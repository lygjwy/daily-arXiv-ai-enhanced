{"id": "2507.19574", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19574", "abs": "https://arxiv.org/abs/2507.19574", "authors": ["Ghufran Abualhail Alhamzawi", "Ali Saeed Alfoudi", "Ali Hakem Alsaeedi", "Suha Mohammed Hadi", "Amjed Abbas Ahmed", "Md. Riad Hassan", "Nurhizam Safie Mohd Satar", "Waeel Yahya Yasseen"], "title": "Tuning adaptive gamma correction (TAGC) for enhancing images in low ligh", "comment": null, "summary": "Enhancing images in low-light conditions is an important challenge in\ncomputer vision. Insufficient illumination negatively affects the quality of\nimages, resulting in low contrast, intensive noise, and blurred details. This\npaper presents a model for enhancing low-light images called tuning adaptive\ngamma correction (TAGC). The model is based on analyzing the color luminance of\nthe low-light image and calculating the average color to determine the adaptive\ngamma coefficient. The gamma value is calculated automatically and adaptively\nat different illumination levels suitable for the image without human\nintervention or manual adjustment. Based on qualitative and quantitative\nevaluation, tuning adaptive gamma correction model has effectively improved\nlow-light images while maintaining details, natural contrast, and correct color\ndistribution. It also provides natural visual quality. It can be considered a\nmore efficient solution for processing low-light images in multiple\napplications such as night surveillance, improving the quality of medical\nimages, and photography in low-light environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTAGC\u7684\u81ea\u9002\u5e94\u4f3d\u9a6c\u6821\u6b63\u6a21\u578b\uff0c\u7528\u4e8e\u589e\u5f3a\u4f4e\u5149\u7167\u56fe\u50cf\uff0c\u901a\u8fc7\u5206\u6790\u989c\u8272\u4eae\u5ea6\u548c\u8ba1\u7b97\u5e73\u5747\u989c\u8272\u6765\u81ea\u52a8\u786e\u5b9a\u4f3d\u9a6c\u503c\uff0c\u6709\u6548\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u4f4e\u5149\u7167\u6761\u4ef6\u4e0b\u56fe\u50cf\u8d28\u91cf\u4e0b\u964d\uff0c\u8868\u73b0\u4e3a\u4f4e\u5bf9\u6bd4\u5ea6\u3001\u566a\u58f0\u548c\u7ec6\u8282\u6a21\u7cca\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u7684\u81ea\u52a8\u589e\u5f3a\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u989c\u8272\u4eae\u5ea6\u5206\u6790\u548c\u5e73\u5747\u989c\u8272\u8ba1\u7b97\uff0c\u81ea\u52a8\u786e\u5b9a\u81ea\u9002\u5e94\u4f3d\u9a6c\u7cfb\u6570\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u5149\u7167\u6c34\u5e73\u7684\u56fe\u50cf\u3002", "result": "TAGC\u6a21\u578b\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u5747\u6709\u6548\u63d0\u5347\u4f4e\u5149\u7167\u56fe\u50cf\u8d28\u91cf\uff0c\u4fdd\u6301\u7ec6\u8282\u3001\u81ea\u7136\u5bf9\u6bd4\u5ea6\u548c\u6b63\u786e\u8272\u5f69\u5206\u5e03\u3002", "conclusion": "TAGC\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u4f4e\u5149\u7167\u56fe\u50cf\u5904\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591c\u95f4\u76d1\u63a7\u3001\u533b\u5b66\u56fe\u50cf\u589e\u5f3a\u548c\u4f4e\u5149\u7167\u6444\u5f71\u7b49\u591a\u79cd\u5e94\u7528\u3002"}}
{"id": "2507.19575", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19575", "abs": "https://arxiv.org/abs/2507.19575", "authors": ["Ayush Roy", "Samin Enam", "Jun Xia", "Vishnu Suresh Lokhande", "Won Hwa Kim"], "title": "Is Exchangeability better than I.I.D to handle Data Distribution Shifts while Pooling Data for Data-scarce Medical image segmentation?", "comment": null, "summary": "Data scarcity is a major challenge in medical imaging, particularly for deep\nlearning models. While data pooling (combining datasets from multiple sources)\nand data addition (adding more data from a new dataset) have been shown to\nenhance model performance, they are not without complications. Specifically,\nincreasing the size of the training dataset through pooling or addition can\ninduce distributional shifts, negatively affecting downstream model\nperformance, a phenomenon known as the \"Data Addition Dilemma\". While the\ntraditional i.i.d. assumption may not hold in multi-source contexts, assuming\nexchangeability across datasets provides a more practical framework for data\npooling. In this work, we investigate medical image segmentation under these\nconditions, drawing insights from causal frameworks to propose a method for\ncontrolling foreground-background feature discrepancies across all layers of\ndeep networks. This approach improves feature representations, which are\ncrucial in data-addition scenarios. Our method achieves state-of-the-art\nsegmentation performance on histopathology and ultrasound images across five\ndatasets, including a novel ultrasound dataset that we have curated and\ncontributed. Qualitative results demonstrate more refined and accurate\nsegmentation maps compared to prominent baselines across three model\narchitectures. The code will be available on Github.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u533b\u5b66\u5f71\u50cf\u4e2d\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u63a7\u5236\u6df1\u5ea6\u7f51\u7edc\u7279\u5f81\u5dee\u5f02\u7684\u65b9\u6cd5\u6765\u4f18\u5316\u6570\u636e\u6c60\u5316\u548c\u6570\u636e\u6dfb\u52a0\u7684\u6548\u679c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u5f71\u50cf\u6570\u636e\u7a00\u7f3a\uff0c\u6570\u636e\u6c60\u5316\u548c\u6570\u636e\u6dfb\u52a0\u867d\u80fd\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u53ef\u80fd\u5f15\u53d1\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff08Data Addition Dilemma\uff09\uff0c\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u3002", "method": "\u57fa\u4e8e\u56e0\u679c\u6846\u67b6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u63a7\u5236\u6df1\u5ea6\u7f51\u7edc\u4e2d\u524d\u666f-\u80cc\u666f\u7279\u5f81\u5dee\u5f02\u7684\u65b9\u6cd5\uff0c\u4f18\u5316\u7279\u5f81\u8868\u793a\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\uff08\u5305\u62ec\u65b0\u6574\u7406\u7684\u8d85\u58f0\u6570\u636e\u96c6\uff09\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5206\u5272\u6027\u80fd\uff0c\u5b9a\u6027\u7ed3\u679c\u663e\u793a\u5206\u5272\u56fe\u66f4\u7cbe\u7ec6\u51c6\u786e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u6dfb\u52a0\u5e26\u6765\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u533b\u5b66\u5f71\u50cf\u5206\u5272\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.19590", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19590", "abs": "https://arxiv.org/abs/2507.19590", "authors": ["Chandravardhan Singh Raghaw", "Jasmer Singh Sanjotra", "Mohammad Zia Ur Rehman", "Shubhi Bansal", "Shahid Shafi Dar", "Nagendra Kumar"], "title": "T-MPEDNet: Unveiling the Synergy of Transformer-aware Multiscale Progressive Encoder-Decoder Network with Feature Recalibration for Tumor and Liver Segmentation", "comment": null, "summary": "Precise and automated segmentation of the liver and its tumor within CT scans\nplays a pivotal role in swift diagnosis and the development of optimal\ntreatment plans for individuals with liver diseases and malignancies. However,\nautomated liver and tumor segmentation faces significant hurdles arising from\nthe inherent heterogeneity of tumors and the diverse visual characteristics of\nlivers across a broad spectrum of patients. Aiming to address these challenges,\nwe present a novel Transformer-aware Multiscale Progressive Encoder-Decoder\nNetwork (T-MPEDNet) for automated segmentation of tumor and liver. T-MPEDNet\nleverages a deep adaptive features backbone through a progressive\nencoder-decoder structure, enhanced by skip connections for recalibrating\nchannel-wise features while preserving spatial integrity. A\nTransformer-inspired dynamic attention mechanism captures long-range contextual\nrelationships within the spatial domain, further enhanced by multi-scale\nfeature utilization for refined local details, leading to accurate prediction.\nMorphological boundary refinement is then employed to address indistinct\nboundaries with neighboring organs, capturing finer details and yielding\nprecise boundary labels. The efficacy of T-MPEDNet is comprehensively assessed\non two widely utilized public benchmark datasets, LiTS and 3DIRCADb. Extensive\nquantitative and qualitative analyses demonstrate the superiority of T-MPEDNet\ncompared to twelve state-of-the-art methods. On LiTS, T-MPEDNet achieves\noutstanding Dice Similarity Coefficients (DSC) of 97.6% and 89.1% for liver and\ntumor segmentation, respectively. Similar performance is observed on 3DIRCADb,\nwith DSCs of 98.3% and 83.3% for liver and tumor segmentation, respectively.\nOur findings prove that T-MPEDNet is an efficacious and reliable framework for\nautomated segmentation of the liver and its tumor in CT scans.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578bTransformer\u611f\u77e5\u7684\u591a\u5c3a\u5ea6\u6e10\u8fdb\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7f51\u7edc\uff08T-MPEDNet\uff09\uff0c\u7528\u4e8eCT\u626b\u63cf\u4e2d\u809d\u810f\u548c\u80bf\u7624\u7684\u81ea\u52a8\u5206\u5272\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u5272\u7cbe\u5ea6\u3002", "motivation": "\u809d\u810f\u548c\u80bf\u7624\u7684\u81ea\u52a8\u5206\u5272\u5728\u5feb\u901f\u8bca\u65ad\u548c\u6cbb\u7597\u8ba1\u5212\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u80bf\u7624\u7684\u5f02\u8d28\u6027\u548c\u809d\u810f\u89c6\u89c9\u7279\u5f81\u7684\u591a\u6837\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u6311\u6218\u3002", "method": "T-MPEDNet\u91c7\u7528\u6e10\u8fdb\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\uff0c\u7ed3\u5408Transformer\u52a8\u6001\u6ce8\u610f\u529b\u673a\u5236\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u5229\u7528\uff0c\u5e76\u901a\u8fc7\u5f62\u6001\u5b66\u8fb9\u754c\u7ec6\u5316\u4f18\u5316\u5206\u5272\u7ed3\u679c\u3002", "result": "\u5728LiTS\u548c3DIRCADb\u6570\u636e\u96c6\u4e0a\uff0cT-MPEDNet\u7684\u809d\u810f\u548c\u80bf\u7624\u5206\u5272Dice\u7cfb\u6570\u5206\u522b\u8fbe\u523097.6%/89.1%\u548c98.3%/83.3%\uff0c\u4f18\u4e8e12\u79cd\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "T-MPEDNet\u662f\u4e00\u79cd\u9ad8\u6548\u53ef\u9760\u7684\u809d\u810f\u548c\u80bf\u7624\u81ea\u52a8\u5206\u5272\u6846\u67b6\uff0c\u5177\u6709\u663e\u8457\u7684\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.19592", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19592", "abs": "https://arxiv.org/abs/2507.19592", "authors": ["Meng Wei", "Charlie Budd", "Oluwatosin Alabi", "Miaojing Shi", "Tom Vercauteren"], "title": "SurgPIS: Surgical-instrument-level Instances and Part-level Semantics for Weakly-supervised Part-aware Instance Segmentation", "comment": null, "summary": "Consistent surgical instrument segmentation is critical for automation in\nrobot-assisted surgery. Yet, existing methods only treat instrument-level\ninstance segmentation (IIS) or part-level semantic segmentation (PSS)\nseparately, without interaction between these tasks. In this work, we formulate\na surgical tool segmentation as a unified part-aware instance segmentation\n(PIS) problem and introduce SurgPIS, the first PIS model for surgical\ninstruments. Our method adopts a transformer-based mask classification approach\nand introduces part-specific queries derived from instrument-level object\nqueries, explicitly linking parts to their parent instrument instances. In\norder to address the lack of large-scale datasets with both instance- and\npart-level labels, we propose a weakly-supervised learning strategy for SurgPIS\nto learn from disjoint datasets labelled for either IIS or PSS purposes. During\ntraining, we aggregate our PIS predictions into IIS or PSS masks, thereby\nallowing us to compute a loss against partially labelled datasets. A\nstudent-teacher approach is developed to maintain prediction consistency for\nmissing PIS information in the partially labelled data, e.g., parts of the IIS\nlabelled data. Extensive experiments across multiple datasets validate the\neffectiveness of SurgPIS, achieving state-of-the-art performance in PIS as well\nas IIS, PSS, and instrument-level semantic segmentation.", "AI": {"tldr": "SurgPIS\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u90e8\u5206\u611f\u77e5\u5b9e\u4f8b\u5206\u5272\uff08PIS\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u624b\u672f\u5668\u68b0\u5206\u5272\uff0c\u7ed3\u5408\u4e86\u5b9e\u4f8b\u7ea7\u548c\u90e8\u5206\u7ea7\u5206\u5272\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u5f31\u76d1\u7763\u5b66\u4e60\u548c\u5e08\u751f\u6846\u67b6\u89e3\u51b3\u4e86\u6570\u636e\u6807\u6ce8\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u5355\u72ec\u5904\u7406\u624b\u672f\u5668\u68b0\u7684\u5b9e\u4f8b\u7ea7\u5206\u5272\uff08IIS\uff09\u6216\u90e8\u5206\u7ea7\u8bed\u4e49\u5206\u5272\uff08PSS\uff09\uff0c\u7f3a\u4e4f\u4efb\u52a1\u95f4\u7684\u4ea4\u4e92\u3002SurgPIS\u65e8\u5728\u7edf\u4e00\u8fd9\u4e24\u9879\u4efb\u52a1\uff0c\u63d0\u5347\u5206\u5272\u6548\u679c\u3002", "method": "\u91c7\u7528\u57fa\u4e8eTransformer\u7684\u63a9\u7801\u5206\u7c7b\u65b9\u6cd5\uff0c\u5f15\u5165\u90e8\u5206\u7279\u5b9a\u67e5\u8be2\uff0c\u660e\u786e\u5c06\u90e8\u5206\u4e0e\u7236\u5668\u68b0\u5b9e\u4f8b\u5173\u8054\u3002\u901a\u8fc7\u5f31\u76d1\u7763\u5b66\u4e60\u548c\u5e08\u751f\u6846\u67b6\u5904\u7406\u90e8\u5206\u6807\u6ce8\u6570\u636e\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cSurgPIS\u5728PIS\u3001IIS\u3001PSS\u548c\u5668\u68b0\u7ea7\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "SurgPIS\u4e3a\u624b\u672f\u5668\u68b0\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\uff0c\u5c24\u5176\u5728\u90e8\u5206\u6807\u6ce8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2507.19511", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19511", "abs": "https://arxiv.org/abs/2507.19511", "authors": ["Khalid Hasan", "Jamil Saquer", "Mukulika Ghosh"], "title": "Advancing Mental Disorder Detection: A Comparative Evaluation of Transformer and LSTM Architectures on Social Media", "comment": "The 49th IEEE International Conference on Computers, Software, and\n  Applications (COMPSAC 2025) (camera-ready)", "summary": "The rising prevalence of mental health disorders necessitates the development\nof robust, automated tools for early detection and monitoring. Recent advances\nin Natural Language Processing (NLP), particularly transformer-based\narchitectures, have demonstrated significant potential in text analysis. This\nstudy provides a comprehensive evaluation of state-of-the-art transformer\nmodels (BERT, RoBERTa, DistilBERT, ALBERT, and ELECTRA) against Long Short-Term\nMemory (LSTM) based approaches using different text embedding techniques for\nmental health disorder classification on Reddit. We construct a large annotated\ndataset, validating its reliability through statistical judgmental analysis and\ntopic modeling. Experimental results demonstrate the superior performance of\ntransformer models over traditional deep-learning approaches. RoBERTa achieved\nthe highest classification performance, with a 99.54% F1 score on the hold-out\ntest set and a 96.05% F1 score on the external test set. Notably, LSTM models\naugmented with BERT embeddings proved highly competitive, achieving F1 scores\nexceeding 94% on the external dataset while requiring significantly fewer\ncomputational resources. These findings highlight the effectiveness of\ntransformer-based models for real-time, scalable mental health monitoring. We\ndiscuss the implications for clinical applications and digital mental health\ninterventions, offering insights into the capabilities and limitations of\nstate-of-the-art NLP methodologies in mental disorder detection.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u591a\u79cdTransformer\u6a21\u578b\u5728\u5fc3\u7406\u5065\u5eb7\u969c\u788d\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0RoBERTa\u6027\u80fd\u6700\u4f73\uff0c\u540c\u65f6LSTM\u7ed3\u5408BERT\u5d4c\u5165\u4e5f\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5fc3\u7406\u5065\u5eb7\u969c\u788d\u65e5\u76ca\u666e\u904d\uff0c\u9700\u8981\u5f00\u53d1\u81ea\u52a8\u5316\u5de5\u5177\u8fdb\u884c\u65e9\u671f\u68c0\u6d4b\u548c\u76d1\u6d4b\u3002", "method": "\u6bd4\u8f83\u4e86BERT\u3001RoBERTa\u7b49Transformer\u6a21\u578b\u4e0eLSTM\u65b9\u6cd5\uff0c\u4f7f\u7528Reddit\u6570\u636e\u6784\u5efa\u6807\u6ce8\u6570\u636e\u96c6\u3002", "result": "RoBERTa\u5728\u6d4b\u8bd5\u96c6\u4e0aF1\u5206\u6570\u8fbe99.54%\uff0cLSTM\u7ed3\u5408BERT\u5d4c\u5165\u4e5f\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "Transformer\u6a21\u578b\u5728\u5b9e\u65f6\u3001\u53ef\u6269\u5c55\u7684\u5fc3\u7406\u5065\u5eb7\u76d1\u6d4b\u4e2d\u6548\u679c\u663e\u8457\uff0c\u9002\u5408\u4e34\u5e8a\u5e94\u7528\u3002"}}
{"id": "2507.19510", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19510", "abs": "https://arxiv.org/abs/2507.19510", "authors": ["Haoxuan Ma", "Xishun Liao", "Yifan Liu", "Chris Stanford", "Jiaqi Ma"], "title": "Beyond 9-to-5: A Generative Model for Augmenting Mobility Data of Underrepresented Shift Workers", "comment": null, "summary": "This paper addresses a critical gap in urban mobility modeling by focusing on\nshift workers, a population segment comprising 15-20% of the workforce in\nindustrialized societies yet systematically underrepresented in traditional\ntransportation surveys and planning. This underrepresentation is revealed in\nthis study by a comparative analysis of GPS and survey data, highlighting stark\ndifferences between the bimodal temporal patterns of shift workers and the\nconventional 9-to-5 schedules recorded in surveys. To address this bias, we\nintroduce a novel transformer-based approach that leverages fragmented GPS\ntrajectory data to generate complete, behaviorally valid activity patterns for\nindividuals working non-standard hours. Our method employs periodaware temporal\nembeddings and a transition-focused loss function specifically designed to\ncapture the unique activity rhythms of shift workers and mitigate the inherent\nbiases in conventional transportation datasets. Evaluation shows that the\ngenerated data achieves remarkable distributional alignment with GPS data from\nLos Angeles County (Average JSD < 0.02 for all evaluation metrics). By\ntransforming incomplete GPS traces into complete, representative activity\npatterns, our approach provides transportation planners with a powerful data\naugmentation tool to fill critical gaps in understanding the 24/7 mobility\nneeds of urban populations, enabling precise and inclusive transportation\nplanning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\uff0c\u5229\u7528GPS\u8f68\u8ff9\u6570\u636e\u751f\u6210\u8f6e\u73ed\u5de5\u4eba\u7684\u5b8c\u6574\u6d3b\u52a8\u6a21\u5f0f\uff0c\u586b\u8865\u4e86\u4f20\u7edf\u4ea4\u901a\u8c03\u67e5\u4e2d\u5bf9\u8fd9\u4e00\u7fa4\u4f53\u7684\u5ffd\u89c6\u3002", "motivation": "\u8f6e\u73ed\u5de5\u4eba\u5360\u5de5\u4e1a\u5316\u793e\u4f1a\u52b3\u52a8\u529b\u768415-20%\uff0c\u4f46\u5728\u4f20\u7edf\u4ea4\u901a\u8c03\u67e5\u548c\u89c4\u5212\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\uff0c\u5bfc\u81f4\u5bf9\u5176\u51fa\u884c\u9700\u6c42\u7684\u7406\u89e3\u4e0d\u5145\u5206\u3002", "method": "\u91c7\u7528\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff0c\u7ed3\u5408\u5468\u671f\u6027\u65f6\u95f4\u5d4c\u5165\u548c\u8fc7\u6e21\u635f\u5931\u51fd\u6570\uff0c\u4ece\u788e\u7247\u5316GPS\u6570\u636e\u751f\u6210\u5b8c\u6574\u7684\u6d3b\u52a8\u6a21\u5f0f\u3002", "result": "\u751f\u6210\u7684\u6a21\u5f0f\u4e0e\u6d1b\u6749\u77f6\u53bf\u7684GPS\u6570\u636e\u9ad8\u5ea6\u4e00\u81f4\uff08\u5e73\u5747JSD < 0.02\uff09\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4ea4\u901a\u89c4\u5212\u8005\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u6570\u636e\u589e\u5f3a\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u66f4\u5168\u9762\u5730\u7406\u89e3\u57ce\u5e0224/7\u7684\u51fa\u884c\u9700\u6c42\u3002"}}
{"id": "2507.19489", "categories": ["cs.AI", "cs.CV", "cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.19489", "abs": "https://arxiv.org/abs/2507.19489", "authors": ["Simone Bendazzoli", "Sanna Persson", "Mehdi Astaraki", "Sebastian Pettersson", "Vitali Grozman", "Rodrigo Moreno"], "title": "MAIA: A Collaborative Medical AI Platform for Integrated Healthcare Innovation", "comment": "26 pages, 12 figures", "summary": "The integration of Artificial Intelligence (AI) into clinical workflows\nrequires robust collaborative platforms that are able to bridge the gap between\ntechnical innovation and practical healthcare applications. This paper\nintroduces MAIA (Medical Artificial Intelligence Assistant), an open-source\nplatform designed to facilitate interdisciplinary collaboration among\nclinicians, researchers, and AI developers. Built on Kubernetes, MAIA offers a\nmodular, scalable environment with integrated tools for data management, model\ndevelopment, annotation, deployment, and clinical feedback. Key features\ninclude project isolation, CI/CD automation, integration with high-computing\ninfrastructures and in clinical workflows. MAIA supports real-world use cases\nin medical imaging AI, with deployments in both academic and clinical\nenvironments. By promoting collaborations and interoperability, MAIA aims to\naccelerate the translation of AI research into impactful clinical solutions\nwhile promoting reproducibility, transparency, and user-centered design. We\nshowcase the use of MAIA with different projects, both at KTH Royal Institute\nof Technology and Karolinska University Hospital.", "AI": {"tldr": "MAIA\u662f\u4e00\u4e2a\u5f00\u6e90\u5e73\u53f0\uff0c\u65e8\u5728\u4fc3\u8fdb\u4e34\u5e8a\u533b\u751f\u3001\u7814\u7a76\u4eba\u5458\u548cAI\u5f00\u53d1\u8005\u4e4b\u95f4\u7684\u8de8\u5b66\u79d1\u5408\u4f5c\uff0c\u52a0\u901fAI\u7814\u7a76\u5411\u4e34\u5e8a\u5e94\u7528\u7684\u8f6c\u5316\u3002", "motivation": "\u89e3\u51b3AI\u6280\u672f\u5728\u5b9e\u9645\u533b\u7597\u5e94\u7528\u4e2d\u7684\u843d\u5730\u95ee\u9898\uff0c\u4fc3\u8fdb\u534f\u4f5c\u548c\u4e92\u64cd\u4f5c\u6027\u3002", "method": "\u57fa\u4e8eKubernetes\u6784\u5efa\uff0c\u63d0\u4f9b\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u7684\u73af\u5883\uff0c\u96c6\u6210\u6570\u636e\u7ba1\u7406\u3001\u6a21\u578b\u5f00\u53d1\u3001\u6807\u6ce8\u3001\u90e8\u7f72\u548c\u4e34\u5e8a\u53cd\u9988\u5de5\u5177\u3002", "result": "\u5728\u5b66\u672f\u548c\u4e34\u5e8a\u73af\u5883\u4e2d\u6210\u529f\u90e8\u7f72\uff0c\u652f\u6301\u533b\u5b66\u5f71\u50cfAI\u7684\u5b9e\u9645\u7528\u4f8b\u3002", "conclusion": "MAIA\u901a\u8fc7\u4fc3\u8fdb\u534f\u4f5c\u548c\u4e92\u64cd\u4f5c\u6027\uff0c\u52a0\u901f\u4e86AI\u7814\u7a76\u5411\u4e34\u5e8a\u89e3\u51b3\u65b9\u6848\u7684\u8f6c\u5316\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u53ef\u91cd\u590d\u6027\u3001\u900f\u660e\u5ea6\u548c\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u8bbe\u8ba1\u3002"}}
{"id": "2507.19599", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19599", "abs": "https://arxiv.org/abs/2507.19599", "authors": ["Haochen Wang", "Qirui Chen", "Cilin Yan", "Jiayin Cai", "Xiaolong Jiang", "Yao Hu", "Weidi Xie", "Stratis Gavves"], "title": "Object-centric Video Question Answering with Visual Grounding and Referring", "comment": null, "summary": "Video Large Language Models (VideoLLMs) have recently demonstrated remarkable\nprogress in general video understanding. However, existing models primarily\nfocus on high-level comprehension and are limited to text-only responses,\nrestricting the flexibility for object-centric, multiround interactions. In\nthis paper, we make three contributions: (i) we address these limitations by\nintroducing a VideoLLM model, capable of performing both object referring for\ninput and grounding for output in video reasoning tasks, i.e., allowing users\nto interact with videos using both textual and visual prompts; (ii) we propose\nSTOM (Spatial-Temporal Overlay Module), a novel approach that propagates\narbitrary visual prompts input at any single timestamp to the remaining frames\nwithin a video; (iii) we present VideoInfer, a manually curated object-centric\nvideo instruction dataset featuring questionanswering pairs that require\nreasoning. We conduct comprehensive experiments on VideoInfer and other\nexisting benchmarks across video question answering and referring object\nsegmentation. The results on 12 benchmarks of 6 tasks show that our proposed\nmodel consistently outperforms baselines in both video question answering and\nsegmentation, underscoring its robustness in multimodal, object-centric video\nand image understanding. Project page:\nhttps://qirui-chen.github.io/RGA3-release/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u652f\u6301\u591a\u6a21\u6001\u4ea4\u4e92\u7684VideoLLM\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u4ec5\u652f\u6301\u6587\u672c\u54cd\u5e94\u7684\u9650\u5236\uff0c\u5e76\u5f15\u5165\u4e86STOM\u6a21\u5757\u548cVideoInfer\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709VideoLLM\u6a21\u578b\u4ec5\u652f\u6301\u6587\u672c\u54cd\u5e94\uff0c\u7f3a\u4e4f\u5bf9\u8c61\u4e2d\u5fc3\u7684\u591a\u8f6e\u4ea4\u4e92\u7075\u6d3b\u6027\uff0c\u9650\u5236\u4e86\u89c6\u9891\u7406\u89e3\u7684\u6df1\u5ea6\u3002", "method": "\u63d0\u51fa\u652f\u6301\u6587\u672c\u548c\u89c6\u89c9\u63d0\u793a\u7684VideoLLM\u6a21\u578b\uff0c\u5f15\u5165STOM\u6a21\u5757\u4f20\u64ad\u89c6\u89c9\u63d0\u793a\uff0c\u5e76\u6784\u5efaVideoInfer\u6570\u636e\u96c6\u3002", "result": "\u572812\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u76846\u9879\u4efb\u52a1\u4e2d\uff0c\u6a21\u578b\u5728\u89c6\u9891\u95ee\u7b54\u548c\u5206\u5272\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u591a\u6a21\u6001\u3001\u5bf9\u8c61\u4e2d\u5fc3\u7684\u89c6\u9891\u7406\u89e3\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2507.19521", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19521", "abs": "https://arxiv.org/abs/2507.19521", "authors": ["Vishakh Padmakumar", "Joseph Chee Chang", "Kyle Lo", "Doug Downey", "Aakanksha Naik"], "title": "Setting The Table with Intent: Intent-aware Schema Generation and Editing for Literature Review Tables", "comment": null, "summary": "The increasing volume of academic literature makes it essential for\nresearchers to organize, compare, and contrast collections of documents. Large\nlanguage models (LLMs) can support this process by generating schemas defining\nshared aspects along which to compare papers. However, progress on schema\ngeneration has been slow due to: (i) ambiguity in reference-based evaluations,\nand (ii) lack of editing/refinement methods. Our work is the first to address\nboth issues. First, we present an approach for augmenting unannotated table\ncorpora with synthesized intents and apply it to create a dataset for studying\nschema generation conditioned on a given information need, thus reducing\nambiguity. With this dataset, we show how incorporating table intents\nsignificantly improves baseline performance in reconstructing reference\nschemas. Next, we propose several LLM-based schema editing techniques. We start\nby comprehensively benchmarking several single-shot schema generation methods,\nincluding prompted LLM workflows and fine-tuned models, showing that smaller,\nopen-weight models can be fine-tuned to be competitive with state-of-the-art\nprompted LLMs. Then we demonstrate that our editing techniques can further\nimprove schemas generated by these methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u6587\u6863\u6bd4\u8f83\u6846\u67b6\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bc4\u4f30\u6a21\u7cca\u548c\u7f3a\u4e4f\u7f16\u8f91\u65b9\u6cd5\u7684\u95ee\u9898\u3002\u901a\u8fc7\u5408\u6210\u610f\u56fe\u589e\u5f3a\u6570\u636e\u96c6\uff0c\u5e76\u5c55\u793a\u4e86\u7f16\u8f91\u6280\u672f\u5bf9\u751f\u6210\u6846\u67b6\u7684\u6539\u8fdb\u6548\u679c\u3002", "motivation": "\u5b66\u672f\u6587\u732e\u6570\u91cf\u6fc0\u589e\uff0c\u9700\u8981\u6709\u6548\u7ec4\u7ec7\u3001\u6bd4\u8f83\u548c\u5bf9\u6bd4\u6587\u6863\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u6bd4\u8f83\u6846\u67b6\u65f6\u5b58\u5728\u8bc4\u4f30\u6a21\u7cca\u548c\u7f3a\u4e4f\u7f16\u8f91\u65b9\u6cd5\u7684\u95ee\u9898\u3002", "method": "1. \u5408\u6210\u610f\u56fe\u589e\u5f3a\u672a\u6807\u6ce8\u8868\u683c\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u7814\u7a76\u57fa\u4e8e\u4fe1\u606f\u9700\u6c42\u7684\u6846\u67b6\u751f\u6210\uff1b2. \u63d0\u51fa\u591a\u79cd\u57fa\u4e8eLLM\u7684\u6846\u67b6\u7f16\u8f91\u6280\u672f\uff0c\u5e76\u5bf9\u6bd4\u5355\u6b21\u751f\u6210\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "result": "\u5408\u6210\u610f\u56fe\u663e\u8457\u63d0\u5347\u4e86\u57fa\u51c6\u6027\u80fd\uff1b\u5c0f\u578b\u5f00\u653e\u6743\u91cd\u6a21\u578b\u901a\u8fc7\u5fae\u8c03\u53ef\u4e0e\u5148\u8fdb\u63d0\u793aLLMs\u7ade\u4e89\uff1b\u7f16\u8f91\u6280\u672f\u8fdb\u4e00\u6b65\u6539\u8fdb\u4e86\u751f\u6210\u7684\u6846\u67b6\u3002", "conclusion": "\u8bba\u6587\u89e3\u51b3\u4e86\u6846\u67b6\u751f\u6210\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u5408\u6210\u610f\u56fe\u548c\u7f16\u8f91\u6280\u672f\u7684\u6709\u6548\u6027\uff0c\u4e3a\u6587\u6863\u6bd4\u8f83\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2507.19513", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19513", "abs": "https://arxiv.org/abs/2507.19513", "authors": ["Khalid Ali", "Zineddine Bettouche", "Andreas Kassler", "Andreas Fischer"], "title": "Enhancing Spatiotemporal Networks with xLSTM: A Scalar LSTM Approach for Cellular Traffic Forecasting", "comment": null, "summary": "Accurate spatiotemporal traffic forecasting is vital for intelligent resource\nmanagement in 5G and beyond. However, conventional AI approaches often fail to\ncapture the intricate spatial and temporal patterns that exist, due to e.g.,\nthe mobility of users. We introduce a lightweight, dual-path Spatiotemporal\nNetwork that leverages a Scalar LSTM (sLSTM) for efficient temporal modeling\nand a three-layer Conv3D module for spatial feature extraction. A fusion layer\nintegrates both streams into a cohesive representation, enabling robust\nforecasting. Our design improves gradient stability and convergence speed while\nreducing prediction error. Evaluations on real-world datasets show superior\nforecast performance over ConvLSTM baselines and strong generalization to\nunseen regions, making it well-suited for large-scale, next-generation network\ndeployments. Experimental evaluation shows a 23% MAE reduction over ConvLSTM,\nwith a 30% improvement in model generalization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u53cc\u8def\u5f84\u65f6\u7a7a\u7f51\u7edc\uff0c\u7ed3\u5408sLSTM\u548cConv3D\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ea4\u901a\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "5G\u53ca\u672a\u6765\u7f51\u7edc\u4e2d\uff0c\u7cbe\u786e\u7684\u65f6\u7a7a\u4ea4\u901a\u9884\u6d4b\u5bf9\u8d44\u6e90\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edfAI\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u590d\u6742\u7684\u65f6\u7a7a\u6a21\u5f0f\u3002", "method": "\u91c7\u7528\u53cc\u8def\u5f84\u8bbe\u8ba1\uff1asLSTM\u7528\u4e8e\u65f6\u95f4\u5efa\u6a21\uff0c\u4e09\u5c42Conv3D\u7528\u4e8e\u7a7a\u95f4\u7279\u5f81\u63d0\u53d6\uff0c\u901a\u8fc7\u878d\u5408\u5c42\u6574\u5408\u4e24\u8005\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8eConvLSTM\u57fa\u7ebf\uff0cMAE\u964d\u4f4e23%\uff0c\u6cdb\u5316\u80fd\u529b\u63d0\u534730%\u3002", "conclusion": "\u8be5\u8bbe\u8ba1\u9002\u5408\u5927\u89c4\u6a21\u4e0b\u4e00\u4ee3\u7f51\u7edc\u90e8\u7f72\uff0c\u5177\u6709\u68af\u5ea6\u7a33\u5b9a\u6027\u548c\u5feb\u901f\u6536\u655b\u7684\u4f18\u52bf\u3002"}}
{"id": "2507.19543", "categories": ["cs.AI", "cs.MA", "I.2.11; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.19543", "abs": "https://arxiv.org/abs/2507.19543", "authors": ["Maria Emilia Mazzolenis", "Ruirui Zhang"], "title": "Agent WARPP: Workflow Adherence via Runtime Parallel Personalization", "comment": "Accepted at the ICML 2025 Workshop on Multi-Agent Systems in the Era\n  of Foundation Models: Opportunities, Challenges, and Futures. Code repo:\n  https://github.com/emiliamazzo/WARPP/", "summary": "Large language models (LLMs) are increasingly applied in task-oriented\ndialogue (TOD) systems but often struggle with long, conditional workflows that\ninvolve external tool calls and depend on user-specific information. We present\nWorkflow Adherence via Runtime Parallel Personalization, or WARPP, a\ntraining-free, modular framework that combines multi-agent orchestration with\nruntime personalization to improve workflow adherence in LLM-based systems. By\ndynamically pruning conditional branches based on user attributes, the\nframework reduces reasoning overhead and narrows tool selection at runtime.\nWARPP deploys a parallelized architecture where a dedicated Personalizer agent\noperates alongside modular, domain-specific agents to dynamically tailor\nexecution paths in real time. The framework is evaluated across five\nrepresentative user intents of varying complexity within three domains:\nbanking, flights, and healthcare. Our evaluation leverages synthetic datasets\nand LLM-powered simulated users to test scenarios with conditional\ndependencies. Our results demonstrate that WARPP outperforms both the\nnon-personalized method and the ReAct baseline, achieving increasingly larger\ngains in parameter fidelity and tool accuracy as intent complexity grows, while\nalso reducing average token usage, without any additional training.", "AI": {"tldr": "WARPP\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u6a21\u5757\u5316\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u7f16\u6392\u548c\u8fd0\u884c\u65f6\u4e2a\u6027\u5316\u63d0\u5347LLM\u5728\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u4e2d\u7684\u5de5\u4f5c\u6d41\u4f9d\u4ece\u6027\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u957f\u6761\u4ef6\u5de5\u4f5c\u6d41\u4e2d\u56e0\u5916\u90e8\u5de5\u5177\u8c03\u7528\u548c\u7528\u6237\u7279\u5b9a\u4fe1\u606f\u4f9d\u8d56\u800c\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u7f16\u6392\u548c\u8fd0\u884c\u65f6\u4e2a\u6027\u5316\uff0c\u52a8\u6001\u4fee\u526a\u6761\u4ef6\u5206\u652f\uff0c\u5e76\u884c\u5316\u67b6\u6784\u4e2d\u90e8\u7f72\u4e2a\u6027\u5316\u4ee3\u7406\u548c\u9886\u57df\u7279\u5b9a\u4ee3\u7406\u3002", "result": "\u5728\u4e94\u4e2a\u590d\u6742\u7528\u6237\u610f\u56fe\u7684\u6d4b\u8bd5\u4e2d\uff0cWARPP\u4f18\u4e8e\u975e\u4e2a\u6027\u5316\u65b9\u6cd5\u548cReAct\u57fa\u7ebf\uff0c\u63d0\u5347\u53c2\u6570\u4fdd\u771f\u5ea6\u548c\u5de5\u5177\u51c6\u786e\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u4ee4\u724c\u4f7f\u7528\u3002", "conclusion": "WARPP\u6709\u6548\u63d0\u5347LLM\u5728\u590d\u6742\u5de5\u4f5c\u6d41\u4e2d\u7684\u8868\u73b0\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002"}}
{"id": "2507.19621", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19621", "abs": "https://arxiv.org/abs/2507.19621", "authors": ["Sheethal Bhat", "Bogdan Georgescu", "Adarsh Bhandary Panambur", "Mathias Zinnen", "Tri-Thien Nguyen", "Awais Mansoor", "Karim Khalifa Elbarbary", "Siming Bayer", "Florin-Cristian Ghesu", "Sasa Grbic", "Andreas Maier"], "title": "Exemplar Med-DETR: Toward Generalized and Robust Lesion Detection in Mammogram Images and beyond", "comment": null, "summary": "Detecting abnormalities in medical images poses unique challenges due to\ndifferences in feature representations and the intricate relationship between\nanatomical structures and abnormalities. This is especially evident in\nmammography, where dense breast tissue can obscure lesions, complicating\nradiological interpretation. Despite leveraging anatomical and semantic\ncontext, existing detection methods struggle to learn effective class-specific\nfeatures, limiting their applicability across different tasks and imaging\nmodalities. In this work, we introduce Exemplar Med-DETR, a novel multi-modal\ncontrastive detector that enables feature-based detection. It employs\ncross-attention with inherently derived, intuitive class-specific exemplar\nfeatures and is trained with an iterative strategy. We achieve state-of-the-art\nperformance across three distinct imaging modalities from four public datasets.\nOn Vietnamese dense breast mammograms, we attain an mAP of 0.7 for mass\ndetection and 0.55 for calcifications, yielding an absolute improvement of 16\npercentage points. Additionally, a radiologist-supported evaluation of 100\nmammograms from an out-of-distribution Chinese cohort demonstrates a twofold\ngain in lesion detection performance. For chest X-rays and angiography, we\nachieve an mAP of 0.25 for mass and 0.37 for stenosis detection, improving\nresults by 4 and 7 percentage points, respectively. These results highlight the\npotential of our approach to advance robust and generalizable detection systems\nfor medical imaging.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aExemplar Med-DETR\u7684\u591a\u6a21\u6001\u5bf9\u6bd4\u68c0\u6d4b\u5668\uff0c\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u79cd\u6210\u50cf\u6a21\u6001\u4e0b\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\u9762\u4e34\u7279\u5f81\u8868\u793a\u5dee\u5f02\u548c\u590d\u6742\u89e3\u5256\u7ed3\u6784\u5173\u7cfb\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5b66\u4e60\u6709\u6548\u7684\u7c7b\u522b\u7279\u5b9a\u7279\u5f81\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001\u5bf9\u6bd4\u68c0\u6d4b\u5668\uff0c\u7ed3\u5408\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u7c7b\u522b\u7279\u5b9a\u793a\u4f8b\u7279\u5f81\uff0c\u901a\u8fc7\u8fed\u4ee3\u7b56\u7565\u8bad\u7ec3\u3002", "result": "\u5728\u56db\u79cd\u516c\u5171\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u4f73\u6027\u80fd\uff0c\u5982\u8d8a\u5357\u5bc6\u96c6\u4e73\u817a\u94bc\u9776\u4e2d\u80bf\u5757\u68c0\u6d4bmAP\u8fbe0.7\uff0c\u4e2d\u56fd\u961f\u5217\u4e2d\u75c5\u53d8\u68c0\u6d4b\u6027\u80fd\u63d0\u5347\u4e24\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u63a8\u52a8\u533b\u5b66\u56fe\u50cf\u68c0\u6d4b\u7cfb\u7edf\u7a33\u5065\u6027\u548c\u901a\u7528\u6027\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.19537", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19537", "abs": "https://arxiv.org/abs/2507.19537", "authors": ["Felix Kraus", "Nicolas Blumenr\u00f6hr", "Danah Tonne", "Achim Streit"], "title": "Mind the Language Gap in Digital Humanities: LLM-Aided Translation of SKOS Thesauri", "comment": null, "summary": "We introduce WOKIE, an open-source, modular, and ready-to-use pipeline for\nthe automated translation of SKOS thesauri. This work addresses a critical need\nin the Digital Humanities (DH), where language diversity can limit access,\nreuse, and semantic interoperability of knowledge resources. WOKIE combines\nexternal translation services with targeted refinement using Large Language\nModels (LLMs), balancing translation quality, scalability, and cost. Designed\nto run on everyday hardware and be easily extended, the application requires no\nprior expertise in machine translation or LLMs. We evaluate WOKIE across\nseveral DH thesauri in 15 languages with different parameters, translation\nservices and LLMs, systematically analysing translation quality, performance,\nand ontology matching improvements. Our results show that WOKIE is suitable to\nenhance the accessibility, reuse, and cross-lingual interoperability of\nthesauri by hurdle-free automated translation and improved ontology matching\nperformance, supporting more inclusive and multilingual research\ninfrastructures.", "AI": {"tldr": "WOKIE\u662f\u4e00\u4e2a\u5f00\u6e90\u3001\u6a21\u5757\u5316\u4e14\u5373\u7528\u7684SKOS\u8bcd\u8868\u81ea\u52a8\u7ffb\u8bd1\u5de5\u5177\uff0c\u65e8\u5728\u89e3\u51b3\u6570\u5b57\u4eba\u6587\u5b66\u79d1\u4e2d\u8bed\u8a00\u591a\u6837\u6027\u5bfc\u81f4\u7684\u8d44\u6e90\u8bbf\u95ee\u548c\u8bed\u4e49\u4e92\u64cd\u4f5c\u6027\u95ee\u9898\u3002", "motivation": "\u6570\u5b57\u4eba\u6587\u5b66\u79d1\u4e2d\u8bed\u8a00\u591a\u6837\u6027\u9650\u5236\u4e86\u77e5\u8bc6\u8d44\u6e90\u7684\u8bbf\u95ee\u3001\u91cd\u7528\u548c\u8bed\u4e49\u4e92\u64cd\u4f5c\u6027\uff0cWOKIE\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u5916\u90e8\u7ffb\u8bd1\u670d\u52a1\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u9488\u5bf9\u6027\u4f18\u5316\uff0c\u5e73\u8861\u7ffb\u8bd1\u8d28\u91cf\u3001\u6269\u5c55\u6027\u548c\u6210\u672c\u3002", "result": "\u572815\u79cd\u8bed\u8a00\u7684\u591a\u4e2a\u8bcd\u8868\u4e0a\u6d4b\u8bd5\u663e\u793a\uff0cWOKIE\u80fd\u63d0\u5347\u8bcd\u8868\u7684\u53ef\u8bbf\u95ee\u6027\u3001\u91cd\u7528\u6027\u548c\u8de8\u8bed\u8a00\u4e92\u64cd\u4f5c\u6027\u3002", "conclusion": "WOKIE\u901a\u8fc7\u65e0\u969c\u788d\u81ea\u52a8\u7ffb\u8bd1\u548c\u6539\u8fdb\u7684ontology\u5339\u914d\u6027\u80fd\uff0c\u652f\u6301\u66f4\u5305\u5bb9\u548c\u591a\u8bed\u8a00\u7684\u7814\u7a76\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2507.19514", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19514", "abs": "https://arxiv.org/abs/2507.19514", "authors": ["Andrew Kiruluta"], "title": "Wavelet Logic Machines: Learning and Reasoning in the Spectral Domain Without Neural Networks", "comment": null, "summary": "We introduce a fully spectral learning framework that eliminates traditional\nneural layers by operating entirely in the wavelet domain. The model applies\nlearnable nonlinear transformations, including soft-thresholding and gain-phase\nmodulation, directly to wavelet coefficients. It also includes a differentiable\nwavelet basis selection mechanism, enabling adaptive processing using families\nsuch as Haar, Daubechies, and Biorthogonal wavelets.\n  Implemented in PyTorch with full 3D support, the model maintains a spectral\npipeline without spatial convolutions or attention. On synthetic 3D denoising\nand natural language tasks from the GLUE benchmark, including SST-2 sentiment\nclassification, the model achieves 89.3 percent accuracy, close to a 4-layer\nTransformer baseline (90.1 percent), while using 72 percent fewer parameters\nand 58 percent less peak memory. Faster early convergence is observed due to\nspectral sparsity priors.\n  In contrast to the quadratic complexity of self-attention and large matrix\nmultiplications in Transformers, our approach uses linear-time wavelet\ntransforms and pointwise nonlinearities, significantly reducing inference cost.\nThis yields a compact, interpretable, and efficient alternative to neural\nmodels. Our results support the viability of principled spectral learning in\nboth vision and language tasks, offering new directions for model design\nwithout overparameterized architectures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u57fa\u4e8e\u5c0f\u6ce2\u57df\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u66ff\u4ee3\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u5c42\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u975e\u7ebf\u6027\u53d8\u6362\u548c\u81ea\u9002\u5e94\u5c0f\u6ce2\u57fa\u9009\u62e9\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u7d27\u51d1\u7684\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\uff08\u5982Transformer\uff09\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u4e14\u53c2\u6570\u8fc7\u591a\uff0c\u5e0c\u671b\u8bbe\u8ba1\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u5728\u5c0f\u6ce2\u57df\u4e2d\u5e94\u7528\u53ef\u5b66\u4e60\u7684\u975e\u7ebf\u6027\u53d8\u6362\uff08\u5982\u8f6f\u9608\u503c\u548c\u589e\u76ca-\u76f8\u4f4d\u8c03\u5236\uff09\uff0c\u5e76\u652f\u6301\u81ea\u9002\u5e94\u9009\u62e9\u5c0f\u6ce2\u57fa\uff08\u5982Haar\u3001Daubechies\uff09\u3002", "result": "\u57283D\u53bb\u566a\u548cGLUE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6a21\u578b\u6027\u80fd\u63a5\u8fd14\u5c42Transformer\uff0889.3% vs 90.1%\uff09\uff0c\u4f46\u53c2\u6570\u548c\u5185\u5b58\u5206\u522b\u51cf\u5c1172%\u548c58%\u3002", "conclusion": "\u5149\u8c31\u5b66\u4e60\u6846\u67b6\u4e3a\u89c6\u89c9\u548c\u8bed\u8a00\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u907f\u514d\u4e86\u8fc7\u53c2\u6570\u5316\u67b6\u6784\u3002"}}
{"id": "2507.19593", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.19593", "abs": "https://arxiv.org/abs/2507.19593", "authors": ["Vince Trencsenyi", "Agnieszka Mensfelt", "Kostas Stathis"], "title": "Hypergames: Modeling Misaligned Perceptions and Nested Beliefs for Multi-agent Systems", "comment": null, "summary": "Classical game-theoretic models typically assume rational agents, complete\ninformation, and common knowledge of payoffs - assumptions that are often\nviolated in real-world MAS characterized by uncertainty, misaligned\nperceptions, and nested beliefs. To overcome these limitations, researchers\nhave proposed extensions that incorporate models of cognitive constraints,\nsubjective beliefs, and heterogeneous reasoning. Among these, hypergame theory\nextends the classical paradigm by explicitly modeling agents' subjective\nperceptions of the strategic scenario, known as perceptual games, in which\nagents may hold divergent beliefs about the structure, payoffs, or available\nactions. We present a systematic review of agent-compatible applications of\nhypergame theory, examining how its descriptive capabilities have been adapted\nto dynamic and interactive MAS contexts. We analyze 44 selected studies from\ncybersecurity, robotics, social simulation, communications, and general\ngame-theoretic modeling. Building on a formal introduction to hypergame theory\nand its two major extensions - hierarchical hypergames and HNF - we develop\nagent-compatibility criteria and an agent-based classification framework to\nassess integration patterns and practical applicability. Our analysis reveals\nprevailing tendencies, including the prevalence of hierarchical and graph-based\nmodels in deceptive reasoning and the simplification of extensive theoretical\nframeworks in practical applications. We identify structural gaps, including\nthe limited adoption of HNF-based models, the lack of formal hypergame\nlanguages, and unexplored opportunities for modeling human-agent and\nagent-agent misalignment. By synthesizing trends, challenges, and open research\ndirections, this review provides a new roadmap for applying hypergame theory to\nenhance the realism and effectiveness of strategic modeling in dynamic\nmulti-agent environments.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u8d85\u535a\u5f08\u7406\u8bba\u5728\u52a8\u6001\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\uff0c\u5206\u6790\u4e8644\u9879\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u667a\u80fd\u4f53\u517c\u5bb9\u6027\u6807\u51c6\u548c\u5206\u7c7b\u6846\u67b6\uff0c\u5e76\u6307\u51fa\u4e86\u7814\u7a76\u7a7a\u767d\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u4f20\u7edf\u535a\u5f08\u7406\u8bba\u5047\u8bbe\u7406\u6027\u3001\u5b8c\u5168\u4fe1\u606f\u548c\u5171\u540c\u77e5\u8bc6\uff0c\u4f46\u73b0\u5b9e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5e38\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\u548c\u8ba4\u77e5\u5dee\u5f02\u3002\u8d85\u535a\u5f08\u7406\u8bba\u901a\u8fc7\u5efa\u6a21\u4e3b\u89c2\u611f\u77e5\u6765\u5f25\u8865\u8fd9\u4e9b\u4e0d\u8db3\u3002", "method": "\u7cfb\u7edf\u56de\u987e\u4e86\u8d85\u535a\u5f08\u7406\u8bba\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u667a\u80fd\u4f53\u517c\u5bb9\u6027\u6807\u51c6\u548c\u5206\u7c7b\u6846\u67b6\uff0c\u5206\u6790\u7814\u7a76\u8d8b\u52bf\u548c\u6311\u6218\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5206\u5c42\u548c\u56fe\u6a21\u578b\u5728\u6b3a\u9a97\u63a8\u7406\u4e2d\u5360\u4e3b\u5bfc\uff0c\u4f46HNF\u6a21\u578b\u5e94\u7528\u8f83\u5c11\uff0c\u4e14\u7f3a\u4e4f\u5f62\u5f0f\u5316\u8bed\u8a00\u3002", "conclusion": "\u672c\u6587\u4e3a\u8d85\u535a\u5f08\u7406\u8bba\u5728\u52a8\u6001\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u8def\u7ebf\u56fe\uff0c\u5f3a\u8c03\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2507.19626", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19626", "abs": "https://arxiv.org/abs/2507.19626", "authors": ["Adrian Celaya", "Tucker Netherton", "Dawid Schellingerhout", "Caroline Chung", "Beatrice Riviere", "David Fuentes"], "title": "Pre- and Post-Treatment Glioma Segmentation with the Medical Imaging Segmentation Toolkit", "comment": null, "summary": "Medical image segmentation continues to advance rapidly, yet rigorous\ncomparison between methods remains challenging due to a lack of standardized\nand customizable tooling. In this work, we present the current state of the\nMedical Imaging Segmentation Toolkit (MIST), with a particular focus on its\nflexible and modular postprocessing framework designed for the BraTS 2025 pre-\nand post-treatment glioma segmentation challenge. Since its debut in the 2024\nBraTS adult glioma post-treatment segmentation challenge, MIST's postprocessing\nmodule has been significantly extended to support a wide range of transforms,\nincluding removal or replacement of small objects, extraction of the largest\nconnected components, and morphological operations such as hole filling and\nclosing. These transforms can be composed into user-defined strategies,\nenabling fine-grained control over the final segmentation output. We evaluate\nthree such strategies - ranging from simple small-object removal to more\ncomplex, class-specific pipelines - and rank their performance using the BraTS\nranking protocol. Our results highlight how MIST facilitates rapid\nexperimentation and targeted refinement, ultimately producing high-quality\nsegmentations for the BraTS 2025 challenge. MIST remains open source and\nextensible, supporting reproducible and scalable research in medical image\nsegmentation.", "AI": {"tldr": "MIST\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u5de5\u5177\u5305\uff0c\u7279\u522b\u9488\u5bf9BraTS 2025\u6311\u6218\u8d5b\u8bbe\u8ba1\u4e86\u7075\u6d3b\u7684\u540e\u671f\u5904\u7406\u6846\u67b6\uff0c\u652f\u6301\u591a\u79cd\u53d8\u6362\u548c\u7528\u6237\u81ea\u5b9a\u4e49\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u5206\u5272\u8d28\u91cf\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u7f3a\u4e4f\u6807\u51c6\u5316\u548c\u53ef\u5b9a\u5236\u5de5\u5177\uff0c\u96be\u4ee5\u8fdb\u884c\u4e25\u683c\u6bd4\u8f83\u3002", "method": "\u6269\u5c55\u4e86MIST\u7684\u540e\u671f\u5904\u7406\u6a21\u5757\uff0c\u652f\u6301\u591a\u79cd\u53d8\u6362\uff08\u5982\u5c0f\u5bf9\u8c61\u79fb\u9664\u3001\u5f62\u6001\u5b66\u64cd\u4f5c\u7b49\uff09\uff0c\u5e76\u53ef\u7ec4\u5408\u6210\u7528\u6237\u81ea\u5b9a\u4e49\u7b56\u7565\u3002", "result": "\u8bc4\u4f30\u4e86\u4e09\u79cd\u7b56\u7565\uff0c\u7ed3\u679c\u663e\u793aMIST\u80fd\u5feb\u901f\u5b9e\u9a8c\u5e76\u4f18\u5316\u5206\u5272\u8d28\u91cf\u3002", "conclusion": "MIST\u5f00\u6e90\u4e14\u53ef\u6269\u5c55\uff0c\u652f\u6301\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u53ef\u91cd\u590d\u548c\u89c4\u6a21\u5316\u7814\u7a76\u3002"}}
{"id": "2507.19586", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19586", "abs": "https://arxiv.org/abs/2507.19586", "authors": ["Shengyuan Wang", "Jie Feng", "Tianhui Liu", "Dan Pei", "Yong Li"], "title": "Mitigating Geospatial Knowledge Hallucination in Large Language Models: Benchmarking and Dynamic Factuality Aligning", "comment": "19 pages, 9 figures", "summary": "Large language models (LLMs) possess extensive world knowledge, including\ngeospatial knowledge, which has been successfully applied to various geospatial\ntasks such as mobility prediction and social indicator prediction. However,\nLLMs often generate inaccurate geospatial knowledge, leading to geospatial\nhallucinations (incorrect or inconsistent representations of geospatial\ninformation) that compromise their reliability. While the phenomenon of general\nknowledge hallucination in LLMs has been widely studied, the systematic\nevaluation and mitigation of geospatial hallucinations remain largely\nunexplored. To address this gap, we propose a comprehensive evaluation\nframework for geospatial hallucinations, leveraging structured geospatial\nknowledge graphs for controlled assessment. Through extensive evaluation across\n20 advanced LLMs, we uncover the hallucinations in their geospatial knowledge.\nBuilding on these insights, we introduce a dynamic factuality aligning method\nbased on Kahneman-Tversky Optimization (KTO) to mitigate geospatial\nhallucinations in LLMs, leading to a performance improvement of over 29.6% on\nthe proposed benchmark. Extensive experimental results demonstrate the\neffectiveness of our benchmark and learning algorithm in enhancing the\ntrustworthiness of LLMs in geospatial knowledge and reasoning tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u548c\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5730\u7406\u7a7a\u95f4\u5e7b\u89c9\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u8bc4\u4f30\u548c\u52a8\u6001\u4e8b\u5b9e\u5bf9\u9f50\u65b9\u6cd5\uff08KTO\uff09\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "LLMs\u5728\u5730\u7406\u7a7a\u95f4\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u77e5\u8bc6\u4e0d\u51c6\u786e\u7684\u95ee\u9898\uff08\u5730\u7406\u7a7a\u95f4\u5e7b\u89c9\uff09\uff0c\u4f46\u76f8\u5173\u7814\u7a76\u8f83\u5c11\uff0c\u4e9f\u9700\u7cfb\u7edf\u8bc4\u4f30\u548c\u89e3\u51b3\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u7ed3\u6784\u5316\u5730\u7406\u7a7a\u95f4\u77e5\u8bc6\u56fe\u8c31\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8eKahneman-Tversky\u4f18\u5316\uff08KTO\uff09\u7684\u52a8\u6001\u4e8b\u5b9e\u5bf9\u9f50\u65b9\u6cd5\u3002", "result": "\u572820\u4e2a\u5148\u8fdbLLMs\u4e0a\u8bc4\u4f30\uff0c\u53d1\u73b0\u5730\u7406\u7a7a\u95f4\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u901a\u8fc7KTO\u65b9\u6cd5\u5c06\u6027\u80fd\u63d0\u534729.6%\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u548c\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86LLMs\u5728\u5730\u7406\u7a7a\u95f4\u77e5\u8bc6\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2507.19515", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.19515", "abs": "https://arxiv.org/abs/2507.19515", "authors": ["Edmund F. Agyemang", "Hansapani Rodrigo", "Vincent Agbenyeavu"], "title": "A Comparative Analysis of Traditional and Deep Learning Time Series Architectures for Influenza A Infectious Disease Forecasting", "comment": null, "summary": "Influenza A is responsible for 290,000 to 650,000 respiratory deaths a year,\nthough this estimate is an improvement from years past due to improvements in\nsanitation, healthcare practices, and vaccination programs. In this study, we\nperform a comparative analysis of traditional and deep learning models to\npredict Influenza A outbreaks. Using historical data from January 2009 to\nDecember 2023, we compared the performance of traditional ARIMA and Exponential\nSmoothing(ETS) models with six distinct deep learning architectures: Simple\nRNN, LSTM, GRU, BiLSTM, BiGRU, and Transformer. The results reveal a clear\nsuperiority of all the deep learning models, especially the state-of-the-art\nTransformer with respective average testing MSE and MAE of 0.0433 \\pm 0.0020\nand 0.1126 \\pm 0.0016 for capturing the temporal complexities associated with\nInfluenza A data, outperforming well known traditional baseline ARIMA and ETS\nmodels. These findings of this study provide evidence that state-of-the-art\ndeep learning architectures can enhance predictive modeling for infectious\ndiseases and indicate a more general trend toward using deep learning methods\nto enhance public health forecasting and intervention planning strategies.\nFuture work should focus on how these models can be incorporated into real-time\nforecasting and preparedness systems at an epidemic level, and integrated into\nexisting surveillance systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u4f20\u7edf\u6a21\u578b\uff08ARIMA\u548cETS\uff09\u4e0e\u516d\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08Simple RNN\u3001LSTM\u3001GRU\u3001BiLSTM\u3001BiGRU\u548cTransformer\uff09\u5728\u9884\u6d4b\u7532\u578b\u6d41\u611f\u7206\u53d1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5c24\u5176\u662fTransformer\u8868\u73b0\u6700\u4f18\u3002", "motivation": "\u7532\u578b\u6d41\u611f\u6bcf\u5e74\u5bfc\u81f4\u5927\u91cf\u6b7b\u4ea1\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u9884\u6d4b\u6a21\u578b\u6765\u63d0\u5347\u516c\u5171\u536b\u751f\u5e72\u9884\u80fd\u529b\u3002", "method": "\u4f7f\u75282009\u5e74\u81f32023\u5e74\u7684\u5386\u53f2\u6570\u636e\uff0c\u5bf9\u6bd4\u4f20\u7edf\u6a21\u578b\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u6240\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5747\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\uff0c\u5176\u4e2dTransformer\u8868\u73b0\u6700\u4f73\uff0c\u6d4b\u8bd5MSE\u548cMAE\u5206\u522b\u4e3a0.0433\u548c0.1126\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u80fd\u663e\u8457\u63d0\u5347\u4f20\u67d3\u75c5\u9884\u6d4b\u80fd\u529b\uff0c\u672a\u6765\u5e94\u63a2\u7d22\u5982\u4f55\u5c06\u5176\u6574\u5408\u5230\u5b9e\u65f6\u9884\u6d4b\u548c\u76d1\u6d4b\u7cfb\u7edf\u4e2d\u3002"}}
{"id": "2507.19608", "categories": ["cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.19608", "abs": "https://arxiv.org/abs/2507.19608", "authors": ["Jiawen Qi", "Chang Gao", "Zhaochun Ren", "Qinyu Chen"], "title": "DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference", "comment": null, "summary": "Deploying Large Language Models (LLMs) on edge devices remains challenging\ndue to their quadratically increasing computations with the sequence length.\nExisting studies for dynamic attention pruning are designed for hardware with\nmassively parallel computation capabilities, such as GPUs or TPUs, and aim at\nlong context lengths (e.g., 64K), making them unsuitable for edge scenarios. We\npresent DeltaLLM, a training-free framework that exploits temporal sparsity in\nattention patterns to enable efficient LLM inference across both the prefilling\nand decoding stages, on resource-constrained edge devices. DeltaLLM introduces\nan accuracy- and memory-aware delta matrix construction strategy that\nintroduces temporal sparsity, and a context-aware hybrid attention mechanism\nthat combines full attention in a local context window with delta approximation\noutside it to increase accuracy. We evaluate our framework on the\nedge-device-friendly BitNet-b1.58-2B-4T model and Llama3.2-1B-Instruct model\nacross diverse language tasks. The results show that on BitNet, our framework\nincreases the attention sparsity from 0% to 60% during the prefilling stage\nwith slight accuracy improvement on the WG task, and 0% to 57% across both the\nprefilling and decoding stages, with even higher F1 score from 29.63 to 30.97\non SQuAD-v2 task. On the Llama model, it can also achieve up to 60% sparsity\nduring the prefilling stage and around 57% across both stages with negligible\naccuracy drop. These results demonstrate that DeltaLLM offers a promising\nsolution for efficient edge deployment, requiring no fine-tuning and seamlessly\nintegrating with existing inference pipelines.", "AI": {"tldr": "DeltaLLM\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u52a8\u6001\u6ce8\u610f\u529b\u526a\u679d\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\uff0c\u663e\u8457\u63d0\u9ad8\u6ce8\u610f\u529b\u7a00\u758f\u6027\u4e14\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u65f6\u56e0\u5e8f\u5217\u957f\u5ea6\u589e\u52a0\u800c\u8ba1\u7b97\u91cf\u6fc0\u589e\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u6ce8\u610f\u529b\u6a21\u5f0f\u7684\u65f6\u95f4\u7a00\u758f\u6027\uff0c\u63d0\u51fa\u57fa\u4e8edelta\u77e9\u9635\u7684\u6784\u9020\u7b56\u7565\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u5728BitNet\u548cLlama\u6a21\u578b\u4e0a\uff0c\u6ce8\u610f\u529b\u7a00\u758f\u6027\u63d0\u5347\u81f360%\uff0c\u90e8\u5206\u4efb\u52a1\u51c6\u786e\u6027\u7565\u6709\u63d0\u9ad8\u3002", "conclusion": "DeltaLLM\u4e3a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548LLM\u63a8\u7406\u63d0\u4f9b\u4e86\u65e0\u9700\u5fae\u8c03\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.19673", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19673", "abs": "https://arxiv.org/abs/2507.19673", "authors": ["Babak Taati", "Muhammad Muzammil", "Yasamin Zarghami", "Abhishek Moturu", "Airhossein Kazerouni", "Hailey Reimer", "Alex Mihailidis", "Thomas Hadjistavropoulos"], "title": "SynPAIN: A Synthetic Dataset of Pain and Non-Pain Facial Expressions", "comment": "10 pages, 4 figures, submitted to IEEE JBHI", "summary": "Accurate pain assessment in patients with limited ability to communicate,\nsuch as older adults with dementia, represents a critical healthcare challenge.\nRobust automated systems of pain detection may facilitate such assessments.\nExisting pain detection datasets, however, suffer from limited ethnic/racial\ndiversity, privacy constraints, and underrepresentation of older adults who are\nthe primary target population for clinical deployment. We present SynPAIN, a\nlarge-scale synthetic dataset containing 10,710 facial expression images (5,355\nneutral/expressive pairs) across five ethnicities/races, two age groups (young:\n20-35, old: 75+), and two genders. Using commercial generative AI tools, we\ncreated demographically balanced synthetic identities with clinically\nmeaningful pain expressions. Our validation demonstrates that synthetic pain\nexpressions exhibit expected pain patterns, scoring significantly higher than\nneutral and non-pain expressions using clinically validated pain assessment\ntools based on facial action unit analysis. We experimentally demonstrate\nSynPAIN's utility in identifying algorithmic bias in existing pain detection\nmodels. Through comprehensive bias evaluation, we reveal substantial\nperformance disparities across demographic characteristics. These performance\ndisparities were previously undetectable with smaller, less diverse datasets.\nFurthermore, we demonstrate that age-matched synthetic data augmentation\nimproves pain detection performance on real clinical data, achieving a 7.0%\nimprovement in average precision. SynPAIN addresses critical gaps in pain\nassessment research by providing the first publicly available, demographically\ndiverse synthetic dataset specifically designed for older adult pain detection,\nwhile establishing a framework for measuring and mitigating algorithmic bias.\nThe dataset is available at https://doi.org/10.5683/SP3/WCXMAP", "AI": {"tldr": "SynPAIN\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u89e3\u51b3\u75bc\u75db\u8bc4\u4f30\u7814\u7a76\u4e2d\u79cd\u65cf/\u5e74\u9f84\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u6570\u636e\u589e\u5f3a\u63d0\u9ad8\u4e86\u75bc\u75db\u68c0\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u75bc\u75db\u8bc4\u4f30\u4e2d\u56e0\u6570\u636e\u96c6\u591a\u6837\u6027\u4e0d\u8db3\u5bfc\u81f4\u7684\u7b97\u6cd5\u504f\u89c1\u95ee\u9898\uff0c\u7279\u522b\u662f\u9488\u5bf9\u8001\u5e74\u75f4\u5446\u75c7\u60a3\u8005\u7b49\u6c9f\u901a\u53d7\u9650\u4eba\u7fa4\u3002", "method": "\u4f7f\u7528\u5546\u4e1a\u751f\u6210AI\u5de5\u5177\u521b\u5efa\u4e86\u5305\u542b\u4e0d\u540c\u79cd\u65cf\u3001\u5e74\u9f84\u548c\u6027\u522b\u7684\u5408\u6210\u75bc\u75db\u8868\u60c5\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u4e34\u5e8a\u9a8c\u8bc1\u5de5\u5177\u8bc4\u4f30\u5176\u6709\u6548\u6027\u3002", "result": "\u5408\u6210\u6570\u636e\u5c55\u793a\u4e86\u9884\u671f\u7684\u75bc\u75db\u6a21\u5f0f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u75bc\u75db\u68c0\u6d4b\u6027\u80fd\uff08\u5e73\u5747\u7cbe\u5ea6\u63d0\u53477.0%\uff09\uff0c\u5e76\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u7b97\u6cd5\u504f\u89c1\u3002", "conclusion": "SynPAIN\u586b\u8865\u4e86\u75bc\u75db\u8bc4\u4f30\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e86\u9996\u4e2a\u516c\u5f00\u7684\u591a\u6837\u5316\u5408\u6210\u6570\u636e\u96c6\uff0c\u5e76\u4e3a\u6d4b\u91cf\u548c\u7f13\u89e3\u7b97\u6cd5\u504f\u89c1\u63d0\u4f9b\u4e86\u6846\u67b6\u3002"}}
{"id": "2507.19595", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19595", "abs": "https://arxiv.org/abs/2507.19595", "authors": ["Yutao Sun", "Zhenyu Li", "Yike Zhang", "Tengyu Pan", "Bowen Dong", "Yuyi Guo", "Jianyong Wang"], "title": "Efficient Attention Mechanisms for Large Language Models: A Survey", "comment": "work in progress", "summary": "Transformer-based architectures have become the prevailing backbone of large\nlanguage models. However, the quadratic time and memory complexity of\nself-attention remains a fundamental obstacle to efficient long-context\nmodeling. To address this limitation, recent research has introduced two\nprincipal categories of efficient attention mechanisms. Linear attention\nmethods achieve linear complexity through kernel approximations, recurrent\nformulations, or fastweight dynamics, thereby enabling scalable inference with\nreduced computational overhead. Sparse attention techniques, in contrast, limit\nattention computation to selected subsets of tokens based on fixed patterns,\nblock-wise routing, or clustering strategies, enhancing efficiency while\npreserving contextual coverage. This survey provides a systematic and\ncomprehensive overview of these developments, integrating both algorithmic\ninnovations and hardware-level considerations. In addition, we analyze the\nincorporation of efficient attention into largescale pre-trained language\nmodels, including both architectures built entirely on efficient attention and\nhybrid designs that combine local and global components. By aligning\ntheoretical foundations with practical deployment strategies, this work aims to\nserve as a foundational reference for advancing the design of scalable and\nefficient language models.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86Transformer\u67b6\u6784\u4e2d\u9ad8\u6548\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e24\u7c7b\u65b9\u6cd5\uff1a\u7ebf\u6027\u6ce8\u610f\u529b\u548c\u7a00\u758f\u6ce8\u610f\u529b\uff0c\u65e8\u5728\u89e3\u51b3\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u63a8\u52a8\u9ad8\u6548\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u7684\u53d1\u5c55\u3002", "motivation": "\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u65f6\u95f4\u548c\u5185\u5b58\u590d\u6742\u5ea6\u9650\u5236\u4e86\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u7684\u6548\u7387\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u9ad8\u6548\u7684\u6ce8\u610f\u529b\u673a\u5236\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u7ebf\u6027\u6ce8\u610f\u529b\u901a\u8fc7\u6838\u8fd1\u4f3c\u3001\u5faa\u73af\u516c\u5f0f\u6216\u5feb\u901f\u6743\u91cd\u52a8\u6001\u5b9e\u73b0\u7ebf\u6027\u590d\u6742\u5ea6\uff1b\u7a00\u758f\u6ce8\u610f\u529b\u5219\u901a\u8fc7\u56fa\u5b9a\u6a21\u5f0f\u3001\u5757\u8def\u7531\u6216\u805a\u7c7b\u7b56\u7565\u9009\u62e9\u90e8\u5206\u4ee4\u724c\u8fdb\u884c\u8ba1\u7b97\u3002", "result": "\u7efc\u8ff0\u4e86\u9ad8\u6548\u6ce8\u610f\u529b\u673a\u5236\u7684\u7b97\u6cd5\u521b\u65b0\u548c\u786c\u4ef6\u7ea7\u4f18\u5316\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5728\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e94\u7528\uff0c\u5305\u62ec\u7eaf\u9ad8\u6548\u6ce8\u610f\u529b\u67b6\u6784\u548c\u6df7\u5408\u8bbe\u8ba1\u3002", "conclusion": "\u672c\u6587\u4e3a\u8bbe\u8ba1\u548c\u90e8\u7f72\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2507.19517", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19517", "abs": "https://arxiv.org/abs/2507.19517", "authors": ["Mohit Gupta", "Debjit Bhowmick", "Ben Beck"], "title": "BikeVAE-GNN: A Variational Autoencoder-Augmented Hybrid Graph Neural Network for Sparse Bicycle Volume Estimation", "comment": "This paper has been accepted for publication in the Proceedings of\n  the $28^{th}$ IEEE International Conference on Intelligent Transportation\n  Systems (ITSC 2025). This is the author's version of the work", "summary": "Accurate link-level bicycle volume estimation is essential for informed urban\nand transport planning but it is challenged by extremely sparse count data in\nurban bicycling networks worldwide. We propose BikeVAE-GNN, a novel dual-task\nframework augmenting a Hybrid Graph Neural Network (GNN) with Variational\nAutoencoder (VAE) to estimate Average Daily Bicycle (ADB) counts, addressing\nsparse bicycle networks. The Hybrid-GNN combines Graph Convolutional Networks\n(GCN), Graph Attention Networks (GAT), and GraphSAGE to effectively model\nintricate spatial relationships in sparse networks while VAE generates\nsynthetic nodes and edges to enrich the graph structure and enhance the\nestimation performance. BikeVAE-GNN simultaneously performs - regression for\nbicycling volume estimation and classification for bicycling traffic level\ncategorization. We demonstrate the effectiveness of BikeVAE-GNN using\nOpenStreetMap data and publicly available bicycle count data within the City of\nMelbourne - where only 141 of 15,933 road segments have labeled counts\n(resulting in 99% count data sparsity). Our experiments show that BikeVAE-GNN\noutperforms machine learning and baseline GNN models, achieving a mean absolute\nerror (MAE) of 30.82 bicycles per day, accuracy of 99% and F1-score of 0.99.\nAblation studies further validate the effective role of Hybrid-GNN and VAE\ncomponents. Our research advances bicycling volume estimation in sparse\nnetworks using novel and state-of-the-art approaches, providing insights for\nsustainable bicycling infrastructures.", "AI": {"tldr": "BikeVAE-GNN\u662f\u4e00\u79cd\u7ed3\u5408\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u53cc\u4efb\u52a1\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u7a00\u758f\u81ea\u884c\u8f66\u7f51\u7edc\u7684\u6d41\u91cf\u4f30\u8ba1\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u57ce\u5e02\u81ea\u884c\u8f66\u7f51\u7edc\u6570\u636e\u7a00\u758f\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u51c6\u786e\u4f30\u8ba1\u81ea\u884c\u8f66\u6d41\u91cf\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u63d0\u5347\u4f30\u8ba1\u7cbe\u5ea6\u3002", "method": "\u63d0\u51faBikeVAE-GNN\u6846\u67b6\uff0c\u7ed3\u5408Hybrid-GNN\uff08GCN\u3001GAT\u3001GraphSAGE\uff09\u548cVAE\uff0c\u901a\u8fc7\u751f\u6210\u5408\u6210\u6570\u636e\u589e\u5f3a\u7a00\u758f\u7f51\u7edc\u7ed3\u6784\uff0c\u540c\u65f6\u8fdb\u884c\u56de\u5f52\u548c\u5206\u7c7b\u4efb\u52a1\u3002", "result": "\u5728\u58a8\u5c14\u672c\u6570\u636e\u4e0a\uff0cBikeVAE-GNN\u7684MAE\u4e3a30.82\uff0c\u51c6\u786e\u7387\u548cF1-score\u5747\u4e3a0.99\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "BikeVAE-GNN\u4e3a\u7a00\u758f\u81ea\u884c\u8f66\u7f51\u7edc\u6d41\u91cf\u4f30\u8ba1\u63d0\u4f9b\u4e86\u5148\u8fdb\u89e3\u51b3\u65b9\u6848\uff0c\u5bf9\u53ef\u6301\u7eed\u4ea4\u901a\u89c4\u5212\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2507.19672", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.19672", "abs": "https://arxiv.org/abs/2507.19672", "authors": ["Haoran Lu", "Luyang Fang", "Ruidong Zhang", "Xinliang Li", "Jiazhang Cai", "Huimin Cheng", "Lin Tang", "Ziyu Liu", "Zeliang Sun", "Tao Wang", "Yingchuan Zhang", "Arif Hassan Zidan", "Jinwen Xu", "Jincheng Yu", "Meizhi Yu", "Hanqi Jiang", "Xilin Gong", "Weidi Luo", "Bolun Sun", "Yongkai Chen", "Terry Ma", "Shushan Wu", "Yifan Zhou", "Junhao Chen", "Haotian Xiang", "Jing Zhang", "Afrar Jahin", "Wei Ruan", "Ke Deng", "Yi Pan", "Peilong Wang", "Jiahui Li", "Zhengliang Liu", "Lu Zhang", "Lin Zhao", "Wei Liu", "Dajiang Zhu", "Xin Xing", "Fei Dou", "Wei Zhang", "Chao Huang", "Rongjie Liu", "Mengrui Zhang", "Yiwen Liu", "Xiaoxiao Sun", "Qin Lu", "Zhen Xiang", "Wenxuan Zhong", "Tianming Liu", "Ping Ma"], "title": "Alignment and Safety in Large Language Models: Safety Mechanisms, Training Paradigms, and Emerging Challenges", "comment": "119 pages, 10 figures, 7 tables", "summary": "Due to the remarkable capabilities and growing impact of large language\nmodels (LLMs), they have been deeply integrated into many aspects of society.\nThus, ensuring their alignment with human values and intentions has emerged as\na critical challenge. This survey provides a comprehensive overview of\npractical alignment techniques, training protocols, and empirical findings in\nLLM alignment. We analyze the development of alignment methods across diverse\nparadigms, characterizing the fundamental trade-offs between core alignment\nobjectives. Our analysis shows that while supervised fine-tuning enables basic\ninstruction-following, preference-based methods offer more flexibility for\naligning with nuanced human intent. We discuss state-of-the-art techniques,\nincluding Direct Preference Optimization (DPO), Constitutional AI,\nbrain-inspired methods, and alignment uncertainty quantification (AUQ),\nhighlighting their approaches to balancing quality and efficiency. We review\nexisting evaluation frameworks and benchmarking datasets, emphasizing\nlimitations such as reward misspecification, distributional robustness, and\nscalable oversight. We summarize strategies adopted by leading AI labs to\nillustrate the current state of practice. We conclude by outlining open\nproblems in oversight, value pluralism, robustness, and continuous alignment.\nThis survey aims to inform both researchers and practitioners navigating the\nevolving landscape of LLM alignment.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u5168\u9762\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bf9\u9f50\u4eba\u7c7b\u4ef7\u503c\u89c2\u548c\u610f\u56fe\u7684\u5b9e\u8df5\u6280\u672f\u3001\u8bad\u7ec3\u534f\u8bae\u548c\u5b9e\u8bc1\u53d1\u73b0\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u8303\u5f0f\u4e0b\u7684\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5e76\u603b\u7ed3\u4e86\u5f53\u524d\u7684\u6700\u65b0\u6280\u672f\u548c\u6311\u6218\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u663e\u8457\u63d0\u5347\u548c\u793e\u4f1a\u5f71\u54cd\u7684\u6269\u5927\uff0c\u786e\u4fdd\u5176\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u548c\u610f\u56fe\u7684\u5bf9\u9f50\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002", "method": "\u7efc\u8ff0\u5206\u6790\u4e86\u76d1\u7763\u5fae\u8c03\u3001\u57fa\u4e8e\u504f\u597d\u7684\u65b9\u6cd5\u7b49\u591a\u79cd\u5bf9\u9f50\u8303\u5f0f\uff0c\u5e76\u8ba8\u8bba\u4e86\u5305\u62ecDPO\u3001Constitutional AI\u7b49\u5728\u5185\u7684\u524d\u6cbf\u6280\u672f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u76d1\u7763\u5fae\u8c03\u80fd\u5b9e\u73b0\u57fa\u672c\u6307\u4ee4\u8ddf\u968f\uff0c\u800c\u57fa\u4e8e\u504f\u597d\u7684\u65b9\u6cd5\u80fd\u66f4\u7075\u6d3b\u5730\u5bf9\u9f50\u590d\u6742\u7684\u4eba\u7c7b\u610f\u56fe\u3002", "conclusion": "\u6587\u7ae0\u603b\u7ed3\u4e86\u5f53\u524d\u5b9e\u8df5\u4e2d\u7684\u7b56\u7565\uff0c\u5e76\u63d0\u51fa\u4e86\u5728\u76d1\u7763\u3001\u4ef7\u503c\u591a\u5143\u6027\u3001\u9c81\u68d2\u6027\u548c\u6301\u7eed\u5bf9\u9f50\u7b49\u65b9\u9762\u7684\u5f00\u653e\u6027\u95ee\u9898\u3002"}}
{"id": "2507.19679", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19679", "abs": "https://arxiv.org/abs/2507.19679", "authors": ["Mandar Kulkarni"], "title": "Efficient Learning for Product Attributes with Compact Multimodal Models", "comment": null, "summary": "Image-based product attribute prediction in e-commerce is a crucial task with\nnumerous applications. The supervised fine-tuning of Vision Language Models\n(VLMs) faces significant scale challenges due to the cost of manual or API\nbased annotation. In this paper, we investigate label-efficient semi-supervised\nfine-tuning strategies for compact VLMs (2B-3B parameters) that leverage\nunlabeled product listings through Direct Preference Optimization (DPO).\nBeginning with a small, API-based, annotated, and labeled set, we first employ\nPEFT to train low-rank adapter modules. To update the adapter weights with\nunlabeled data, we generate multiple reasoning-and-answer chains per unlabeled\nsample and segregate these chains into preferred and dispreferred based on\nself-consistency. We then fine-tune the model with DPO loss and use the updated\nmodel for the next iteration. By using PEFT fine-tuning with DPO, our method\nachieves efficient convergence with minimal compute overhead. On a dataset\nspanning twelve e-commerce verticals, DPO-based fine-tuning, which utilizes\nonly unlabeled data, demonstrates a significant improvement over the supervised\nmodel. Moreover, experiments demonstrate that accuracy with DPO training\nimproves with more unlabeled data, indicating that a large pool of unlabeled\nsamples can be effectively leveraged to improve performance.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u534a\u76d1\u7763\u5b66\u4e60\u7684\u6807\u7b7e\u9ad8\u6548\u5fae\u8c03\u7b56\u7565\uff0c\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\u901a\u8fc7\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u63d0\u5347\u7d27\u51d1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u624b\u52a8\u6216API\u6807\u6ce8\u6210\u672c\u9ad8\u6602\uff0c\u76d1\u7763\u5fae\u8c03\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u89c4\u6a21\u6311\u6218\uff0c\u56e0\u6b64\u63a2\u7d22\u5982\u4f55\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u91c7\u7528PEFT\u8bad\u7ec3\u4f4e\u79e9\u9002\u914d\u5668\u6a21\u5757\uff0c\u901a\u8fc7DPO\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\u751f\u6210\u504f\u597d\u94fe\u5e76\u66f4\u65b0\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u6548\u6536\u655b\u3002", "result": "\u572812\u4e2a\u7535\u5546\u9886\u57df\u7684\u6570\u636e\u96c6\u4e0a\uff0cDPO\u5fae\u8c03\u663e\u8457\u4f18\u4e8e\u76d1\u7763\u6a21\u578b\uff0c\u4e14\u672a\u6807\u6ce8\u6570\u636e\u8d8a\u591a\uff0c\u6027\u80fd\u63d0\u5347\u8d8a\u660e\u663e\u3002", "conclusion": "\u901a\u8fc7DPO\u548c\u672a\u6807\u6ce8\u6570\u636e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5f00\u9500\u6700\u5c0f\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u672a\u6807\u6ce8\u6570\u636e\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.19598", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19598", "abs": "https://arxiv.org/abs/2507.19598", "authors": ["Muntasir Wahed", "Xiaona Zhou", "Kiet A. Nguyen", "Tianjiao Yu", "Nirav Diwan", "Gang Wang", "Dilek Hakkani-T\u00fcr", "Ismini Lourentzou"], "title": "MOCHA: Are Code Language Models Robust Against Multi-Turn Malicious Coding Prompts?", "comment": "Winner Defender Team at Amazon Nova AI Challenge 2025", "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their code generation capabilities. However, their robustness against\nadversarial misuse, particularly through multi-turn malicious coding prompts,\nremains underexplored. In this work, we introduce code decomposition attacks,\nwhere a malicious coding task is broken down into a series of seemingly benign\nsubtasks across multiple conversational turns to evade safety filters. To\nfacilitate systematic evaluation, we introduce \\benchmarkname{}, a large-scale\nbenchmark designed to evaluate the robustness of code LLMs against both\nsingle-turn and multi-turn malicious prompts. Empirical results across open-\nand closed-source models reveal persistent vulnerabilities, especially under\nmulti-turn scenarios. Fine-tuning on MOCHA improves rejection rates while\npreserving coding ability, and importantly, enhances robustness on external\nadversarial datasets with up to 32.4% increase in rejection rates without any\nadditional supervision.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u8f6e\u6076\u610f\u63d0\u793a\u653b\u51fb\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u3002\u901a\u8fc7\u5fae\u8c03\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u6297\u6027\u653b\u51fb\u7684\u9632\u5fa1\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u5bf9\u591a\u8f6e\u6076\u610f\u63d0\u793a\u7684\u9c81\u68d2\u6027\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4ee3\u7801\u5206\u89e3\u653b\u51fb\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\\benchmarkname{}\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u5355\u8f6e\u548c\u591a\u8f6e\u6076\u610f\u63d0\u793a\u4e0b\u7684\u8868\u73b0\u3002\u901a\u8fc7\u5fae\u8c03\u6a21\u578b\uff08MOCHA\uff09\u63d0\u5347\u9632\u5fa1\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6a21\u578b\u5728\u591a\u8f6e\u653b\u51fb\u4e0b\u5b58\u5728\u6f0f\u6d1e\uff0c\u5fae\u8c03\u540e\u62d2\u7edd\u7387\u663e\u8457\u63d0\u5347\uff08\u6700\u9ad832.4%\uff09\uff0c\u4e14\u4e0d\u5f71\u54cd\u4ee3\u7801\u751f\u6210\u80fd\u529b\u3002", "conclusion": "\u591a\u8f6e\u6076\u610f\u63d0\u793a\u5bf9LLMs\u6784\u6210\u5a01\u80c1\uff0c\u4f46\u901a\u8fc7\u5fae\u8c03\u53ef\u663e\u8457\u63d0\u9ad8\u9c81\u68d2\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\u548c\u65b9\u6cd5\u3002"}}
{"id": "2507.19518", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19518", "abs": "https://arxiv.org/abs/2507.19518", "authors": ["Sangwoo Seo", "Jimin Seo", "Yoonho Lee", "Donghyeon Kim", "Hyejin Shin", "Banghyun Sung", "Chanyoung Park"], "title": "Target Circuit Matching in Large-Scale Netlists using GNN-Based Region Prediction", "comment": "ICCAD 2025", "summary": "Subgraph matching plays an important role in electronic design automation\n(EDA) and circuit verification. Traditional rule-based methods have limitations\nin generalizing to arbitrary target circuits. Furthermore, node-to-node\nmatching approaches tend to be computationally inefficient, particularly for\nlarge-scale circuits. Deep learning methods have emerged as a potential\nsolution to address these challenges, but existing models fail to efficiently\ncapture global subgraph embeddings or rely on inefficient matching matrices,\nwhich limits their effectiveness for large circuits. In this paper, we propose\nan efficient graph matching approach that utilizes Graph Neural Networks (GNNs)\nto predict regions of high probability for containing the target circuit.\nSpecifically, we construct various negative samples to enable GNNs to\naccurately learn the presence of target circuits and develop an approach to\ndirectly extracting subgraph embeddings from the entire circuit, which captures\nglobal subgraph information and addresses the inefficiency of applying GNNs to\nall candidate subgraphs. Extensive experiments demonstrate that our approach\nsignificantly outperforms existing methods in terms of time efficiency and\ntarget region prediction, offering a scalable and effective solution for\nsubgraph matching in large-scale circuits.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u9ad8\u6548\u5b50\u56fe\u5339\u914d\u65b9\u6cd5\uff0c\u7528\u4e8e\u7535\u5b50\u8bbe\u8ba1\u81ea\u52a8\u5316\uff08EDA\uff09\u548c\u7535\u8def\u9a8c\u8bc1\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u8282\u70b9\u5230\u8282\u70b9\u5339\u914d\u65b9\u6cd5\u8ba1\u7b97\u6548\u7387\u4f4e\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u65e0\u6cd5\u9ad8\u6548\u6355\u83b7\u5168\u5c40\u5b50\u56fe\u5d4c\u5165\u6216\u4f9d\u8d56\u4f4e\u6548\u5339\u914d\u77e9\u9635\u3002", "method": "\u5229\u7528GNN\u9884\u6d4b\u76ee\u6807\u7535\u8def\u7684\u9ad8\u6982\u7387\u533a\u57df\uff0c\u6784\u5efa\u8d1f\u6837\u672c\u4ee5\u51c6\u786e\u5b66\u4e60\u76ee\u6807\u7535\u8def\u7684\u5b58\u5728\uff0c\u5e76\u76f4\u63a5\u4ece\u6574\u4e2a\u7535\u8def\u4e2d\u63d0\u53d6\u5b50\u56fe\u5d4c\u5165\u4ee5\u6355\u83b7\u5168\u5c40\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u65f6\u95f4\u6548\u7387\u548c\u76ee\u6807\u533a\u57df\u9884\u6d4b\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21\u7535\u8def\u4e2d\u7684\u5b50\u56fe\u5339\u914d\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.19703", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19703", "abs": "https://arxiv.org/abs/2507.19703", "authors": ["Peter V. Coveney", "Sauro Succi"], "title": "The wall confronting large language models", "comment": null, "summary": "We show that the scaling laws which determine the performance of large\nlanguage models (LLMs) severely limit their ability to improve the uncertainty\nof their predictions. As a result, raising their reliability to meet the\nstandards of scientific inquiry is intractable by any reasonable measure. We\nargue that the very mechanism which fuels much of the learning power of LLMs,\nnamely the ability to generate non-Gaussian output distributions from Gaussian\ninput ones, might well be at the roots of their propensity to produce error\npileup, ensuing information catastrophes and degenerative AI behaviour. This\ntension between learning and accuracy is a likely candidate mechanism\nunderlying the observed low values of the scaling components. It is\nsubstantially compounded by the deluge of spurious correlations pointed out by\nCalude and Longo which rapidly increase in any data set merely as a function of\nits size, regardless of its nature. The fact that a degenerative AI pathway is\na very probable feature of the LLM landscape does not mean that it must\ninevitably arise in all future AI research. Its avoidance, which we also\ndiscuss in this paper, necessitates putting a much higher premium on insight\nand understanding of the structural characteristics of the problems being\ninvestigated.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6027\u80fd\u53d7\u9650\u4e8e\u5176\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u7684\u63d0\u5347\u80fd\u529b\uff0c\u96be\u4ee5\u6ee1\u8db3\u79d1\u5b66\u7814\u7a76\u7684\u53ef\u9760\u6027\u6807\u51c6\u3002\u5176\u5b66\u4e60\u673a\u5236\u53ef\u80fd\u5bfc\u81f4\u9519\u8bef\u7d2f\u79ef\u548c\u4fe1\u606f\u707e\u96be\uff0c\u800c\u6570\u636e\u89c4\u6a21\u7684\u6269\u5927\u52a0\u5267\u4e86\u865a\u5047\u76f8\u5173\u6027\u3002\u907f\u514d\u9000\u5316AI\u8def\u5f84\u9700\u66f4\u91cd\u89c6\u95ee\u9898\u7ed3\u6784\u7279\u6027\u7684\u7406\u89e3\u3002", "motivation": "\u63a2\u8ba8LLM\u5728\u63d0\u5347\u9884\u6d4b\u53ef\u9760\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u63ed\u793a\u5176\u5b66\u4e60\u673a\u5236\u4e0e\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u5e76\u63d0\u51fa\u907f\u514dAI\u9000\u5316\u7684\u53ef\u80fd\u9014\u5f84\u3002", "method": "\u901a\u8fc7\u5206\u6790LLM\u7684\u7f29\u653e\u5b9a\u5f8b\u548c\u5b66\u4e60\u673a\u5236\uff0c\u7ed3\u5408\u6570\u636e\u89c4\u6a21\u5bf9\u865a\u5047\u76f8\u5173\u6027\u7684\u5f71\u54cd\uff0c\u8bba\u8bc1\u5176\u6027\u80fd\u9650\u5236\u3002", "result": "LLM\u7684\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u96be\u4ee5\u6539\u5584\uff0c\u5b66\u4e60\u673a\u5236\u53ef\u80fd\u5bfc\u81f4\u9519\u8bef\u7d2f\u79ef\u548c\u4fe1\u606f\u707e\u96be\uff0c\u6570\u636e\u89c4\u6a21\u6269\u5927\u52a0\u5267\u95ee\u9898\u3002", "conclusion": "\u4e3a\u907f\u514dAI\u9000\u5316\uff0c\u9700\u66f4\u91cd\u89c6\u5bf9\u95ee\u9898\u7ed3\u6784\u7279\u6027\u7684\u6df1\u5165\u7406\u89e3\uff0c\u800c\u975e\u5355\u7eaf\u4f9d\u8d56\u6570\u636e\u89c4\u6a21\u548c\u6a21\u578b\u6269\u5c55\u3002"}}
{"id": "2507.19682", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19682", "abs": "https://arxiv.org/abs/2507.19682", "authors": ["Matthew Drexler", "Benjamin Risk", "James J Lah", "Suprateek Kundu", "Deqiang Qiu"], "title": "DeepJIVE: Learning Joint and Individual Variation Explained from Multimodal Data Using Deep Learning", "comment": "26 pages, 10 figures", "summary": "Conventional multimodal data integration methods provide a comprehensive\nassessment of the shared or unique structure within each individual data type\nbut suffer from several limitations such as the inability to handle\nhigh-dimensional data and identify nonlinear structures. In this paper, we\nintroduce DeepJIVE, a deep-learning approach to performing Joint and Individual\nVariance Explained (JIVE). We perform mathematical derivation and experimental\nvalidations using both synthetic and real-world 1D, 2D, and 3D datasets.\nDifferent strategies of achieving the identity and orthogonality constraints\nfor DeepJIVE were explored, resulting in three viable loss functions. We found\nthat DeepJIVE can successfully uncover joint and individual variations of\nmultimodal datasets. Our application of DeepJIVE to the Alzheimer's Disease\nNeuroimaging Initiative (ADNI) also identified biologically plausible\ncovariation patterns between the amyloid positron emission tomography (PET) and\nmagnetic resonance (MR) images. In conclusion, the proposed DeepJIVE can be a\nuseful tool for multimodal data analysis.", "AI": {"tldr": "DeepJIVE\u662f\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u6210\u65b9\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u9ad8\u7ef4\u6570\u636e\u5e76\u8bc6\u522b\u975e\u7ebf\u6027\u7ed3\u6784\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u591a\u6a21\u6001\u6570\u636e\u96c6\u6210\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u9ad8\u7ef4\u6570\u636e\u548c\u975e\u7ebf\u6027\u7ed3\u6784\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u5f3a\u5927\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDeepJIVE\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u5b9e\u73b0\u8054\u5408\u548c\u4e2a\u4f53\u65b9\u5dee\u89e3\u91ca\uff08JIVE\uff09\uff0c\u63a2\u7d22\u4e86\u4e09\u79cd\u53ef\u884c\u7684\u635f\u5931\u51fd\u6570\u3002", "result": "DeepJIVE\u6210\u529f\u63ed\u793a\u4e86\u591a\u6a21\u6001\u6570\u636e\u7684\u8054\u5408\u548c\u4e2a\u4f53\u53d8\u5316\uff0c\u5e76\u5728ADNI\u6570\u636e\u4e2d\u53d1\u73b0\u4e86\u751f\u7269\u5b66\u4e0a\u5408\u7406\u7684\u534f\u53d8\u6a21\u5f0f\u3002", "conclusion": "DeepJIVE\u662f\u591a\u6a21\u6001\u6570\u636e\u5206\u6790\u7684\u6709\u7528\u5de5\u5177\u3002"}}
{"id": "2507.19616", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19616", "abs": "https://arxiv.org/abs/2507.19616", "authors": ["Xuchen Wei", "Yangxin Wu", "Yaoyin Zhang", "Henglyu Liu", "Kehai Chen", "Xuefeng Bai", "Min Zhang"], "title": "HITSZ's End-To-End Speech Translation Systems Combining Sequence-to-Sequence Auto Speech Recognition Model and Indic Large Language Model for IWSLT 2025 in Indic Track", "comment": "7 pages, 1 figure, submitted to IWSLT 2025", "summary": "This paper presents HITSZ's submission for the IWSLT 2025 Indic track,\nfocusing on speech-to-text translation (ST) for English-to-Indic and\nIndic-to-English language pairs. To enhance translation quality in this\nlow-resource scenario, we propose an end-to-end system integrating the\npre-trained Whisper automated speech recognition (ASR) model with Krutrim, an\nIndic-specialized large language model (LLM). Experimental results demonstrate\nthat our end-to-end system achieved average BLEU scores of $28.88$ for\nEnglish-to-Indic directions and $27.86$ for Indic-to-English directions.\nFurthermore, we investigated the Chain-of-Thought (CoT) method. While this\nmethod showed potential for significant translation quality improvements on\nsuccessfully parsed outputs (e.g. a $13.84$ BLEU increase for\nTamil-to-English), we observed challenges in ensuring the model consistently\nadheres to the required CoT output format.", "AI": {"tldr": "HITSZ\u7684IWSLT 2025 Indic\u8d5b\u9053\u63d0\u4ea4\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u7ed3\u5408Whisper ASR\u548cKrutrim LLM\uff0c\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u5bf9\u7684\u8bed\u97f3\u5230\u6587\u672c\u7ffb\u8bd1\u8d28\u91cf\u3002", "motivation": "\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u63d0\u5347\u82f1\u8bed\u4e0e\u5370\u5ea6\u8bed\u8a00\u4e4b\u95f4\u7684\u8bed\u97f3\u5230\u6587\u672c\u7ffb\u8bd1\u8d28\u91cf\u3002", "method": "\u96c6\u6210\u9884\u8bad\u7ec3\u7684Whisper ASR\u6a21\u578b\u548c\u5370\u5ea6\u8bed\u8a00\u4e13\u7528LLM Krutrim\uff0c\u5f62\u6210\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u5e76\u63a2\u7d22Chain-of-Thought\u65b9\u6cd5\u3002", "result": "\u5e73\u5747BLEU\u5206\u6570\u4e3a\u82f1\u8bed\u5230\u5370\u5ea6\u8bed\u8a0028.88\uff0c\u5370\u5ea6\u8bed\u8a00\u5230\u82f1\u8bed27.86\uff1bCoT\u65b9\u6cd5\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\uff08\u5982\u6cf0\u7c73\u5c14\u8bed\u5230\u82f1\u8bedBLEU\u589e\u52a013.84\uff09\u3002", "conclusion": "\u7aef\u5230\u7aef\u7cfb\u7edf\u6709\u6548\uff0c\u4f46CoT\u65b9\u6cd5\u7684\u8f93\u51fa\u683c\u5f0f\u4e00\u81f4\u6027\u4ecd\u9700\u6539\u8fdb\u3002"}}
{"id": "2507.19519", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19519", "abs": "https://arxiv.org/abs/2507.19519", "authors": ["J. Poole", "P. Gardner", "A. J. Hughes", "N. Dervilis", "R. S. Mills", "T. A. Dardeno", "K. Worden"], "title": "Physics-informed transfer learning for SHM via feature selection", "comment": null, "summary": "Data used for training structural health monitoring (SHM) systems are\nexpensive and often impractical to obtain, particularly labelled data.\nPopulation-based SHM presents a potential solution to this issue by considering\nthe available data across a population of structures. However, differences\nbetween structures will mean the training and testing distributions will\ndiffer; thus, conventional machine learning methods cannot be expected to\ngeneralise between structures. To address this issue, transfer learning (TL),\ncan be used to leverage information across related domains. An important\nconsideration is that the lack of labels in the target domain limits data-based\nmetrics to quantifying the discrepancy between the marginal distributions.\nThus, a prerequisite for the application of typical unsupervised TL methods is\nto identify suitable source structures (domains), and a set of features, for\nwhich the conditional distributions are related to the target structure.\nGenerally, the selection of domains and features is reliant on domain\nexpertise; however, for complex mechanisms, such as the influence of damage on\nthe dynamic response of a structure, this task is not trivial. In this paper,\nknowledge of physics is leveraged to select more similar features, the modal\nassurance criterion (MAC) is used to quantify the correspondence between the\nmodes of healthy structures. The MAC is shown to have high correspondence with\na supervised metric that measures joint-distribution similarity, which is the\nprimary indicator of whether a classifier will generalise between domains. The\nMAC is proposed as a measure for selecting a set of features that behave\nconsistently across domains when subjected to damage, i.e. features with\ninvariance in the conditional distributions. This approach is demonstrated on\nnumerical and experimental case studies to verify its effectiveness in various\napplications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u77e5\u8bc6\u7684\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\uff0c\u5229\u7528\u6a21\u6001\u4fdd\u8bc1\u51c6\u5219\uff08MAC\uff09\u6765\u91cf\u5316\u5065\u5eb7\u7ed3\u6784\u6a21\u6001\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u4ee5\u89e3\u51b3\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\uff08SHM\uff09\u4e2d\u6570\u636e\u7a00\u7f3a\u548c\u5206\u5e03\u5dee\u5f02\u7684\u95ee\u9898\u3002", "motivation": "\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\uff08SHM\uff09\u7684\u8bad\u7ec3\u6570\u636e\u6602\u8d35\u4e14\u96be\u4ee5\u83b7\u53d6\uff0c\u5c24\u5176\u662f\u6807\u8bb0\u6570\u636e\u3002\u4e0d\u540c\u7ed3\u6784\u95f4\u7684\u5206\u5e03\u5dee\u5f02\u5bfc\u81f4\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u6cdb\u5316\u3002", "method": "\u5229\u7528\u8f6c\u79fb\u5b66\u4e60\uff08TL\uff09\u8de8\u57df\u5171\u4eab\u4fe1\u606f\uff0c\u63d0\u51fa\u57fa\u4e8e\u7269\u7406\u77e5\u8bc6\u7684\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\uff0c\u4f7f\u7528MAC\u91cf\u5316\u6a21\u6001\u76f8\u4f3c\u6027\uff0c\u9009\u62e9\u6761\u4ef6\u5206\u5e03\u4e00\u81f4\u7684\u8de8\u57df\u7279\u5f81\u3002", "result": "MAC\u4e0e\u76d1\u7763\u5ea6\u91cf\u9ad8\u5ea6\u4e00\u81f4\uff0c\u80fd\u6709\u6548\u9009\u62e9\u6761\u4ef6\u5206\u5e03\u4e0d\u53d8\u7684\u7279\u5f81\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u6570\u503c\u548c\u5b9e\u9a8c\u6848\u4f8b\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "MAC\u53ef\u4f5c\u4e3a\u9009\u62e9\u8de8\u57df\u4e00\u81f4\u6027\u7279\u5f81\u7684\u5ea6\u91cf\uff0c\u4e3aSHM\u4e2d\u7684\u8f6c\u79fb\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2507.19725", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19725", "abs": "https://arxiv.org/abs/2507.19725", "authors": ["Leonardo Villalobos-Arias", "Grant Forbes", "Jianxun Wang", "David L Roberts", "Arnav Jhala"], "title": "Minding Motivation: The Effect of Intrinsic Motivation on Agent Behaviors", "comment": "11 pages, 7 figures, 3 tables", "summary": "Games are challenging for Reinforcement Learning~(RL) agents due to their\nreward-sparsity, as rewards are only obtainable after long sequences of\ndeliberate actions. Intrinsic Motivation~(IM) methods -- which introduce\nexploration rewards -- are an effective solution to reward-sparsity. However,\nIM also causes an issue known as `reward hacking' where the agent optimizes for\nthe new reward at the expense of properly playing the game. The larger problem\nis that reward hacking itself is largely unknown; there is no answer to\nwhether, and to what extent, IM rewards change the behavior of RL agents. This\nstudy takes a first step by empirically evaluating the impact on behavior of\nthree IM techniques on the MiniGrid game-like environment. We compare these IM\nmodels with Generalized Reward Matching~(GRM), a method that can be used with\nany intrinsic reward function to guarantee optimality. Our results suggest that\nIM causes noticeable change by increasing the initial rewards, but also\naltering the way the agent plays; and that GRM mitigated reward hacking in some\nscenarios.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5185\u5728\u52a8\u673a\uff08IM\uff09\u65b9\u6cd5\u5728\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e2d\u5bf9\u6e38\u620f\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u53d1\u73b0IM\u4f1a\u5bfc\u81f4\u5956\u52b1\u9ed1\u5ba2\u884c\u4e3a\uff0c\u5e76\u63d0\u51fa\u5e7f\u4e49\u5956\u52b1\u5339\u914d\uff08GRM\uff09\u65b9\u6cd5\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u6e38\u620f\u4e2d\u7684\u5956\u52b1\u7a00\u758f\u6027\u4f7f\u5f97RL\u4ee3\u7406\u96be\u4ee5\u5b66\u4e60\uff0cIM\u65b9\u6cd5\u867d\u80fd\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u53ef\u80fd\u5bfc\u81f4\u5956\u52b1\u9ed1\u5ba2\u884c\u4e3a\uff0c\u76ee\u524d\u5176\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u5728MiniGrid\u73af\u5883\u4e2d\u8bc4\u4f30\u4e86\u4e09\u79cdIM\u6280\u672f\u7684\u884c\u4e3a\u5f71\u54cd\uff0c\u5e76\u4e0eGRM\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "IM\u663e\u8457\u6539\u53d8\u4e86\u4ee3\u7406\u7684\u884c\u4e3a\uff0c\u589e\u52a0\u4e86\u521d\u59cb\u5956\u52b1\u4f46\u4e5f\u5bfc\u81f4\u5956\u52b1\u9ed1\u5ba2\u884c\u4e3a\uff0cGRM\u5728\u90e8\u5206\u573a\u666f\u4e2d\u7f13\u89e3\u4e86\u8fd9\u4e00\u95ee\u9898\u3002", "conclusion": "IM\u786e\u5b9e\u4f1a\u6539\u53d8RL\u4ee3\u7406\u7684\u884c\u4e3a\uff0cGRM\u662f\u4e00\u79cd\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2507.19691", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19691", "abs": "https://arxiv.org/abs/2507.19691", "authors": ["Haichuan Li", "Tomi Westerlund"], "title": "Co-Win: Joint Object Detection and Instance Segmentation in LiDAR Point Clouds via Collaborative Window Processing", "comment": null, "summary": "Accurate perception and scene understanding in complex urban environments is\na critical challenge for ensuring safe and efficient autonomous navigation. In\nthis paper, we present Co-Win, a novel bird's eye view (BEV) perception\nframework that integrates point cloud encoding with efficient parallel\nwindow-based feature extraction to address the multi-modality inherent in\nenvironmental understanding. Our method employs a hierarchical architecture\ncomprising a specialized encoder, a window-based backbone, and a query-based\ndecoder head to effectively capture diverse spatial features and object\nrelationships. Unlike prior approaches that treat perception as a simple\nregression task, our framework incorporates a variational approach with\nmask-based instance segmentation, enabling fine-grained scene decomposition and\nunderstanding. The Co-Win architecture processes point cloud data through\nprogressive feature extraction stages, ensuring that predicted masks are both\ndata-consistent and contextually relevant. Furthermore, our method produces\ninterpretable and diverse instance predictions, enabling enhanced downstream\ndecision-making and planning in autonomous driving systems.", "AI": {"tldr": "Co-Win\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u9e1f\u77b0\u56fe\uff08BEV\uff09\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u70b9\u4e91\u7f16\u7801\u548c\u5e76\u884c\u7a97\u53e3\u7279\u5f81\u63d0\u53d6\u89e3\u51b3\u590d\u6742\u73af\u5883\u7684\u591a\u6a21\u6001\u95ee\u9898\u3002", "motivation": "\u5728\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u5b9e\u73b0\u51c6\u786e\u7684\u611f\u77e5\u548c\u573a\u666f\u7406\u89e3\u662f\u786e\u4fdd\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u9ad8\u6548\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u91c7\u7528\u5206\u5c42\u67b6\u6784\uff0c\u5305\u62ec\u4e13\u7528\u7f16\u7801\u5668\u3001\u7a97\u53e3\u5f0f\u4e3b\u5e72\u548c\u67e5\u8be2\u5f0f\u89e3\u7801\u5668\uff0c\u7ed3\u5408\u53d8\u5206\u65b9\u6cd5\u548c\u63a9\u7801\u5b9e\u4f8b\u5206\u5272\u3002", "result": "\u6846\u67b6\u80fd\u591f\u751f\u6210\u6570\u636e\u4e00\u81f4\u4e14\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u9884\u6d4b\u63a9\u7801\uff0c\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u591a\u6837\u5316\u5b9e\u4f8b\u9884\u6d4b\u3002", "conclusion": "Co-Win\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u4e0b\u6e38\u51b3\u7b56\u548c\u89c4\u5212\u80fd\u529b\u3002"}}
{"id": "2507.19634", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.19634", "abs": "https://arxiv.org/abs/2507.19634", "authors": ["Sara Papi", "Maike Z\u00fcfle", "Marco Gaido", "Beatrice Savoldi", "Danni Liu", "Ioannis Douros", "Luisa Bentivogli", "Jan Niehues"], "title": "MCIF: Multimodal Crosslingual Instruction-Following Benchmark from Scientific Talks", "comment": "Work in progress", "summary": "Recent advances in large language models have catalyzed the development of\nmultimodal LLMs (MLLMs) that integrate text, speech, and vision within unified\nframeworks. As MLLMs evolve from narrow, monolingual, task-specific systems to\ngeneral-purpose instruction-following models, a key frontier lies in evaluating\ntheir multilingual and multimodal capabilities over both long and short\ncontexts. However, existing benchmarks fall short in evaluating these\ndimensions jointly: they are often limited to English, mostly focus on one\nsingle modality at a time, rely on short-form contexts, or lack human\nannotations -- hindering comprehensive assessment of model performance across\nlanguages, modalities, and task complexity. To address these gaps, we introduce\nMCIF (Multimodal Crosslingual Instruction Following), the first multilingual\nhuman-annotated benchmark based on scientific talks that is designed to\nevaluate instruction-following in crosslingual, multimodal settings over both\nshort- and long-form inputs. MCIF spans three core modalities -- speech,\nvision, and text -- and four diverse languages (English, German, Italian, and\nChinese), enabling a comprehensive evaluation of MLLMs' abilities to interpret\ninstructions across languages and combine them with multimodal contextual\ninformation. MCIF is released under a CC-BY 4.0 license to encourage open\nresearch and progress in MLLMs development.", "AI": {"tldr": "MCIF\u662f\u4e00\u4e2a\u591a\u8bed\u8a00\u3001\u591a\u6a21\u6001\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u548c\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u8bc4\u4f30\u591a\u8bed\u8a00\u548c\u591a\u6a21\u6001\u80fd\u529b\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u5982\u5c40\u9650\u4e8e\u82f1\u8bed\u3001\u5355\u4e00\u6a21\u6001\u6216\u77ed\u6587\u672c\uff0c\u7f3a\u4e4f\u5168\u9762\u6027\u3002", "method": "\u5f15\u5165MCIF\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u8bed\u97f3\u3001\u89c6\u89c9\u548c\u6587\u672c\u4e09\u79cd\u6a21\u6001\u53ca\u56db\u79cd\u8bed\u8a00\uff0c\u57fa\u4e8e\u79d1\u5b66\u8bb2\u5ea7\u6570\u636e\uff0c\u652f\u6301\u957f\u77ed\u671f\u4e0a\u4e0b\u6587\u8bc4\u4f30\u3002", "result": "MCIF\u4e3a\u591a\u6a21\u6001\u8de8\u8bed\u8a00\u6307\u4ee4\u8ddf\u968f\u63d0\u4f9b\u4e86\u9996\u4e2a\u5168\u9762\u8bc4\u4f30\u5de5\u5177\uff0c\u652f\u6301\u5f00\u653e\u7814\u7a76\u3002", "conclusion": "MCIF\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u7a7a\u767d\uff0c\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.19520", "categories": ["cs.LG", "astro-ph.EP", "astro-ph.IM", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19520", "abs": "https://arxiv.org/abs/2507.19520", "authors": ["Ethan Lo", "Dan C. Lo"], "title": "Exoplanet Detection Using Machine Learning Models Trained on Synthetic Light Curves", "comment": null, "summary": "With manual searching processes, the rate at which scientists and astronomers\ndiscover exoplanets is slow because of inefficiencies that require an extensive\ntime of laborious inspections. In fact, as of now there have been about only\n5,000 confirmed exoplanets since the late 1900s. Recently, machine learning\n(ML) has proven to be extremely valuable and efficient in various fields,\ncapable of processing massive amounts of data in addition to increasing its\naccuracy by learning. Though ML models for discovering exoplanets owned by\nlarge corporations (e.g. NASA) exist already, they largely depend on complex\nalgorithms and supercomputers. In an effort to reduce such complexities, in\nthis paper, we report the results and potential benefits of various, well-known\nML models in the discovery and validation of extrasolar planets. The ML models\nthat are examined in this study include logistic regression, k-nearest\nneighbors, and random forest. The dataset on which the models train and predict\nis acquired from NASA's Kepler space telescope. The initial results show\npromising scores for each model. However, potential biases and dataset\nimbalances necessitate the use of data augmentation techniques to further\nensure fairer predictions and improved generalization. This study concludes\nthat, in the context of searching for exoplanets, data augmentation techniques\nsignificantly improve the recall and precision, while the accuracy varies for\neach model.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u5982\u903b\u8f91\u56de\u5f52\u3001K\u8fd1\u90bb\u548c\u968f\u673a\u68ee\u6797\uff09\u5728\u53d1\u73b0\u548c\u9a8c\u8bc1\u7cfb\u5916\u884c\u661f\u4e2d\u7684\u5e94\u7528\uff0c\u4f7f\u7528NASA\u5f00\u666e\u52d2\u592a\u7a7a\u671b\u8fdc\u955c\u7684\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u6280\u672f\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u53ec\u56de\u7387\u548c\u7cbe\u786e\u5ea6\u3002", "motivation": "\u4f20\u7edf\u624b\u52a8\u641c\u7d22\u7cfb\u5916\u884c\u661f\u6548\u7387\u4f4e\u4e0b\uff0c\u673a\u5668\u5b66\u4e60\u80fd\u9ad8\u6548\u5904\u7406\u5927\u91cf\u6570\u636e\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u4f9d\u8d56\u590d\u6742\u7b97\u6cd5\u548c\u8d85\u7ea7\u8ba1\u7b97\u673a\u3002\u672c\u7814\u7a76\u65e8\u5728\u7b80\u5316\u590d\u6742\u6027\u5e76\u9a8c\u8bc1\u591a\u79cdML\u6a21\u578b\u7684\u6548\u679c\u3002", "method": "\u4f7f\u7528\u903b\u8f91\u56de\u5f52\u3001K\u8fd1\u90bb\u548c\u968f\u673a\u68ee\u6797\u7b49ML\u6a21\u578b\uff0c\u57fa\u4e8eNASA\u5f00\u666e\u52d2\u592a\u7a7a\u671b\u8fdc\u955c\u7684\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u548c\u9884\u6d4b\uff0c\u5e76\u91c7\u7528\u6570\u636e\u589e\u5f3a\u6280\u672f\u4f18\u5316\u7ed3\u679c\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\u5404\u6a21\u578b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5b58\u5728\u6f5c\u5728\u504f\u5dee\u548c\u6570\u636e\u96c6\u4e0d\u5e73\u8861\u95ee\u9898\u3002\u6570\u636e\u589e\u5f3a\u663e\u8457\u63d0\u9ad8\u4e86\u53ec\u56de\u7387\u548c\u7cbe\u786e\u5ea6\uff0c\u4f46\u51c6\u786e\u7387\u56e0\u6a21\u578b\u800c\u5f02\u3002", "conclusion": "\u5728\u7cfb\u5916\u884c\u661f\u641c\u7d22\u4e2d\uff0c\u6570\u636e\u589e\u5f3a\u6280\u672f\u80fd\u663e\u8457\u63d0\u5347ML\u6a21\u578b\u7684\u53ec\u56de\u7387\u548c\u7cbe\u786e\u5ea6\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u89e3\u51b3\u51c6\u786e\u7387\u5dee\u5f02\u95ee\u9898\u3002"}}
{"id": "2507.19726", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19726", "abs": "https://arxiv.org/abs/2507.19726", "authors": ["Yuzhang Xie", "Xu Han", "Ran Xu", "Xiao Hu", "Jiaying Lu", "Carl Yang"], "title": "HypKG: Hypergraph-based Knowledge Graph Contextualization for Precision Healthcare", "comment": "Extended version of paper accepted at the 24th International Semantic\n  Web Conference (ISWC 2025), Main Tracks, Research Track, Oral", "summary": "Knowledge graphs (KGs) are important products of the semantic web, which are\nwidely used in various application domains. Healthcare is one of such domains\nwhere KGs are intensively used, due to the high requirement for knowledge\naccuracy and interconnected nature of healthcare data. However, KGs storing\ngeneral factual information often lack the ability to account for important\ncontexts of the knowledge such as the status of specific patients, which are\ncrucial in precision healthcare. Meanwhile, electronic health records (EHRs)\nprovide rich personal data, including various diagnoses and medications, which\nprovide natural contexts for general KGs. In this paper, we propose HypKG, a\nframework that integrates patient information from EHRs into KGs to generate\ncontextualized knowledge representations for accurate healthcare predictions.\nUsing advanced entity-linking techniques, we connect relevant knowledge from\ngeneral KGs with patient information from EHRs, and then utilize a hypergraph\nmodel to \"contextualize\" the knowledge with the patient information. Finally,\nwe employ hypergraph transformers guided by downstream prediction tasks to\njointly learn proper contextualized representations for both KGs and patients,\nfully leveraging existing knowledge in KGs and patient contexts in EHRs. In\nexperiments using a large biomedical KG and two real-world EHR datasets, HypKG\ndemonstrates significant improvements in healthcare prediction tasks across\nmultiple evaluation metrics. Additionally, by integrating external contexts,\nHypKG can learn to adjust the representations of entities and relations in KG,\npotentially improving the quality and real-world utility of knowledge.", "AI": {"tldr": "HypKG\u6846\u67b6\u901a\u8fc7\u6574\u5408\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHRs\uff09\u5230\u77e5\u8bc6\u56fe\u8c31\uff08KGs\uff09\u4e2d\uff0c\u751f\u6210\u4e0a\u4e0b\u6587\u77e5\u8bc6\u8868\u793a\uff0c\u63d0\u5347\u533b\u7597\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u901a\u7528\u77e5\u8bc6\u56fe\u8c31\u7f3a\u4e4f\u60a3\u8005\u7279\u5b9a\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u800c\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u4e2a\u4eba\u6570\u636e\uff0c\u4e24\u8005\u7ed3\u5408\u53ef\u63d0\u5347\u7cbe\u51c6\u533b\u7597\u7684\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u5b9e\u4f53\u94fe\u63a5\u6280\u672f\u8fde\u63a5\u901a\u7528KGs\u548cEHRs\uff0c\u901a\u8fc7\u8d85\u56fe\u6a21\u578b\u5c06\u77e5\u8bc6\u201c\u4e0a\u4e0b\u6587\u5316\u201d\uff0c\u5e76\u5229\u7528\u8d85\u56fe\u53d8\u6362\u5668\u5b66\u4e60\u4e0a\u4e0b\u6587\u5316\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHypKG\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u533b\u7597\u9884\u6d4b\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5e76\u4f18\u5316\u4e86\u77e5\u8bc6\u56fe\u8c31\u7684\u5b9e\u4f53\u548c\u5173\u7cfb\u8868\u793a\u3002", "conclusion": "HypKG\u901a\u8fc7\u6574\u5408\u5916\u90e8\u4e0a\u4e0b\u6587\uff0c\u63d0\u5347\u4e86\u77e5\u8bc6\u56fe\u8c31\u7684\u5b9e\u7528\u6027\u548c\u8d28\u91cf\uff0c\u4e3a\u7cbe\u51c6\u533b\u7597\u63d0\u4f9b\u4e86\u6709\u6548\u652f\u6301\u3002"}}
{"id": "2507.19705", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19705", "abs": "https://arxiv.org/abs/2507.19705", "authors": ["Asmae Lamsaf", "Lucia Cascone", "Hugo Proen\u00e7a", "Jo\u00e3o Neves"], "title": "Bias Analysis for Synthetic Face Detection: A Case Study of the Impact of Facial Attribute", "comment": null, "summary": "Bias analysis for synthetic face detection is bound to become a critical\ntopic in the coming years. Although many detection models have been developed\nand several datasets have been released to reliably identify synthetic content,\none crucial aspect has been largely overlooked: these models and training\ndatasets can be biased, leading to failures in detection for certain\ndemographic groups and raising significant social, legal, and ethical issues.\nIn this work, we introduce an evaluation framework to contribute to the\nanalysis of bias of synthetic face detectors with respect to several facial\nattributes. This framework exploits synthetic data generation, with evenly\ndistributed attribute labels, for mitigating any skew in the data that could\notherwise influence the outcomes of bias analysis. We build on the proposed\nframework to provide an extensive case study of the bias level of five\nstate-of-the-art detectors in synthetic datasets with 25 controlled facial\nattributes. While the results confirm that, in general, synthetic face\ndetectors are biased towards the presence/absence of specific facial\nattributes, our study also sheds light on the origins of the observed bias\nthrough the analysis of the correlations with the balancing of facial\nattributes in the training sets of the detectors, and the analysis of detectors\nactivation maps in image pairs with controlled attribute modifications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u5408\u6210\u4eba\u8138\u68c0\u6d4b\u5668\u7684\u504f\u89c1\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u5c5e\u6027\u5e73\u8861\u6765\u51cf\u5c11\u6570\u636e\u504f\u5dee\u3002\u7814\u7a76\u8fd8\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u63ed\u793a\u4e86\u4e94\u79cd\u5148\u8fdb\u68c0\u6d4b\u5668\u7684\u504f\u89c1\u6765\u6e90\u3002", "motivation": "\u5408\u6210\u4eba\u8138\u68c0\u6d4b\u5668\u7684\u504f\u89c1\u53ef\u80fd\u5bfc\u81f4\u5bf9\u67d0\u4e9b\u4eba\u53e3\u7fa4\u4f53\u7684\u68c0\u6d4b\u5931\u8d25\uff0c\u5f15\u53d1\u793e\u4f1a\u3001\u6cd5\u5f8b\u548c\u4f26\u7406\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u5206\u6790\u3002", "method": "\u5229\u7528\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u5747\u5300\u5206\u5e03\u7684\u5c5e\u6027\u6807\u7b7e\u6784\u5efa\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5206\u6790\u4e94\u79cd\u68c0\u6d4b\u5668\u7684\u504f\u89c1\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5408\u6210\u4eba\u8138\u68c0\u6d4b\u5668\u666e\u904d\u5b58\u5728\u5bf9\u7279\u5b9a\u9762\u90e8\u5c5e\u6027\u7684\u504f\u89c1\uff0c\u5e76\u63ed\u793a\u4e86\u504f\u89c1\u4e0e\u8bad\u7ec3\u6570\u636e\u5c5e\u6027\u5e73\u8861\u53ca\u68c0\u6d4b\u5668\u6fc0\u6d3b\u56fe\u7684\u76f8\u5173\u6027\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u504f\u89c1\u5206\u6790\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u51cf\u5c11\u504f\u89c1\u7684\u6846\u67b6\u548c\u65b9\u6cd5\u3002"}}
{"id": "2507.19666", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19666", "abs": "https://arxiv.org/abs/2507.19666", "authors": ["Andrei Vlad Man", "R\u0103zvan-Alexandru Sm\u0103du", "Cristian-George Craciun", "Dumitru-Clementin Cercel", "Florin Pop", "Mihaela-Claudia Cercel"], "title": "RoD-TAL: A Benchmark for Answering Questions in Romanian Driving License Exams", "comment": "49 pages, 52 figures", "summary": "The intersection of AI and legal systems presents a growing need for tools\nthat support legal education, particularly in under-resourced languages such as\nRomanian. In this work, we aim to evaluate the capabilities of Large Language\nModels (LLMs) and Vision-Language Models (VLMs) in understanding and reasoning\nabout Romanian driving law through textual and visual question-answering tasks.\nTo facilitate this, we introduce RoD-TAL, a novel multimodal dataset comprising\nRomanian driving test questions, text-based and image-based, alongside\nannotated legal references and human explanations. We implement and assess\nretrieval-augmented generation (RAG) pipelines, dense retrievers, and\nreasoning-optimized models across tasks including Information Retrieval (IR),\nQuestion Answering (QA), Visual IR, and Visual QA. Our experiments demonstrate\nthat domain-specific fine-tuning significantly enhances retrieval performance.\nAt the same time, chain-of-thought prompting and specialized reasoning models\nimprove QA accuracy, surpassing the minimum grades required to pass driving\nexams. However, visual reasoning remains challenging, highlighting the\npotential and the limitations of applying LLMs and VLMs to legal education.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86AI\u5728\u6cd5\u5f8b\u6559\u80b2\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u9488\u5bf9\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u7b49\u8d44\u6e90\u4e0d\u8db3\u7684\u8bed\u8a00\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u96c6RoD-TAL\u8bc4\u4f30\u4e86LLMs\u548cVLMs\u5728\u7f57\u9a6c\u5c3c\u4e9a\u9a7e\u9a76\u6cd5\u5f8b\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u8d44\u6e90\u4e0d\u8db3\u8bed\u8a00\uff08\u5982\u7f57\u9a6c\u5c3c\u4e9a\u8bed\uff09\u5728\u6cd5\u5f8b\u6559\u80b2\u4e2d\u7684AI\u652f\u6301\u9700\u6c42\uff0c\u8bc4\u4f30LLMs\u548cVLMs\u5728\u6cd5\u5f8b\u63a8\u7406\u4e2d\u7684\u80fd\u529b\u3002", "method": "\u5f15\u5165RoD-TAL\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u7ed3\u5408RAG\u7ba1\u9053\u3001\u5bc6\u96c6\u68c0\u7d22\u5668\u548c\u63a8\u7406\u4f18\u5316\u6a21\u578b\uff0c\u8bc4\u4f30\u6587\u672c\u548c\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u3002", "result": "\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u663e\u8457\u63d0\u5347\u68c0\u7d22\u6027\u80fd\uff0c\u63a8\u7406\u4f18\u5316\u6a21\u578b\u5728\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u89c6\u89c9\u63a8\u7406\u4ecd\u6709\u6311\u6218\u3002", "conclusion": "LLMs\u548cVLMs\u5728\u6cd5\u5f8b\u6559\u80b2\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u89c6\u89c9\u63a8\u7406\u4ecd\u9700\u6539\u8fdb\u3002"}}
{"id": "2507.19522", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19522", "abs": "https://arxiv.org/abs/2507.19522", "authors": ["Aarush Gupta", "Kendric Hsu", "Syna Mathod"], "title": "Applications and Manipulations of Physics-Informed Neural Networks in Solving Differential Equations", "comment": null, "summary": "Mathematical models in neural networks are powerful tools for solving complex\ndifferential equations and optimizing their parameters; that is, solving the\nforward and inverse problems, respectively. A forward problem predicts the\noutput of a network for a given input by optimizing weights and biases. An\ninverse problem finds equation parameters or coefficients that effectively\nmodel the data. A Physics-Informed Neural Network (PINN) can solve both\nproblems. PINNs inject prior analytical information about the data into the\ncost function to improve model performance outside the training set boundaries.\nThis also allows PINNs to efficiently solve problems with sparse data without\noverfitting by extrapolating the model to fit larger trends in the data. The\nprior information we implement is in the form of differential equations.\nResiduals are the differences between the left-hand and right-hand sides of\ncorresponding differential equations; PINNs minimize these residuals to\neffectively solve the differential equation and take advantage of prior\nknowledge. In this way, the solution and parameters are embedded into the loss\nfunction and optimized, allowing both the weights of the neural network and the\nmodel parameters to be found simultaneously, solving both the forward and\ninverse problems in the process. In this paper, we will create PINNs with\nresiduals of varying complexity, beginning with linear and quadratic models and\nthen expanding to fit models for the heat equation and other complex\ndifferential equations. We will mainly use Python as the computing language,\nusing the PyTorch library to aid us in our research.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u5982\u4f55\u901a\u8fc7\u7ed3\u5408\u5fae\u5206\u65b9\u7a0b\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u540c\u65f6\u89e3\u51b3\u6b63\u5411\u548c\u9006\u5411\u95ee\u9898\uff0c\u5e76\u5728\u7a00\u758f\u6570\u636e\u4e0b\u907f\u514d\u8fc7\u62df\u5408\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u5fae\u5206\u65b9\u7a0b\u7684\u6b63\u5411\u548c\u9006\u5411\u95ee\u9898\uff0c\u540c\u65f6\u5229\u7528\u5148\u9a8c\u77e5\u8bc6\u63d0\u5347\u6a21\u578b\u5728\u8bad\u7ec3\u96c6\u5916\u7684\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u5c06\u5fae\u5206\u65b9\u7a0b\u7684\u6b8b\u5dee\u5d4c\u5165\u635f\u5931\u51fd\u6570\uff0c\u4f18\u5316\u795e\u7ecf\u7f51\u7edc\u6743\u91cd\u548c\u6a21\u578b\u53c2\u6570\uff0c\u4f7f\u7528PyTorch\u5b9e\u73b0\u3002", "result": "PINN\u80fd\u591f\u6709\u6548\u89e3\u51b3\u7ebf\u6027\u3001\u4e8c\u6b21\u53ca\u70ed\u65b9\u7a0b\u7b49\u590d\u6742\u5fae\u5206\u65b9\u7a0b\u95ee\u9898\u3002", "conclusion": "PINN\u662f\u4e00\u79cd\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u80fd\u591f\u540c\u65f6\u5904\u7406\u6b63\u5411\u548c\u9006\u5411\u95ee\u9898\uff0c\u5e76\u5229\u7528\u5148\u9a8c\u77e5\u8bc6\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.19733", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2507.19733", "abs": "https://arxiv.org/abs/2507.19733", "authors": ["Alec Scully", "Cameron Stockton", "Forrest Hare"], "title": "Integrating Activity Predictions in Knowledge Graphs", "comment": "7 pages. 18 figures. Semantic Technology for Intelligence, Defense,\n  and Security (STIDS 2024)", "summary": "We argue that ontology-structured knowledge graphs can play a crucial role in\ngenerating predictions about future events. By leveraging the semantic\nframework provided by Basic Formal Ontology (BFO) and Common Core Ontologies\n(CCO), we demonstrate how data such as the movements of a fishing vessel can be\norganized in and retrieved from a knowledge graph. These query results are then\nused to create Markov chain models, allowing us to predict future states based\non the vessel's history. To fully support this process, we introduce the term\n`spatiotemporal instant' to complete the necessary structural semantics.\nAdditionally, we critique the prevailing ontological model of probability,\nwhich conflates probability with likelihood and relies on the problematic\nconcept of modal measurements: measurements of future entities. We propose an\nalternative view, where probabilities are treated as being about process\nprofiles, which better captures the dynamics of real world phenomena. Finally,\nwe demonstrate how our Markov chain based probability calculations can be\nseamlessly integrated back into the knowledge graph, enabling further analysis\nand decision-making. Keywords: predictive analytics, ontology, Markov chains,\nprobability, Basic Formal Ontology (BFO), knowledge graphs, SPARQL.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5229\u7528\u672c\u4f53\u7ed3\u6784\u77e5\u8bc6\u56fe\u8c31\u9884\u6d4b\u672a\u6765\u4e8b\u4ef6\uff0c\u901a\u8fc7BFO\u548cCCO\u7ec4\u7ec7\u6570\u636e\u5e76\u751f\u6210\u9a6c\u5c14\u53ef\u592b\u94fe\u6a21\u578b\uff0c\u540c\u65f6\u6279\u5224\u73b0\u6709\u6982\u7387\u6a21\u578b\u5e76\u63d0\u51fa\u66ff\u4ee3\u65b9\u6848\u3002", "motivation": "\u63a2\u8ba8\u672c\u4f53\u7ed3\u6784\u77e5\u8bc6\u56fe\u8c31\u5728\u9884\u6d4b\u672a\u6765\u4e8b\u4ef6\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u6539\u8fdb\u73b0\u6709\u6982\u7387\u6a21\u578b\u3002", "method": "\u5229\u7528BFO\u548cCCO\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\uff0c\u7ec4\u7ec7\u6570\u636e\u5e76\u751f\u6210\u9a6c\u5c14\u53ef\u592b\u94fe\u6a21\u578b\uff0c\u5f15\u5165\u201c\u65f6\u7a7a\u5b9e\u4f8b\u201d\u6982\u5ff5\u3002", "result": "\u6210\u529f\u9884\u6d4b\u672a\u6765\u72b6\u6001\uff0c\u5e76\u5c06\u6982\u7387\u8ba1\u7b97\u65e0\u7f1d\u96c6\u6210\u56de\u77e5\u8bc6\u56fe\u8c31\u3002", "conclusion": "\u672c\u4f53\u7ed3\u6784\u77e5\u8bc6\u56fe\u8c31\u7ed3\u5408\u9a6c\u5c14\u53ef\u592b\u94fe\u6a21\u578b\u80fd\u6709\u6548\u652f\u6301\u9884\u6d4b\u5206\u6790\u548c\u51b3\u7b56\u3002"}}
{"id": "2507.19730", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19730", "abs": "https://arxiv.org/abs/2507.19730", "authors": ["Liyang Wang", "Shiqian Wu", "Shun Fang", "Qile Zhu", "Jiaxin Wu", "Sos Again"], "title": "Quaternion-Based Robust PCA for Efficient Moving Target Detection and Background Recovery in Color Videos", "comment": null, "summary": "Moving target detection is a challenging computer vision task aimed at\ngenerating accurate segmentation maps in diverse in-the-wild color videos\ncaptured by static cameras. If backgrounds and targets can be simultaneously\nextracted and recombined, such synthetic data can significantly enrich\nannotated in-the-wild datasets and enhance the generalization ability of deep\nmodels. Quaternion-based RPCA (QRPCA) is a promising unsupervised paradigm for\ncolor image processing. However, in color video processing, Quaternion Singular\nValue Decomposition (QSVD) incurs high computational costs, and rank-1\nquaternion matrix fails to yield rank-1 color channels. In this paper, we\nreduce the computational complexity of QSVD to o(1) by utilizing a quaternion\nRiemannian manifold. Furthermor, we propose the universal QRPCA (uQRPCA)\nframework, which achieves a balance in simultaneously segmenting targets and\nrecovering backgrounds from color videos. Moreover, we expand to uQRPCA+ by\nintroducing the Color Rank-1 Batch (CR1B) method to further process and obtain\nthe ideal low-rank background across color channels. Experiments demonstrate\nour uQRPCA+ achieves State Of The Art (SOTA) performance on moving target\ndetection and background recovery tasks compared to existing open-source\nmethods. Our implementation is publicly available on GitHub at\nhttps://github.com/Ruchtech/uQRPCA", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56db\u5143\u6570\u9ece\u66fc\u6d41\u5f62\u7684uQRPCA+\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u5904\u7406\u5f69\u8272\u89c6\u9891\u4e2d\u7684\u79fb\u52a8\u76ee\u6807\u68c0\u6d4b\u548c\u80cc\u666f\u6062\u590d\u4efb\u52a1\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5f69\u8272\u89c6\u9891\u5904\u7406\u4e2dQSVD\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u4f20\u7edfQRPCA\u5728\u989c\u8272\u901a\u9053\u4e2d\u65e0\u6cd5\u5b9e\u73b0rank-1\u5206\u89e3\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u56db\u5143\u6570\u9ece\u66fc\u6d41\u5f62\u964d\u4f4eQSVD\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u63d0\u51fauQRPCA\u6846\u67b6\u5e73\u8861\u76ee\u6807\u5206\u5272\u548c\u80cc\u666f\u6062\u590d\uff0c\u5e76\u901a\u8fc7CR1B\u65b9\u6cd5\u8fdb\u4e00\u6b65\u4f18\u5316\u80cc\u666f\u6062\u590d\u3002", "result": "uQRPCA+\u5728\u79fb\u52a8\u76ee\u6807\u68c0\u6d4b\u548c\u80cc\u666f\u6062\u590d\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\u3002", "conclusion": "uQRPCA+\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u5f69\u8272\u89c6\u9891\u5904\u7406\u65b9\u6cd5\uff0c\u5176\u5b9e\u73b0\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.19699", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19699", "abs": "https://arxiv.org/abs/2507.19699", "authors": ["Maitha Alshehhi", "Ahmed Sharshar", "Mohsen Guizani"], "title": "Towards Inclusive NLP: Assessing Compressed Multilingual Transformers across Diverse Language Benchmarks", "comment": "Published in the 3rd International Workshop on Generalizing from\n  Limited Resources in the Open World. Workshop at International Joint\n  Conference on Artificial Intelligence (IJCAI) 2025", "summary": "Although LLMs have attained significant success in high-resource languages,\ntheir capacity in low-resource linguistic environments like Kannada and Arabic\nis not yet fully understood. This work benchmarking the performance of\nmultilingual and monolingual Large Language Models (LLMs) across Arabic,\nEnglish, and Indic languages, with particular emphasis on the effects of model\ncompression strategies such as pruning and quantization. Findings shows\nsignificant performance differences driven by linguistic diversity and resource\navailability on SOTA LLMS as BLOOMZ, AceGPT, Jais, LLaMA-2, XGLM, and AraGPT2.\nWe find that multilingual versions of the model outperform their\nlanguage-specific counterparts across the board, indicating substantial\ncross-lingual transfer benefits. Quantization (4-bit and 8-bit) is effective in\nmaintaining model accuracy while promoting efficiency, but aggressive pruning\nsignificantly compromises performance, especially in bigger models. Our\nfindings pinpoint key strategies to construct scalable and fair multilingual\nNLP solutions and underscore the need for interventions to address\nhallucination and generalization errors in the low-resource setting.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u591a\u8bed\u8a00\u548c\u5355\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u963f\u62c9\u4f2f\u8bed\u3001\u82f1\u8bed\u548c\u5370\u5ea6\u8bed\u4e2d\u7684\u8868\u73b0\uff0c\u91cd\u70b9\u5173\u6ce8\u6a21\u578b\u538b\u7f29\u7b56\u7565\uff08\u5982\u526a\u679d\u548c\u91cf\u5316\uff09\u7684\u5f71\u54cd\u3002\u53d1\u73b0\u591a\u8bed\u8a00\u6a21\u578b\u666e\u904d\u4f18\u4e8e\u5355\u8bed\u8a00\u6a21\u578b\uff0c\u91cf\u5316\u80fd\u4fdd\u6301\u51c6\u786e\u6027\uff0c\u800c\u526a\u679d\u4f1a\u663e\u8457\u964d\u4f4e\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\uff08\u5982\u963f\u62c9\u4f2f\u8bed\u548c\u5370\u5ea6\u8bed\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u53ca\u6a21\u578b\u538b\u7f29\u7b56\u7565\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u5bf9\u591a\u79cdLLMs\uff08\u5982BLOOMZ\u3001AceGPT\u7b49\uff09\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5206\u6790\u91cf\u5316\uff084\u4f4d\u548c8\u4f4d\uff09\u548c\u526a\u679d\u7b56\u7565\u7684\u6548\u679c\u3002", "result": "\u591a\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u66f4\u4f18\uff0c\u91cf\u5316\u6709\u6548\u4f46\u526a\u679d\u635f\u5bb3\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u5927\u578b\u6a21\u578b\u4e2d\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6784\u5efa\u53ef\u6269\u5c55\u4e14\u516c\u5e73\u7684\u591a\u8bed\u8a00NLP\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u5173\u952e\u7b56\u7565\uff0c\u5e76\u6307\u51fa\u9700\u89e3\u51b3\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\u7684\u5e7b\u89c9\u548c\u6cdb\u5316\u9519\u8bef\u3002"}}
{"id": "2507.19523", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19523", "abs": "https://arxiv.org/abs/2507.19523", "authors": ["Xingyu Su", "Xiner Li", "Yuchao Lin", "Ziqian Xie", "Degui Zhi", "Shuiwang Ji"], "title": "Language Models for Controllable DNA Sequence Design", "comment": null, "summary": "We consider controllable DNA sequence design, where sequences are generated\nby conditioning on specific biological properties. While language models (LMs)\nsuch as GPT and BERT have achieved remarkable success in natural language\ngeneration, their application to DNA sequence generation remains largely\nunderexplored. In this work, we introduce ATGC-Gen, an Automated Transformer\nGenerator for Controllable Generation, which leverages cross-modal encoding to\nintegrate diverse biological signals. ATGC-Gen is instantiated with both\ndecoder-only and encoder-only transformer architectures, allowing flexible\ntraining and generation under either autoregressive or masked recovery\nobjectives. We evaluate ATGC-Gen on representative tasks including promoter and\nenhancer sequence design, and further introduce a new dataset based on ChIP-Seq\nexperiments for modeling protein binding specificity. Our experiments\ndemonstrate that ATGC-Gen can generate fluent, diverse, and biologically\nrelevant sequences aligned with the desired properties. Compared to prior\nmethods, our model achieves notable improvements in controllability and\nfunctional relevance, highlighting the potential of language models in\nadvancing programmable genomic design. The source code is released at\n(https://github.com/divelab/AIRS/blob/main/OpenBio/ATGC_Gen).", "AI": {"tldr": "ATGC-Gen\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684DNA\u5e8f\u5217\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u7f16\u7801\u6574\u5408\u751f\u7269\u4fe1\u53f7\uff0c\u652f\u6301\u81ea\u56de\u5f52\u6216\u63a9\u7801\u6062\u590d\u76ee\u6807\uff0c\u751f\u6210\u4e0e\u7279\u5b9a\u751f\u7269\u5c5e\u6027\u5bf9\u9f50\u7684\u5e8f\u5217\u3002", "motivation": "\u63a2\u7d22\u8bed\u8a00\u6a21\u578b\u5728DNA\u5e8f\u5217\u751f\u6210\u4e2d\u7684\u5e94\u7528\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u53ef\u63a7\u6027\u548c\u529f\u80fd\u76f8\u5173\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u89e3\u7801\u5668-\u7f16\u7801\u5668Transformer\u67b6\u6784\uff0c\u7ed3\u5408\u8de8\u6a21\u6001\u7f16\u7801\uff0c\u652f\u6301\u81ea\u56de\u5f52\u6216\u63a9\u7801\u6062\u590d\u76ee\u6807\u8bad\u7ec3\u3002", "result": "\u5728\u542f\u52a8\u5b50\u548c\u589e\u5f3a\u5b50\u5e8f\u5217\u8bbe\u8ba1\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u751f\u6210\u5e8f\u5217\u6d41\u7545\u3001\u591a\u6837\u4e14\u751f\u7269\u76f8\u5173\u3002", "conclusion": "ATGC-Gen\u5c55\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u53ef\u7f16\u7a0b\u57fa\u56e0\u7ec4\u8bbe\u8ba1\u4e2d\u7684\u6f5c\u529b\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.19749", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19749", "abs": "https://arxiv.org/abs/2507.19749", "authors": ["Lin Ren", "Guohui Xiao", "Guilin Qi", "Yishuai Geng", "Haohan Xue"], "title": "Can LLMs Solve ASP Problems? Insights from a Benchmarking Study (Extended Version)", "comment": "Accepted for publication at the 22nd International Conference on\n  Principles of Knowledge Representation and Reasoning (KR 2025). The code is\n  available at https://github.com/HomuraT/ASPBench", "summary": "Answer Set Programming (ASP) is a powerful paradigm for non-monotonic\nreasoning. Recently, large language models (LLMs) have demonstrated promising\ncapabilities in logical reasoning. Despite this potential, current evaluations\nof LLM capabilities in ASP are often limited. Existing works normally employ\noverly simplified ASP programs, do not support negation, disjunction, or\nmultiple answer sets. Furthermore, there is a lack of benchmarks that introduce\ntasks specifically designed for ASP solving. To bridge this gap, we introduce\nASPBench, a comprehensive ASP benchmark, including three ASP specific tasks:\nASP entailment, answer set verification, and answer set computation. Our\nextensive evaluations on ASPBench reveal that while 14 state-of-the-art LLMs,\nincluding \\emph{deepseek-r1}, \\emph{o4-mini}, and\n\\emph{gemini-2.5-flash-thinking}, perform relatively well on the first two\nsimpler tasks, they struggle with answer set computation, which is the core of\nASP solving. These findings offer insights into the current limitations of LLMs\nin ASP solving. This highlights the need for new approaches that integrate\nsymbolic reasoning capabilities more effectively. The code and dataset are\navailable at https://github.com/HomuraT/ASPBench.", "AI": {"tldr": "ASPBench\u662f\u4e00\u4e2a\u5168\u9762\u7684ASP\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728ASP\u6c42\u89e3\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u6838\u5fc3\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5728ASP\uff08\u7b54\u6848\u96c6\u7f16\u7a0b\uff09\u4e2d\u7684\u8bc4\u4f30\u8fc7\u4e8e\u7b80\u5316\uff0c\u7f3a\u4e4f\u652f\u6301\u590d\u6742\u903b\u8f91\u7ed3\u6784\u548c\u591a\u7b54\u6848\u96c6\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u5f15\u5165ASPBench\uff0c\u5305\u542b\u4e09\u4e2aASP\u7279\u5b9a\u4efb\u52a1\uff1aASP\u8574\u542b\u3001\u7b54\u6848\u96c6\u9a8c\u8bc1\u548c\u7b54\u6848\u96c6\u8ba1\u7b97\uff0c\u5e76\u5bf914\u79cd\u5148\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u524d\u4e24\u4e2a\u7b80\u5355\u4efb\u52a1\u4e0a\u8868\u73b0\u8f83\u597d\uff0c\u4f46\u5728\u6838\u5fc3\u7684\u7b54\u6848\u96c6\u8ba1\u7b97\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u9700\u8981\u66f4\u6709\u6548\u6574\u5408\u7b26\u53f7\u63a8\u7406\u80fd\u529b\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728ASP\u6c42\u89e3\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2507.19738", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19738", "abs": "https://arxiv.org/abs/2507.19738", "authors": ["Jinsu Yoo", "Sooyoung Jeon", "Zanming Huang", "Tai-Yu Pan", "Wei-Lun Chao"], "title": "Leveraging Sparse LiDAR for RAFT-Stereo: A Depth Pre-Fill Perspective", "comment": null, "summary": "We investigate LiDAR guidance within the RAFT-Stereo framework, aiming to\nimprove stereo matching accuracy by injecting precise LiDAR depth into the\ninitial disparity map. We find that the effectiveness of LiDAR guidance\ndrastically degrades when the LiDAR points become sparse (e.g., a few hundred\npoints per frame), and we offer a novel explanation from a signal processing\nperspective. This insight leads to a surprisingly simple solution that enables\nLiDAR-guided RAFT-Stereo to thrive: pre-filling the sparse initial disparity\nmap with interpolation. Interestingly, we find that pre-filling is also\neffective when injecting LiDAR depth into image features via early fusion, but\nfor a fundamentally different reason, necessitating a distinct pre-filling\napproach. By combining both solutions, the proposed Guided RAFT-Stereo\n(GRAFT-Stereo) significantly outperforms existing LiDAR-guided methods under\nsparse LiDAR conditions across various datasets. We hope this study inspires\nmore effective LiDAR-guided stereo methods.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728RAFT-Stereo\u6846\u67b6\u4e2d\u5f15\u5165LiDAR\u5f15\u5bfc\u4ee5\u63d0\u9ad8\u7acb\u4f53\u5339\u914d\u7cbe\u5ea6\uff0c\u53d1\u73b0\u7a00\u758fLiDAR\u70b9\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u63d2\u503c\u9884\u586b\u5145\u89e3\u51b3\u65b9\u6848\u3002\u7ed3\u5408\u4e24\u79cd\u9884\u586b\u5145\u65b9\u6cd5\uff0cGRAFT-Stereo\u5728\u7a00\u758fLiDAR\u6761\u4ef6\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u901a\u8fc7LiDAR\u6df1\u5ea6\u4fe1\u606f\u63d0\u5347\u7acb\u4f53\u5339\u914d\u7cbe\u5ea6\uff0c\u5c24\u5176\u662f\u5728LiDAR\u70b9\u7a00\u758f\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u5728\u521d\u59cb\u89c6\u5dee\u56fe\u4e2d\u901a\u8fc7\u63d2\u503c\u9884\u586b\u5145\u7a00\u758fLiDAR\u70b9\uff0c\u5e76\u7ed3\u5408\u65e9\u671f\u878d\u5408\u4e2d\u7684\u9884\u586b\u5145\u65b9\u6cd5\u3002", "result": "GRAFT-Stereo\u5728\u7a00\u758fLiDAR\u6761\u4ef6\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u4e3aLiDAR\u5f15\u5bfc\u7684\u7acb\u4f53\u5339\u914d\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5e76\u5c55\u793a\u4e86\u7b80\u5355\u63d2\u503c\u9884\u586b\u5145\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.19710", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19710", "abs": "https://arxiv.org/abs/2507.19710", "authors": ["Ronak Upasham", "Tathagata Dey", "Pushpak Bhattacharyya"], "title": "Ta-G-T: Subjectivity Capture in Table to Text Generation via RDF Graphs", "comment": null, "summary": "In Table-to-Text (T2T) generation, existing approaches predominantly focus on\nproviding objective descriptions of tabular data. However, generating text that\nincorporates subjectivity, where subjectivity refers to interpretations beyond\nraw numerical data, remains underexplored. To address this, we introduce a\nnovel pipeline that leverages intermediate representations to generate both\nobjective and subjective text from tables. Our three-stage pipeline consists\nof: 1) extraction of Resource Description Framework (RDF) triples, 2)\naggregation of text into coherent narratives, and 3) infusion of subjectivity\nto enrich the generated text. By incorporating RDFs, our approach enhances\nfactual accuracy while maintaining interpretability. Unlike large language\nmodels (LLMs) such as GPT-3.5, Mistral-7B, and Llama-2, our pipeline employs\nsmaller, fine-tuned T5 models while achieving comparable performance to GPT-3.5\nand outperforming Mistral-7B and Llama-2 in several metrics. We evaluate our\napproach through quantitative and qualitative analyses, demonstrating its\neffectiveness in balancing factual accuracy with subjective interpretation. To\nthe best of our knowledge, this is the first work to propose a structured\npipeline for T2T generation that integrates intermediate representations to\nenhance both factual correctness and subjectivity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e09\u9636\u6bb5\u6d41\u6c34\u7ebf\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u8868\u683c\u6570\u636e\u751f\u6210\u5305\u542b\u4e3b\u89c2\u6027\u7684\u6587\u672c\uff0c\u901a\u8fc7RDF\u4e09\u5143\u7ec4\u63d0\u53d6\u3001\u6587\u672c\u805a\u5408\u548c\u4e3b\u89c2\u6027\u6ce8\u5165\u5b9e\u73b0\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8868\u683c\u6570\u636e\u7684\u5ba2\u89c2\u63cf\u8ff0\uff0c\u800c\u751f\u6210\u5305\u542b\u4e3b\u89c2\u6027\uff08\u5373\u8d85\u8d8a\u539f\u59cb\u6570\u636e\u7684\u89e3\u91ca\uff09\u7684\u6587\u672c\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "1) \u63d0\u53d6RDF\u4e09\u5143\u7ec4\uff1b2) \u5c06\u6587\u672c\u805a\u5408\u6210\u8fde\u8d2f\u53d9\u8ff0\uff1b3) \u6ce8\u5165\u4e3b\u89c2\u6027\u4ee5\u4e30\u5bcc\u6587\u672c\u3002\u4f7f\u7528\u5c0f\u578bT5\u6a21\u578b\u800c\u975e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5728\u591a\u9879\u6307\u6807\u4e0a\u8868\u73b0\u4e0eGPT-3.5\u76f8\u5f53\uff0c\u4f18\u4e8eMistral-7B\u548cLlama-2\uff0c\u540c\u65f6\u4fdd\u6301\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u901a\u8fc7\u4e2d\u95f4\u8868\u793a\u589e\u5f3a\u4e8b\u5b9e\u6b63\u786e\u6027\u548c\u4e3b\u89c2\u6027\u7684\u7ed3\u6784\u5316T2T\u751f\u6210\u65b9\u6cd5\u3002"}}
{"id": "2507.19524", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19524", "abs": "https://arxiv.org/abs/2507.19524", "authors": ["Ugo Lomoio", "Pierangelo Veltri", "Pietro Hiram Guzzi"], "title": "Kolmogorov Arnold Network Autoencoder in Medicine", "comment": null, "summary": "Deep learning neural networks architectures such Multi Layer Perceptrons\n(MLP) and Convolutional blocks still play a crucial role in nowadays research\nadvancements. From a topological point of view, these architecture may be\nrepresented as graphs in which we learn the functions related to the nodes\nwhile fixed edges convey the information from the input to the output. A recent\nwork introduced a new architecture called Kolmogorov Arnold Networks (KAN) that\nreports how putting learnable activation functions on the edges of the neural\nnetwork leads to better performances in multiple scenarios. Multiple studies\nare focusing on optimizing the KAN architecture by adding important features\nsuch as dropout regularization, Autoencoders (AE), model benchmarking and last,\nbut not least, the KAN Convolutional Network (KCN) that introduced matrix\nconvolution with KANs learning. This study aims to benchmark multiple versions\nof vanilla AEs (such as Linear, Convolutional and Variational) against their\nKolmogorov-Arnold counterparts that have same or less number of parameters.\nUsing cardiological signals as model input, a total of five different classic\nAE tasks were studied: reconstruction, generation, denoising, inpainting and\nanomaly detection. The proposed experiments uses a medical dataset\n\\textit{AbnormalHeartbeat} that contains audio signals obtained from the\nstethoscope.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86\u4f20\u7edf\u81ea\u7f16\u7801\u5668\uff08AE\uff09\u4e0eKolmogorov-Arnold\u7f51\u7edc\uff08KAN\uff09\u53d8\u4f53\u5728\u5fc3\u810f\u4fe1\u53f7\u5904\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22KAN\u67b6\u6784\u5728\u81ea\u7f16\u7801\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u5fc3\u810f\u4fe1\u53f7\u5904\u7406\u9886\u57df\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5bf9\u591a\u79cd\u4f20\u7edfAE\uff08\u7ebf\u6027\u3001\u5377\u79ef\u3001\u53d8\u5206\uff09\u4e0e\u53c2\u6570\u76f8\u5f53\u7684KAN\u53d8\u4f53\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4efb\u52a1\u5305\u62ec\u91cd\u5efa\u3001\u751f\u6210\u3001\u53bb\u566a\u3001\u4fee\u590d\u548c\u5f02\u5e38\u68c0\u6d4b\u3002", "result": "\u5b9e\u9a8c\u4f7f\u7528\u4e86\u5fc3\u810f\u97f3\u9891\u6570\u636e\u96c6\uff0c\u7ed3\u679c\u8868\u660eKAN\u53d8\u4f53\u5728\u67d0\u4e9b\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u7ed3\u8bba\u662fKAN\u67b6\u6784\u5728\u5fc3\u810f\u4fe1\u53f7\u5904\u7406\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u53c2\u6570\u6548\u7387\u65b9\u9762\u3002"}}
{"id": "2507.19788", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19788", "abs": "https://arxiv.org/abs/2507.19788", "authors": ["Rifny Rachman", "Josh Tingey", "Richard Allmendinger", "Pradyumn Shukla", "Wei Pan"], "title": "Reinforcement Learning for Multi-Objective Multi-Echelon Supply Chain Optimisation", "comment": null, "summary": "This study develops a generalised multi-objective, multi-echelon supply chain\noptimisation model with non-stationary markets based on a Markov decision\nprocess, incorporating economic, environmental, and social considerations. The\nmodel is evaluated using a multi-objective reinforcement learning (RL) method,\nbenchmarked against an originally single-objective RL algorithm modified with\nweighted sum using predefined weights, and a multi-objective evolutionary\nalgorithm (MOEA)-based approach. We conduct experiments on varying network\ncomplexities, mimicking typical real-world challenges using a customisable\nsimulator. The model determines production and delivery quantities across\nsupply chain routes to achieve near-optimal trade-offs between competing\nobjectives, approximating Pareto front sets. The results demonstrate that the\nprimary approach provides the most balanced trade-off between optimality,\ndiversity, and density, further enhanced with a shared experience buffer that\nallows knowledge transfer among policies. In complex settings, it achieves up\nto 75\\% higher hypervolume than the MOEA-based method and generates solutions\nthat are approximately eleven times denser, signifying better robustness, than\nthose produced by the modified single-objective RL method. Moreover, it ensures\nstable production and inventory levels while minimising demand loss.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u7684\u591a\u76ee\u6807\u3001\u591a\u5c42\u6b21\u4f9b\u5e94\u94fe\u4f18\u5316\u6a21\u578b\uff0c\u7ed3\u5408\u7ecf\u6d4e\u3001\u73af\u5883\u548c\u793e\u4f1a\u56e0\u7d20\uff0c\u5e76\u901a\u8fc7\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u8bc4\u4f30\u3002", "motivation": "\u89e3\u51b3\u975e\u7a33\u6001\u5e02\u573a\u4e2d\u4f9b\u5e94\u94fe\u7684\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u5e73\u8861\u7ecf\u6d4e\u3001\u73af\u5883\u548c\u793e\u4f1a\u6548\u76ca\u3002", "method": "\u91c7\u7528\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\uff0c\u4e0e\u6539\u8fdb\u7684\u5355\u76ee\u6807RL\u7b97\u6cd5\u548c\u591a\u76ee\u6807\u8fdb\u5316\u7b97\u6cd5\uff08MOEA\uff09\u8fdb\u884c\u5bf9\u6bd4\uff0c\u901a\u8fc7\u53ef\u5b9a\u5236\u6a21\u62df\u5668\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u4e3b\u65b9\u6cd5\u5728\u6700\u4f18\u6027\u3001\u591a\u6837\u6027\u548c\u5bc6\u5ea6\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u590d\u6742\u573a\u666f\u4e0b\u8d85\u4f53\u79ef\u6bd4MOEA\u9ad875%\uff0c\u89e3\u5bc6\u5ea6\u662f\u6539\u8fdb\u5355\u76ee\u6807RL\u768411\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u4f9b\u5e94\u94fe\u4e2d\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u751f\u4ea7\u548c\u5e93\u5b58\u6c34\u5e73\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u9700\u6c42\u635f\u5931\uff0c\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u591a\u76ee\u6807\u6743\u8861\u3002"}}
{"id": "2507.19754", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19754", "abs": "https://arxiv.org/abs/2507.19754", "authors": ["Seunghun Lee", "Jiwan Seo", "Minwoo Choi", "Kiljoon Han", "Jaehoon Jeong", "Zane Durante", "Ehsan Adeli", "Sang Hyun Park", "Sunghoon Im"], "title": "Latest Object Memory Management for Temporally Consistent Video Instance Segmentation", "comment": "ICCV 2025. Code: https://github.com/Seung-Hun-Lee/LOMM", "summary": "In this paper, we present Latest Object Memory Management (LOMM) for\ntemporally consistent video instance segmentation that significantly improves\nlong-term instance tracking. At the core of our method is Latest Object Memory\n(LOM), which robustly tracks and continuously updates the latest states of\nobjects by explicitly modeling their presence in each frame. This enables\nconsistent tracking and accurate identity management across frames, enhancing\nboth performance and reliability through the VIS process. Moreover, we\nintroduce Decoupled Object Association (DOA), a strategy that separately\nhandles newly appearing and already existing objects. By leveraging our memory\nsystem, DOA accurately assigns object indices, improving matching accuracy and\nensuring stable identity consistency, even in dynamic scenes where objects\nfrequently appear and disappear. Extensive experiments and ablation studies\ndemonstrate the superiority of our method over traditional approaches, setting\na new benchmark in VIS. Notably, our LOMM achieves state-of-the-art AP score of\n54.0 on YouTube-VIS 2022, a dataset known for its challenging long videos.\nProject page: https://seung-hun-lee.github.io/projects/LOMM/", "AI": {"tldr": "LOMM\u65b9\u6cd5\u901a\u8fc7\u6700\u65b0\u5bf9\u8c61\u5185\u5b58\uff08LOM\uff09\u548c\u89e3\u8026\u5bf9\u8c61\u5173\u8054\uff08DOA\uff09\u663e\u8457\u63d0\u5347\u89c6\u9891\u5b9e\u4f8b\u5206\u5272\u7684\u957f\u671f\u8ddf\u8e2a\u80fd\u529b\uff0c\u5b9e\u73b0\u4e8654.0\u7684AP\u9ad8\u5206\u3002", "motivation": "\u89e3\u51b3\u89c6\u9891\u5b9e\u4f8b\u5206\u5272\u4e2d\u957f\u671f\u5b9e\u4f8b\u8ddf\u8e2a\u7684\u6311\u6218\uff0c\u63d0\u5347\u8ddf\u8e2a\u7684\u4e00\u81f4\u6027\u548c\u8eab\u4efd\u7ba1\u7406\u7684\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528LOM\u8ddf\u8e2a\u548c\u66f4\u65b0\u5bf9\u8c61\u72b6\u6001\uff0cDOA\u7b56\u7565\u5206\u522b\u5904\u7406\u65b0\u51fa\u73b0\u548c\u5df2\u5b58\u5728\u7684\u5bf9\u8c61\u3002", "result": "\u5728YouTube-VIS 2022\u6570\u636e\u96c6\u4e0a\u8fbe\u523054.0\u7684AP\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "LOMM\u5728\u89c6\u9891\u5b9e\u4f8b\u5206\u5272\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u6210\u4e3a\u65b0\u7684\u57fa\u51c6\u3002"}}
{"id": "2507.19741", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19741", "abs": "https://arxiv.org/abs/2507.19741", "authors": ["Zhi Zhou", "Sirui Miao", "Xiangyu Duan", "Hao Yang", "Min Zhang"], "title": "Basic Reading Distillation", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable abilities in\nvarious natural language processing areas, but they demand high computation\nresources which limits their deployment in real-world. Distillation is one\ntechnique to solve this problem through either knowledge distillation or task\ndistillation. Both distillation approaches train small models to imitate\nspecific features of LLMs, but they all neglect basic reading education for\nsmall models on generic texts that are \\emph{unrelated} to downstream tasks. In\nthis paper, we propose basic reading distillation (BRD) which educates a small\nmodel to imitate LLMs basic reading behaviors, such as named entity\nrecognition, question raising and answering, on each sentence. After such basic\neducation, we apply the small model on various tasks including language\ninference benchmarks and BIG-bench tasks. It shows that the small model can\noutperform or perform comparable to over 20x bigger LLMs. Analysis reveals that\nBRD effectively influences the probability distribution of the small model, and\nhas orthogonality to either knowledge distillation or task distillation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u57fa\u672c\u9605\u8bfb\u84b8\u998f\uff08BRD\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6559\u80b2\u5c0f\u578b\u6a21\u578b\u6a21\u4eff\u5927\u8bed\u8a00\u6a21\u578b\u7684\u57fa\u672c\u9605\u8bfb\u884c\u4e3a\uff0c\u4f7f\u5176\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u6216\u63a5\u8fd120\u500d\u5927\u7684\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9ad8\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u7684\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u73b0\u6709\u84b8\u998f\u65b9\u6cd5\u5ffd\u89c6\u5bf9\u901a\u7528\u6587\u672c\u7684\u57fa\u672c\u9605\u8bfb\u6559\u80b2\u3002", "method": "\u63d0\u51faBRD\u65b9\u6cd5\uff0c\u6559\u80b2\u5c0f\u578b\u6a21\u578b\u6a21\u4effLLMs\u7684\u57fa\u672c\u9605\u8bfb\u884c\u4e3a\uff08\u5982\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3001\u95ee\u7b54\u7b49\uff09\uff0c\u518d\u5e94\u7528\u4e8e\u4e0b\u6e38\u4efb\u52a1\u3002", "result": "\u5c0f\u578b\u6a21\u578b\u5728\u8bed\u8a00\u63a8\u7406\u548cBIG-bench\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6216\u63a5\u8fd120\u500d\u5927\u7684LLMs\u3002", "conclusion": "BRD\u80fd\u6709\u6548\u5f71\u54cd\u5c0f\u578b\u6a21\u578b\u7684\u6982\u7387\u5206\u5e03\uff0c\u5e76\u4e0e\u77e5\u8bc6\u84b8\u998f\u6216\u4efb\u52a1\u84b8\u998f\u5177\u6709\u6b63\u4ea4\u6027\u3002"}}
{"id": "2507.19525", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19525", "abs": "https://arxiv.org/abs/2507.19525", "authors": ["Chenchen Zhao", "Zhengyuan Shi", "Xiangyu Wen", "Chengjie Liu", "Yi Liu", "Yunhao Zhou", "Yuxiang Zhao", "Hefei Feng", "Yinan Zhu", "Gwok-Waa Wan", "Xin Cheng", "Weiyu Chen", "Yongqi Fu", "Chujie Chen", "Chenhao Xue", "Guangyu Sun", "Ying Wang", "Yibo Lin", "Jun Yang", "Ning Xu", "Xi Wang", "Qiang Xu"], "title": "MMCircuitEval: A Comprehensive Multimodal Circuit-Focused Benchmark for Evaluating LLMs", "comment": "10 pages, 1 figure, 5 tables. To appear in ICCAD 2025", "summary": "The emergence of multimodal large language models (MLLMs) presents promising\nopportunities for automation and enhancement in Electronic Design Automation\n(EDA). However, comprehensively evaluating these models in circuit design\nremains challenging due to the narrow scope of existing benchmarks. To bridge\nthis gap, we introduce MMCircuitEval, the first multimodal benchmark\nspecifically designed to assess MLLM performance comprehensively across diverse\nEDA tasks. MMCircuitEval comprises 3614 meticulously curated question-answer\n(QA) pairs spanning digital and analog circuits across critical EDA stages -\nranging from general knowledge and specifications to front-end and back-end\ndesign. Derived from textbooks, technical question banks, datasheets, and\nreal-world documentation, each QA pair undergoes rigorous expert review for\naccuracy and relevance. Our benchmark uniquely categorizes questions by design\nstage, circuit type, tested abilities (knowledge, comprehension, reasoning,\ncomputation), and difficulty level, enabling detailed analysis of model\ncapabilities and limitations. Extensive evaluations reveal significant\nperformance gaps among existing LLMs, particularly in back-end design and\ncomplex computations, highlighting the critical need for targeted training\ndatasets and modeling approaches. MMCircuitEval provides a foundational\nresource for advancing MLLMs in EDA, facilitating their integration into\nreal-world circuit design workflows. Our benchmark is available at\nhttps://github.com/cure-lab/MMCircuitEval.", "AI": {"tldr": "MMCircuitEval\u662f\u9996\u4e2a\u9488\u5bf9EDA\u4efb\u52a1\u7684\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u5168\u9762\u8bc4\u4f30MLLM\u5728\u7535\u8def\u8bbe\u8ba1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u8303\u56f4\u72ed\u7a84\uff0c\u96be\u4ee5\u5168\u9762\u8bc4\u4f30MLLM\u5728EDA\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u76843614\u4e2aQA\u5bf9\uff0c\u6db5\u76d6\u6570\u5b57\u548c\u6a21\u62df\u7535\u8def\u7684\u5173\u952eEDA\u9636\u6bb5\uff0c\u95ee\u9898\u6765\u6e90\u591a\u6837\u4e14\u7ecf\u8fc7\u4e13\u5bb6\u5ba1\u6838\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u73b0\u6709LLM\u5728\u540e\u7aef\u8bbe\u8ba1\u548c\u590d\u6742\u8ba1\u7b97\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u9488\u5bf9\u6027\u6539\u8fdb\u3002", "conclusion": "MMCircuitEval\u4e3aMLLM\u5728EDA\u4e2d\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u57fa\u7840\u8d44\u6e90\uff0c\u4fc3\u8fdb\u5176\u5728\u5b9e\u9645\u7535\u8def\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2507.19882", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19882", "abs": "https://arxiv.org/abs/2507.19882", "authors": ["Xinshu Li", "Ruoyu Wang", "Erdun Gao", "Mingming Gong", "Lina Yao"], "title": "Causality-aligned Prompt Learning via Diffusion-based Counterfactual Generation", "comment": null, "summary": "Prompt learning has garnered attention for its efficiency over traditional\nmodel training and fine-tuning. However, existing methods, constrained by\ninadequate theoretical foundations, encounter difficulties in achieving\ncausally invariant prompts, ultimately falling short of capturing robust\nfeatures that generalize effectively across categories. To address these\nchallenges, we introduce the $\\textit{\\textbf{DiCap}}$ model, a theoretically\ngrounded $\\textbf{Di}$ffusion-based $\\textbf{C}$ounterf$\\textbf{a}$ctual\n$\\textbf{p}$rompt learning framework, which leverages a diffusion process to\niteratively sample gradients from the marginal and conditional distributions of\nthe causal model, guiding the generation of counterfactuals that satisfy the\nminimal sufficiency criterion. Grounded in rigorous theoretical derivations,\nthis approach guarantees the identifiability of counterfactual outcomes while\nimposing strict bounds on estimation errors. We further employ a contrastive\nlearning framework that leverages the generated counterfactuals, thereby\nenabling the refined extraction of prompts that are precisely aligned with the\ncausal features of the data. Extensive experimental results demonstrate that\nour method performs excellently across tasks such as image classification,\nimage-text retrieval, and visual question answering, with particularly strong\nadvantages in unseen categories.", "AI": {"tldr": "DiCap\u6a21\u578b\u901a\u8fc7\u6269\u6563\u8fc7\u7a0b\u548c\u53cd\u4e8b\u5b9e\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u5728\u56e0\u679c\u4e0d\u53d8\u6027\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u7406\u8bba\u652f\u6301\uff0c\u96be\u4ee5\u751f\u6210\u56e0\u679c\u4e0d\u53d8\u7684\u63d0\u793a\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "DiCap\u5229\u7528\u6269\u6563\u8fc7\u7a0b\u4ece\u56e0\u679c\u6a21\u578b\u7684\u8fb9\u9645\u548c\u6761\u4ef6\u5206\u5e03\u4e2d\u8fed\u4ee3\u91c7\u6837\u68af\u5ea6\uff0c\u751f\u6210\u6ee1\u8db3\u6700\u5c0f\u5145\u5206\u6027\u51c6\u5219\u7684\u53cd\u4e8b\u5b9e\u63d0\u793a\uff0c\u5e76\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u4f18\u5316\u63d0\u793a\u63d0\u53d6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDiCap\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u56fe\u6587\u68c0\u7d22\u548c\u89c6\u89c9\u95ee\u7b54\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u672a\u89c1\u7c7b\u522b\u4e0a\u4f18\u52bf\u660e\u663e\u3002", "conclusion": "DiCap\u901a\u8fc7\u7406\u8bba\u9a71\u52a8\u7684\u53cd\u4e8b\u5b9e\u63d0\u793a\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63d0\u793a\u5b66\u4e60\u7684\u56e0\u679c\u4e0d\u53d8\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.19770", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19770", "abs": "https://arxiv.org/abs/2507.19770", "authors": ["Jiaxin Liu", "Qichao Ying", "Zhenxing Qian", "Sheng Li", "Runqi Zhang", "Jian Liu", "Xinpeng Zhang"], "title": "MoFRR: Mixture of Diffusion Models for Face Retouching Restoration", "comment": null, "summary": "The widespread use of face retouching on social media platforms raises\nconcerns about the authenticity of face images. While existing methods focus on\ndetecting face retouching, how to accurately recover the original faces from\nthe retouched ones has yet to be answered. This paper introduces Face\nRetouching Restoration (FRR), a novel computer vision task aimed at restoring\noriginal faces from their retouched counterparts. FRR differs from traditional\nimage restoration tasks by addressing the complex retouching operations with\nvarious types and degrees, which focuses more on the restoration of the\nlow-frequency information of the faces. To tackle this challenge, we propose\nMoFRR, Mixture of Diffusion Models for FRR. Inspired by DeepSeek's expert\nisolation strategy, the MoFRR uses sparse activation of specialized experts\nhandling distinct retouching types and the engagement of a shared expert\ndealing with universal retouching traces. Each specialized expert follows a\ndual-branch structure with a DDIM-based low-frequency branch guided by an\nIterative Distortion Evaluation Module (IDEM) and a Cross-Attention-based\nHigh-Frequency branch (HFCAM) for detail refinement. Extensive experiments on a\nnewly constructed face retouching dataset, RetouchingFFHQ++, demonstrate the\neffectiveness of MoFRR for FRR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u4efb\u52a1\u2014\u2014\u4eba\u8138\u4fee\u56fe\u6062\u590d\uff08FRR\uff09\uff0c\u65e8\u5728\u4ece\u4fee\u56fe\u540e\u7684\u4eba\u8138\u56fe\u50cf\u4e2d\u6062\u590d\u539f\u59cb\u4eba\u8138\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6269\u6563\u6a21\u578b\uff08MoFRR\uff09\uff0c\u901a\u8fc7\u4e13\u5bb6\u9694\u79bb\u7b56\u7565\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u7684\u4fee\u56fe\u64cd\u4f5c\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e0a\u5e7f\u6cdb\u4f7f\u7528\u7684\u4eba\u8138\u4fee\u56fe\u6280\u672f\u5f15\u53d1\u4e86\u5173\u4e8e\u56fe\u50cf\u771f\u5b9e\u6027\u7684\u62c5\u5fe7\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u5173\u6ce8\u68c0\u6d4b\u4fee\u56fe\uff0c\u800c\u5982\u4f55\u4ece\u4fee\u56fe\u540e\u56fe\u50cf\u4e2d\u6062\u590d\u539f\u59cb\u4eba\u8138\u5c1a\u672a\u89e3\u51b3\u3002", "method": "\u63d0\u51faMoFRR\u6a21\u578b\uff0c\u7ed3\u5408\u7a00\u758f\u6fc0\u6d3b\u7684\u4e13\u7528\u4e13\u5bb6\u5904\u7406\u7279\u5b9a\u4fee\u56fe\u7c7b\u578b\uff0c\u4ee5\u53ca\u5171\u4eab\u4e13\u5bb6\u5904\u7406\u901a\u7528\u4fee\u56fe\u75d5\u8ff9\u3002\u4e13\u7528\u4e13\u5bb6\u91c7\u7528\u53cc\u5206\u652f\u7ed3\u6784\uff0c\u5305\u62ec\u57fa\u4e8eDDIM\u7684\u4f4e\u9891\u5206\u652f\u548c\u57fa\u4e8e\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u9ad8\u9891\u5206\u652f\u3002", "result": "\u5728\u65b0\u5efa\u7684\u6570\u636e\u96c6RetouchingFFHQ++\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86MoFRR\u5728FRR\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "MoFRR\u901a\u8fc7\u6df7\u5408\u6269\u6563\u6a21\u578b\u548c\u4e13\u5bb6\u9694\u79bb\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4eba\u8138\u4fee\u56fe\u6062\u590d\u7684\u590d\u6742\u95ee\u9898\uff0c\u4e3a\u6062\u590d\u539f\u59cb\u4eba\u8138\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2507.19748", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19748", "abs": "https://arxiv.org/abs/2507.19748", "authors": ["Yifan Hao", "Fangning Chao", "Yaqian Hao", "Zhaojun Cui", "Huan Bai", "Haiyu Zhang", "Yankai Liu", "Chao Deng", "Junlan Feng"], "title": "JT-Math: A Multi-Stage Framework for Advanced Mathematical Reasoning in Large Language Models", "comment": null, "summary": "Mathematical reasoning is a cornerstone of artificial general intelligence\nand a primary benchmark for evaluating the capabilities of Large Language\nModels (LLMs). While state-of-the-art models show promise, they often falter\nwhen faced with complex problems that demand deep conceptual understanding and\nintricate, multi-step deliberation. To address this challenge, we introduce\nJT-Math-8B, a series of open-source models comprising base, instruct, and\nthinking versions, built upon a systematic, multi-stage optimization framework.\nOur pre-training corpus is a high-quality, 210B-token dataset curated through a\ndedicated data pipeline that uses model-based validation to ensure quality and\ndiversity. The Instruct Model is optimized for direct, concise answers through\nSupervised Fine-Tuning (SFT) and a GRPO-based reinforcement learning (RL)\nmethod. The Thinking Model is trained for complex problem-solving using a Long\nChain-of-Thought (Long CoT) approach, combining SFT with a novel, multi-stage\nRL curriculum that progressively increases task difficulty and context length\nup to 32K tokens. JT-Math-8B achieves state-of-the-art results among\nopen-source models of similar size, surpassing prominent models like OpenAI's\nO1-mini and GPT-4o , and demonstrating superior performance on\ncompetition-level mathematics.", "AI": {"tldr": "JT-Math-8B\u662f\u4e00\u7cfb\u5217\u5f00\u6e90\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u4f18\u5316\u6846\u67b6\u63d0\u5347\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u5728\u7c7b\u4f3c\u89c4\u6a21\u7684\u5f00\u6e90\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u6570\u5b66\u95ee\u9898\u4e2d\u8868\u73b0\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u4f18\u5316\u6846\u67b6\uff0c\u5305\u62ec\u57fa\u7840\u3001\u6307\u5bfc\u548c\u601d\u8003\u4e09\u4e2a\u7248\u672c\uff0c\u7ed3\u5408\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5728\u5f00\u6e90\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u8d85\u8d8aOpenAI\u7684O1-mini\u548cGPT-4o\u3002", "conclusion": "JT-Math-8B\u5c55\u793a\u4e86\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5f3a\u5927\u80fd\u529b\uff0c\u4e3a\u5f00\u6e90\u6a21\u578b\u6811\u7acb\u4e86\u65b0\u6807\u6746\u3002"}}
{"id": "2507.19526", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19526", "abs": "https://arxiv.org/abs/2507.19526", "authors": ["Jianyuan Bo", "Hao Wu", "Yuan Fang"], "title": "Quantizing Text-attributed Graphs for Semantic-Structural Integration", "comment": "Accepted at KDD'2025", "summary": "Text-attributed graphs (TAGs) have emerged as a powerful representation for\nmodeling complex relationships across diverse domains. With the rise of large\nlanguage models (LLMs), there is growing interest in leveraging their\ncapabilities for graph learning. However, current approaches face significant\nchallenges in embedding structural information into LLM-compatible formats,\nrequiring either computationally expensive alignment mechanisms or manual graph\nverbalization techniques that often lose critical structural details. Moreover,\nthese methods typically require labeled data from source domains for effective\ntransfer learning, significantly constraining their adaptability. We propose\nSTAG, a novel self-supervised framework that directly quantizes graph\nstructural information into discrete tokens using a frozen codebook. Unlike\ntraditional quantization approaches, our method employs soft assignment and KL\ndivergence guided quantization to address the unique challenges of graph data,\nwhich lacks natural tokenization structures. Our framework enables both\nLLM-based and traditional learning approaches, supporting true zero-shot\ntransfer learning without requiring labeled data even in the source domain.\nExtensive experiments demonstrate state-of-the-art performance across multiple\nnode classification benchmarks while maintaining compatibility with different\nLLM architectures, offering an elegant solution to bridging graph learning with\nLLMs.", "AI": {"tldr": "STAG\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5316\u56fe\u7ed3\u6784\u4fe1\u606f\u4e3a\u79bb\u6563\u4ee4\u724c\uff0c\u89e3\u51b3\u4e86\u5c06\u7ed3\u6784\u4fe1\u606f\u5d4c\u5165LLM\u517c\u5bb9\u683c\u5f0f\u7684\u6311\u6218\uff0c\u652f\u6301\u96f6\u6837\u672c\u8fc1\u79fb\u5b66\u4e60\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5728\u5c06\u56fe\u7ed3\u6784\u4fe1\u606f\u5d4c\u5165LLM\u517c\u5bb9\u683c\u5f0f\u65f6\u9762\u4e34\u8ba1\u7b97\u6210\u672c\u9ad8\u6216\u4e22\u5931\u5173\u952e\u7ec6\u8282\u7684\u95ee\u9898\uff0c\u4e14\u4f9d\u8d56\u6e90\u57df\u6807\u8bb0\u6570\u636e\uff0c\u9650\u5236\u4e86\u9002\u5e94\u6027\u3002", "method": "STAG\u901a\u8fc7\u8f6f\u5206\u914d\u548cKL\u6563\u5ea6\u5f15\u5bfc\u7684\u91cf\u5316\uff0c\u5c06\u56fe\u7ed3\u6784\u4fe1\u606f\u76f4\u63a5\u91cf\u5316\u4e3a\u79bb\u6563\u4ee4\u724c\uff0c\u65e0\u9700\u81ea\u7136\u6807\u8bb0\u5316\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSTAG\u5728\u591a\u4e2a\u8282\u70b9\u5206\u7c7b\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e14\u517c\u5bb9\u4e0d\u540cLLM\u67b6\u6784\u3002", "conclusion": "STAG\u4e3a\u56fe\u5b66\u4e60\u4e0eLLM\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u4f18\u96c5\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u96f6\u6837\u672c\u8fc1\u79fb\u5b66\u4e60\u3002"}}
{"id": "2507.19960", "categories": ["cs.AI", "I.2.0; K.2; K.4.0"], "pdf": "https://arxiv.org/pdf/2507.19960", "abs": "https://arxiv.org/abs/2507.19960", "authors": ["Olivia Guest"], "title": "What Does 'Human-Centred AI' Mean?", "comment": null, "summary": "While it seems sensible that human-centred artificial intelligence (AI) means\ncentring \"human behaviour and experience,\" it cannot be any other way. AI, I\nargue, is usefully seen as a relationship between technology and humans where\nit appears that artifacts can perform, to a greater or lesser extent, human\ncognitive labour. This is evinced using examples that juxtapose technology with\ncognition, inter alia: abacus versus mental arithmetic; alarm clock versus\nknocker-upper; camera versus vision; and sweatshop versus tailor. Using novel\ndefinitions and analyses, sociotechnical relationships can be analysed into\nvarying types of: displacement (harmful), enhancement (beneficial), and/or\nreplacement (neutral) of human cognitive labour. Ultimately, all AI implicates\nhuman cognition; no matter what. Obfuscation of cognition in the AI context --\nfrom clocks to artificial neural networks -- results in distortion, in slowing\ncritical engagement, perverting cognitive science, and indeed in limiting our\nability to truly centre humans and humanity in the engineering of AI systems.\nTo even begin to de-fetishise AI, we must look the human-in-the-loop in the\neyes.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u672c\u8d28\u4e0a\u662f\u6280\u672f\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u7684\u5173\u7cfb\uff0c\u5206\u6790\u4e86AI\u5bf9\u4eba\u7c7b\u8ba4\u77e5\u52b3\u52a8\u7684\u66ff\u4ee3\u3001\u589e\u5f3a\u6216\u53d6\u4ee3\uff0c\u5e76\u5f3a\u8c03\u5ffd\u89c6\u8ba4\u77e5\u4f1a\u5bfc\u81f4AI\u8bbe\u8ba1\u7684\u626d\u66f2\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u6f84\u6e05AI\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u7684\u5173\u7cfb\uff0c\u907f\u514d\u56e0\u5ffd\u89c6\u8ba4\u77e5\u800c\u5bfc\u81f4\u7684AI\u8bbe\u8ba1\u95ee\u9898\uff0c\u4ece\u800c\u771f\u6b63\u5b9e\u73b0\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684AI\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u6280\u672f\uff08\u5982\u7b97\u76d8\u3001\u95f9\u949f\uff09\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u52b3\u52a8\uff08\u5982\u5fc3\u7b97\u3001\u4eba\u5de5\u53eb\u9192\uff09\uff0c\u63d0\u51fa\u65b0\u7684\u5b9a\u4e49\u548c\u5206\u6790\u6846\u67b6\uff0c\u5c06\u793e\u4f1a\u6280\u672f\u5173\u7cfb\u5206\u4e3a\u66ff\u4ee3\uff08\u6709\u5bb3\uff09\u3001\u589e\u5f3a\uff08\u6709\u76ca\uff09\u548c\u53d6\u4ee3\uff08\u4e2d\u6027\uff09\u3002", "result": "\u5206\u6790\u8868\u660e\uff0c\u6240\u6709AI\u90fd\u6d89\u53ca\u4eba\u7c7b\u8ba4\u77e5\uff0c\u5ffd\u89c6\u8fd9\u4e00\u70b9\u4f1a\u626d\u66f2AI\u8bbe\u8ba1\uff0c\u963b\u788d\u6279\u5224\u6027\u601d\u8003\uff0c\u5e76\u9650\u5236\u771f\u6b63\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684AI\u5de5\u7a0b\u3002", "conclusion": "\u7ed3\u8bba\u5f3a\u8c03\u5fc5\u987b\u6b63\u89c6AI\u4e2d\u7684\u4eba\u7c7b\u8ba4\u77e5\u56e0\u7d20\uff0c\u624d\u80fd\u771f\u6b63\u53bb\u795e\u79d8\u5316AI\uff0c\u5b9e\u73b0\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u8bbe\u8ba1\u3002"}}
{"id": "2507.19773", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19773", "abs": "https://arxiv.org/abs/2507.19773", "authors": ["Jeongwoo Shin", "Inseo Lee", "Junho Lee", "Joonseok Lee"], "title": "Self-Guided Masked Autoencoder", "comment": null, "summary": "Masked Autoencoder (MAE) is a self-supervised approach for representation\nlearning, widely applicable to a variety of downstream tasks in computer\nvision. In spite of its success, it is still not fully uncovered what and how\nMAE exactly learns. In this paper, with an in-depth analysis, we discover that\nMAE intrinsically learns pattern-based patch-level clustering from surprisingly\nearly stages of pretraining. Upon this understanding, we propose self-guided\nmasked autoencoder, which internally generates informed mask by utilizing its\nprogress in patch clustering, substituting the naive random masking of the\nvanilla MAE. Our approach significantly boosts its learning process without\nrelying on any external models or supplementary information, keeping the\nbenefit of self-supervised nature of MAE intact. Comprehensive experiments on\nvarious downstream tasks verify the effectiveness of the proposed method.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u5f15\u5bfc\u7684\u63a9\u7801\u81ea\u7f16\u7801\u5668\uff08MAE\uff09\uff0c\u901a\u8fc7\u5229\u7528\u5176\u5185\u90e8\u5b66\u4e60\u7684\u8865\u4e01\u805a\u7c7b\u4fe1\u606f\u751f\u6210\u63a9\u7801\uff0c\u66ff\u4ee3\u4e86\u539f\u59cbMAE\u7684\u968f\u673a\u63a9\u7801\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u5c3d\u7ba1MAE\u5728\u81ea\u76d1\u7763\u8868\u793a\u5b66\u4e60\u4e2d\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5176\u5177\u4f53\u5b66\u4e60\u673a\u5236\u5c1a\u672a\u5b8c\u5168\u660e\u786e\u3002\u672c\u6587\u65e8\u5728\u63ed\u793aMAE\u7684\u5b66\u4e60\u673a\u5236\uff0c\u5e76\u57fa\u4e8e\u6b64\u6539\u8fdb\u5176\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u6df1\u5165\u5206\u6790\u53d1\u73b0MAE\u5728\u9884\u8bad\u7ec3\u65e9\u671f\u5c31\u5b66\u4e60\u5230\u4e86\u57fa\u4e8e\u6a21\u5f0f\u7684\u8865\u4e01\u7ea7\u805a\u7c7b\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u81ea\u5f15\u5bfcMAE\uff0c\u5229\u7528\u5185\u90e8\u805a\u7c7b\u4fe1\u606f\u751f\u6210\u63a9\u7801\uff0c\u66ff\u4ee3\u968f\u673a\u63a9\u7801\u3002", "result": "\u5728\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5b66\u4e60\u6548\u679c\uff0c\u4e14\u65e0\u9700\u4f9d\u8d56\u5916\u90e8\u6a21\u578b\u6216\u989d\u5916\u4fe1\u606f\u3002", "conclusion": "\u81ea\u5f15\u5bfcMAE\u901a\u8fc7\u5229\u7528\u5185\u90e8\u805a\u7c7b\u4fe1\u606f\u6539\u8fdb\u63a9\u7801\u751f\u6210\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u4f18\u52bf\u3002"}}
{"id": "2507.19756", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19756", "abs": "https://arxiv.org/abs/2507.19756", "authors": ["Rebecca M. M. Hicke", "Brian Haggard", "Mia Ferrante", "Rayhan Khanna", "David Mimno"], "title": "Are You There God? Lightweight Narrative Annotation of Christian Fiction with LMs", "comment": null, "summary": "In addition to its more widely studied political activities, the American\nEvangelical movement has a well-developed but less externally visible cultural\nand literary side. Christian Fiction, however, has been little studied, and\nwhat scholarly attention there is has focused on the explosively popular Left\nBehind series. In this work, we use computational tools to provide both a broad\ntopical overview of Christian Fiction as a genre and a more directed\nexploration of how its authors depict divine acts. Working with human\nannotators we first developed definitions and a codebook for \"acts of God.\" We\nthen adapted those instructions designed for human annotators for use by a\nrecent, lightweight LM with the assistance of a much larger model. The\nlaptop-scale LM is capable of matching human annotations, even when the task is\nsubtle and challenging. Using these annotations, we show that significant and\nmeaningful differences exist between the Left Behind books and Christian\nFiction more broadly and between books by male and female authors.", "AI": {"tldr": "\u8bba\u6587\u4f7f\u7528\u8ba1\u7b97\u5de5\u5177\u5206\u6790\u57fa\u7763\u6559\u5c0f\u8bf4\u4e2d\u7684\u2018\u795e\u7684\u884c\u4e3a\u2019\uff0c\u53d1\u73b0\u300a\u672b\u65e5\u8ff7\u8e2a\u300b\u7cfb\u5217\u4e0e\u5176\u4ed6\u57fa\u7763\u6559\u5c0f\u8bf4\u53ca\u7537\u5973\u4f5c\u8005\u4f5c\u54c1\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u7814\u7a76\u57fa\u7763\u6559\u5c0f\u8bf4\u7684\u6587\u5316\u548c\u6587\u5b66\u5c42\u9762\uff0c\u586b\u8865\u5b66\u672f\u7a7a\u767d\uff0c\u5c24\u5176\u662f\u5bf9\u2018\u795e\u7684\u884c\u4e3a\u2019\u7684\u63cf\u7ed8\u3002", "method": "\u7ed3\u5408\u4eba\u7c7b\u6ce8\u91ca\u548c\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\uff0c\u5b9a\u4e49\u5e76\u6807\u6ce8\u2018\u795e\u7684\u884c\u4e3a\u2019\uff0c\u6bd4\u8f83\u4e0d\u540c\u4f5c\u54c1\u95f4\u7684\u5dee\u5f02\u3002", "result": "\u8f7b\u91cf\u7ea7\u6a21\u578b\u80fd\u5339\u914d\u4eba\u7c7b\u6ce8\u91ca\uff0c\u53d1\u73b0\u300a\u672b\u65e5\u8ff7\u8e2a\u300b\u4e0e\u5176\u4ed6\u4f5c\u54c1\u53ca\u7537\u5973\u4f5c\u8005\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u8ba1\u7b97\u5de5\u5177\u80fd\u6709\u6548\u5206\u6790\u6587\u5b66\u4e3b\u9898\uff0c\u63ed\u793a\u57fa\u7763\u6559\u5c0f\u8bf4\u4e2d\u7684\u591a\u6837\u6027\u3002"}}
{"id": "2507.19527", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19527", "abs": "https://arxiv.org/abs/2507.19527", "authors": ["Yihan Wang", "Jianing Zhao"], "title": "Research on the application of graph data structure and graph neural network in node classification/clustering tasks", "comment": null, "summary": "Graph-structured data are pervasive across domains including social networks,\nbiological networks, and knowledge graphs. Due to their non-Euclidean nature,\nsuch data pose significant challenges to conventional machine learning methods.\nThis study investigates graph data structures, classical graph algorithms, and\nGraph Neural Networks (GNNs), providing comprehensive theoretical analysis and\ncomparative evaluation. Through comparative experiments, we quantitatively\nassess performance differences between traditional algorithms and GNNs in node\nclassification and clustering tasks. Results show GNNs achieve substantial\naccuracy improvements of 43% to 70% over traditional methods. We further\nexplore integration strategies between classical algorithms and GNN\narchitectures, providing theoretical guidance for advancing graph\nrepresentation learning research.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u56fe\u6570\u636e\u7ed3\u6784\u3001\u7ecf\u5178\u56fe\u7b97\u6cd5\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u4e86\u4f20\u7edf\u7b97\u6cd5\u4e0eGNNs\u5728\u8282\u70b9\u5206\u7c7b\u548c\u805a\u7c7b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\u3002\u7ed3\u679c\u663e\u793aGNNs\u6bd4\u4f20\u7edf\u65b9\u6cd5\u51c6\u786e\u7387\u63d0\u5347\u4e8643%\u81f370%\u3002", "motivation": "\u7531\u4e8e\u56fe\u6570\u636e\u7684\u975e\u6b27\u51e0\u91cc\u5f97\u7279\u6027\uff0c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\uff0c\u56e0\u6b64\u7814\u7a76\u56fe\u6570\u636e\u7ed3\u6784\u548cGNNs\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u6bd4\u8f83\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u4f20\u7edf\u7b97\u6cd5\u4e0eGNNs\u5728\u8282\u70b9\u5206\u7c7b\u548c\u805a\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "GNNs\u5728\u51c6\u786e\u7387\u4e0a\u6bd4\u4f20\u7edf\u65b9\u6cd5\u63d0\u5347\u4e8643%\u81f370%\u3002", "conclusion": "\u7814\u7a76\u4e3a\u56fe\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\uff0c\u5e76\u63a2\u7d22\u4e86\u7ecf\u5178\u7b97\u6cd5\u4e0eGNN\u67b6\u6784\u7684\u6574\u5408\u7b56\u7565\u3002"}}
{"id": "2507.19973", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.19973", "abs": "https://arxiv.org/abs/2507.19973", "authors": ["Ebrahim Rasromani", "Stella K. Kang", "Yanqi Xu", "Beisong Liu", "Garvit Luhadia", "Wan Fung Chui", "Felicia L. Pasadyn", "Yu Chih Hung", "Julie Y. An", "Edwin Mathieu", "Zehui Gu", "Carlos Fernandez-Granda", "Ammar A. Javed", "Greg D. Sacks", "Tamas Gonda", "Chenchan Huang", "Yiqiu Shen"], "title": "Leveraging Fine-Tuned Large Language Models for Interpretable Pancreatic Cystic Lesion Feature Extraction and Risk Categorization", "comment": null, "summary": "Background: Manual extraction of pancreatic cystic lesion (PCL) features from\nradiology reports is labor-intensive, limiting large-scale studies needed to\nadvance PCL research. Purpose: To develop and evaluate large language models\n(LLMs) that automatically extract PCL features from MRI/CT reports and assign\nrisk categories based on guidelines. Materials and Methods: We curated a\ntraining dataset of 6,000 abdominal MRI/CT reports (2005-2024) from 5,134\npatients that described PCLs. Labels were generated by GPT-4o using\nchain-of-thought (CoT) prompting to extract PCL and main pancreatic duct\nfeatures. Two open-source LLMs were fine-tuned using QLoRA on GPT-4o-generated\nCoT data. Features were mapped to risk categories per institutional guideline\nbased on the 2017 ACR White Paper. Evaluation was performed on 285 held-out\nhuman-annotated reports. Model outputs for 100 cases were independently\nreviewed by three radiologists. Feature extraction was evaluated using exact\nmatch accuracy, risk categorization with macro-averaged F1 score, and\nradiologist-model agreement with Fleiss' Kappa. Results: CoT fine-tuning\nimproved feature extraction accuracy for LLaMA (80% to 97%) and DeepSeek (79%\nto 98%), matching GPT-4o (97%). Risk categorization F1 scores also improved\n(LLaMA: 0.95; DeepSeek: 0.94), closely matching GPT-4o (0.97), with no\nstatistically significant differences. Radiologist inter-reader agreement was\nhigh (Fleiss' Kappa = 0.888) and showed no statistically significant difference\nwith the addition of DeepSeek-FT-CoT (Fleiss' Kappa = 0.893) or GPT-CoT\n(Fleiss' Kappa = 0.897), indicating that both models achieved agreement levels\non par with radiologists. Conclusion: Fine-tuned open-source LLMs with CoT\nsupervision enable accurate, interpretable, and efficient phenotyping for\nlarge-scale PCL research, achieving performance comparable to GPT-4o.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u4eceMRI/CT\u62a5\u544a\u4e2d\u63d0\u53d6\u80f0\u817a\u56ca\u6027\u75c5\u53d8\uff08PCL\uff09\u7279\u5f81\u5e76\u5206\u7c7b\u98ce\u9669\uff0c\u6027\u80fd\u63a5\u8fd1GPT-4o\u3002", "motivation": "\u624b\u52a8\u63d0\u53d6PCL\u7279\u5f81\u8017\u65f6\u4e14\u96be\u4ee5\u5927\u89c4\u6a21\u7814\u7a76\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u5316\u5de5\u5177\u3002", "method": "\u4f7f\u7528GPT-4o\u751f\u6210\u7684\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u6570\u636e\u5fae\u8c03\u5f00\u6e90LLMs\uff08LLaMA\u548cDeepSeek\uff09\uff0c\u5e76\u57fa\u4e8e\u6307\u5357\u6620\u5c04\u98ce\u9669\u7c7b\u522b\u3002", "result": "\u5fae\u8c03\u540e\u6a21\u578b\u5728\u7279\u5f81\u63d0\u53d6\u548c\u98ce\u9669\u5206\u7c7b\u4e0a\u8868\u73b0\u4f18\u5f02\uff08\u51c6\u786e\u738797%-98%\uff0cF1\u5206\u65700.94-0.97\uff09\uff0c\u4e0e\u653e\u5c04\u79d1\u533b\u751f\u4e00\u81f4\u6027\u9ad8\u3002", "conclusion": "\u5fae\u8c03\u7684\u5f00\u6e90LLMs\u7ed3\u5408CoT\u76d1\u7763\u53ef\u5b9e\u73b0\u9ad8\u6548\u3001\u51c6\u786e\u7684PCL\u7814\u7a76\uff0c\u6027\u80fd\u5ab2\u7f8eGPT-4o\u3002"}}
{"id": "2507.19778", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19778", "abs": "https://arxiv.org/abs/2507.19778", "authors": ["Kanglin Qu", "Pan Gao", "Qun Dai", "Yuanhao Sun"], "title": "HydraMamba: Multi-Head State Space Model for Global Point Cloud Learning", "comment": "Accepted by MM '25", "summary": "The attention mechanism has become a dominant operator in point cloud\nlearning, but its quadratic complexity leads to limited inter-point\ninteractions, hindering long-range dependency modeling between objects. Due to\nexcellent long-range modeling capability with linear complexity, the selective\nstate space model (S6), as the core of Mamba, has been exploited in point cloud\nlearning for long-range dependency interactions over the entire point cloud.\nDespite some significant progress, related works still suffer from imperfect\npoint cloud serialization and lack of locality learning. To this end, we\nexplore a state space model-based point cloud network termed HydraMamba to\naddress the above challenges. Specifically, we design a shuffle serialization\nstrategy, making unordered point sets better adapted to the causal nature of\nS6. Meanwhile, to overcome the deficiency of existing techniques in locality\nlearning, we propose a ConvBiS6 layer, which is capable of capturing local\ngeometries and global context dependencies synergistically. Besides, we propose\nMHS6 by extending the multi-head design to S6, further enhancing its modeling\ncapability. HydraMamba achieves state-of-the-art results on various tasks at\nboth object-level and scene-level. The code is available at\nhttps://github.com/Point-Cloud-Learning/HydraMamba.", "AI": {"tldr": "HydraMamba\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u70b9\u4e91\u7f51\u7edc\uff0c\u901a\u8fc7\u6539\u8fdb\u70b9\u4e91\u5e8f\u5217\u5316\u548c\u5c40\u90e8\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6ce8\u610f\u529b\u673a\u5236\u5728\u70b9\u4e91\u5b66\u4e60\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u6ce8\u610f\u529b\u673a\u5236\u5728\u70b9\u4e91\u5b66\u4e60\u4e2d\u5b58\u5728\u4e8c\u6b21\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u9650\u5236\u4e86\u957f\u8ddd\u79bb\u4f9d\u8d56\u5efa\u6a21\u3002\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08S6\uff09\u56e0\u5176\u7ebf\u6027\u590d\u6742\u5ea6\u548c\u4f18\u79c0\u7684\u957f\u8ddd\u79bb\u5efa\u6a21\u80fd\u529b\u88ab\u5f15\u5165\u70b9\u4e91\u5b66\u4e60\uff0c\u4f46\u4ecd\u5b58\u5728\u5e8f\u5217\u5316\u548c\u5c40\u90e8\u5b66\u4e60\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86shuffle\u5e8f\u5217\u5316\u7b56\u7565\u4ee5\u9002\u5e94S6\u7684\u56e0\u679c\u6027\uff0c\u5e76\u63d0\u51faConvBiS6\u5c42\u4ee5\u534f\u540c\u6355\u83b7\u5c40\u90e8\u51e0\u4f55\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u4f9d\u8d56\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u591a\u5934\u8bbe\u8ba1\u6269\u5c55S6\u4e3aMHS6\uff0c\u589e\u5f3a\u5efa\u6a21\u80fd\u529b\u3002", "result": "HydraMamba\u5728\u5bf9\u8c61\u7ea7\u548c\u573a\u666f\u7ea7\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "HydraMamba\u901a\u8fc7\u6539\u8fdb\u5e8f\u5217\u5316\u548c\u5c40\u90e8\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u70b9\u4e91\u5b66\u4e60\u7684\u957f\u8ddd\u79bb\u4f9d\u8d56\u5efa\u6a21\u80fd\u529b\u3002"}}
{"id": "2507.19766", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19766", "abs": "https://arxiv.org/abs/2507.19766", "authors": ["Dong Du", "Shulin Liu", "Tao Yang", "Shaohua Chen", "Yang Li"], "title": "UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models' Reasoning Abilities", "comment": "12 pages", "summary": "Recent advances in large language models (LLMs) have highlighted the\npotential of reinforcement learning with verifiable rewards (RLVR) to enhance\nreasoning capabilities through extended output sequences. However, traditional\nRL frameworks face inefficiencies when handling ultra-long outputs due to\nlong-tail sequence distributions and entropy collapse during training. To\naddress these challenges, we propose an Ultra-Long Output Reinforcement\nLearning (UloRL) approach for advancing large language models' reasoning\nabilities. Specifically, we divide ultra long output decoding into short\nsegments, enabling efficient training by mitigating delays caused by long-tail\nsamples. Additionally, we introduce dynamic masking of well-Mastered Positive\nTokens (MPTs) to prevent entropy collapse. Experimental results demonstrate the\neffectiveness of our approach. On the Qwen3-30B-A3B model, RL with segment\nrollout achieved 2.06x increase in training speed, while RL training with\n128k-token outputs improves the model's performance on AIME2025 from 70.9\\% to\n85.1\\% and on BeyondAIME from 50.7\\% to 61.9\\%, even surpassing Qwen3-235B-A22B\nwith remarkable gains. These findings underscore the potential of our methods\nto advance the reasoning capabilities of LLMs with ultra-long sequence\ngeneration. We will release our code and model for further use by the\ncommunity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u8d85\u957f\u8f93\u51fa\u5e8f\u5217\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08UloRL\uff09\uff0c\u901a\u8fc7\u5206\u6bb5\u89e3\u7801\u548c\u52a8\u6001\u63a9\u7801\u6280\u672f\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5728\u5904\u7406\u8d85\u957f\u8f93\u51fa\u5e8f\u5217\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u5b58\u5728\u957f\u5c3e\u5206\u5e03\u548c\u71b5\u5d29\u6e83\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u5c06\u8d85\u957f\u8f93\u51fa\u89e3\u7801\u5206\u4e3a\u77ed\u6bb5\uff0c\u5f15\u5165\u52a8\u6001\u63a9\u7801\u6280\u672f\u9632\u6b62\u71b5\u5d29\u6e83\u3002", "result": "\u5728Qwen3-30B-A3B\u6a21\u578b\u4e0a\uff0c\u8bad\u7ec3\u901f\u5ea6\u63d0\u53472.06\u500d\uff0c\u6027\u80fd\u5728AIME2025\u548cBeyondAIME\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u3002", "conclusion": "UloRL\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86LLMs\u5728\u8d85\u957f\u5e8f\u5217\u751f\u6210\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.19529", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19529", "abs": "https://arxiv.org/abs/2507.19529", "authors": ["Obumneme Nwafor", "Mohammed Abdul Majeed Al Hooti"], "title": "Machine Learning Risk Intelligence for Green Hydrogen Investment: Insights for Duqm R3 Auction", "comment": null, "summary": "As green hydrogen emerges as a major component of global decarbonisation,\nOman has positioned itself strategically through national auctions and\ninternational partnerships. Following two successful green hydrogen project\nrounds, the country launched its third auction (R3) in the Duqm region. While\nthis area exhibits relative geospatial homogeneity, it is still vulnerable to\nenvironmental fluctuations that pose inherent risks to productivity. Despite\ngrowing global investment in green hydrogen, operational data remains scarce,\nwith major projects like Saudi Arabia's NEOM facility not expected to commence\nproduction until 2026, and Oman's ACME Duqm project scheduled for 2028. This\nabsence of historical maintenance and performance data from large-scale\nhydrogen facilities in desert environments creates a major knowledge gap for\naccurate risk assessment for infrastructure planning and auction decisions.\nGiven this data void, environmental conditions emerge as accessible and\nreliable proxy for predicting infrastructure maintenance pressures, because\nharsh desert conditions such as dust storms, extreme temperatures, and humidity\nfluctuations are well-documented drivers of equipment degradation in renewable\nenergy systems. To address this challenge, this paper proposes an Artificial\nIntelligence decision support system that leverages publicly available\nmeteorological data to develop a predictive Maintenance Pressure Index (MPI),\nwhich predicts risk levels and future maintenance demands on hydrogen\ninfrastructure. This tool strengthens regulatory foresight and operational\ndecision-making by enabling temporal benchmarking to assess and validate\nperformance claims over time. It can be used to incorporate temporal risk\nintelligence into auction evaluation criteria despite the absence of historical\noperational benchmarks.", "AI": {"tldr": "\u963f\u66fc\u901a\u8fc7\u56fd\u5bb6\u62cd\u5356\u548c\u56fd\u9645\u5408\u4f5c\u63a8\u52a8\u7eff\u6c22\u53d1\u5c55\uff0c\u4f46\u7f3a\u4e4f\u5927\u89c4\u6a21\u6c22\u8bbe\u65bd\u7684\u5386\u53f2\u6570\u636e\u3002\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u6c14\u8c61\u6570\u636e\u7684AI\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff0c\u9884\u6d4b\u7ef4\u62a4\u538b\u529b\u6307\u6570\uff08MPI\uff09\uff0c\u4ee5\u586b\u8865\u98ce\u9669\u8bc4\u4f30\u7684\u77e5\u8bc6\u7a7a\u767d\u3002", "motivation": "\u5168\u7403\u7eff\u6c22\u6295\u8d44\u589e\u957f\uff0c\u4f46\u6c99\u6f20\u73af\u5883\u4e0b\u5927\u89c4\u6a21\u6c22\u8bbe\u65bd\u7684\u5386\u53f2\u7ef4\u62a4\u548c\u6027\u80fd\u6570\u636e\u7a00\u7f3a\uff0c\u5bfc\u81f4\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\u548c\u62cd\u5356\u51b3\u7b56\u7684\u98ce\u9669\u8bc4\u4f30\u56f0\u96be\u3002", "method": "\u5229\u7528\u516c\u5f00\u6c14\u8c61\u6570\u636e\u5f00\u53d1AI\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff0c\u751f\u6210\u7ef4\u62a4\u538b\u529b\u6307\u6570\uff08MPI\uff09\uff0c\u9884\u6d4b\u6c22\u57fa\u7840\u8bbe\u65bd\u7684\u7ef4\u62a4\u9700\u6c42\u548c\u98ce\u9669\u6c34\u5e73\u3002", "result": "\u63d0\u51fa\u7684MPI\u5de5\u5177\u80fd\u591f\u586b\u8865\u5386\u53f2\u6570\u636e\u7a7a\u767d\uff0c\u4e3a\u62cd\u5356\u8bc4\u4f30\u548c\u8fd0\u8425\u51b3\u7b56\u63d0\u4f9b\u65f6\u95f4\u57fa\u51c6\u548c\u98ce\u9669\u60c5\u62a5\u3002", "conclusion": "AI\u652f\u6301\u7684MPI\u7cfb\u7edf\u53ef\u589e\u5f3a\u76d1\u7ba1\u9884\u89c1\u6027\uff0c\u4f18\u5316\u7eff\u6c22\u57fa\u7840\u8bbe\u65bd\u7684\u89c4\u5212\u548c\u8fd0\u8425\u51b3\u7b56\u3002"}}
{"id": "2507.19974", "categories": ["cs.AI", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.19974", "abs": "https://arxiv.org/abs/2507.19974", "authors": ["Tongjie Li", "Jianhua Zhang", "Li Yu", "Yuxiang Zhang", "Yunlong Cai", "Fan Xu", "Guangyi Liu"], "title": "Digital Twin Channel-Enabled Online Resource Allocation for 6G: Principle, Architecture and Application", "comment": null, "summary": "Emerging applications such as holographic communication, autonomous driving,\nand the industrial Internet of Things impose stringent requirements on\nflexible, low-latency, and reliable resource allocation in 6G networks.\nConventional methods, which rely on statistical modeling, have proven effective\nin general contexts but may fail to achieve optimal performance in specific and\ndynamic environments. Furthermore, acquiring real-time channel state\ninformation (CSI) typically requires excessive pilot overhead. To address these\nchallenges, a digital twin channel (DTC)-enabled online optimization framework\nis proposed, in which DTC is employed to predict CSI based on environmental\nsensing. The predicted CSI is then utilized by lightweight game-theoretic\nalgorithms to perform online resource allocation in a timely and efficient\nmanner. Simulation results based on a digital replica of a realistic industrial\nworkshop demonstrate that the proposed method achieves throughput improvements\nof up to 11.5\\% compared with pilot-based ideal CSI schemes, validating its\neffectiveness for scalable, low-overhead, and environment-aware communication\nin future 6G networks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u5b57\u5b6a\u751f\u4fe1\u9053\uff08DTC\uff09\u7684\u5728\u7ebf\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e6G\u7f51\u7edc\u4e2d\u7075\u6d3b\u3001\u4f4e\u5ef6\u8fdf\u548c\u53ef\u9760\u7684\u8d44\u6e90\u5206\u914d\u3002", "motivation": "6G\u7f51\u7edc\u4e2d\u65b0\u5174\u5e94\u7528\uff08\u5982\u5168\u606f\u901a\u4fe1\u3001\u81ea\u52a8\u9a7e\u9a76\u548c\u5de5\u4e1a\u7269\u8054\u7f51\uff09\u5bf9\u8d44\u6e90\u5206\u914d\u63d0\u51fa\u4e86\u4e25\u683c\u8981\u6c42\uff0c\u4f20\u7edf\u7edf\u8ba1\u5efa\u6a21\u65b9\u6cd5\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u5b9e\u65f6\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\u83b7\u53d6\u6210\u672c\u9ad8\u3002", "method": "\u5229\u7528DTC\u9884\u6d4bCSI\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u535a\u5f08\u8bba\u7b97\u6cd5\u8fdb\u884c\u5728\u7ebf\u8d44\u6e90\u5206\u914d\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u6bd4\u57fa\u4e8e\u5bfc\u9891\u7684\u7406\u60f3CSI\u65b9\u6848\u541e\u5410\u91cf\u63d0\u5347\u9ad8\u8fbe11.5%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u672a\u67656G\u7f51\u7edc\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u4f4e\u5f00\u9500\u548c\u73af\u5883\u611f\u77e5\u7684\u901a\u4fe1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.19780", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19780", "abs": "https://arxiv.org/abs/2507.19780", "authors": ["Zhiming Liu", "Paul Hill", "Nantheera Anantrasirichai"], "title": "JDATT: A Joint Distillation Framework for Atmospheric Turbulence Mitigation and Target Detection", "comment": "Accepted by the 36th British Machine Vision Conference", "summary": "Atmospheric turbulence (AT) introduces severe degradations, such as rippling,\nblur, and intensity fluctuations, that hinder both image quality and downstream\nvision tasks like target detection. While recent deep learning-based approaches\nhave advanced AT mitigation using transformer and Mamba architectures, their\nhigh complexity and computational cost make them unsuitable for real-time\napplications, especially in resource-constrained settings such as remote\nsurveillance. Moreover, the common practice of separating turbulence mitigation\nand object detection leads to inefficiencies and suboptimal performance. To\naddress these challenges, we propose JDATT, a Joint Distillation framework for\nAtmospheric Turbulence mitigation and Target detection. JDATT integrates\nstate-of-the-art AT mitigation and detection modules and introduces a unified\nknowledge distillation strategy that compresses both components while\nminimizing performance loss. We employ a hybrid distillation scheme:\nfeature-level distillation via Channel-Wise Distillation (CWD) and Masked\nGenerative Distillation (MGD), and output-level distillation via\nKullback-Leibler divergence. Experiments on synthetic and real-world turbulence\ndatasets demonstrate that JDATT achieves superior visual restoration and\ndetection accuracy while significantly reducing model size and inference time,\nmaking it well-suited for real-time deployment.", "AI": {"tldr": "JDATT\u662f\u4e00\u4e2a\u8054\u5408\u84b8\u998f\u6846\u67b6\uff0c\u7528\u4e8e\u5927\u6c14\u6e4d\u6d41\u6291\u5236\u548c\u76ee\u6807\u68c0\u6d4b\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u538b\u7f29\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\u3002", "motivation": "\u5927\u6c14\u6e4d\u6d41\uff08AT\uff09\u5bfc\u81f4\u56fe\u50cf\u8d28\u91cf\u4e0b\u964d\uff0c\u5f71\u54cd\u4e0b\u6e38\u89c6\u89c9\u4efb\u52a1\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8ba1\u7b97\u590d\u6742\uff0c\u4e0d\u9002\u5408\u5b9e\u65f6\u5e94\u7528\uff0c\u4e14\u6e4d\u6d41\u6291\u5236\u4e0e\u76ee\u6807\u68c0\u6d4b\u5206\u79bb\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faJDATT\u6846\u67b6\uff0c\u7ed3\u5408AT\u6291\u5236\u548c\u76ee\u6807\u68c0\u6d4b\u6a21\u5757\uff0c\u91c7\u7528\u7279\u5f81\u7ea7\uff08CWD\u548cMGD\uff09\u548c\u8f93\u51fa\u7ea7\uff08KL\u6563\u5ea6\uff09\u77e5\u8bc6\u84b8\u998f\u7b56\u7565\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6e4d\u6d41\u6570\u636e\u96c6\u4e0a\uff0cJDATT\u5728\u89c6\u89c9\u6062\u590d\u548c\u68c0\u6d4b\u7cbe\u5ea6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u6a21\u578b\u5927\u5c0f\u548c\u63a8\u7406\u65f6\u95f4\u3002", "conclusion": "JDATT\u901a\u8fc7\u8054\u5408\u84b8\u998f\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5927\u6c14\u6e4d\u6d41\u6291\u5236\u548c\u76ee\u6807\u68c0\u6d4b\uff0c\u9002\u5408\u5b9e\u65f6\u90e8\u7f72\u3002"}}
{"id": "2507.19786", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19786", "abs": "https://arxiv.org/abs/2507.19786", "authors": ["Tianxiang Chen", "Zhentao Tan", "Xiaofan Bo", "Yue Wu", "Tao Gong", "Qi Chu", "Jieping Ye", "Nenghai Yu"], "title": "Flora: Effortless Context Construction to Arbitrary Length and Scale", "comment": null, "summary": "Effectively handling long contexts is challenging for Large Language Models\n(LLMs) due to the rarity of long texts, high computational demands, and\nsubstantial forgetting of short-context abilities. Recent approaches have\nattempted to construct long contexts for instruction tuning, but these methods\noften require LLMs or human interventions, which are both costly and limited in\nlength and diversity. Also, the drop in short-context performances of present\nlong-context LLMs remains significant. In this paper, we introduce Flora, an\neffortless (human/LLM-free) long-context construction strategy. Flora can\nmarkedly enhance the long-context performance of LLMs by arbitrarily assembling\nshort instructions based on categories and instructing LLMs to generate\nresponses based on long-context meta-instructions. This enables Flora to\nproduce contexts of arbitrary length and scale with rich diversity, while only\nslightly compromising short-context performance. Experiments on\nLlama3-8B-Instruct and QwQ-32B show that LLMs enhanced by Flora excel in three\nlong-context benchmarks while maintaining strong performances in short-context\ntasks. Our data-construction code is available at\n\\href{https://github.com/txchen-USTC/Flora}{https://github.com/txchen-USTC/Flora}.", "AI": {"tldr": "Flora\u662f\u4e00\u79cd\u65e0\u9700\u4eba\u5de5\u6216LLM\u5e72\u9884\u7684\u957f\u4e0a\u4e0b\u6587\u6784\u5efa\u7b56\u7565\uff0c\u901a\u8fc7\u7ec4\u5408\u77ed\u6307\u4ee4\u751f\u6210\u4efb\u610f\u957f\u5ea6\u548c\u591a\u6837\u6027\u7684\u957f\u4e0a\u4e0b\u6587\uff0c\u663e\u8457\u63d0\u5347LLMs\u7684\u957f\u4e0a\u4e0b\u6587\u6027\u80fd\uff0c\u540c\u65f6\u51e0\u4e4e\u4e0d\u5f71\u54cd\u77ed\u4e0a\u4e0b\u6587\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u6784\u5efa\u957f\u4e0a\u4e0b\u6587\u4f9d\u8d56LLMs\u6216\u4eba\u5de5\u5e72\u9884\uff0c\u6210\u672c\u9ad8\u4e14\u957f\u5ea6\u548c\u591a\u6837\u6027\u6709\u9650\uff0c\u4e14\u957f\u4e0a\u4e0b\u6587LLMs\u7684\u77ed\u4e0a\u4e0b\u6587\u6027\u80fd\u4e0b\u964d\u660e\u663e\u3002", "method": "Flora\u901a\u8fc7\u57fa\u4e8e\u7c7b\u522b\u7684\u77ed\u6307\u4ee4\u7ec4\u5408\u548c\u957f\u4e0a\u4e0b\u6587\u5143\u6307\u4ee4\u751f\u6210\u54cd\u5e94\uff0c\u5b9e\u73b0\u4efb\u610f\u957f\u5ea6\u548c\u591a\u6837\u6027\u7684\u4e0a\u4e0b\u6587\u6784\u5efa\u3002", "result": "\u5728Llama3-8B-Instruct\u548cQwQ-32B\u4e0a\uff0cFlora\u589e\u5f3a\u7684LLMs\u5728\u4e09\u4e2a\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u4fdd\u6301\u77ed\u4e0a\u4e0b\u6587\u4efb\u52a1\u7684\u5f3a\u6027\u80fd\u3002", "conclusion": "Flora\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u957f\u4e0a\u4e0b\u6587\u6784\u5efa\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347LLMs\u6027\u80fd\u4e14\u51e0\u4e4e\u4e0d\u5f71\u54cd\u77ed\u4e0a\u4e0b\u6587\u80fd\u529b\u3002"}}
{"id": "2507.19530", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19530", "abs": "https://arxiv.org/abs/2507.19530", "authors": ["Md Basit Azam", "Sarangthem Ibotombi Singh"], "title": "Clinical-Grade Blood Pressure Prediction in ICU Settings: An Ensemble Framework with Uncertainty Quantification and Cross-Institutional Validation", "comment": null, "summary": "Blood pressure (BP) monitoring is critical in in tensive care units (ICUs)\nwhere hemodynamic instability can\n  rapidly progress to cardiovascular collapse. Current machine\n  learning (ML) approaches suffer from three limitations: lack of\n  external validation, absence of uncertainty quantification, and\n  inadequate data leakage prevention. This study presents the\n  first comprehensive framework with novel algorithmic leakage\n  prevention, uncertainty quantification, and cross-institutional\n  validation for electronic health records (EHRs) based BP pre dictions. Our\nmethodology implemented systematic data leakage\n  prevention, uncertainty quantification through quantile regres sion, and\nexternal validation between the MIMIC-III and eICU\n  databases. An ensemble framework combines Gradient Boosting,\n  Random Forest, and XGBoost with 74 features across five\n  physiological domains. Internal validation achieved a clinically\n  acceptable performance (for SBP: R^2 = 0.86, RMSE = 6.03\n  mmHg; DBP: R^2 = 0.49, RMSE = 7.13 mmHg), meeting AAMI\n  standards. External validation showed 30% degradation with\n  critical limitations in patients with hypotensive. Uncertainty\n  quantification generated valid prediction intervals (80.3% SBP\n  and 79.9% DBP coverage), enabling risk-stratified protocols\n  with narrow intervals (< 15 mmHg) for standard monitoring\n  and wide intervals (> 30 mmHg) for manual verification. This\n  framework provides realistic deployment expectations for cross institutional\nAI-assisted BP monitoring in critical care settings.\n  The source code is publicly available at https://github.com/\n  mdbasit897/clinical-bp-prediction-ehr.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u7684\u8840\u538b\u9884\u6d4b\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u5916\u90e8\u9a8c\u8bc1\u3001\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u6570\u636e\u6cc4\u6f0f\u9884\u9632\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u91cd\u75c7\u76d1\u62a4\u75c5\u623f\uff08ICU\uff09\u4e2d\u8840\u538b\u76d1\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5916\u90e8\u9a8c\u8bc1\u4e0d\u8db3\u3001\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u6570\u636e\u6cc4\u6f0f\u95ee\u9898\u3002", "method": "\u91c7\u7528\u96c6\u6210\u6846\u67b6\uff08Gradient Boosting\u3001Random Forest\u3001XGBoost\uff09\u7ed3\u540874\u4e2a\u751f\u7406\u7279\u5f81\uff0c\u901a\u8fc7\u5206\u4f4d\u6570\u56de\u5f52\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u9632\u6b62\u6570\u636e\u6cc4\u6f0f\u3002", "result": "\u5185\u90e8\u9a8c\u8bc1\u8868\u73b0\u826f\u597d\uff08SBP: R\u00b2=0.86, RMSE=6.03 mmHg; DBP: R\u00b2=0.49, RMSE=7.13 mmHg\uff09\uff0c\u5916\u90e8\u9a8c\u8bc1\u6027\u80fd\u4e0b\u964d30%\u3002\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u751f\u6210\u6709\u6548\u9884\u6d4b\u533a\u95f4\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8de8\u673a\u6784AI\u8f85\u52a9\u8840\u538b\u76d1\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.20000", "categories": ["cs.AI", "cs.DL"], "pdf": "https://arxiv.org/pdf/2507.20000", "abs": "https://arxiv.org/abs/2507.20000", "authors": ["Renaud Fabre", "Daniel Egret", "Patrice Bellot"], "title": "Matching Game Preferences Through Dialogical Large Language Models: A Perspective", "comment": "28 pages, 1 figure. Published in Applied Sciences", "summary": "This perspective paper explores the future potential of \"conversational\nintelligence\" by examining how Large Language Models (LLMs) could be combined\nwith GRAPHYP's network system to better understand human conversations and\npreferences. Using recent research and case studies, we propose a conceptual\nframework that could make AI rea-soning transparent and traceable, allowing\nhumans to see and understand how AI reaches its conclusions. We present the\nconceptual perspective of \"Matching Game Preferences through Dialogical Large\nLanguage Models (D-LLMs),\" a proposed system that would allow multiple users to\nshare their different preferences through structured conversations. This\napproach envisions personalizing LLMs by embedding individual user preferences\ndirectly into how the model makes decisions. The proposed D-LLM framework would\nrequire three main components: (1) reasoning processes that could analyze\ndifferent search experiences and guide performance, (2) classification systems\nthat would identify user preference patterns, and (3) dialogue approaches that\ncould help humans resolve conflicting information. This perspective framework\naims to create an interpretable AI system where users could examine,\nunderstand, and combine the different human preferences that influence AI\nresponses, detected through GRAPHYP's search experience networks. The goal of\nthis perspective is to envision AI systems that would not only provide answers\nbut also show users how those answers were reached, making artificial\nintelligence more transparent and trustworthy for human decision-making.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548cGRAPHYP\u7f51\u7edc\u7cfb\u7edf\uff0c\u901a\u8fc7\u5bf9\u8bdd\u5f0f\u667a\u80fd\u63d0\u5347\u5bf9\u4eba\u7c7b\u5bf9\u8bdd\u548c\u504f\u597d\u7684\u7406\u89e3\uff0c\u63d0\u51fa\u900f\u660e\u53ef\u8ffd\u6eaf\u7684AI\u6846\u67b6D-LLM\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u900f\u660e\u5316\u7684AI\u63a8\u7406\u8fc7\u7a0b\uff0c\u589e\u5f3a\u7528\u6237\u5bf9AI\u51b3\u7b56\u7684\u7406\u89e3\u548c\u4fe1\u4efb\uff0c\u5b9e\u73b0\u4e2a\u6027\u5316\u504f\u597d\u5d4c\u5165\u3002", "method": "\u63d0\u51faD-LLM\u6846\u67b6\uff0c\u5305\u542b\u63a8\u7406\u8fc7\u7a0b\u3001\u504f\u597d\u5206\u7c7b\u7cfb\u7edf\u548c\u5bf9\u8bdd\u65b9\u6cd5\uff0c\u7ed3\u5408GRAPHYP\u7f51\u7edc\u5206\u6790\u7528\u6237\u504f\u597d\u3002", "result": "\u8bbe\u60f3\u6784\u5efa\u53ef\u89e3\u91ca\u7684AI\u7cfb\u7edf\uff0c\u7528\u6237\u53ef\u67e5\u770b\u548c\u7406\u89e3AI\u51b3\u7b56\u8fc7\u7a0b\uff0c\u63d0\u5347\u900f\u660e\u5ea6\u548c\u4fe1\u4efb\u5ea6\u3002", "conclusion": "\u901a\u8fc7D-LLM\u6846\u67b6\uff0c\u672a\u6765AI\u4e0d\u4ec5\u80fd\u63d0\u4f9b\u7b54\u6848\uff0c\u8fd8\u80fd\u5c55\u793a\u63a8\u7406\u8fc7\u7a0b\uff0c\u589e\u5f3a\u4eba\u673a\u534f\u4f5c\u7684\u900f\u660e\u6027\u3002"}}
{"id": "2507.19789", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19789", "abs": "https://arxiv.org/abs/2507.19789", "authors": ["Suhwan Cho", "Minhyeok Lee", "Jungho Lee", "Sunghun Yang", "Sangyoun Lee"], "title": "TransFlow: Motion Knowledge Transfer from Video Diffusion Models to Video Salient Object Detection", "comment": "ICCVW 2025", "summary": "Video salient object detection (SOD) relies on motion cues to distinguish\nsalient objects from backgrounds, but training such models is limited by scarce\nvideo datasets compared to abundant image datasets. Existing approaches that\nuse spatial transformations to create video sequences from static images fail\nfor motion-guided tasks, as these transformations produce unrealistic optical\nflows that lack semantic understanding of motion. We present TransFlow, which\ntransfers motion knowledge from pre-trained video diffusion models to generate\nrealistic training data for video SOD. Video diffusion models have learned rich\nsemantic motion priors from large-scale video data, understanding how different\nobjects naturally move in real scenes. TransFlow leverages this knowledge to\ngenerate semantically-aware optical flows from static images, where objects\nexhibit natural motion patterns while preserving spatial boundaries and\ntemporal coherence. Our method achieves improved performance across multiple\nbenchmarks, demonstrating effective motion knowledge transfer.", "AI": {"tldr": "TransFlow\u5229\u7528\u9884\u8bad\u7ec3\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\u751f\u6210\u903c\u771f\u7684\u89c6\u9891\u6570\u636e\uff0c\u63d0\u5347\u89c6\u9891\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u89c6\u9891\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u4f9d\u8d56\u8fd0\u52a8\u7ebf\u7d22\uff0c\u4f46\u89c6\u9891\u6570\u636e\u96c6\u7a00\u7f3a\uff0c\u73b0\u6709\u65b9\u6cd5\u751f\u6210\u7684\u89c6\u9891\u5e8f\u5217\u7f3a\u4e4f\u771f\u5b9e\u7684\u8fd0\u52a8\u8bed\u4e49\u3002", "method": "\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\u8fc1\u79fb\u8fd0\u52a8\u77e5\u8bc6\uff0c\u4ece\u9759\u6001\u56fe\u50cf\u751f\u6210\u8bed\u4e49\u611f\u77e5\u7684\u5149\u6d41\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u8fd0\u52a8\u77e5\u8bc6\u8fc1\u79fb\u7684\u6709\u6548\u6027\u3002", "conclusion": "TransFlow\u901a\u8fc7\u751f\u6210\u903c\u771f\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u89c6\u9891\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u4e2d\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002"}}
{"id": "2507.19823", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19823", "abs": "https://arxiv.org/abs/2507.19823", "authors": ["Dongquan Yang", "Yifan Yang", "Xiaotian Yu", "Xianbiao Qi", "Rong Xiao"], "title": "HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs", "comment": null, "summary": "Processing long-context inputs with large language models presents a\nsignificant challenge due to the enormous memory requirements of the Key-Value\n(KV) cache during inference. Existing KV cache compression methods exhibit\nnoticeable performance degradation when memory is reduced by more than 85%.\nAdditionally, strategies that leverage GPU-CPU collaboration for approximate\nattention remain underexplored in this setting. We propose HCAttention, a\nheterogeneous attention computation framework that integrates key quantization,\nvalue offloading, and dynamic KV eviction to enable efficient inference under\nextreme memory constraints. The method is compatible with existing transformer\narchitectures and does not require model fine-tuning. Experimental results on\nthe LongBench benchmark demonstrate that our approach preserves the accuracy of\nfull-attention model while shrinking the KV cache memory footprint to 25% of\nits original size. Remarkably, it stays competitive with only 12.5% of the\ncache, setting a new state-of-the-art in LLM KV cache compression. To the best\nof our knowledge, HCAttention is the first to extend the Llama-3-8B model to\nprocess 4 million tokens on a single A100 GPU with 80GB memory.", "AI": {"tldr": "HCAttention\u63d0\u51fa\u4e86\u4e00\u79cd\u5f02\u6784\u6ce8\u610f\u529b\u8ba1\u7b97\u6846\u67b6\uff0c\u901a\u8fc7\u5173\u952e\u91cf\u5316\u3001\u503c\u5378\u8f7d\u548c\u52a8\u6001KV\u6dd8\u6c70\uff0c\u5728\u6781\u7aef\u5185\u5b58\u9650\u5236\u4e0b\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\uff0c\u5c06KV\u7f13\u5b58\u5185\u5b58\u5360\u7528\u7f29\u5c0f\u81f325%\uff0c\u4e14\u65e0\u9700\u5fae\u8c03\u6a21\u578b\u3002", "motivation": "\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u8f93\u5165\u65f6\uff0cKV\u7f13\u5b58\u7684\u5de8\u5927\u5185\u5b58\u9700\u6c42\u6210\u4e3a\u4e3b\u8981\u6311\u6218\uff0c\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u5728\u5185\u5b58\u51cf\u5c11\u8d85\u8fc785%\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "method": "HCAttention\u7ed3\u5408\u5173\u952e\u91cf\u5316\u3001\u503c\u5378\u8f7d\u548c\u52a8\u6001KV\u6dd8\u6c70\uff0c\u517c\u5bb9\u73b0\u6709Transformer\u67b6\u6784\uff0c\u65e0\u9700\u5fae\u8c03\u3002", "result": "\u5728LongBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHCAttention\u5728KV\u7f13\u5b58\u5185\u5b58\u5360\u7528\u7f29\u5c0f\u81f325%\u65f6\u4fdd\u6301\u5168\u6ce8\u610f\u529b\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u751a\u81f3\u572812.5%\u65f6\u4ecd\u5177\u7ade\u4e89\u529b\u3002", "conclusion": "HCAttention\u5728\u6781\u7aef\u5185\u5b58\u9650\u5236\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u63a8\u7406\uff0c\u6269\u5c55\u4e86Llama-3-8B\u6a21\u578b\u5904\u74064\u767e\u4e07token\u7684\u80fd\u529b\uff0c\u6210\u4e3aKV\u7f13\u5b58\u538b\u7f29\u7684\u65b0\u6807\u6746\u3002"}}
{"id": "2507.19534", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2; I.7"], "pdf": "https://arxiv.org/pdf/2507.19534", "abs": "https://arxiv.org/abs/2507.19534", "authors": ["Ali Shakeri", "Wei Emma Zhang", "Amin Beheshti", "Weitong Chen", "Jian Yang", "Lishan Yang"], "title": "FedDPG: An Adaptive Yet Efficient Prompt-tuning Approach in Federated Learning Settings", "comment": "12 pages; Published to PAKDD'2025", "summary": "Pre-trained Language Models (PLMs) have demonstrated impressive performance\nin various NLP tasks. However, traditional fine-tuning methods for leveraging\nPLMs for downstream tasks entail significant computational overhead.\nPrompt-tuning has emerged as an efficient alternative that involves prepending\na limited number of parameters to the input sequence and only updating them\nwhile the PLM's parameters are frozen. However, this technique's prompts remain\nfixed for all inputs, reducing the model's flexibility. The Federated Learning\n(FL) technique has gained attention in recent years to address the growing\nconcerns around data privacy. However, challenges such as communication and\ncomputation limitations of clients still need to be addressed. To mitigate\nthese challenges, this paper introduces the Federated Dynamic Prompt Generator\n(FedDPG), which incorporates a dynamic prompt generator network to generate\ncontext-aware prompts based on the given input, ensuring flexibility and\nadaptability while prioritising data privacy in federated learning settings.\nOur experiments on three NLP benchmark datasets showcase that FedDPG\noutperforms the state-of-the-art parameter-efficient fine-tuning methods in\nterms of global model performance, and has significantly reduced the\ncalculation time and the number of parameters to be sent through the FL\nnetwork.", "AI": {"tldr": "FedDPG\u662f\u4e00\u79cd\u7ed3\u5408\u52a8\u6001\u63d0\u793a\u751f\u6210\u548c\u8054\u90a6\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u9ad8\u6a21\u578b\u7075\u6d3b\u6027\u548c\u6570\u636e\u9690\u79c1\u4fdd\u62a4\uff0c\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u548c\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u4f20\u7edf\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u800c\u8054\u90a6\u5b66\u4e60\u5728\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u5b58\u5728\u8ba1\u7b97\u548c\u901a\u4fe1\u9650\u5236\u3002", "method": "\u63d0\u51faFedDPG\uff0c\u901a\u8fc7\u52a8\u6001\u63d0\u793a\u751f\u6210\u7f51\u7edc\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u63d0\u793a\uff0c\u7ed3\u5408\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u5728\u4e09\u4e2aNLP\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cFedDPG\u5728\u5168\u5c40\u6a21\u578b\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u65f6\u95f4\u548c\u901a\u4fe1\u53c2\u6570\u3002", "conclusion": "FedDPG\u5728\u4fdd\u6301\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u548c\u6548\u7387\uff0c\u4e3a\u8054\u90a6\u5b66\u4e60\u4e2d\u7684NLP\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.20010", "categories": ["cs.AI", "cs.GT", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.20010", "abs": "https://arxiv.org/abs/2507.20010", "authors": ["M\u00fcge Fidan", "Esra Erdem"], "title": "Finding Personalized Good-Enough Solutions to Unsatisfiable Stable Roommates Problems", "comment": null, "summary": "The Stable Roommates problems are characterized by the preferences of agents\nover other agents as roommates. A solution is a partition of the agents into\npairs that are acceptable to each other (i.e., they are in the preference lists\nof each other), and the matching is stable (i.e., there do not exist any two\nagents who prefer each other to their roommates, and thus block the matching).\nMotivated by real-world applications, and considering that stable roommates\nproblems do not always have solutions, we continue our studies to compute\n\"good-enough\" matchings. In addition to the agents' habits and habitual\npreferences, we consider their networks of preferred friends, and introduce a\nmethod to generate personalized solutions to stable roommates problems. We\nillustrate the usefulness of our method with examples and empirical\nevaluations.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u7a33\u5b9a\u5ba4\u53cb\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7406\u4eba\u504f\u597d\u548c\u670b\u53cb\u7f51\u7edc\u7684\u4e2a\u6027\u5316\u5339\u914d\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u65e0\u7a33\u5b9a\u89e3\u65f6\u7684\u201c\u8db3\u591f\u597d\u201d\u5339\u914d\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u5e94\u7528\u4e2d\uff0c\u7a33\u5b9a\u5ba4\u53cb\u95ee\u9898\u5e76\u4e0d\u603b\u6709\u89e3\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5982\u4f55\u751f\u6210\u201c\u8db3\u591f\u597d\u201d\u7684\u5339\u914d\u3002", "method": "\u7ed3\u5408\u4ee3\u7406\u4eba\u7684\u4e60\u60ef\u504f\u597d\u548c\u670b\u53cb\u7f51\u7edc\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u4e2a\u6027\u5316\u5339\u914d\u7684\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u793a\u4f8b\u548c\u5b9e\u8bc1\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u89e3\u51b3\u65e0\u7a33\u5b9a\u89e3\u7684\u5ba4\u53cb\u5339\u914d\u95ee\u9898\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\u3002"}}
{"id": "2507.19790", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19790", "abs": "https://arxiv.org/abs/2507.19790", "authors": ["Suhwan Cho", "Minhyeok Lee", "Jungho Lee", "Donghyeong Kim", "Sangyoun Lee"], "title": "DepthFlow: Exploiting Depth-Flow Structural Correlations for Unsupervised Video Object Segmentation", "comment": "ICCVW 2025", "summary": "Unsupervised video object segmentation (VOS) aims to detect the most\nprominent object in a video. Recently, two-stream approaches that leverage both\nRGB images and optical flow have gained significant attention, but their\nperformance is fundamentally constrained by the scarcity of training data. To\naddress this, we propose DepthFlow, a novel data generation method that\nsynthesizes optical flow from single images. Our approach is driven by the key\ninsight that VOS models depend more on structural information embedded in flow\nmaps than on their geometric accuracy, and that this structure is highly\ncorrelated with depth. We first estimate a depth map from a source image and\nthen convert it into a synthetic flow field that preserves essential structural\ncues. This process enables the transformation of large-scale image-mask pairs\ninto image-flow-mask training pairs, dramatically expanding the data available\nfor network training. By training a simple encoder-decoder architecture with\nour synthesized data, we achieve new state-of-the-art performance on all public\nVOS benchmarks, demonstrating a scalable and effective solution to the data\nscarcity problem.", "AI": {"tldr": "DepthFlow\u901a\u8fc7\u4ece\u5355\u5f20\u56fe\u50cf\u5408\u6210\u5149\u6d41\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u65e0\u76d1\u7763\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u4e2d\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u65e0\u76d1\u7763\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u4e2d\u56e0\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u5bfc\u81f4\u7684\u6027\u80fd\u9650\u5236\u95ee\u9898\u3002", "method": "\u63d0\u51faDepthFlow\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u5355\u5f20\u56fe\u50cf\u4f30\u8ba1\u6df1\u5ea6\u56fe\u5e76\u8f6c\u6362\u4e3a\u5408\u6210\u5149\u6d41\uff0c\u751f\u6210\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5728\u6240\u6709\u516c\u5f00VOS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "DepthFlow\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u7f13\u89e3\u4e86\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002"}}
{"id": "2507.19867", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19867", "abs": "https://arxiv.org/abs/2507.19867", "authors": ["Anshul Chavda", "M Jagadeesh", "Chintalapalli Raja Kullayappa", "B Jayaprakash", "Medchalimi Sruthi", "Pushpak Bhattacharyya"], "title": "DRIVE: Disfluency-Rich Synthetic Dialog Data Generation Framework for Intelligent Vehicle Environments", "comment": null, "summary": "In-car conversational AI is becoming increasingly critical as autonomous\nvehicles and smart assistants gain widespread adoption. Yet, existing datasets\nfail to capture the spontaneous disfluencies such as hesitations, false starts,\nrepetitions, and self-corrections that characterize real driver-AI dialogs. To\naddress this, we introduce DiscoDrive, a synthetic corpus of 3500 multi-turn\ndialogs across seven automotive domains, generated using a two-stage,\nprompt-driven pipeline that dynamically integrates disfluencies during\nsynthesis. We show that DiscoDrive is effective both as a training resource,\nenabling DialoGPT-Medium and T5-Base to match or exceed KVRET-trained models on\nthe MultiWOZ 2.2 and Schema-Guided Dialogue (SGD) relevant test sets (BLEU-4\nimprovements of 0.26 to 0.61; METEOR +2.10; ROUGE-L +3.48; BERTScore F1\nimprovements of 1.35 to 3.48), and as a data augmentation resource in\nlow-resource scenarios, delivering additional gains of up to BLEU-4 +0.38,\nMETEOR +1.95, ROUGE-L +2.87, and BERTScore F1 +4.00 when combined with 10\npercent of KVRET. Human evaluations further confirm that dialogs sampled from\nDiscoDrive are rated higher than KVRET's human-collected dialogs in naturalness\n(3.8 vs 3.6) and coherence (4.1 vs 4.0), and are perceived as more\ncontext-appropriate than leading post-hoc methods (such as LARD), without\ncompromising clarity. DiscoDrive fills a critical gap in existing resources and\nserves as a versatile corpus for both training and augmenting conversational\nAI, enabling robust handling of real-world, disfluent in-car interactions.", "AI": {"tldr": "DiscoDrive\u662f\u4e00\u4e2a\u5408\u6210\u7684\u8f66\u8f7d\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u5305\u542b3500\u4e2a\u591a\u8f6e\u5bf9\u8bdd\uff0c\u52a8\u6001\u6574\u5408\u4e86\u771f\u5b9e\u5bf9\u8bdd\u4e2d\u7684\u4e0d\u6d41\u7545\u73b0\u8c61\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u8bddAI\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u672a\u80fd\u6355\u6349\u771f\u5b9e\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u4e0d\u6d41\u7545\u5bf9\u8bdd\u73b0\u8c61\uff0c\u5982\u72b9\u8c6b\u3001\u91cd\u590d\u548c\u81ea\u6211\u7ea0\u6b63\uff0c\u9650\u5236\u4e86\u5bf9\u8bddAI\u7684\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u3001\u63d0\u793a\u9a71\u52a8\u7684\u5408\u6210\u65b9\u6cd5\uff0c\u52a8\u6001\u751f\u6210\u5305\u542b\u4e0d\u6d41\u7545\u73b0\u8c61\u7684\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\u96c6\u3002", "result": "DiscoDrive\u5728\u8bad\u7ec3\u548c\u4f4e\u8d44\u6e90\u6570\u636e\u589e\u5f3a\u573a\u666f\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u4e86BLEU-4\u3001METEOR\u7b49\u6307\u6807\uff0c\u5e76\u5728\u4eba\u7c7b\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u73b0\u6709\u6570\u636e\u96c6\u3002", "conclusion": "DiscoDrive\u586b\u8865\u4e86\u73b0\u6709\u8d44\u6e90\u7684\u7a7a\u767d\uff0c\u4e3a\u8f66\u8f7d\u5bf9\u8bddAI\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u3001\u9c81\u68d2\u7684\u8bad\u7ec3\u548c\u589e\u5f3a\u6570\u636e\u3002"}}
{"id": "2507.19536", "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.mtrl-sci", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19536", "abs": "https://arxiv.org/abs/2507.19536", "authors": ["K. -C. Ouyang", "S. -Y. Zhang", "S. -L. Liu", "J. Tian", "Y. -H. Li", "H. Tong", "H. -Y. Bai", "W. -H. Wang", "Y. -C. Hu"], "title": "Graph Learning Metallic Glass Discovery from Wikipedia", "comment": "7 figures", "summary": "Synthesizing new materials efficiently is highly demanded in various research\nfields. However, this process is usually slow and expensive, especially for\nmetallic glasses, whose formation strongly depends on the optimal combinations\nof multiple elements to resist crystallization. This constraint renders only\nseveral thousands of candidates explored in the vast material space since 1960.\nRecently, data-driven approaches armed by advanced machine learning techniques\nprovided alternative routes for intelligent materials design. Due to data\nscarcity and immature material encoding, the conventional tabular data is\nusually mined by statistical learning algorithms, giving limited model\npredictability and generalizability. Here, we propose sophisticated data\nlearning from material network representations. The node elements are encoded\nfrom the Wikipedia by a language model. Graph neural networks with versatile\narchitectures are designed to serve as recommendation systems to explore hidden\nrelationships among materials. By employing Wikipedia embeddings from different\nlanguages, we assess the capability of natural languages in materials design.\nOur study proposes a new paradigm to harvesting new amorphous materials and\nbeyond with artificial intelligence.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6750\u6599\u7f51\u7edc\u8868\u793a\u548c\u8bed\u8a00\u6a21\u578b\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u5408\u6210\u65b0\u6750\u6599\uff0c\u7279\u522b\u662f\u91d1\u5c5e\u73bb\u7483\u3002", "motivation": "\u4f20\u7edf\u6750\u6599\u5408\u6210\u65b9\u6cd5\u6548\u7387\u4f4e\u4e14\u6210\u672c\u9ad8\uff0c\u6570\u636e\u7a00\u7f3a\u548c\u6750\u6599\u7f16\u7801\u4e0d\u6210\u719f\u9650\u5236\u4e86\u673a\u5668\u5b66\u4e60\u5728\u6750\u6599\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5229\u7528\u8bed\u8a00\u6a21\u578b\u7f16\u7801\u7ef4\u57fa\u767e\u79d1\u4e2d\u7684\u5143\u7d20\u8282\u70b9\uff0c\u8bbe\u8ba1\u56fe\u795e\u7ecf\u7f51\u7edc\u4f5c\u4e3a\u63a8\u8350\u7cfb\u7edf\uff0c\u63a2\u7d22\u6750\u6599\u95f4\u7684\u9690\u85cf\u5173\u7cfb\u3002", "result": "\u901a\u8fc7\u591a\u8bed\u8a00\u7ef4\u57fa\u767e\u79d1\u5d4c\u5165\u8bc4\u4f30\u81ea\u7136\u8bed\u8a00\u5728\u6750\u6599\u8bbe\u8ba1\u4e2d\u7684\u80fd\u529b\uff0c\u4e3a\u65b0\u6750\u6599\u53d1\u73b0\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4eba\u5de5\u667a\u80fd\u5728\u975e\u6676\u6750\u6599\u53ca\u5176\u4ed6\u9886\u57df\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.20067", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20067", "abs": "https://arxiv.org/abs/2507.20067", "authors": ["Sarat Chandra Bobbili", "Ujwal Dinesha", "Dheeraj Narasimha", "Srinivas Shakkottai"], "title": "PITA: Preference-Guided Inference-Time Alignment for LLM Post-Training", "comment": null, "summary": "Inference-time alignment enables large language models (LLMs) to generate\noutputs aligned with end-user preferences without further training. Recent\npost-training methods achieve this by using small guidance models to modify\ntoken generation during inference. These methods typically optimize a reward\nfunction KL-regularized by the original LLM taken as the reference policy. A\ncritical limitation, however, is their dependence on a pre-trained reward\nmodel, which requires fitting to human preference feedback--a potentially\nunstable process. In contrast, we introduce PITA, a novel framework that\nintegrates preference feedback directly into the LLM's token generation,\neliminating the need for a reward model. PITA learns a small preference-based\nguidance policy to modify token probabilities at inference time without LLM\nfine-tuning, reducing computational cost and bypassing the pre-trained reward\nmodel dependency. The problem is framed as identifying an underlying preference\ndistribution, solved through stochastic search and iterative refinement of the\npreference-based guidance model. We evaluate PITA across diverse tasks,\nincluding mathematical reasoning and sentiment classification, demonstrating\nits effectiveness in aligning LLM outputs with user preferences.", "AI": {"tldr": "PITA\u6846\u67b6\u901a\u8fc7\u76f4\u63a5\u6574\u5408\u504f\u597d\u53cd\u9988\u5230LLM\u7684token\u751f\u6210\u4e2d\uff0c\u65e0\u9700\u9884\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\uff0c\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u9ad8\u4e86\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9884\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\uff0c\u53ef\u80fd\u5bfc\u81f4\u4e0d\u7a33\u5b9a\uff1bPITA\u65e8\u5728\u6d88\u9664\u8fd9\u4e00\u4f9d\u8d56\uff0c\u76f4\u63a5\u5229\u7528\u504f\u597d\u53cd\u9988\u3002", "method": "PITA\u901a\u8fc7\u5b66\u4e60\u5c0f\u578b\u504f\u597d\u5f15\u5bfc\u7b56\u7565\uff0c\u5728\u63a8\u7406\u65f6\u4fee\u6539token\u6982\u7387\uff0c\u65e0\u9700\u5fae\u8c03LLM\uff0c\u91c7\u7528\u968f\u673a\u641c\u7d22\u548c\u8fed\u4ee3\u4f18\u5316\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u548c\u60c5\u611f\u5206\u7c7b\u7b49\u4efb\u52a1\u4e2d\uff0cPITA\u6709\u6548\u5bf9\u9f50LLM\u8f93\u51fa\u4e0e\u7528\u6237\u504f\u597d\u3002", "conclusion": "PITA\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u65b9\u6cd5\uff0c\u76f4\u63a5\u5229\u7528\u504f\u597d\u53cd\u9988\u4f18\u5316LLM\u8f93\u51fa\u3002"}}
{"id": "2507.19795", "categories": ["cs.CV", "cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19795", "abs": "https://arxiv.org/abs/2507.19795", "authors": ["Steven Walton"], "title": "Smaller, Faster, Cheaper: Architectural Designs for Efficient Machine Learning", "comment": "Ph.D. Thesis", "summary": "Major advancements in the capabilities of computer vision models have been\nprimarily fueled by rapid expansion of datasets, model parameters, and\ncomputational budgets, leading to ever-increasing demands on computational\ninfrastructure. However, as these models are deployed in increasingly diverse\nand resource-constrained environments, there is a pressing need for\narchitectures that can deliver high performance while requiring fewer\ncomputational resources.\n  This dissertation focuses on architectural principles through which models\ncan achieve increased performance while reducing their computational demands.\nWe discuss strides towards this goal through three directions. First, we focus\non data ingress and egress, investigating how information may be passed into\nand retrieved from our core neural processing units. This ensures that our\nmodels make the most of available data, allowing smaller architectures to\nbecome more performant. Second, we investigate modifications to the core neural\narchitecture, applied to restricted attention in vision transformers. This\nsection explores how removing uniform context windows in restricted attention\nincreases the expressivity of the underlying neural architecture. Third, we\nexplore the natural structures of Normalizing Flows and how we can leverage\nthese properties to better distill model knowledge.\n  These contributions demonstrate that careful design of neural architectures\ncan increase the efficiency of machine learning algorithms, allowing them to\nbecome smaller, faster, and cheaper.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u6539\u8fdb\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u8bbe\u8ba1\uff0c\u5728\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u7684\u540c\u65f6\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5bf9\u8ba1\u7b97\u8d44\u6e90\u7684\u9700\u6c42\u4e0d\u65ad\u589e\u52a0\uff0c\u4f46\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u67b6\u6784\u3002", "method": "\u901a\u8fc7\u4e09\u4e2a\u65b9\u5411\u5b9e\u73b0\u76ee\u6807\uff1a\u4f18\u5316\u6570\u636e\u8f93\u5165\u8f93\u51fa\u3001\u6539\u8fdb\u6838\u5fc3\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff08\u5982\u53d7\u9650\u6ce8\u610f\u529b\u673a\u5236\uff09\u3001\u5229\u7528Normalizing Flows\u7684\u7279\u6027\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u67b6\u6784\u53ef\u4ee5\u63d0\u9ad8\u7b97\u6cd5\u6548\u7387\uff0c\u4f7f\u6a21\u578b\u66f4\u5c0f\u3001\u66f4\u5feb\u3001\u66f4\u7ecf\u6d4e\u3002", "conclusion": "\u901a\u8fc7\u67b6\u6784\u4f18\u5316\uff0c\u53ef\u4ee5\u5728\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u7684\u540c\u65f6\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u3002"}}
{"id": "2507.19869", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19869", "abs": "https://arxiv.org/abs/2507.19869", "authors": ["Danil Fokin", "Monika P\u0142u\u017cyczka", "Grigory Golovin"], "title": "The Polish Vocabulary Size Test: A Novel Adaptive Test for Receptive Vocabulary Assessment", "comment": null, "summary": "We present the Polish Vocabulary Size Test (PVST), a novel tool for assessing\nthe receptive vocabulary size of both native and non-native Polish speakers.\nBased on Item Response Theory and Computerized Adaptive Testing, PVST\ndynamically adjusts to each test-taker's proficiency level, ensuring high\naccuracy while keeping the test duration short. To validate the test, a pilot\nstudy was conducted with 1.475 participants. Native Polish speakers\ndemonstrated significantly larger vocabularies compared to non-native speakers.\nFor native speakers, vocabulary size showed a strong positive correlation with\nage. The PVST is available online at myvocab.info/pl.", "AI": {"tldr": "PVST\u662f\u4e00\u79cd\u57fa\u4e8e\u9879\u76ee\u53cd\u5e94\u7406\u8bba\u548c\u8ba1\u7b97\u673a\u81ea\u9002\u5e94\u6d4b\u8bd5\u7684\u65b0\u5de5\u5177\uff0c\u7528\u4e8e\u8bc4\u4f30\u6ce2\u5170\u8bed\u6bcd\u8bed\u548c\u975e\u6bcd\u8bed\u8005\u7684\u8bcd\u6c47\u91cf\uff0c\u6d4b\u8bd5\u65f6\u95f4\u77ed\u4e14\u51c6\u786e\u5ea6\u9ad8\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u9ad8\u6548\u3001\u51c6\u786e\u7684\u5de5\u5177\u6765\u8bc4\u4f30\u6ce2\u5170\u8bed\u5b66\u4e60\u8005\u548c\u6bcd\u8bed\u8005\u7684\u8bcd\u6c47\u91cf\u3002", "method": "\u57fa\u4e8e\u9879\u76ee\u53cd\u5e94\u7406\u8bba\u548c\u8ba1\u7b97\u673a\u81ea\u9002\u5e94\u6d4b\u8bd5\uff0c\u52a8\u6001\u8c03\u6574\u6d4b\u8bd5\u5185\u5bb9\u4ee5\u9002\u5e94\u53d7\u8bd5\u8005\u7684\u6c34\u5e73\u3002", "result": "\u9a8c\u8bc1\u7814\u7a76\u8868\u660e\uff0c\u6bcd\u8bed\u8005\u7684\u8bcd\u6c47\u91cf\u663e\u8457\u5927\u4e8e\u975e\u6bcd\u8bed\u8005\uff0c\u4e14\u6bcd\u8bed\u8005\u7684\u8bcd\u6c47\u91cf\u4e0e\u5e74\u9f84\u5448\u6b63\u76f8\u5173\u3002", "conclusion": "PVST\u662f\u4e00\u79cd\u6709\u6548\u7684\u8bcd\u6c47\u91cf\u8bc4\u4f30\u5de5\u5177\uff0c\u5df2\u5728myvocab.info/pl\u4e0a\u7ebf\u3002"}}
{"id": "2507.19539", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.19539", "abs": "https://arxiv.org/abs/2507.19539", "authors": ["Khurram Javed", "Richard S. Sutton"], "title": "Swift-Sarsa: Fast and Robust Linear Control", "comment": "Presented at RLDM 2025", "summary": "Javed, Sharifnassab, and Sutton (2024) introduced a new algorithm for TD\nlearning -- SwiftTD -- that augments True Online TD($\\lambda$) with step-size\noptimization, a bound on the effective learning rate, and step-size decay. In\ntheir experiments SwiftTD outperformed True Online TD($\\lambda$) and\nTD($\\lambda$) on a variety of prediction tasks derived from Atari games, and\nits performance was robust to the choice of hyper-parameters. In this extended\nabstract we extend SwiftTD to work for control problems. We combine the key\nideas behind SwiftTD with True Online Sarsa($\\lambda$) to develop an on-policy\nreinforcement learning algorithm called $\\textit{Swift-Sarsa}$.\n  We propose a simple benchmark for linear on-policy control called the\n$\\textit{operant conditioning benchmark}$. The key challenge in the operant\nconditioning benchmark is that a very small subset of input signals are\nrelevant for decision making. The majority of the signals are noise sampled\nfrom a non-stationary distribution. To learn effectively, the agent must learn\nto differentiate between the relevant signals and the noisy signals, and\nminimize prediction errors by assigning credit to the weight parameters\nassociated with the relevant signals.\n  Swift-Sarsa, when applied to the operant conditioning benchmark, learned to\nassign credit to the relevant signals without any prior knowledge of the\nstructure of the problem. It opens the door for solution methods that learn\nrepresentations by searching over hundreds of millions of features in parallel\nwithout performance degradation due to noisy or bad features.", "AI": {"tldr": "Javed\u7b49\u4eba\uff082024\uff09\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684TD\u5b66\u4e60\u7b97\u6cd5SwiftTD\uff0c\u5e76\u5c06\u5176\u6269\u5c55\u5230\u63a7\u5236\u95ee\u9898\uff0c\u5f00\u53d1\u4e86Swift-Sarsa\u7b97\u6cd5\u3002\u8be5\u7b97\u6cd5\u5728\u64cd\u4f5c\u6761\u4ef6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u533a\u5206\u76f8\u5173\u4fe1\u53f7\u4e0e\u566a\u58f0\u3002", "motivation": "\u6539\u8fdb\u73b0\u6709\u7684TD\u5b66\u4e60\u7b97\u6cd5\uff0c\u4f7f\u5176\u5728\u63a7\u5236\u95ee\u9898\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u5e76\u89e3\u51b3\u4fe1\u53f7\u566a\u58f0\u95ee\u9898\u3002", "method": "\u7ed3\u5408SwiftTD\u548cTrue Online Sarsa(\u03bb)\u5f00\u53d1Swift-Sarsa\u7b97\u6cd5\uff0c\u5e94\u7528\u4e8e\u64cd\u4f5c\u6761\u4ef6\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "Swift-Sarsa\u80fd\u6709\u6548\u533a\u5206\u76f8\u5173\u4fe1\u53f7\u4e0e\u566a\u58f0\uff0c\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\u3002", "conclusion": "Swift-Sarsa\u4e3a\u5927\u89c4\u6a21\u7279\u5f81\u641c\u7d22\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u6027\u80fd\u4e0d\u53d7\u566a\u58f0\u7279\u5f81\u5f71\u54cd\u3002"}}
{"id": "2507.20143", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20143", "abs": "https://arxiv.org/abs/2507.20143", "authors": ["Zhonghan Ge", "Yuanyang Zhu", "Chunlin Chen"], "title": "Concept Learning for Cooperative Multi-Agent Reinforcement Learning", "comment": "IEEE-China Conference on System Simulation Technology and its\n  Applications, 2025", "summary": "Despite substantial progress in applying neural networks (NN) to multi-agent\nreinforcement learning (MARL) areas, they still largely suffer from a lack of\ntransparency and interoperability. However, its implicit cooperative mechanism\nis not yet fully understood due to black-box networks. In this work, we study\nan interpretable value decomposition framework via concept bottleneck models,\nwhich promote trustworthiness by conditioning credit assignment on an\nintermediate level of human-like cooperation concepts. To address this problem,\nwe propose a novel value-based method, named Concepts learning for Multi-agent\nQ-learning (CMQ), that goes beyond the current performance-vs-interpretability\ntrade-off by learning interpretable cooperation concepts. CMQ represents each\ncooperation concept as a supervised vector, as opposed to existing models where\nthe information flowing through their end-to-end mechanism is concept-agnostic.\nIntuitively, using individual action value conditioning on global state\nembeddings to represent each concept allows for extra cooperation\nrepresentation capacity. Empirical evaluations on the StarCraft II\nmicromanagement challenge and level-based foraging (LBF) show that CMQ achieves\nsuperior performance compared with the state-of-the-art counterparts. The\nresults also demonstrate that CMQ provides more cooperation concept\nrepresentation capturing meaningful cooperation modes, and supports test-time\nconcept interventions for detecting potential biases of cooperation mode and\nidentifying spurious artifacts that impact cooperation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u4ef7\u503c\u5206\u89e3\u6846\u67b6CMQ\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u900f\u660e\u6027\u548c\u4e92\u64cd\u4f5c\u6027\u95ee\u9898\uff0c\u5e76\u5728\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "motivation": "\u5c3d\u7ba1\u795e\u7ecf\u7f51\u7edc\u5728\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5176\u9ed1\u76d2\u7279\u6027\u5bfc\u81f4\u900f\u660e\u6027\u548c\u4e92\u64cd\u4f5c\u6027\u4e0d\u8db3\uff0c\u9690\u542b\u7684\u5408\u4f5c\u673a\u5236\u5c1a\u672a\u5b8c\u5168\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u4e86CMQ\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u53ef\u89e3\u91ca\u7684\u5408\u4f5c\u6982\u5ff5\uff0c\u5c06\u6bcf\u4e2a\u6982\u5ff5\u8868\u793a\u4e3a\u76d1\u7763\u5411\u91cf\uff0c\u5229\u7528\u5168\u5c40\u72b6\u6001\u5d4c\u5165\u5bf9\u4e2a\u4f53\u52a8\u4f5c\u4ef7\u503c\u8fdb\u884c\u6761\u4ef6\u5316\u8868\u793a\u3002", "result": "\u5728StarCraft II\u548cLBF\u4efb\u52a1\u4e2d\uff0cCMQ\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u6355\u6349\u6709\u610f\u4e49\u7684\u5408\u4f5c\u6a21\u5f0f\u5e76\u652f\u6301\u6982\u5ff5\u5e72\u9884\u3002", "conclusion": "CMQ\u5728\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u7a81\u7834\uff0c\u4e3a\u5408\u4f5c\u6a21\u5f0f\u7684\u68c0\u6d4b\u548c\u6f5c\u5728\u504f\u5dee\u7684\u8bc6\u522b\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2507.19804", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19804", "abs": "https://arxiv.org/abs/2507.19804", "authors": ["Peng Cai", "Qiang Li", "Kaicheng Yang", "Dong Guo", "Jia Li", "Nan Zhou", "Xiang An", "Ninghua Yang", "Jiankang Deng"], "title": "ForCenNet: Foreground-Centric Network for Document Image Rectification", "comment": "Accepted by ICCV25, 16 pages, 14 figures", "summary": "Document image rectification aims to eliminate geometric deformation in\nphotographed documents to facilitate text recognition. However, existing\nmethods often neglect the significance of foreground elements, which provide\nessential geometric references and layout information for document image\ncorrection. In this paper, we introduce Foreground-Centric Network (ForCenNet)\nto eliminate geometric distortions in document images. Specifically, we\ninitially propose a foreground-centric label generation method, which extracts\ndetailed foreground elements from an undistorted image. Then we introduce a\nforeground-centric mask mechanism to enhance the distinction between readable\nand background regions. Furthermore, we design a curvature consistency loss to\nleverage the detailed foreground labels to help the model understand the\ndistorted geometric distribution. Extensive experiments demonstrate that\nForCenNet achieves new state-of-the-art on four real-world benchmarks, such as\nDocUNet, DIR300, WarpDoc, and DocReal. Quantitative analysis shows that the\nproposed method effectively undistorts layout elements, such as text lines and\ntable borders. The resources for further comparison are provided at\nhttps://github.com/caipeng328/ForCenNet.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faForeground-Centric Network (ForCenNet)\uff0c\u901a\u8fc7\u5173\u6ce8\u524d\u666f\u5143\u7d20\u6d88\u9664\u6587\u6863\u56fe\u50cf\u4e2d\u7684\u51e0\u4f55\u53d8\u5f62\uff0c\u663e\u8457\u63d0\u5347\u6587\u672c\u8bc6\u522b\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u524d\u666f\u5143\u7d20\u7684\u91cd\u8981\u6027\uff0c\u800c\u524d\u666f\u5143\u7d20\u4e3a\u6587\u6863\u56fe\u50cf\u6821\u6b63\u63d0\u4f9b\u5173\u952e\u51e0\u4f55\u53c2\u8003\u548c\u5e03\u5c40\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u524d\u666f\u4e2d\u5fc3\u6807\u7b7e\u751f\u6210\u65b9\u6cd5\u3001\u524d\u666f\u4e2d\u5fc3\u63a9\u7801\u673a\u5236\u548c\u66f2\u7387\u4e00\u81f4\u6027\u635f\u5931\uff0c\u5229\u7528\u8be6\u7ec6\u524d\u666f\u6807\u7b7e\u5e2e\u52a9\u6a21\u578b\u7406\u89e3\u53d8\u5f62\u51e0\u4f55\u5206\u5e03\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u65b0\u6700\u4f18\u6027\u80fd\uff0c\u6709\u6548\u6821\u6b63\u6587\u672c\u884c\u548c\u8868\u683c\u8fb9\u6846\u7b49\u5e03\u5c40\u5143\u7d20\u3002", "conclusion": "ForCenNet\u901a\u8fc7\u5173\u6ce8\u524d\u666f\u5143\u7d20\uff0c\u663e\u8457\u63d0\u5347\u6587\u6863\u56fe\u50cf\u6821\u6b63\u6548\u679c\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u8d44\u6e90\u652f\u6301\u3002"}}
{"id": "2507.19885", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19885", "abs": "https://arxiv.org/abs/2507.19885", "authors": ["Cesar Augusto Madid Truyts", "Amanda Gomes Rabelo", "Gabriel Mesquita de Souza", "Daniel Scaldaferri Lages", "Adriano Jose Pereira", "Uri Adrian Prync Flato", "Eduardo Pontes dos Reis", "Joaquim Edson Vieira", "Paulo Sergio Panse Silveira", "Edson Amaro Junior"], "title": "Zero-shot Performance of Generative AI in Brazilian Portuguese Medical Exam", "comment": null, "summary": "Artificial intelligence (AI) has shown the potential to revolutionize\nhealthcare by improving diagnostic accuracy, optimizing workflows, and\npersonalizing treatment plans. Large Language Models (LLMs) and Multimodal\nLarge Language Models (MLLMs) have achieved notable advancements in natural\nlanguage processing and medical applications. However, the evaluation of these\nmodels has focused predominantly on the English language, leading to potential\nbiases in their performance across different languages.\n  This study investigates the capability of six LLMs (GPT-4.0 Turbo,\nLLaMA-3-8B, LLaMA-3-70B, Mixtral 8x7B Instruct, Titan Text G1-Express, and\nCommand R+) and four MLLMs (Claude-3.5-Sonnet, Claude-3-Opus, Claude-3-Sonnet,\nand Claude-3-Haiku) to answer questions written in Brazilian spoken portuguese\nfrom the medical residency entrance exam of the Hospital das Cl\\'inicas da\nFaculdade de Medicina da Universidade de S\\~ao Paulo (HCFMUSP) - the largest\nhealth complex in South America. The performance of the models was benchmarked\nagainst human candidates, analyzing accuracy, processing time, and coherence of\nthe generated explanations.\n  The results show that while some models, particularly Claude-3.5-Sonnet and\nClaude-3-Opus, achieved accuracy levels comparable to human candidates,\nperformance gaps persist, particularly in multimodal questions requiring image\ninterpretation. Furthermore, the study highlights language disparities,\nemphasizing the need for further fine-tuning and data set augmentation for\nnon-English medical AI applications.\n  Our findings reinforce the importance of evaluating generative AI in various\nlinguistic and clinical settings to ensure a fair and reliable deployment in\nhealthcare. Future research should explore improved training methodologies,\nimproved multimodal reasoning, and real-world clinical integration of AI-driven\nmedical assistance.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u516d\u79cdLLMs\u548c\u56db\u79cdMLLMs\u5728\u5df4\u897f\u8461\u8404\u7259\u8bed\u533b\u5b66\u8003\u8bd5\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u90e8\u5206\u6a21\u578b\u8868\u73b0\u63a5\u8fd1\u4eba\u7c7b\uff0c\u4f46\u591a\u6a21\u6001\u95ee\u9898\u548c\u8bed\u8a00\u5dee\u5f02\u4ecd\u9700\u6539\u8fdb\u3002", "motivation": "\u5f53\u524dAI\u6a21\u578b\u8bc4\u4f30\u591a\u96c6\u4e2d\u4e8e\u82f1\u8bed\uff0c\u53ef\u80fd\u5bfc\u81f4\u5176\u4ed6\u8bed\u8a00\u8868\u73b0\u504f\u5dee\uff0c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u5df4\u897f\u8461\u8404\u7259\u8bed\u533b\u5b66\u8003\u8bd5\u9898\u76ee\uff0c\u5bf9\u6bd4\u516d\u79cdLLMs\u548c\u56db\u79cdMLLMs\u4e0e\u4eba\u7c7b\u8003\u751f\u7684\u51c6\u786e\u6027\u3001\u5904\u7406\u65f6\u95f4\u548c\u89e3\u91ca\u8fde\u8d2f\u6027\u3002", "result": "\u90e8\u5206\u6a21\u578b\uff08\u5982Claude-3.5-Sonnet\u548cClaude-3-Opus\uff09\u51c6\u786e\u6027\u63a5\u8fd1\u4eba\u7c7b\uff0c\u4f46\u591a\u6a21\u6001\u95ee\u9898\u548c\u8bed\u8a00\u5dee\u5f02\u4ecd\u5b58\u5728\u5dee\u8ddd\u3002", "conclusion": "\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u975e\u82f1\u8bed\u533b\u5b66AI\u5e94\u7528\uff0c\u5f3a\u8c03\u591a\u8bed\u8a00\u548c\u4e34\u5e8a\u73af\u5883\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.19547", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.19547", "abs": "https://arxiv.org/abs/2507.19547", "authors": ["Pablo Peiro-Corbacho", "Long Lin", "Pablo \u00c1vila", "Alejandro Carta-Bergaz", "\u00c1ngel Arenal", "Carlos Sevilla-Salcedo", "Gonzalo R. R\u00edos-Mu\u00f1oz"], "title": "Latent Representations of Intracardiac Electrograms for Atrial Fibrillation Driver Detection", "comment": null, "summary": "Atrial Fibrillation (AF) is the most prevalent sustained arrhythmia, yet\ncurrent ablation therapies, including pulmonary vein isolation, are frequently\nineffective in persistent AF due to the involvement of non-pulmonary vein\ndrivers. This study proposes a deep learning framework using convolutional\nautoencoders for unsupervised feature extraction from unipolar and bipolar\nintracavitary electrograms (EGMs) recorded during AF in ablation studies. These\nlatent representations of atrial electrical activity enable the\ncharacterization and automation of EGM analysis, facilitating the detection of\nAF drivers.\n  The database consisted of 11,404 acquisitions recorded from 291 patients,\ncontaining 228,080 unipolar EGMs and 171,060 bipolar EGMs. The autoencoders\nsuccessfully learned latent representations with low reconstruction loss,\npreserving the morphological features. The extracted embeddings allowed\ndownstream classifiers to detect rotational and focal activity with moderate\nperformance (AUC 0.73-0.76) and achieved high discriminative performance in\nidentifying atrial EGM entanglement (AUC 0.93).\n  The proposed method can operate in real-time and enables integration into\nclinical electroanatomical mapping systems to assist in identifying\narrhythmogenic regions during ablation procedures. This work highlights the\npotential of unsupervised learning to uncover physiologically meaningful\nfeatures from intracardiac signals.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5377\u79ef\u81ea\u7f16\u7801\u5668\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5fc3\u623f\u98a4\u52a8\uff08AF\uff09\u7684\u5fc3\u5185\u7535\u56fe\u4e2d\u65e0\u76d1\u7763\u63d0\u53d6\u7279\u5f81\uff0c\u4ee5\u68c0\u6d4bAF\u9a71\u52a8\u6e90\u3002", "motivation": "\u5f53\u524d\u6d88\u878d\u7597\u6cd5\u5bf9\u6301\u7eed\u6027AF\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u975e\u80ba\u9759\u8109\u9a71\u52a8\u6e90\u7684\u5b58\u5728\u3002\u9700\u81ea\u52a8\u5316\u5206\u6790\u5fc3\u5185\u7535\u56fe\u4ee5\u8bc6\u522bAF\u9a71\u52a8\u6e90\u3002", "method": "\u4f7f\u7528\u5377\u79ef\u81ea\u7f16\u7801\u5668\u4ece\u5355\u6781\u548c\u53cc\u6781\u5fc3\u5185\u7535\u56fe\u4e2d\u65e0\u76d1\u7763\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u5229\u7528\u4e0b\u6e38\u5206\u7c7b\u5668\u68c0\u6d4b\u65cb\u8f6c\u548c\u5c40\u7076\u6d3b\u52a8\u3002", "result": "\u6a21\u578b\u5728\u68c0\u6d4b\u65cb\u8f6c\u548c\u5c40\u7076\u6d3b\u52a8\u65f6\u8868\u73b0\u4e2d\u7b49\uff08AUC 0.73-0.76\uff09\uff0c\u8bc6\u522b\u5fc3\u623f\u7535\u56fe\u7ea0\u7f20\u65f6\u8868\u73b0\u4f18\u5f02\uff08AUC 0.93\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u5b9e\u65f6\u8fd0\u884c\u5e76\u6574\u5408\u5230\u4e34\u5e8a\u7cfb\u7edf\u4e2d\uff0c\u5c55\u793a\u4e86\u65e0\u76d1\u7763\u5b66\u4e60\u5728\u63d0\u53d6\u751f\u7406\u7279\u5f81\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.20150", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20150", "abs": "https://arxiv.org/abs/2507.20150", "authors": ["Xingcheng Xu"], "title": "The Policy Cliff: A Theoretical Analysis of Reward-Policy Maps in Large Language Models", "comment": null, "summary": "Reinforcement learning (RL) plays a crucial role in shaping the behavior of\nlarge language and reasoning models (LLMs/LRMs). However, it often produces\nbrittle and unstable policies, leading to critical failures such as spurious\nreasoning, deceptive alignment, and instruction disobedience that undermine the\ntrustworthiness and safety of LLMs/LRMs. Currently, these issues lack a unified\ntheoretical explanation and are typically addressed using ad-hoc heuristics.\nThis paper presents a rigorous mathematical framework for analyzing the\nstability of the mapping from a reward function to the optimal policy. We show\nthat policy brittleness often stems from non-unique optimal actions, a common\noccurrence when multiple valid traces exist in a reasoning task. This\ntheoretical lens provides a unified explanation for a range of seemingly\ndisparate failures, reframing them as rational outcomes of optimizing rewards\nthat may be incomplete or noisy, especially in the presence of action\ndegeneracy. We extend this analysis from the fundamental single-reward setting\nto the more realistic multi-reward RL across diverse domains, showing how\nstability is governed by an \"effective reward\" aggregation mechanism. We also\nprove that entropy regularization restores policy stability at the cost of\nincreased stochasticity. Our framework provides a unified explanation for\nrecent empirical findings on deceptive reasoning, instruction-following\ntrade-offs, and RLHF-induced sophistry, and is further validated through\nperturbation experiments in multi-reward RL. This work advances\npolicy-stability analysis from empirical heuristics towards a principled\ntheory, offering essential insights for designing safer and more trustworthy AI\nsystems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6570\u5b66\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e2d\u5956\u52b1\u51fd\u6570\u5230\u6700\u4f18\u7b56\u7565\u6620\u5c04\u7684\u7a33\u5b9a\u6027\uff0c\u89e3\u91ca\u4e86\u7b56\u7565\u8106\u5f31\u6027\u7684\u6839\u6e90\uff0c\u5e76\u63a2\u8ba8\u4e86\u71b5\u6b63\u5219\u5316\u5bf9\u7a33\u5b9a\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u5927\u578b\u8bed\u8a00\u548c\u63a8\u7406\u6a21\u578b\uff08LLMs/LRMs\uff09\u4e2d\u8868\u73b0\u4e0d\u7a33\u5b9a\uff0c\u5bfc\u81f4\u865a\u5047\u63a8\u7406\u3001\u6b3a\u9a97\u6027\u5bf9\u9f50\u548c\u6307\u4ee4\u4e0d\u670d\u4ece\u7b49\u95ee\u9898\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u8bba\u89e3\u91ca\u3002", "method": "\u901a\u8fc7\u6570\u5b66\u6846\u67b6\u5206\u6790\u5956\u52b1\u51fd\u6570\u5230\u6700\u4f18\u7b56\u7565\u7684\u6620\u5c04\u7a33\u5b9a\u6027\uff0c\u63a2\u8ba8\u975e\u552f\u4e00\u6700\u4f18\u52a8\u4f5c\u7684\u5f71\u54cd\uff0c\u5e76\u6269\u5c55\u5230\u591a\u5956\u52b1RL\u548c\u71b5\u6b63\u5219\u5316\u7684\u4f5c\u7528\u3002", "result": "\u7406\u8bba\u5206\u6790\u63ed\u793a\u4e86\u7b56\u7565\u8106\u5f31\u6027\u7684\u6839\u6e90\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u71b5\u6b63\u5219\u5316\u80fd\u6062\u590d\u7a33\u5b9a\u6027\u4f46\u589e\u52a0\u968f\u673a\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8bbe\u8ba1\u66f4\u5b89\u5168\u3001\u53ef\u4fe1\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5c06\u653f\u7b56\u7a33\u5b9a\u6027\u5206\u6790\u4ece\u7ecf\u9a8c\u542f\u53d1\u63d0\u5347\u5230\u539f\u5219\u6027\u7406\u8bba\u3002"}}
