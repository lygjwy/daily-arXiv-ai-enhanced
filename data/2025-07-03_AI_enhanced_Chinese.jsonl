{"id": "2507.01019", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.01019", "abs": "https://arxiv.org/abs/2507.01019", "authors": ["Imran Mirza", "Cole Huang", "Ishwara Vasista", "Rohan Patil", "Asli Akalin", "Sean O'Brien", "Kevin Zhu"], "title": "MALIBU Benchmark: Multi-Agent LLM Implicit Bias Uncovered", "comment": "Accepted to Building Trust in LLMs @ ICLR 2025 and NAACL SRW 2025", "summary": "Multi-agent systems, which consist of multiple AI models interacting within a\nshared environment, are increasingly used for persona-based interactions.\nHowever, if not carefully designed, these systems can reinforce implicit biases\nin large language models (LLMs), raising concerns about fairness and equitable\nrepresentation. We present MALIBU, a novel benchmark developed to assess the\ndegree to which LLM-based multi-agent systems implicitly reinforce social\nbiases and stereotypes. MALIBU evaluates bias in LLM-based multi-agent systems\nthrough scenario-based assessments. AI models complete tasks within predefined\ncontexts, and their responses undergo evaluation by an LLM-based multi-agent\njudging system in two phases. In the first phase, judges score responses\nlabeled with specific demographic personas (e.g., gender, race, religion)\nacross four metrics. In the second phase, judges compare paired responses\nassigned to different personas, scoring them and selecting the superior\nresponse. Our study quantifies biases in LLM-generated outputs, revealing that\nbias mitigation may favor marginalized personas over true neutrality,\nemphasizing the need for nuanced detection, balanced fairness strategies, and\ntransparent evaluation benchmarks in multi-agent systems.", "AI": {"tldr": "MALIBU\u662f\u4e00\u4e2a\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u9690\u542b\u7684\u793e\u4f1a\u504f\u89c1\u548c\u523b\u677f\u5370\u8c61\u3002\u901a\u8fc7\u573a\u666f\u8bc4\u4f30\u548c\u4e24\u9636\u6bb5\u8bc4\u5206\uff0c\u7814\u7a76\u53d1\u73b0\u504f\u89c1\u7f13\u89e3\u53ef\u80fd\u504f\u5411\u8fb9\u7f18\u5316\u7fa4\u4f53\uff0c\u800c\u975e\u771f\u6b63\u4e2d\u7acb\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u53ef\u80fd\u5f3a\u5316LLM\u4e2d\u7684\u9690\u6027\u504f\u89c1\uff0c\u5f15\u53d1\u516c\u5e73\u6027\u548c\u4ee3\u8868\u6027\u62c5\u5fe7\uff0c\u9700\u5f00\u53d1\u8bc4\u4f30\u5de5\u5177\u3002", "method": "MALIBU\u901a\u8fc7\u573a\u666f\u4efb\u52a1\u548c\u4e24\u9636\u6bb5\u8bc4\u5206\uff08\u5355\u54cd\u5e94\u8bc4\u5206\u548c\u6210\u5bf9\u6bd4\u8f83\uff09\u8bc4\u4f30LLM\u8f93\u51fa\u7684\u504f\u89c1\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u504f\u89c1\u7f13\u89e3\u53ef\u80fd\u8fc7\u5ea6\u504f\u5411\u8fb9\u7f18\u5316\u7fa4\u4f53\uff0c\u800c\u975e\u5b9e\u73b0\u771f\u6b63\u4e2d\u7acb\uff0c\u9700\u66f4\u7ec6\u81f4\u7684\u516c\u5e73\u7b56\u7565\u3002", "conclusion": "\u9700\u5f00\u53d1\u900f\u660e\u3001\u5e73\u8861\u7684\u8bc4\u4f30\u57fa\u51c6\u548c\u7b56\u7565\uff0c\u4ee5\u51cf\u5c11\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u504f\u89c1\u3002"}}
{"id": "2507.01160", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01160", "abs": "https://arxiv.org/abs/2507.01160", "authors": ["Huiling You", "Samia Touileb", "Erik Velldal", "Lilja \u00d8vrelid"], "title": "Event-based evaluation of abstractive news summarization", "comment": "to appear at GEM2 workshop@ACL 2025", "summary": "An abstractive summary of a news article contains its most important\ninformation in a condensed version. The evaluation of automatically generated\nsummaries by generative language models relies heavily on human-authored\nsummaries as gold references, by calculating overlapping units or similarity\nscores. News articles report events, and ideally so should the summaries. In\nthis work, we propose to evaluate the quality of abstractive summaries by\ncalculating overlapping events between generated summaries, reference\nsummaries, and the original news articles. We experiment on a richly annotated\nNorwegian dataset comprising both events annotations and summaries authored by\nexpert human annotators. Our approach provides more insight into the event\ninformation contained in the summaries.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u91cd\u53e0\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u751f\u6210\u5f0f\u6458\u8981\u7684\u8d28\u91cf\uff0c\u901a\u8fc7\u6bd4\u8f83\u751f\u6210\u6458\u8981\u3001\u53c2\u8003\u6458\u8981\u548c\u539f\u6587\u4e2d\u7684\u4e8b\u4ef6\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4eba\u5de5\u6458\u8981\u4f5c\u4e3a\u9ec4\u91d1\u6807\u51c6\uff0c\u4f46\u5ffd\u7565\u4e86\u4e8b\u4ef6\u4fe1\u606f\u7684\u5b8c\u6574\u6027\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u5f0f\u3002", "method": "\u63d0\u51fa\u8ba1\u7b97\u751f\u6210\u6458\u8981\u3001\u53c2\u8003\u6458\u8981\u548c\u539f\u6587\u4e4b\u95f4\u7684\u4e8b\u4ef6\u91cd\u53e0\uff0c\u5e76\u5728\u632a\u5a01\u8bed\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u65b9\u6cd5\u80fd\u66f4\u6df1\u5165\u5206\u6790\u6458\u8981\u4e2d\u5305\u542b\u7684\u4e8b\u4ef6\u4fe1\u606f\u3002", "conclusion": "\u57fa\u4e8e\u4e8b\u4ef6\u91cd\u53e0\u7684\u8bc4\u4f30\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u6458\u8981\u8d28\u91cf\u5206\u6790\u3002"}}
{"id": "2507.01170", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01170", "abs": "https://arxiv.org/abs/2507.01170", "authors": ["Simon B\u00f6rjesson", "Erik Ersmark", "Pierre Nugues"], "title": "Matching and Linking Entries in Historical Swedish Encyclopedias", "comment": "10 pages, 3 figures", "summary": "The \\textit{Nordisk familjebok} is a Swedish encyclopedia from the 19th and\n20th centuries. It was written by a team of experts and aimed to be an\nintellectual reference, stressing precision and accuracy. This encyclopedia had\nfour main editions remarkable by their size, ranging from 20 to 38 volumes. As\na consequence, the \\textit{Nordisk familjebok} had a considerable influence in\nuniversities, schools, the media, and society overall. As new editions were\nreleased, the selection of entries and their content evolved, reflecting\nintellectual changes in Sweden.\n  In this paper, we used digitized versions from \\textit{Project Runeberg}. We\nfirst resegmented the raw text into entries and matched pairs of entries\nbetween the first and second editions using semantic sentence embeddings. We\nthen extracted the geographical entries from both editions using a\ntransformer-based classifier and linked them to Wikidata. This enabled us to\nidentify geographic trends and possible shifts between the first and second\neditions, written between 1876-1899 and 1904-1926, respectively.\n  Interpreting the results, we observe a small but significant shift in\ngeographic focus away from Europe and towards North America, Africa, Asia,\nAustralia, and northern Scandinavia from the first to the second edition,\nconfirming the influence of the First World War and the rise of new powers. The\ncode and data are available on GitHub at\nhttps://github.com/sibbo/nordisk-familjebok.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u300aNordisk familjebok\u300b\u767e\u79d1\u5168\u4e66\u7b2c\u4e00\u7248\u548c\u7b2c\u4e8c\u7248\u4e2d\u5730\u7406\u6761\u76ee\u7684\u53d8\u5316\uff0c\u53d1\u73b0\u5730\u7406\u7126\u70b9\u4ece\u6b27\u6d32\u8f6c\u5411\u5317\u7f8e\u3001\u975e\u6d32\u3001\u4e9a\u6d32\u3001\u6fb3\u5927\u5229\u4e9a\u548c\u5317\u6b27\uff0c\u53cd\u6620\u4e86\u7b2c\u4e00\u6b21\u4e16\u754c\u5927\u6218\u548c\u65b0\u52bf\u529b\u5d1b\u8d77\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u76ee\u7684\u662f\u901a\u8fc7\u6570\u5b57\u5316\u7248\u672c\u5206\u6790\u767e\u79d1\u5168\u4e66\u6761\u76ee\u5185\u5bb9\u7684\u53d8\u5316\uff0c\u63ed\u793a\u745e\u5178\u77e5\u8bc6\u7ed3\u6784\u7684\u5386\u53f2\u6f14\u53d8\u3002", "method": "\u4f7f\u7528\u8bed\u4e49\u53e5\u5b50\u5d4c\u5165\u5339\u914d\u6761\u76ee\uff0c\u57fa\u4e8eTransformer\u7684\u5206\u7c7b\u5668\u63d0\u53d6\u5730\u7406\u6761\u76ee\u5e76\u94fe\u63a5\u5230Wikidata\uff0c\u5206\u6790\u5730\u7406\u8d8b\u52bf\u53d8\u5316\u3002", "result": "\u53d1\u73b0\u5730\u7406\u7126\u70b9\u4ece\u6b27\u6d32\u8f6c\u5411\u5176\u4ed6\u5730\u533a\uff0c\u8bc1\u5b9e\u4e86\u7b2c\u4e00\u6b21\u4e16\u754c\u5927\u6218\u548c\u65b0\u52bf\u529b\u5d1b\u8d77\u7684\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u767e\u79d1\u5168\u4e66\u5185\u5bb9\u5982\u4f55\u53cd\u6620\u793e\u4f1a\u5386\u53f2\u53d8\u8fc1\uff0c\u6570\u636e\u548c\u65b9\u6cd5\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2507.01213", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01213", "abs": "https://arxiv.org/abs/2507.01213", "authors": ["Adamu Lawan", "Juhua Pu", "Haruna Yunusa", "Jawad Muhammad", "Muhammad Lawan"], "title": "MEGA: xLSTM with Multihead Exponential Gated Fusion for Precise Aspect-based Sentiment Analysis", "comment": "6, 1 figure", "summary": "Aspect-based Sentiment Analysis (ABSA) is a critical Natural Language\nProcessing (NLP) task that extracts aspects from text and determines their\nassociated sentiments, enabling fine-grained analysis of user opinions.\nExisting ABSA methods struggle to balance computational efficiency with high\nperformance: deep learning models often lack global context, transformers\ndemand significant computational resources, and Mamba-based approaches face\nCUDA dependency and diminished local correlations. Recent advancements in\nExtended Long Short-Term Memory (xLSTM) models, particularly their efficient\nmodeling of long-range dependencies, have significantly advanced the NLP\ncommunity. However, their potential in ABSA remains untapped. To this end, we\npropose xLSTM with Multihead Exponential Gated Fusion (MEGA), a novel framework\nintegrating a bi-directional mLSTM architecture with forward and partially\nflipped backward (PF-mLSTM) streams. The PF-mLSTM enhances localized context\nmodeling by processing the initial sequence segment in reverse with dedicated\nparameters, preserving critical short-range patterns. We further introduce an\nmLSTM-based multihead cross exponential gated fusion mechanism (MECGAF) that\ndynamically combines forward mLSTM outputs as query and key with PF-mLSTM\noutputs as value, optimizing short-range dependency capture while maintaining\nglobal context and efficiency. Experimental results on three benchmark datasets\ndemonstrate that MEGA outperforms state-of-the-art baselines, achieving\nsuperior accuracy and efficiency in ABSA tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMEGA\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u53cc\u5411mLSTM\u548c\u90e8\u5206\u7ffb\u8f6c\u53cd\u5411\u6d41\uff08PF-mLSTM\uff09\uff0c\u901a\u8fc7\u591a\u5934\u6307\u6570\u95e8\u63a7\u878d\u5408\u673a\u5236\uff08MECGAF\uff09\u4f18\u5316ABSA\u4efb\u52a1\u4e2d\u7684\u5c40\u90e8\u548c\u5168\u5c40\u4f9d\u8d56\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709ABSA\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u4e0e\u9ad8\u6027\u80fd\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7f3a\u4e4f\u5168\u5c40\u4e0a\u4e0b\u6587\uff0cTransformer\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\uff0c\u800cMamba\u65b9\u6cd5\u5b58\u5728CUDA\u4f9d\u8d56\u548c\u5c40\u90e8\u76f8\u5173\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002xLSTM\u5728\u957f\u7a0b\u4f9d\u8d56\u5efa\u6a21\u4e0a\u7684\u6f5c\u529b\u5c1a\u672a\u5728ABSA\u4e2d\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faxLSTM\u7ed3\u5408\u591a\u5934\u6307\u6570\u95e8\u63a7\u878d\u5408\uff08MEGA\uff09\uff0c\u91c7\u7528\u53cc\u5411mLSTM\u67b6\u6784\u548c\u90e8\u5206\u7ffb\u8f6c\u53cd\u5411\u6d41\uff08PF-mLSTM\uff09\uff0c\u5e76\u5f15\u5165MECGAF\u673a\u5236\u52a8\u6001\u878d\u5408\u524d\u5411\u548c\u53cd\u5411\u8f93\u51fa\uff0c\u4f18\u5316\u5c40\u90e8\u4f9d\u8d56\u6355\u83b7\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMEGA\u5728ABSA\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "conclusion": "MEGA\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u53cc\u5411\u6d41\u548c\u878d\u5408\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86ABSA\u4efb\u52a1\u4e2d\u7684\u5c40\u90e8\u4e0e\u5168\u5c40\u4f9d\u8d56\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.01231", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01231", "abs": "https://arxiv.org/abs/2507.01231", "authors": ["I\u00f1aki Dellibarda Varela", "Pablo Romero-Sorozabal", "Eduardo Rocon", "Manuel Cebrian"], "title": "Rethinking the Illusion of Thinking", "comment": "8 pages, 4 figures", "summary": "Earlier this year, Apple ignited controversy by publishing \"The Illusion of\nThinking,\" prompting heated debate within the AI community. Critics seized upon\nthe findings as conclusive evidence that Large Reasoning Models (LRMs) lack\ngenuine reasoning capabilities, branding them as mere stochastic parrots.\nMeanwhile, defenders-spearheaded by Lawsen et al. (2025)-fired back, condemning\nthe experimental setup as flawed and the conclusions overstated. We clarify\nthis debate by replicating and refining two of the original study's most\ncontentious benchmarks: Towers of Hanoi and River Crossing. By introducing\nincremental stepwise prompting and agentic collaborative dialogue, we show that\npreviously reported failures solving the Towers of Hanoi were not purely result\nof output constraints, but also partly a result of cognition limitations: LRMs\nstill stumble when complexity rises moderately (around 8 disks). Moreover, the\nRiver Crossing results initially heralded as catastrophic failures turn out to\nhinge upon testing unsolvable configurations. Once we limit tests strictly to\nsolvable problems-LRMs effortlessly solve large instances involving over 100\nagent pairs. Our findings ultimately defy simplistic narratives: today's LRMs\nare stochastic, RL-tuned searchers in a discrete state space we barely\nunderstand. Real progress in symbolic, long-horizon reasoning demands mapping\nthat terrain through fine-grained ablations like those introduced here.", "AI": {"tldr": "\u8bba\u6587\u6f84\u6e05\u4e86\u5173\u4e8e\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u662f\u5426\u5177\u5907\u771f\u6b63\u63a8\u7406\u80fd\u529b\u7684\u4e89\u8bae\uff0c\u901a\u8fc7\u6539\u8fdb\u5b9e\u9a8c\u65b9\u6cd5\uff0c\u53d1\u73b0LRMs\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u4f46\u5728\u53ef\u89e3\u95ee\u9898\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3AI\u793e\u533a\u5bf9LRMs\u662f\u5426\u5177\u5907\u771f\u6b63\u63a8\u7406\u80fd\u529b\u7684\u4e89\u8bae\uff0c\u6f84\u6e05\u5b9e\u9a8c\u8bbe\u8ba1\u548c\u7ed3\u8bba\u7684\u8bef\u89e3\u3002", "method": "\u590d\u5236\u5e76\u6539\u8fdb\u539f\u59cb\u7814\u7a76\u4e2d\u7684\u4e24\u4e2a\u4e89\u8bae\u6027\u57fa\u51c6\uff08\u6c49\u8bfa\u5854\u548c\u8fc7\u6cb3\u95ee\u9898\uff09\uff0c\u5f15\u5165\u9010\u6b65\u63d0\u793a\u548c\u534f\u4f5c\u5bf9\u8bdd\u65b9\u6cd5\u3002", "result": "LRMs\u5728\u6c49\u8bfa\u5854\u4efb\u52a1\u4e2d\u56e0\u8ba4\u77e5\u9650\u5236\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u5728\u53ef\u89e3\u7684\u8fc7\u6cb3\u95ee\u9898\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "LRMs\u662f\u79bb\u6563\u72b6\u6001\u7a7a\u95f4\u4e2d\u7684\u968f\u673a\u641c\u7d22\u5668\uff0c\u672a\u6765\u9700\u8981\u5728\u7b26\u53f7\u63a8\u7406\u548c\u957f\u671f\u89c4\u5212\u65b9\u9762\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2507.01026", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01026", "abs": "https://arxiv.org/abs/2507.01026", "authors": ["Md Shakil Ahamed Shohag", "Q. M. Jonathan Wu", "Farhad Pourpanah"], "title": "Few-Shot Inspired Generative Zero-Shot Learning", "comment": null, "summary": "Generative zero-shot learning (ZSL) methods typically synthesize visual\nfeatures for unseen classes using predefined semantic attributes, followed by\ntraining a fully supervised classification model. While effective, these\nmethods require substantial computational resources and extensive synthetic\ndata, thereby relaxing the original ZSL assumptions. In this paper, we propose\nFSIGenZ, a few-shot-inspired generative ZSL framework that reduces reliance on\nlarge-scale feature synthesis. Our key insight is that class-level attributes\nexhibit instance-level variability, i.e., some attributes may be absent or\npartially visible, yet conventional ZSL methods treat them as uniformly\npresent. To address this, we introduce Model-Specific Attribute Scoring (MSAS),\nwhich dynamically re-scores class attributes based on model-specific\noptimization to approximate instance-level variability without access to unseen\ndata. We further estimate group-level prototypes as clusters of instances based\non MSAS-adjusted attribute scores, which serve as representative synthetic\nfeatures for each unseen class. To mitigate the resulting data imbalance, we\nintroduce a Dual-Purpose Semantic Regularization (DPSR) strategy while training\na semantic-aware contrastive classifier (SCC) using these prototypes.\nExperiments on SUN, AwA2, and CUB benchmarks demonstrate that FSIGenZ achieves\ncompetitive performance using far fewer synthetic features.", "AI": {"tldr": "FSIGenZ\u63d0\u51fa\u4e86\u4e00\u79cd\u5c11\u6837\u672c\u542f\u53d1\u7684\u751f\u6210\u96f6\u6837\u672c\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5c5e\u6027\u8bc4\u5206\u548c\u539f\u578b\u4f30\u8ba1\uff0c\u51cf\u5c11\u5bf9\u5927\u89c4\u6a21\u7279\u5f81\u5408\u6210\u7684\u4f9d\u8d56\u3002", "motivation": "\u4f20\u7edf\u751f\u6210\u96f6\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u548c\u5408\u6210\u6570\u636e\uff0c\u8fdd\u80cc\u4e86\u96f6\u6837\u672c\u5b66\u4e60\u7684\u521d\u8877\u3002FSIGenZ\u65e8\u5728\u901a\u8fc7\u52a8\u6001\u5c5e\u6027\u8bc4\u5206\u548c\u539f\u578b\u4f30\u8ba1\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f15\u5165\u6a21\u578b\u7279\u5b9a\u5c5e\u6027\u8bc4\u5206\uff08MSAS\uff09\u52a8\u6001\u8c03\u6574\u5c5e\u6027\u8bc4\u5206\uff0c\u4f30\u8ba1\u7ec4\u7ea7\u539f\u578b\u4f5c\u4e3a\u672a\u89c1\u7c7b\u522b\u7684\u5408\u6210\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u53cc\u91cd\u76ee\u7684\u8bed\u4e49\u6b63\u5219\u5316\uff08DPSR\uff09\u8bad\u7ec3\u8bed\u4e49\u611f\u77e5\u5bf9\u6bd4\u5206\u7c7b\u5668\uff08SCC\uff09\u3002", "result": "\u5728SUN\u3001AwA2\u548cCUB\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFSIGenZ\u4f7f\u7528\u66f4\u5c11\u7684\u5408\u6210\u7279\u5f81\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u6027\u80fd\u3002", "conclusion": "FSIGenZ\u901a\u8fc7\u52a8\u6001\u5c5e\u6027\u8bc4\u5206\u548c\u539f\u578b\u4f30\u8ba1\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u96f6\u6837\u672c\u5b66\u4e60\u7684\u6027\u80fd\u3002"}}
{"id": "2507.01099", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01099", "abs": "https://arxiv.org/abs/2507.01099", "authors": ["Zeyi Liu", "Shuang Li", "Eric Cousineau", "Siyuan Feng", "Benjamin Burchfiel", "Shuran Song"], "title": "Geometry-aware 4D Video Generation for Robot Manipulation", "comment": "Project website: https://robot4dgen.github.io", "summary": "Understanding and predicting the dynamics of the physical world can enhance a\nrobot's ability to plan and interact effectively in complex environments. While\nrecent video generation models have shown strong potential in modeling dynamic\nscenes, generating videos that are both temporally coherent and geometrically\nconsistent across camera views remains a significant challenge. To address\nthis, we propose a 4D video generation model that enforces multi-view 3D\nconsistency of videos by supervising the model with cross-view pointmap\nalignment during training. This geometric supervision enables the model to\nlearn a shared 3D representation of the scene, allowing it to predict future\nvideo sequences from novel viewpoints based solely on the given RGB-D\nobservations, without requiring camera poses as inputs. Compared to existing\nbaselines, our method produces more visually stable and spatially aligned\npredictions across multiple simulated and real-world robotic datasets. We\nfurther show that the predicted 4D videos can be used to recover robot\nend-effector trajectories using an off-the-shelf 6DoF pose tracker, supporting\nrobust robot manipulation and generalization to novel camera viewpoints.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd4D\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u8de8\u89c6\u89d2\u70b9\u56fe\u5bf9\u9f50\u76d1\u7763\uff0c\u5b9e\u73b0\u591a\u89c6\u89d23D\u4e00\u81f4\u6027\uff0c\u63d0\u5347\u673a\u5668\u4eba\u5bf9\u52a8\u6001\u573a\u666f\u7684\u7406\u89e3\u548c\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u589e\u5f3a\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u89c4\u5212\u548c\u4ea4\u4e92\u7684\u80fd\u529b\uff0c\u89e3\u51b3\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u65f6\u95f4\u8fde\u8d2f\u6027\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u5229\u7528RGB-D\u89c2\u6d4b\u6570\u636e\uff0c\u901a\u8fc7\u8de8\u89c6\u89d2\u70b9\u56fe\u5bf9\u9f50\u76d1\u7763\u8bad\u7ec3\u6a21\u578b\uff0c\u5b66\u4e60\u5171\u4eab\u76843D\u573a\u666f\u8868\u793a\uff0c\u65e0\u9700\u76f8\u673a\u59ff\u6001\u8f93\u5165\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u673a\u5668\u4eba\u6570\u636e\u96c6\u4e0a\uff0c\u751f\u6210\u66f4\u7a33\u5b9a\u4e14\u7a7a\u95f4\u5bf9\u9f50\u7684\u89c6\u9891\u9884\u6d4b\uff0c\u5e76\u80fd\u6062\u590d\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\u8f68\u8ff9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u652f\u6301\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u9002\u5e94\u65b0\u89c6\u89d2\uff0c\u4e3a\u52a8\u6001\u573a\u666f\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01234", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01234", "abs": "https://arxiv.org/abs/2507.01234", "authors": ["Yu Fan", "Yang Tian", "Shauli Ravfogel", "Mrinmaya Sachan", "Elliott Ash", "Alexander Hoyle"], "title": "The Medium Is Not the Message: Deconfounding Text Embeddings via Linear Concept Erasure", "comment": null, "summary": "Embedding-based similarity metrics between text sequences can be influenced\nnot just by the content dimensions we most care about, but can also be biased\nby spurious attributes like the text's source or language. These document\nconfounders cause problems for many applications, but especially those that\nneed to pool texts from different corpora. This paper shows that a debiasing\nalgorithm that removes information about observed confounders from the encoder\nrepresentations substantially reduces these biases at a minimal computational\ncost. Document similarity and clustering metrics improve across every embedding\nvariant and task we evaluate -- often dramatically. Interestingly, performance\non out-of-distribution benchmarks is not impacted, indicating that the\nembeddings are not otherwise degraded.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53bb\u504f\u7b97\u6cd5\uff0c\u901a\u8fc7\u4ece\u7f16\u7801\u5668\u8868\u793a\u4e2d\u79fb\u9664\u89c2\u5bdf\u5230\u7684\u6df7\u6742\u56e0\u7d20\u4fe1\u606f\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6587\u672c\u76f8\u4f3c\u6027\u5ea6\u91cf\u4e2d\u7684\u504f\u5dee\u3002", "motivation": "\u6587\u672c\u5e8f\u5217\u7684\u5d4c\u5165\u76f8\u4f3c\u6027\u5ea6\u91cf\u53ef\u80fd\u53d7\u5230\u65e0\u5173\u5c5e\u6027\uff08\u5982\u6587\u672c\u6765\u6e90\u6216\u8bed\u8a00\uff09\u7684\u5e72\u6270\uff0c\u5f71\u54cd\u591a\u8bed\u6599\u5e93\u6587\u672c\u7684\u5e94\u7528\u6548\u679c\u3002", "method": "\u91c7\u7528\u53bb\u504f\u7b97\u6cd5\uff0c\u4ece\u7f16\u7801\u5668\u8868\u793a\u4e2d\u79fb\u9664\u6df7\u6742\u56e0\u7d20\u4fe1\u606f\u3002", "result": "\u53bb\u504f\u7b97\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u504f\u5dee\uff0c\u63d0\u5347\u4e86\u6587\u6863\u76f8\u4f3c\u6027\u548c\u805a\u7c7b\u5ea6\u91cf\uff0c\u4e14\u4e0d\u5f71\u54cd\u5206\u5e03\u5916\u57fa\u51c6\u6027\u80fd\u3002", "conclusion": "\u53bb\u504f\u7b97\u6cd5\u9ad8\u6548\u4e14\u6709\u6548\uff0c\u80fd\u5728\u4e0d\u964d\u4f4e\u5d4c\u5165\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u51cf\u5c11\u504f\u5dee\u3002"}}
{"id": "2507.01282", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.01282", "abs": "https://arxiv.org/abs/2507.01282", "authors": ["Matthew JY Kang", "Wenli Yang", "Monica R Roberts", "Byeong Ho Kang", "Charles B Malpas"], "title": "Beyond Black-Box AI: Interpretable Hybrid Systems for Dementia Care", "comment": null, "summary": "The recent boom of large language models (LLMs) has re-ignited the hope that\nartificial intelligence (AI) systems could aid medical diagnosis. Yet despite\ndazzling benchmark scores, LLM assistants have yet to deliver measurable\nimprovements at the bedside. This scoping review aims to highlight the areas\nwhere AI is limited to make practical contributions in the clinical setting,\nspecifically in dementia diagnosis and care.\n  Standalone machine-learning models excel at pattern recognition but seldom\nprovide actionable, interpretable guidance, eroding clinician trust. Adjacent\nuse of LLMs by physicians did not result in better diagnostic accuracy or\nspeed. Key limitations trace to the data-driven paradigm: black-box outputs\nwhich lack transparency, vulnerability to hallucinations, and weak causal\nreasoning. Hybrid approaches that combine statistical learning with expert\nrule-based knowledge, and involve clinicians throughout the process help bring\nback interpretability. They also fit better with existing clinical workflows,\nas seen in examples like PEIRS and ATHENA-CDS.\n  Future decision-support should prioritise explanatory coherence by linking\npredictions to clinically meaningful causes. This can be done through\nneuro-symbolic or hybrid AI that combines the language ability of LLMs with\nhuman causal expertise. AI researchers have addressed this direction, with\nexplainable AI and neuro-symbolic AI being the next logical steps in further\nadvancement in AI. However, they are still based on data-driven knowledge\nintegration instead of human-in-the-loop approaches. Future research should\nmeasure success not only by accuracy but by improvements in clinician\nunderstanding, workflow fit, and patient outcomes. A better understanding of\nwhat helps improve human-computer interactions is greatly needed for AI systems\nto become part of clinical practice.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u533b\u7597\u8bca\u65ad\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u75f4\u5446\u75c7\u8bca\u65ad\u548c\u62a4\u7406\u4e2d\uff0c\u63d0\u51fa\u6df7\u5408\u65b9\u6cd5\uff08\u7ed3\u5408\u7edf\u8ba1\u5b66\u4e60\u548c\u4e13\u5bb6\u77e5\u8bc6\uff09\u4ee5\u63d0\u9ad8\u900f\u660e\u5ea6\u548c\u4e34\u5e8a\u5b9e\u7528\u6027\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5b9e\u9645\u4e34\u5e8a\u73af\u5883\u4e2d\u672a\u80fd\u663e\u8457\u63d0\u5347\u8bca\u65ad\u6548\u679c\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u5176\u5c40\u9650\u6027\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u5411\u3002", "method": "\u901a\u8fc7\u8303\u56f4\u7efc\u8ff0\u5206\u6790AI\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u7ed3\u5408\u7edf\u8ba1\u5b66\u4e60\u548c\u4e13\u5bb6\u77e5\u8bc6\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u5982PEIRS\u548cATHENA-CDS\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u7eaf\u6570\u636e\u9a71\u52a8\u7684AI\u7f3a\u4e4f\u900f\u660e\u5ea6\u548c\u56e0\u679c\u63a8\u7406\u80fd\u529b\uff0c\u800c\u6df7\u5408\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u878d\u5165\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u5e76\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u672a\u6765AI\u51b3\u7b56\u652f\u6301\u5e94\u6ce8\u91cd\u89e3\u91ca\u6027\u548c\u4e34\u5e8a\u5b9e\u7528\u6027\uff0c\u7ed3\u5408\u795e\u7ecf\u7b26\u53f7AI\u548c\u4eba\u7c7b\u56e0\u679c\u77e5\u8bc6\uff0c\u5e76\u8861\u91cf\u5176\u5bf9\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u548c\u60a3\u8005\u7ed3\u679c\u7684\u5f71\u54cd\u3002"}}
{"id": "2507.01027", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01027", "abs": "https://arxiv.org/abs/2507.01027", "authors": ["Zijian Ye", "Wei Huang", "Yifei Yu", "Tianhe Ren", "Zhongrui Wang", "Xiaojuan Qi"], "title": "DBellQuant: Breaking the Bell with Double-Bell Transformation for LLMs Post Training Binarization", "comment": "19 pages; Appendix added", "summary": "Large language models (LLMs) demonstrate remarkable performance but face\nsubstantial computational and memory challenges that limit their practical\ndeployment. Quantization has emerged as a promising solution; however, its\neffectiveness is often limited by quantization errors arising from weight\ndistributions that are not quantization-friendly and the presence of activation\noutliers. To address these challenges, we introduce DBellQuant, an innovative\npost-training quantization (PTQ) framework that achieves nearly 1-bit weight\ncompression and 6-bit activation quantization with minimal performance\ndegradation. DBellQuant uses Learnable Transformation for Dual-Bell (LTDB)\nalgorithm, which transforms single-bell weight distributions into dual-bell\nforms to reduce binarization errors and applies inverse transformations to\nsmooth activations. DBellQuant sets a new state-of-the-art by preserving\nsuperior model performance under aggressive weight and activation quantization.\nFor example, on the Wikitext2 dataset, DBellQuant achieves a perplexity of\n14.39 on LLaMA2-13B with 6-bit activation quantization, significantly\noutperforming BiLLM's 21.35 without activation quantization, underscoring its\npotential in compressing LLMs for real-world applications.", "AI": {"tldr": "DBellQuant\u662f\u4e00\u79cd\u521b\u65b0\u7684\u540e\u8bad\u7ec3\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u949f\u5f62\u5206\u5e03\u8f6c\u6362\u51cf\u5c11\u91cf\u5316\u8bef\u5dee\uff0c\u5b9e\u73b01\u6bd4\u7279\u6743\u91cd\u538b\u7f29\u548c6\u6bd4\u7279\u6fc0\u6d3b\u91cf\u5316\uff0c\u6027\u80fd\u635f\u5931\u6781\u5c0f\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9762\u4e34\u8ba1\u7b97\u548c\u5185\u5b58\u6311\u6218\uff0c\u91cf\u5316\u662f\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u91cf\u5316\u8bef\u5dee\u548c\u6fc0\u6d3b\u5f02\u5e38\u503c\u9650\u5236\u4e86\u5176\u6548\u679c\u3002", "method": "DBellQuant\u91c7\u7528\u53ef\u5b66\u4e60\u7684\u53cc\u949f\u5f62\u8f6c\u6362\u7b97\u6cd5\uff08LTDB\uff09\uff0c\u5c06\u5355\u949f\u5f62\u6743\u91cd\u5206\u5e03\u8f6c\u6362\u4e3a\u53cc\u949f\u5f62\u4ee5\u51cf\u5c11\u4e8c\u503c\u5316\u8bef\u5dee\uff0c\u5e76\u901a\u8fc7\u9006\u8f6c\u6362\u5e73\u6ed1\u6fc0\u6d3b\u3002", "result": "\u5728Wikitext2\u6570\u636e\u96c6\u4e0a\uff0cDBellQuant\u5728LLaMA2-13B\u4e0a\u5b9e\u73b0\u4e8614.39\u7684\u56f0\u60d1\u5ea6\uff0c\u663e\u8457\u4f18\u4e8eBiLLM\u768421.35\u3002", "conclusion": "DBellQuant\u5728\u6fc0\u8fdb\u91cf\u5316\u7684\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\uff0c\u4e3aLLMs\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002"}}
{"id": "2507.01123", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.01123", "abs": "https://arxiv.org/abs/2507.01123", "authors": ["Rahul A. Burange", "Harsh K. Shinde", "Omkar Mutyalwar"], "title": "Landslide Detection and Mapping Using Deep Learning Across Multi-Source Satellite Data and Geographic Regions", "comment": "20 pages, 24 figures", "summary": "Landslides pose severe threats to infrastructure, economies, and human lives,\nnecessitating accurate detection and predictive mapping across diverse\ngeographic regions. With advancements in deep learning and remote sensing,\nautomated landslide detection has become increasingly effective. This study\npresents a comprehensive approach integrating multi-source satellite imagery\nand deep learning models to enhance landslide identification and prediction. We\nleverage Sentinel-2 multispectral data and ALOS PALSAR-derived slope and\nDigital Elevation Model (DEM) layers to capture critical environmental features\ninfluencing landslide occurrences. Various geospatial analysis techniques are\nemployed to assess the impact of terra in characteristics, vegetation cover,\nand rainfall on detection accuracy. Additionally, we evaluate the performance\nof multiple stateof-the-art deep learning segmentation models, including U-Net,\nDeepLabV3+, and Res-Net, to determine their effectiveness in landslide\ndetection. The proposed framework contributes to the development of reliable\nearly warning systems, improved disaster risk management, and sustainable\nland-use planning. Our findings provide valuable insights into the potential of\ndeep learning and multi-source remote sensing in creating robust, scalable, and\ntransferable landslide prediction models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u6e90\u536b\u661f\u5f71\u50cf\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u7efc\u5408\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u6ed1\u5761\u8bc6\u522b\u548c\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u6ed1\u5761\u5bf9\u57fa\u7840\u8bbe\u65bd\u3001\u7ecf\u6d4e\u548c\u4eba\u7c7b\u751f\u547d\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u9700\u8981\u8de8\u5730\u7406\u533a\u57df\u7684\u51c6\u786e\u68c0\u6d4b\u548c\u9884\u6d4b\u3002", "method": "\u5229\u7528Sentinel-2\u591a\u5149\u8c31\u6570\u636e\u548cALOS PALSAR\u884d\u751f\u7684\u5761\u5ea6\u53ca\u6570\u5b57\u9ad8\u7a0b\u6a21\u578b\uff08DEM\uff09\u5c42\uff0c\u7ed3\u5408\u591a\u79cd\u5730\u7406\u7a7a\u95f4\u5206\u6790\u6280\u672f\u548c\u6df1\u5ea6\u5b66\u4e60\u5206\u5272\u6a21\u578b\uff08\u5982U-Net\u3001DeepLabV3+\u548cRes-Net\uff09\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5f00\u53d1\u53ef\u9760\u7684\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u3001\u6539\u8fdb\u707e\u5bb3\u98ce\u9669\u7ba1\u7406\u548c\u53ef\u6301\u7eed\u571f\u5730\u5229\u7528\u89c4\u5212\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u4e0e\u591a\u6e90\u9065\u611f\u6570\u636e\u7ed3\u5408\uff0c\u53ef\u6784\u5efa\u7a33\u5065\u3001\u53ef\u6269\u5c55\u4e14\u53ef\u8fc1\u79fb\u7684\u6ed1\u5761\u9884\u6d4b\u6a21\u578b\u3002"}}
{"id": "2507.01259", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01259", "abs": "https://arxiv.org/abs/2507.01259", "authors": ["Micha\u0142 Matak", "Jaros\u0142aw A. Chudziak"], "title": "GAIus: Combining Genai with Legal Clauses Retrieval for Knowledge-based Assistant", "comment": "8 pages, 2 figures, presented at ICAART 2025, in proceedings of the\n  17th International Conference on Agents and Artificial Intelligence - Volume\n  3: ICAART", "summary": "In this paper we discuss the capability of large language models to base\ntheir answer and provide proper references when dealing with legal matters of\nnon-english and non-chinese speaking country. We discuss the history of legal\ninformation retrieval, the difference between case law and statute law, its\nimpact on the legal tasks and analyze the latest research in this field. Basing\non that background we introduce gAIus, the architecture of the cognitive\nLLM-based agent, whose responses are based on the knowledge retrieved from\ncertain legal act, which is Polish Civil Code. We propose a retrieval mechanism\nwhich is more explainable, human-friendly and achieves better results than\nembedding-based approaches. To evaluate our method we create special dataset\nbased on single-choice questions from entrance exams for law apprenticeships\nconducted in Poland. The proposed architecture critically leveraged the\nabilities of used large language models, improving the gpt-3.5-turbo-0125 by\n419%, allowing it to beat gpt-4o and lifting gpt-4o-mini score from 31% to 86%.\nAt the end of our paper we show the possible future path of research and\npotential applications of our findings.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u975e\u82f1\u8bed\u548c\u975e\u4e2d\u6587\u56fd\u5bb6\u7684\u6cd5\u5f8b\u95ee\u9898\u65f6\u63d0\u4f9b\u7b54\u6848\u548c\u5f15\u7528\u9002\u5f53\u53c2\u8003\u6587\u732e\u7684\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce2\u5170\u6c11\u6cd5\u5178\u7684\u8ba4\u77e5LLM\u4ee3\u7406\u67b6\u6784gAIus\uff0c\u5176\u68c0\u7d22\u673a\u5236\u6bd4\u57fa\u4e8e\u5d4c\u5165\u7684\u65b9\u6cd5\u66f4\u6613\u89e3\u91ca\u4e14\u6548\u679c\u66f4\u597d\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u975e\u4e3b\u6d41\u8bed\u8a00\u6cd5\u5f8b\u95ee\u9898\u65f6\u7684\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u7f3a\u4e4f\u5bf9\u7279\u5b9a\u6cd5\u5f8b\u4f53\u7cfb\uff08\u5982\u6ce2\u5170\u6c11\u6cd5\u5178\uff09\u7684\u9002\u5e94\u6027\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u63d0\u51fagAIus\u67b6\u6784\uff0c\u91c7\u7528\u66f4\u6613\u89e3\u91ca\u7684\u68c0\u7d22\u673a\u5236\uff0c\u5e76\u57fa\u4e8e\u6ce2\u5170\u6cd5\u5f8b\u5b66\u5f92\u5165\u5b66\u8003\u8bd5\u7684\u5355\u9009\u95ee\u9898\u521b\u5efa\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cgAIus\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4f7fgpt-3.5-turbo-0125\u63d0\u9ad8\u4e86419%\uff0c\u5e76\u8d85\u8d8a\u4e86gpt-4o\uff0c\u540c\u65f6\u5c06gpt-4o-mini\u7684\u5f97\u5206\u4ece31%\u63d0\u5347\u523086%\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\u8be5\u67b6\u6784\u5728\u63d0\u5347\u6a21\u578b\u6027\u80fd\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u7814\u7a76\u548c\u5e94\u7528\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2507.01376", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01376", "abs": "https://arxiv.org/abs/2507.01376", "authors": ["Yinwang Ren", "Yangyang Liu", "Tang Ji", "Xun Xu"], "title": "AI Agents and Agentic AI-Navigating a Plethora of Concepts for Future Manufacturing", "comment": "Submitted to JMS(March 2025)", "summary": "AI agents are autonomous systems designed to perceive, reason, and act within\ndynamic environments. With the rapid advancements in generative AI (GenAI),\nlarge language models (LLMs) and multimodal large language models (MLLMs) have\nsignificantly improved AI agents' capabilities in semantic comprehension,\ncomplex reasoning, and autonomous decision-making. At the same time, the rise\nof Agentic AI highlights adaptability and goal-directed autonomy in dynamic and\ncomplex environments. LLMs-based AI Agents (LLM-Agents), MLLMs-based AI Agents\n(MLLM-Agents), and Agentic AI contribute to expanding AI's capabilities in\ninformation processing, environmental perception, and autonomous\ndecision-making, opening new avenues for smart manufacturing. However, the\ndefinitions, capability boundaries, and practical applications of these\nemerging AI paradigms in smart manufacturing remain unclear. To address this\ngap, this study systematically reviews the evolution of AI and AI agent\ntechnologies, examines the core concepts and technological advancements of\nLLM-Agents, MLLM-Agents, and Agentic AI, and explores their potential\napplications in and integration into manufacturing, along with the potential\nchallenges they may face.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8e\u751f\u6210\u5f0fAI\uff08GenAI\uff09\u7684AI\u4ee3\u7406\uff08\u5982LLM-Agents\u548cMLLM-Agents\uff09\u53ca\u5176\u5728\u667a\u80fd\u5236\u9020\u4e2d\u7684\u6f5c\u529b\u4e0e\u6311\u6218\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u7684\u5feb\u901f\u53d1\u5c55\uff0cAI\u4ee3\u7406\u5728\u8bed\u4e49\u7406\u89e3\u3001\u590d\u6742\u63a8\u7406\u548c\u81ea\u4e3b\u51b3\u7b56\u65b9\u9762\u7684\u80fd\u529b\u663e\u8457\u63d0\u5347\uff0c\u4f46\u5176\u5728\u667a\u80fd\u5236\u9020\u4e2d\u7684\u5b9a\u4e49\u3001\u80fd\u529b\u8fb9\u754c\u548c\u5b9e\u9645\u5e94\u7528\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u7814\u7a76\u7cfb\u7edf\u56de\u987e\u4e86AI\u548cAI\u4ee3\u7406\u6280\u672f\u7684\u6f14\u8fdb\uff0c\u5206\u6790\u4e86LLM-Agents\u3001MLLM-Agents\u548cAgentic AI\u7684\u6838\u5fc3\u6982\u5ff5\u4e0e\u6280\u672f\u8fdb\u5c55\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5728\u667a\u80fd\u5236\u9020\u4e2d\u7684\u5e94\u7528\u4e0e\u96c6\u6210\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8fd9\u4e9b\u65b0\u5174AI\u8303\u5f0f\u5728\u4fe1\u606f\u5904\u7406\u3001\u73af\u5883\u611f\u77e5\u548c\u81ea\u4e3b\u51b3\u7b56\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u80fd\u9762\u4e34\u6311\u6218\u3002", "conclusion": "\u7814\u7a76\u4e3a\u667a\u80fd\u5236\u9020\u4e2dAI\u4ee3\u7406\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u7efc\u8ff0\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u9700\u8981\u89e3\u51b3\u7684\u95ee\u9898\u3002"}}
{"id": "2507.01028", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01028", "abs": "https://arxiv.org/abs/2507.01028", "authors": ["Jean Ponce", "Martial Hebert", "Basile Terver"], "title": "Dual Perspectives on Non-Contrastive Self-Supervised Learning", "comment": null, "summary": "The objective of non-contrastive approaches to self-supervised learning is to\ntrain on pairs of different views of the data an encoder and a predictor that\nminimize the mean discrepancy between the code predicted from the embedding of\nthe first view and the embedding of the second one. In this setting, the stop\ngradient and exponential moving average iterative procedures are commonly used\nto avoid representation collapse, with excellent performance in downstream\nsupervised applications. This presentation investigates these procedures from\nthe dual theoretical viewpoints of optimization and dynamical systems. We first\nshow that, in general, although they do not optimize the original objective, or\nfor that matter, any other smooth function, they do avoid collapse. Following\nTian et al. [2021], but without any of the extra assumptions used in their\nproofs, we then show using a dynamical system perspective that, in the linear\ncase, minimizing the original objective function without the use of a stop\ngradient or exponential moving average always leads to collapse. Conversely, we\nfinally show that the limit points of the dynamical systems associated with\nthese two procedures are, in general, asymptotically stable equilibria, with no\nrisk of degenerating to trivial solutions.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\u975e\u5bf9\u6bd4\u65b9\u6cd5\u7684\u4f18\u5316\u548c\u52a8\u6001\u7cfb\u7edf\u89c6\u89d2\uff0c\u8bc1\u660e\u4e86\u505c\u6b62\u68af\u5ea6\u548c\u6307\u6570\u79fb\u52a8\u5e73\u5747\u80fd\u907f\u514d\u8868\u793a\u5d29\u6e83\uff0c\u5e76\u5206\u6790\u4e86\u5176\u7a33\u5b9a\u6027\u3002", "motivation": "\u63a2\u8ba8\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\u975e\u5bf9\u6bd4\u65b9\u6cd5\u5982\u4f55\u901a\u8fc7\u505c\u6b62\u68af\u5ea6\u548c\u6307\u6570\u79fb\u52a8\u5e73\u5747\u907f\u514d\u8868\u793a\u5d29\u6e83\uff0c\u5e76\u4ece\u7406\u8bba\u548c\u52a8\u6001\u7cfb\u7edf\u89d2\u5ea6\u5206\u6790\u5176\u6709\u6548\u6027\u3002", "method": "\u4ece\u4f18\u5316\u548c\u52a8\u6001\u7cfb\u7edf\u7406\u8bba\u89d2\u5ea6\u5206\u6790\u505c\u6b62\u68af\u5ea6\u548c\u6307\u6570\u79fb\u52a8\u5e73\u5747\u7684\u4f5c\u7528\uff0c\u8bc1\u660e\u5176\u5728\u907f\u514d\u5d29\u6e83\u4e2d\u7684\u6709\u6548\u6027\u3002", "result": "\u8bc1\u660e\u505c\u6b62\u68af\u5ea6\u548c\u6307\u6570\u79fb\u52a8\u5e73\u5747\u80fd\u907f\u514d\u5d29\u6e83\uff0c\u4e14\u5176\u52a8\u6001\u7cfb\u7edf\u6781\u9650\u70b9\u662f\u7a33\u5b9a\u7684\u5e73\u8861\u70b9\u3002", "conclusion": "\u505c\u6b62\u68af\u5ea6\u548c\u6307\u6570\u79fb\u52a8\u5e73\u5747\u662f\u907f\u514d\u8868\u793a\u5d29\u6e83\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4e14\u5177\u6709\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2507.01163", "categories": ["cs.CV", "q-bio.CB", "q-bio.QM", "I.4.7"], "pdf": "https://arxiv.org/pdf/2507.01163", "abs": "https://arxiv.org/abs/2507.01163", "authors": ["Al\u00e1n F. Mu\u00f1oz", "Tim Treis", "Alexandr A. Kalinin", "Shatavisha Dasgupta", "Fabian Theis", "Anne E. Carpenter", "Shantanu Singh"], "title": "cp_measure: API-first feature extraction for image-based profiling workflows", "comment": "10 pages, 4 figures, 4 supplementary figures. CODEML Workshop paper\n  accepted (non-archival), as a part of ICML2025 events", "summary": "Biological image analysis has traditionally focused on measuring specific\nvisual properties of interest for cells or other entities. A complementary\nparadigm gaining increasing traction is image-based profiling - quantifying\nmany distinct visual features to form comprehensive profiles which may reveal\nhidden patterns in cellular states, drug responses, and disease mechanisms.\nWhile current tools like CellProfiler can generate these feature sets, they\npose significant barriers to automated and reproducible analyses, hindering\nmachine learning workflows. Here we introduce cp_measure, a Python library that\nextracts CellProfiler's core measurement capabilities into a modular, API-first\ntool designed for programmatic feature extraction. We demonstrate that\ncp_measure features retain high fidelity with CellProfiler features while\nenabling seamless integration with the scientific Python ecosystem. Through\napplications to 3D astrocyte imaging and spatial transcriptomics, we showcase\nhow cp_measure enables reproducible, automated image-based profiling pipelines\nthat scale effectively for machine learning applications in computational\nbiology.", "AI": {"tldr": "cp_measure\u662f\u4e00\u4e2aPython\u5e93\uff0c\u5c06CellProfiler\u7684\u6838\u5fc3\u6d4b\u91cf\u529f\u80fd\u6a21\u5757\u5316\uff0c\u4fbf\u4e8e\u7a0b\u5e8f\u5316\u7279\u5f81\u63d0\u53d6\uff0c\u652f\u6301\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u6d41\u3002", "motivation": "\u4f20\u7edf\u751f\u7269\u56fe\u50cf\u5206\u6790\u5de5\u5177\uff08\u5982CellProfiler\uff09\u5728\u81ea\u52a8\u5316\u3001\u53ef\u91cd\u590d\u6027\u65b9\u9762\u5b58\u5728\u969c\u788d\uff0c\u963b\u788d\u4e86\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u6d41\u3002", "method": "\u5f00\u53d1cp_measure\u5e93\uff0c\u63d0\u53d6CellProfiler\u7684\u6838\u5fc3\u6d4b\u91cf\u529f\u80fd\uff0c\u8bbe\u8ba1\u4e3a\u6a21\u5757\u5316\u3001API\u4f18\u5148\u7684\u5de5\u5177\u3002", "result": "cp_measure\u7279\u5f81\u4e0eCellProfiler\u7279\u5f81\u9ad8\u5ea6\u4e00\u81f4\uff0c\u5e76\u80fd\u65e0\u7f1d\u96c6\u6210\u5230\u79d1\u5b66Python\u751f\u6001\u7cfb\u7edf\u4e2d\u3002", "conclusion": "cp_measure\u652f\u6301\u53ef\u91cd\u590d\u3001\u81ea\u52a8\u5316\u7684\u56fe\u50cf\u5206\u6790\u6d41\u7a0b\uff0c\u9002\u7528\u4e8e\u8ba1\u7b97\u751f\u7269\u5b66\u4e2d\u7684\u673a\u5668\u5b66\u4e60\u5e94\u7528\u3002"}}
{"id": "2507.01278", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01278", "abs": "https://arxiv.org/abs/2507.01278", "authors": ["Cindy Lie Tabuse", "David Restepo", "Carolina Gracitelli", "Fernando Korn Malerbi", "Caio Regatieri", "Luis Filipe Nakayama"], "title": "Evaluating Large Language Models for Multimodal Simulated Ophthalmic Decision-Making in Diabetic Retinopathy and Glaucoma Screening", "comment": null, "summary": "Large language models (LLMs) can simulate clinical reasoning based on natural\nlanguage prompts, but their utility in ophthalmology is largely unexplored.\nThis study evaluated GPT-4's ability to interpret structured textual\ndescriptions of retinal fundus photographs and simulate clinical decisions for\ndiabetic retinopathy (DR) and glaucoma screening, including the impact of\nadding real or synthetic clinical metadata. We conducted a retrospective\ndiagnostic validation study using 300 annotated fundus images. GPT-4 received\nstructured prompts describing each image, with or without patient metadata. The\nmodel was tasked with assigning an ICDR severity score, recommending DR\nreferral, and estimating the cup-to-disc ratio for glaucoma referral.\nPerformance was evaluated using accuracy, macro and weighted F1 scores, and\nCohen's kappa. McNemar's test and change rate analysis were used to assess the\ninfluence of metadata. GPT-4 showed moderate performance for ICDR\nclassification (accuracy 67.5%, macro F1 0.33, weighted F1 0.67, kappa 0.25),\ndriven mainly by correct identification of normal cases. Performance improved\nin the binary DR referral task (accuracy 82.3%, F1 0.54, kappa 0.44). For\nglaucoma referral, performance was poor across all settings (accuracy ~78%, F1\n<0.04, kappa <0.03). Metadata inclusion did not significantly affect outcomes\n(McNemar p > 0.05), and predictions remained consistent across conditions.\nGPT-4 can simulate basic ophthalmic decision-making from structured prompts but\nlacks precision for complex tasks. While not suitable for clinical use, LLMs\nmay assist in education, documentation, or image annotation workflows in\nophthalmology.", "AI": {"tldr": "GPT-4\u5728\u773c\u79d1\u4e2d\u6a21\u62df\u4e34\u5e8a\u51b3\u7b56\u7684\u80fd\u529b\u6709\u9650\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u53ef\u80fd\u9002\u7528\u4e8e\u6559\u80b2\u6216\u6587\u6863\u8f85\u52a9\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u773c\u79d1\u4e2d\u6a21\u62df\u4e34\u5e8a\u63a8\u7406\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\uff08DR\uff09\u548c\u9752\u5149\u773c\u7684\u7b5b\u67e5\u3002", "method": "\u4f7f\u7528300\u5f20\u6807\u6ce8\u7684\u773c\u5e95\u56fe\u50cf\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u63cf\u8ff0\u56fe\u50cf\uff08\u542b\u6216\u4e0d\u542b\u60a3\u8005\u5143\u6570\u636e\uff09\uff0c\u8bc4\u4f30GPT-4\u7684ICDR\u8bc4\u5206\u3001DR\u8f6c\u8bca\u5efa\u8bae\u548c\u676f\u76d8\u6bd4\u4f30\u8ba1\u80fd\u529b\u3002", "result": "GPT-4\u5728ICDR\u5206\u7c7b\u4e2d\u8868\u73b0\u4e2d\u7b49\uff08\u51c6\u786e\u738767.5%\uff09\uff0cDR\u8f6c\u8bca\u4efb\u52a1\u8868\u73b0\u8f83\u597d\uff08\u51c6\u786e\u738782.3%\uff09\uff0c\u4f46\u9752\u5149\u773c\u8f6c\u8bca\u8868\u73b0\u5dee\uff08\u51c6\u786e\u7387\u7ea678%\uff09\u3002\u5143\u6570\u636e\u5bf9\u7ed3\u679c\u65e0\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "GPT-4\u53ef\u4ece\u7ed3\u6784\u5316\u63d0\u793a\u4e2d\u6a21\u62df\u57fa\u672c\u773c\u79d1\u51b3\u7b56\uff0c\u4f46\u590d\u6742\u4efb\u52a1\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u4e0d\u9002\u5408\u4e34\u5e8a\u4f7f\u7528\uff0c\u53ef\u80fd\u7528\u4e8e\u6559\u80b2\u6216\u6587\u6863\u8f85\u52a9\u3002"}}
{"id": "2507.01410", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01410", "abs": "https://arxiv.org/abs/2507.01410", "authors": ["Abeer Dyoub", "Francesca A. Lisi"], "title": "A Fuzzy Approach to the Specification, Verification and Validation of Risk-Based Ethical Decision Making Models", "comment": null, "summary": "The ontological and epistemic complexities inherent in the moral domain make\nit challenging to establish clear standards for evaluating the performance of a\nmoral machine. In this paper, we present a formal method to describe Ethical\nDecision Making models based on ethical risk assessment. Then, we show how\nthese models that are specified as fuzzy rules can be verified and validated\nusing fuzzy Petri nets. A case study from the medical field is considered to\nillustrate the proposed approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f26\u7406\u98ce\u9669\u8bc4\u4f30\u7684\u6a21\u7cca\u89c4\u5219\u65b9\u6cd5\uff0c\u7528\u4e8e\u63cf\u8ff0\u548c\u9a8c\u8bc1\u9053\u5fb7\u51b3\u7b56\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u533b\u5b66\u6848\u4f8b\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u9053\u5fb7\u9886\u57df\u7684\u672c\u4f53\u8bba\u548c\u8ba4\u8bc6\u8bba\u590d\u6742\u6027\u4f7f\u5f97\u8bc4\u4f30\u9053\u5fb7\u673a\u5668\u6027\u80fd\u7684\u6807\u51c6\u96be\u4ee5\u660e\u786e\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u5f62\u5f0f\u5316\u65b9\u6cd5\u6765\u63cf\u8ff0\u548c\u9a8c\u8bc1\u9053\u5fb7\u51b3\u7b56\u6a21\u578b\u3002", "method": "\u91c7\u7528\u6a21\u7cca\u89c4\u5219\u63cf\u8ff0\u4f26\u7406\u51b3\u7b56\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u6a21\u7ccaPetri\u7f51\u8fdb\u884c\u9a8c\u8bc1\u548c\u9a8c\u8bc1\u3002", "result": "\u901a\u8fc7\u533b\u5b66\u9886\u57df\u7684\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9053\u5fb7\u673a\u5668\u7684\u4f26\u7406\u51b3\u7b56\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u5f62\u5f0f\u5316\u63cf\u8ff0\u548c\u9a8c\u8bc1\u9014\u5f84\u3002"}}
{"id": "2507.01029", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01029", "abs": "https://arxiv.org/abs/2507.01029", "authors": ["Junjie Zhou", "Yingli Zuo", "Shichang Feng", "Peng Wan", "Qi Zhu", "Daoqiang Zhang", "Wei Shao"], "title": "PathCoT: Chain-of-Thought Prompting for Zero-shot Pathology Visual Reasoning", "comment": null, "summary": "With the development of generative artificial intelligence and instruction\ntuning techniques, multimodal large language models (MLLMs) have made\nimpressive progress on general reasoning tasks. Benefiting from the\nchain-of-thought (CoT) methodology, MLLMs can solve the visual reasoning\nproblem step-by-step. However, existing MLLMs still face significant challenges\nwhen applied to pathology visual reasoning tasks: (1) LLMs often underperforms\nbecause they lack domain-specific information, which can lead to model\nhallucinations. (2) The additional reasoning steps in CoT may introduce errors,\nleading to the divergence of answers. To address these limitations, we propose\nPathCoT, a novel zero-shot CoT prompting method which integrates the pathology\nexpert-knowledge into the reasoning process of MLLMs and incorporates\nself-evaluation to mitigate divergence of answers. Specifically, PathCoT guides\nthe MLLM with prior knowledge to perform as pathology experts, and provides\ncomprehensive analysis of the image with their domain-specific knowledge. By\nincorporating the experts' knowledge, PathCoT can obtain the answers with CoT\nreasoning. Furthermore, PathCoT incorporates a self-evaluation step that\nassesses both the results generated directly by MLLMs and those derived through\nCoT, finally determining the reliable answer. The experimental results on the\nPathMMU dataset demonstrate the effectiveness of our method on pathology visual\nunderstanding and reasoning.", "AI": {"tldr": "PathCoT\u662f\u4e00\u79cd\u65b0\u578b\u96f6\u6837\u672cCoT\u63d0\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u75c5\u7406\u5b66\u4e13\u5bb6\u77e5\u8bc6\u548c\u81ea\u8bc4\u4f30\u673a\u5236\uff0c\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u75c5\u7406\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709MLLMs\u5728\u75c5\u7406\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u56e0\u7f3a\u4e4f\u9886\u57df\u77e5\u8bc6\u548cCoT\u63a8\u7406\u6b65\u9aa4\u53ef\u80fd\u5f15\u5165\u9519\u8bef\u3002", "method": "PathCoT\u7ed3\u5408\u75c5\u7406\u5b66\u4e13\u5bb6\u77e5\u8bc6\u6307\u5bfcMLLMs\u63a8\u7406\uff0c\u5e76\u5f15\u5165\u81ea\u8bc4\u4f30\u6b65\u9aa4\u4ee5\u51cf\u5c11\u7b54\u6848\u504f\u5dee\u3002", "result": "\u5728PathMMU\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86PathCoT\u5728\u75c5\u7406\u89c6\u89c9\u7406\u89e3\u548c\u63a8\u7406\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "PathCoT\u901a\u8fc7\u6574\u5408\u4e13\u5bb6\u77e5\u8bc6\u548c\u81ea\u8bc4\u4f30\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86MLLMs\u5728\u75c5\u7406\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2507.01182", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01182", "abs": "https://arxiv.org/abs/2507.01182", "authors": ["Zhuo Su", "Li Liu", "Matthias M\u00fcller", "Jiehua Zhang", "Diana Wofk", "Ming-Ming Cheng", "Matti Pietik\u00e4inen"], "title": "Rapid Salient Object Detection with Difference Convolutional Neural Networks", "comment": "16 pages, accepted in TPAMI", "summary": "This paper addresses the challenge of deploying salient object detection\n(SOD) on resource-constrained devices with real-time performance. While recent\nadvances in deep neural networks have improved SOD, existing top-leading models\nare computationally expensive. We propose an efficient network design that\ncombines traditional wisdom on SOD and the representation power of modern CNNs.\nLike biologically-inspired classical SOD methods relying on computing contrast\ncues to determine saliency of image regions, our model leverages Pixel\nDifference Convolutions (PDCs) to encode the feature contrasts. Differently,\nPDCs are incorporated in a CNN architecture so that the valuable contrast cues\nare extracted from rich feature maps. For efficiency, we introduce a difference\nconvolution reparameterization (DCR) strategy that embeds PDCs into standard\nconvolutions, eliminating computation and parameters at inference.\nAdditionally, we introduce SpatioTemporal Difference Convolution (STDC) for\nvideo SOD, enhancing the standard 3D convolution with spatiotemporal contrast\ncapture. Our models, SDNet for image SOD and STDNet for video SOD, achieve\nsignificant improvements in efficiency-accuracy trade-offs. On a Jetson Orin\ndevice, our models with $<$ 1M parameters operate at 46 FPS and 150 FPS on\nstreamed images and videos, surpassing the second-best lightweight models in\nour experiments by more than $2\\times$ and $3\\times$ in speed with superior\naccuracy. Code will be available at https://github.com/hellozhuo/stdnet.git.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7f51\u7edc\u8bbe\u8ba1\uff0c\u7ed3\u5408\u4f20\u7edf\u663e\u8457\u76ee\u6807\u68c0\u6d4b\uff08SOD\uff09\u4e0e\u73b0\u4ee3CNN\uff0c\u901a\u8fc7\u50cf\u7d20\u5dee\u5f02\u5377\u79ef\uff08PDCs\uff09\u548c\u5dee\u5f02\u5377\u79ef\u91cd\u53c2\u6570\u5316\uff08DCR\uff09\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709SOD\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\u3002", "method": "\u63d0\u51faSDNet\u548cSTDNet\uff0c\u5206\u522b\u7528\u4e8e\u56fe\u50cf\u548c\u89c6\u9891SOD\uff0c\u7ed3\u5408PDCs\u548cDCR\u7b56\u7565\uff0c\u4ee5\u53ca\u65f6\u7a7a\u5dee\u5f02\u5377\u79ef\uff08STDC\uff09\u3002", "result": "\u6a21\u578b\u5728Jetson Orin\u8bbe\u5907\u4e0a\u4ee546 FPS\uff08\u56fe\u50cf\uff09\u548c150 FPS\uff08\u89c6\u9891\uff09\u8fd0\u884c\uff0c\u53c2\u6570\u5c11\u4e8e1M\uff0c\u901f\u5ea6\u548c\u51c6\u786e\u6027\u5747\u4f18\u4e8e\u5176\u4ed6\u8f7b\u91cf\u7ea7\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u663e\u8457\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u5b9e\u65f6\u5e94\u7528\u3002"}}
{"id": "2507.01281", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01281", "abs": "https://arxiv.org/abs/2507.01281", "authors": ["Juan Chen", "Baolong Bi", "Wei Zhang", "Jingyan Sui", "Xiaofei Zhu", "Yuanzhuo Wang", "Lingrui Mei", "Shenghua Liu"], "title": "Rethinking All Evidence: Enhancing Trustworthy Retrieval-Augmented Generation via Conflict-Driven Summarization", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nintegrating their parametric knowledge with external retrieved content.\nHowever, knowledge conflicts caused by internal inconsistencies or noisy\nretrieved content can severely undermine the generation reliability of RAG\nsystems.In this work, we argue that LLMs should rethink all evidence, including\nboth retrieved content and internal knowledge, before generating responses.We\npropose CARE-RAG (Conflict-Aware and Reliable Evidence for RAG), a novel\nframework that improves trustworthiness through Conflict-Driven Summarization\nof all available evidence.CARE-RAG first derives parameter-aware evidence by\ncomparing parameter records to identify diverse internal perspectives. It then\nrefines retrieved evidences to produce context-aware evidence, removing\nirrelevant or misleading content. To detect and summarize conflicts, we distill\na 3B LLaMA3.2 model to perform conflict-driven summarization, enabling reliable\nsynthesis across multiple sources.To further ensure evaluation integrity, we\nintroduce a QA Repair step to correct outdated or ambiguous benchmark\nanswers.Experiments on revised QA datasets with retrieval data show that\nCARE-RAG consistently outperforms strong RAG baselines, especially in scenarios\nwith noisy or conflicting evidence.", "AI": {"tldr": "CARE-RAG \u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u51b2\u7a81\u9a71\u52a8\u7684\u8bc1\u636e\u603b\u7ed3\u63d0\u5347 RAG \u7cfb\u7edf\u7684\u53ef\u9760\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u89e3\u51b3 RAG \u7cfb\u7edf\u4e2d\u56e0\u77e5\u8bc6\u51b2\u7a81\u5bfc\u81f4\u7684\u751f\u6210\u4e0d\u53ef\u9760\u95ee\u9898\u3002", "method": "\u63d0\u51fa CARE-RAG \u6846\u67b6\uff0c\u5305\u62ec\u53c2\u6570\u611f\u77e5\u8bc1\u636e\u63d0\u53d6\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u8bc1\u636e\u4f18\u5316\u3001\u51b2\u7a81\u9a71\u52a8\u603b\u7ed3\u548c QA \u4fee\u590d\u6b65\u9aa4\u3002", "result": "\u5728\u566a\u58f0\u6216\u51b2\u7a81\u8bc1\u636e\u573a\u666f\u4e0b\uff0cCARE-RAG \u8868\u73b0\u4f18\u4e8e\u73b0\u6709 RAG \u57fa\u7ebf\u3002", "conclusion": "CARE-RAG \u901a\u8fc7\u51b2\u7a81\u9a71\u52a8\u603b\u7ed3\u663e\u8457\u63d0\u5347\u4e86 RAG \u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2507.01431", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01431", "abs": "https://arxiv.org/abs/2507.01431", "authors": ["Yoonseok Yang", "Minjune Kim", "Marlon Rondinelli", "Keren Shao"], "title": "Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading", "comment": "7 pages, 5 figues, 1 table", "summary": "Grading handwritten, open-ended responses remains a major bottleneck in large\nuniversity STEM courses. We introduce Pensieve (https://www.pensieve.co), an\nAI-assisted grading platform that leverages large language models (LLMs) to\ntranscribe and evaluate student work, providing instructors with rubric-aligned\nscores, transcriptions, and confidence ratings. Unlike prior tools that focus\nnarrowly on specific tasks like transcription or rubric generation, Pensieve\nsupports the entire grading pipeline-from scanned student submissions to final\nfeedback-within a human-in-the-loop interface.\n  Pensieve has been deployed in real-world courses at over 20 institutions and\nhas graded more than 300,000 student responses. We present system details and\nempirical results across four core STEM disciplines: Computer Science,\nMathematics, Physics, and Chemistry. Our findings show that Pensieve reduces\ngrading time by an average of 65%, while maintaining a 95.4% agreement rate\nwith instructor-assigned grades for high-confidence predictions.", "AI": {"tldr": "Pensieve\u662f\u4e00\u4e2aAI\u8f85\u52a9\u8bc4\u5206\u5e73\u53f0\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8f6c\u5f55\u548c\u8bc4\u4f30\u5b66\u751f\u4f5c\u4e1a\uff0c\u663e\u8457\u51cf\u5c11\u8bc4\u5206\u65f6\u95f4\u5e76\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21STEM\u8bfe\u7a0b\u4e2d\u624b\u5199\u5f00\u653e\u6027\u7b54\u6848\u8bc4\u5206\u7684\u6548\u7387\u74f6\u9888\u95ee\u9898\u3002", "method": "\u5f00\u53d1Pensieve\u5e73\u53f0\uff0c\u7ed3\u5408LLMs\u548c\u4eba\u5de5\u53cd\u9988\uff0c\u652f\u6301\u4ece\u626b\u63cf\u4f5c\u4e1a\u5230\u6700\u7ec8\u53cd\u9988\u7684\u5b8c\u6574\u8bc4\u5206\u6d41\u7a0b\u3002", "result": "\u572820\u591a\u6240\u673a\u6784\u4e2d\u90e8\u7f72\uff0c\u8bc4\u5206\u8d85\u8fc730\u4e07\u4efd\u4f5c\u4e1a\uff0c\u8bc4\u5206\u65f6\u95f4\u51cf\u5c1165%\uff0c\u9ad8\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u4e0e\u6559\u5e08\u8bc4\u5206\u4e00\u81f4\u7387\u8fbe95.4%\u3002", "conclusion": "Pensieve\u6709\u6548\u63d0\u5347\u8bc4\u5206\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u8bc4\u5206\u8d28\u91cf\uff0c\u9002\u7528\u4e8e\u591a\u5b66\u79d1STEM\u8bfe\u7a0b\u3002"}}
{"id": "2507.01030", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01030", "abs": "https://arxiv.org/abs/2507.01030", "authors": ["Reza Lotfi Navaei", "Mohammad Safarzadeh", "Seyed Mohammad Jafar Sobhani"], "title": "Optimizing Flamelet Generated Manifold Models: A Machine Learning Performance Study", "comment": "It has been submitted to ASME Journal of Heat and Mass Transfer", "summary": "In chemistry tabulations and Flamelet combustion models, the Flamelet\nGenerated Manifold (FGM) is recognized for its precision and physical\nrepresentation. The practical implementation of FGM requires a significant\nallocation of memory resources. FGM libraries are developed specifically for a\nspecific fuel and subsequently utilized for all numerical problems using\nmachine learning techniques. This research aims to develop libraries of Laminar\nFGM utilizing machine learning algorithms for application in combustion\nsimulations of methane fuel. This study employs four Machine Learning\nalgorithms to regenerate Flamelet libraries, based on an understanding of data\nsources, techniques, and data-driven concepts. 1. Multi-Layer Perceptron; 2.\nRandom Forest; 3. Linear Regression; 4. Support Vector Machine. Seven libraries\nwere identified as appropriate for constructing a database for training machine\nlearning models, giving an error rate of 2.30%. The default architectures of\neach method were evaluated to determine the optimal approach, leading to the\nselection of the MLP method as the primary choice. The method was enhanced\nthrough hyperparameter tuning to improve accuracy. The quantity of hidden\nlayers and neurons significantly influences method performance. The optimal\nmodel, comprising four hidden layers with 10, 15, 20, and 25 neurons\nrespectively, achieved an accuracy of 99.81%.", "AI": {"tldr": "\u7814\u7a76\u5229\u7528\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff08MLP\u3001\u968f\u673a\u68ee\u6797\u3001\u7ebf\u6027\u56de\u5f52\u3001SVM\uff09\u91cd\u5efa\u7532\u70f7\u71c3\u6599\u71c3\u70e7\u6a21\u62df\u4e2d\u7684FGM\u5e93\uff0cMLP\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u8fbe99.81%\u3002", "motivation": "FGM\u5e93\u5728\u71c3\u70e7\u6a21\u62df\u4e2d\u7cbe\u5ea6\u9ad8\u4f46\u5185\u5b58\u9700\u6c42\u5927\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u4f18\u5316FGM\u5e93\u7684\u751f\u6210\u3002", "method": "\u4f7f\u7528\u56db\u79cd\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff08MLP\u3001\u968f\u673a\u68ee\u6797\u3001\u7ebf\u6027\u56de\u5f52\u3001SVM\uff09\u91cd\u5efaFGM\u5e93\uff0c\u5e76\u901a\u8fc7\u8d85\u53c2\u6570\u8c03\u4f18\u4f18\u5316MLP\u3002", "result": "MLP\u65b9\u6cd5\u8868\u73b0\u6700\u4f18\uff0c\u56db\u9690\u85cf\u5c42\u7ed3\u6784\uff0810,15,20,25\u795e\u7ecf\u5143\uff09\u51c6\u786e\u7387\u8fbe99.81%\uff0c\u8bef\u5dee\u73872.30%\u3002", "conclusion": "MLP\u662f\u91cd\u5efaFGM\u5e93\u7684\u6700\u4f73\u9009\u62e9\uff0c\u8d85\u53c2\u6570\u8c03\u4f18\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u7532\u70f7\u71c3\u6599\u71c3\u70e7\u6a21\u62df\u3002"}}
{"id": "2507.01254", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01254", "abs": "https://arxiv.org/abs/2507.01254", "authors": ["Runze Cheng", "Xihang Qiu", "Ming Li", "Ye Zhang", "Chun Li", "Fei Yu"], "title": "Robust Brain Tumor Segmentation with Incomplete MRI Modalities Using H\u00f6lder Divergence and Mutual Information-Enhanced Knowledge Transfer", "comment": null, "summary": "Multimodal MRI provides critical complementary information for accurate brain\ntumor segmentation. However, conventional methods struggle when certain\nmodalities are missing due to issues such as image quality, protocol\ninconsistencies, patient allergies, or financial constraints. To address this,\nwe propose a robust single-modality parallel processing framework that achieves\nhigh segmentation accuracy even with incomplete modalities. Leveraging Holder\ndivergence and mutual information, our model maintains modality-specific\nfeatures while dynamically adjusting network parameters based on the available\ninputs. By using these divergence- and information-based loss functions, the\nframework effectively quantifies discrepancies between predictions and\nground-truth labels, resulting in consistently accurate segmentation. Extensive\nevaluations on the BraTS 2018 and BraTS 2020 datasets demonstrate superior\nperformance over existing methods in handling missing modalities.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5355\u6a21\u6001\u5e76\u884c\u5904\u7406\u7684\u9c81\u68d2\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406MRI\u6a21\u6001\u7f3a\u5931\u60c5\u51b5\u4e0b\u7684\u8111\u80bf\u7624\u5206\u5272\u95ee\u9898\uff0c\u901a\u8fc7Holder\u6563\u5ea6\u548c\u4e92\u4fe1\u606f\u4fdd\u6301\u6a21\u6001\u7279\u5f81\u5e76\u52a8\u6001\u8c03\u6574\u7f51\u7edc\u53c2\u6570\uff0c\u5728BraTS\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u591a\u6a21\u6001MRI\u5728\u8111\u80bf\u7624\u5206\u5272\u4e2d\u63d0\u4f9b\u4e92\u8865\u4fe1\u606f\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5728\u6a21\u6001\u7f3a\u5931\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5355\u6a21\u6001\u5e76\u884c\u5904\u7406\u6846\u67b6\uff0c\u7ed3\u5408Holder\u6563\u5ea6\u548c\u4e92\u4fe1\u606f\u8bbe\u8ba1\u635f\u5931\u51fd\u6570\uff0c\u52a8\u6001\u8c03\u6574\u7f51\u7edc\u53c2\u6570\u4ee5\u9002\u5e94\u8f93\u5165\u6a21\u6001\u3002", "result": "\u5728BraTS 2018\u548c2020\u6570\u636e\u96c6\u4e0a\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u6a21\u6001\u7f3a\u5931\u60c5\u51b5\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u6709\u6548\u5904\u7406\u6a21\u6001\u7f3a\u5931\u95ee\u9898\uff0c\u63d0\u5347\u5206\u5272\u51c6\u786e\u6027\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.01297", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.01297", "abs": "https://arxiv.org/abs/2507.01297", "authors": ["Xinxi Lyu", "Michael Duan", "Rulin Shao", "Pang Wei Koh", "Sewon Min"], "title": "Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive Benchmarks", "comment": "33 pages, 2 figures, 27 tables", "summary": "Retrieval-augmented Generation (RAG) has primarily been studied in limited\nsettings, such as factoid question answering; more challenging,\nreasoning-intensive benchmarks have seen limited success from minimal RAG. In\nthis work, we challenge this prevailing view on established,\nreasoning-intensive benchmarks: MMLU, MMLU Pro, AGI Eval, GPQA, and MATH. We\nidentify a key missing component in prior work: a usable, web-scale datastore\naligned with the breadth of pretraining data. To this end, we introduce\nCompactDS: a diverse, high-quality, web-scale datastore that achieves high\nretrieval accuracy and subsecond latency on a single-node. The key insights are\n(1) most web content can be filtered out without sacrificing coverage, and a\ncompact, high-quality subset is sufficient; and (2) combining in-memory\napproximate nearest neighbor (ANN) retrieval and on-disk exact search balances\nspeed and recall. Using CompactDS, we show that a minimal RAG pipeline achieves\nconsistent accuracy improvements across all benchmarks and model sizes\n(8B--70B), with relative gains of 10% on MMLU, 33% on MMLU Pro, 14% on GPQA,\nand 19% on MATH. No single data source suffices alone, highlighting the\nimportance of diversity of sources (web crawls, curated math, academic papers,\ntextbooks). Finally, we show that our carefully designed in-house datastore\nmatches or outperforms web search engines such as Google Search, as well as\nrecently proposed, complex agent-based RAG systems--all while maintaining\nsimplicity, reproducibility, and self-containment. We release CompactDS and our\nretrieval pipeline, supporting future research exploring retrieval-based AI\nsystems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCompactDS\uff0c\u4e00\u4e2a\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u7684\u7f51\u7edc\u89c4\u6a21\u6570\u636e\u5b58\u50a8\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5728\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RAG\u5728\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u4e0e\u9884\u8bad\u7ec3\u6570\u636e\u5e7f\u5ea6\u5bf9\u9f50\u7684\u6570\u636e\u5b58\u50a8\u3002", "method": "\u5f15\u5165CompactDS\uff0c\u901a\u8fc7\u8fc7\u6ee4\u4f4e\u8d28\u91cf\u5185\u5bb9\u5e76\u7ed3\u5408\u5185\u5b58\u8fd1\u4f3c\u6700\u8fd1\u90bb\uff08ANN\uff09\u68c0\u7d22\u548c\u78c1\u76d8\u7cbe\u786e\u641c\u7d22\uff0c\u5b9e\u73b0\u9ad8\u6548\u68c0\u7d22\u3002", "result": "CompactDS\u5728\u6240\u6709\u57fa\u51c6\u6d4b\u8bd5\uff08MMLU\u3001MMLU Pro\u3001GPQA\u3001MATH\uff09\u4e2d\u5747\u5e26\u6765\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u76f8\u5bf9\u589e\u76ca\u8fbe10%-33%\u3002", "conclusion": "CompactDS\u8bc1\u660e\u4e86\u9ad8\u8d28\u91cf\u591a\u6837\u5316\u6570\u636e\u5b58\u50a8\u7684\u91cd\u8981\u6027\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u590d\u6742\u7cfb\u7edf\uff0c\u4e14\u5177\u5907\u7b80\u5355\u6027\u548c\u53ef\u590d\u73b0\u6027\u3002"}}
{"id": "2507.01446", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01446", "abs": "https://arxiv.org/abs/2507.01446", "authors": ["Abd Elrahman Amer", "Magdi Amer"], "title": "Using multi-agent architecture to mitigate the risk of LLM hallucinations", "comment": null, "summary": "Improving customer service quality and response time are critical factors for\nmaintaining customer loyalty and increasing a company's market share. While\nadopting emerging technologies such as Large Language Models (LLMs) is becoming\na necessity to achieve these goals, the risk of hallucination remains a major\nchallenge. In this paper, we present a multi-agent system to handle customer\nrequests sent via SMS. This system integrates LLM based agents with fuzzy logic\nto mitigate hallucination risks.", "AI": {"tldr": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7ed3\u5408\u6a21\u7cca\u903b\u8f91\u4ee5\u51cf\u5c11LLM\u5e7b\u89c9\u98ce\u9669\uff0c\u63d0\u5347\u5ba2\u670d\u8d28\u91cf\u3002", "motivation": "\u63d0\u9ad8\u5ba2\u670d\u8d28\u91cf\u548c\u54cd\u5e94\u901f\u5ea6\u4ee5\u4fdd\u6301\u5ba2\u6237\u5fe0\u8bda\u5ea6\u548c\u5e02\u573a\u4efd\u989d\u3002", "method": "\u96c6\u6210\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u4e0e\u6a21\u7cca\u903b\u8f91\u5904\u7406\u5ba2\u6237\u77ed\u4fe1\u8bf7\u6c42\u3002", "result": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6709\u6548\u51cf\u5c11LLM\u5e7b\u89c9\u98ce\u9669\u3002", "conclusion": "\u7ed3\u5408\u6a21\u7cca\u903b\u8f91\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u662f\u63d0\u5347\u5ba2\u670d\u8d28\u91cf\u7684\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2507.01031", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.01031", "abs": "https://arxiv.org/abs/2507.01031", "authors": ["Fanchen Bu", "Kijung Shin"], "title": "PyTorch-based Geometric Learning with Non-CUDA Processing Units: Experiences from Intel Gaudi-v2 HPUs", "comment": "Conference paper: Accepted in Korea Computer Congress (KCC) 2025. The\n  library is available at https://github.com/bokveizen/gaudi-geometric-learning", "summary": "Geometric learning has emerged as a powerful paradigm for modeling\nnon-Euclidean data, especially graph-structured ones, with applications\nspanning social networks, molecular structures, knowledge graphs, and\nrecommender systems. While Nvidia's CUDA-enabled graphics processing units\n(GPUs) largely dominate the hardware landscape, emerging accelerators such as\nIntel's Gaudi Habana Processing Units (HPUs) offer competitive performance and\nenergy efficiency. However, the usage of such non-CUDA processing units\nrequires significant engineering effort and novel software adaptations. In this\nwork, we present our experiences porting PyTorch-based geometric learning\nframeworks to Gaudi-v2 HPUs. We introduce a collection of core utilities that\nrestore essential operations (e.g., scatter, sparse indexing, k-nearest\nneighbors) on Gaudi-v2 HPUs, and we consolidate sixteen guided tutorials and\neleven real-world examples with diagnostic analyses of encountered failures and\ndetailed workarounds. We collect all our experiences into a publicly accessible\nGitHub repository. Our contributions lower the barrier for researchers to\nexperiment with geometric-learning algorithms and models on non-CUDA hardware,\nproviding a foundation for further optimization and cross-platform portability.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u5c06\u57fa\u4e8ePyTorch\u7684\u51e0\u4f55\u5b66\u4e60\u6846\u67b6\u79fb\u690d\u5230Gaudi-v2 HPUs\u7684\u7ecf\u9a8c\uff0c\u63d0\u4f9b\u4e86\u6838\u5fc3\u5de5\u5177\u548c\u6559\u7a0b\uff0c\u964d\u4f4e\u4e86\u975eCUDA\u786c\u4ef6\u4e0a\u7684\u7814\u7a76\u95e8\u69db\u3002", "motivation": "\u5c3d\u7ba1Nvidia\u7684CUDA GPU\u4e3b\u5bfc\u786c\u4ef6\u5e02\u573a\uff0c\u4f46\u65b0\u5174\u52a0\u901f\u5668\u5982Gaudi HPUs\u5728\u6027\u80fd\u548c\u80fd\u6548\u4e0a\u5177\u6709\u7ade\u4e89\u529b\uff0c\u4f46\u9700\u8981\u5927\u91cf\u5de5\u7a0b\u548c\u8f6f\u4ef6\u9002\u914d\u3002", "method": "\u5f00\u53d1\u4e86\u6838\u5fc3\u5de5\u5177\u4ee5\u652f\u6301Gaudi-v2 HPUs\u4e0a\u7684\u57fa\u672c\u64cd\u4f5c\uff0c\u5e76\u63d0\u4f9b\u4e86\u6559\u7a0b\u548c\u5b9e\u9645\u6848\u4f8b\uff0c\u5206\u6790\u4e86\u5931\u8d25\u539f\u56e0\u548c\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u6210\u529f\u79fb\u690d\u4e86\u6846\u67b6\uff0c\u5e76\u516c\u5f00\u4e86GitHub\u8d44\u6e90\u5e93\uff0c\u4e3a\u7814\u7a76\u8005\u5728\u975eCUDA\u786c\u4ef6\u4e0a\u5b9e\u9a8c\u51e0\u4f55\u5b66\u4e60\u63d0\u4f9b\u4e86\u4fbf\u5229\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u51e0\u4f55\u5b66\u4e60\u5728\u975eCUDA\u786c\u4ef6\u4e0a\u7684\u4f18\u5316\u548c\u8de8\u5e73\u53f0\u79fb\u690d\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.01255", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01255", "abs": "https://arxiv.org/abs/2507.01255", "authors": ["Xiao Liu", "Jiawei Zhang"], "title": "AIGVE-MACS: Unified Multi-Aspect Commenting and Scoring Model for AI-Generated Video Evaluation", "comment": "Working in Progress", "summary": "The rapid advancement of AI-generated video models has created a pressing\nneed for robust and interpretable evaluation frameworks. Existing metrics are\nlimited to producing numerical scores without explanatory comments, resulting\nin low interpretability and human evaluation alignment. To address those\nchallenges, we introduce AIGVE-MACS, a unified model for AI-Generated Video\nEvaluation(AIGVE), which can provide not only numerical scores but also\nmulti-aspect language comment feedback in evaluating these generated videos.\nCentral to our approach is AIGVE-BENCH 2, a large-scale benchmark comprising\n2,500 AI-generated videos and 22,500 human-annotated detailed comments and\nnumerical scores across nine critical evaluation aspects. Leveraging\nAIGVE-BENCH 2, AIGVE-MACS incorporates recent Vision-Language Models with a\nnovel token-wise weighted loss and a dynamic frame sampling strategy to better\nalign with human evaluators. Comprehensive experiments across supervised and\nzero-shot benchmarks demonstrate that AIGVE-MACS achieves state-of-the-art\nperformance in both scoring correlation and comment quality, significantly\noutperforming prior baselines including GPT-4o and VideoScore. In addition, we\nfurther showcase a multi-agent refinement framework where feedback from\nAIGVE-MACS drives iterative improvements in video generation, leading to 53.5%\nquality enhancement. This work establishes a new paradigm for comprehensive,\nhuman-aligned evaluation of AI-generated videos. We release the AIGVE-BENCH 2\nand AIGVE-MACS at https://huggingface.co/xiaoliux/AIGVE-MACS.", "AI": {"tldr": "\u63d0\u51faAIGVE-MACS\u6a21\u578b\uff0c\u7528\u4e8eAI\u751f\u6210\u89c6\u9891\u7684\u8bc4\u4f30\uff0c\u63d0\u4f9b\u5206\u6570\u548c\u591a\u65b9\u9762\u8bed\u8a00\u53cd\u9988\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709AI\u751f\u6210\u89c6\u9891\u8bc4\u4f30\u6307\u6807\u7f3a\u4e4f\u89e3\u91ca\u6027\uff0c\u96be\u4ee5\u4e0e\u4eba\u7c7b\u8bc4\u4ef7\u5bf9\u9f50\u3002", "method": "\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001\u52a0\u6743\u635f\u5931\u548c\u52a8\u6001\u5e27\u91c7\u6837\u7b56\u7565\uff0c\u6784\u5efaAIGVE-BENCH 2\u57fa\u51c6\u3002", "result": "\u5728\u8bc4\u5206\u76f8\u5173\u6027\u548c\u8bc4\u8bba\u8d28\u91cf\u4e0a\u8fbe\u5230\u6700\u4f18\uff0c\u89c6\u9891\u751f\u6210\u8d28\u91cf\u63d0\u534753.5%\u3002", "conclusion": "AIGVE-MACS\u4e3aAI\u751f\u6210\u89c6\u9891\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5168\u9762\u3001\u4eba\u7c7b\u5bf9\u9f50\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2507.01299", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01299", "abs": "https://arxiv.org/abs/2507.01299", "authors": ["Kai Liu", "Bowen Xu", "Shaoyu Wu", "Xin Chen", "Hao Zhou", "Yongliang Tao", "Lulu Hu"], "title": "La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation", "comment": "ICML 2025 Acceptance", "summary": "Activation sparsity can reduce the computational overhead and memory\ntransfers during the forward pass of Large Language Model (LLM) inference.\nExisting methods face limitations, either demanding time-consuming recovery\ntraining that hinders real-world adoption, or relying on empirical\nmagnitude-based pruning, which causes fluctuating sparsity and unstable\ninference speed-up. This paper introduces LaRoSA (Layerwise Rotated Sparse\nActivation), a novel method for activation sparsification designed to improve\nLLM efficiency without requiring additional training or magnitude-based\npruning. We leverage layerwise orthogonal rotations to transform input\nactivations into rotated forms that are more suitable for sparsification. By\nemploying a Top-K selection approach within the rotated activations, we achieve\nconsistent model-level sparsity and reliable wall-clock time speed-up. LaRoSA\nis effective across various sizes and types of LLMs, demonstrating minimal\nperformance degradation and robust inference acceleration. Specifically, for\nLLaMA2-7B at 40% sparsity, LaRoSA achieves a mere 0.17 perplexity gap with a\nconsistent 1.30x wall-clock time speed-up, and reduces the accuracy gap in\nzero-shot tasks compared to the dense model to just 0.54%, while surpassing\nTEAL by 1.77% and CATS by 17.14%.", "AI": {"tldr": "LaRoSA\u662f\u4e00\u79cd\u65b0\u7684\u6fc0\u6d3b\u7a00\u758f\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c42\u95f4\u6b63\u4ea4\u65cb\u8f6c\u548cTop-K\u9009\u62e9\uff0c\u63d0\u5347LLM\u6548\u7387\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u57fa\u4e8e\u5e45\u5ea6\u7684\u526a\u679d\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u6062\u590d\u8bad\u7ec3\u8017\u65f6\u6216\u57fa\u4e8e\u5e45\u5ea6\u7684\u526a\u679d\u5bfc\u81f4\u7a00\u758f\u6027\u6ce2\u52a8\u7684\u95ee\u9898\uff0cLaRoSA\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5229\u7528\u5c42\u95f4\u6b63\u4ea4\u65cb\u8f6c\u5c06\u8f93\u5165\u6fc0\u6d3b\u8f6c\u6362\u4e3a\u66f4\u9002\u5408\u7a00\u758f\u5316\u7684\u5f62\u5f0f\uff0c\u5e76\u901a\u8fc7Top-K\u9009\u62e9\u5b9e\u73b0\u4e00\u81f4\u7684\u7a00\u758f\u6027\u3002", "result": "\u5728LLaMA2-7B\u4e0a\uff0c40%\u7a00\u758f\u6027\u4e0b\uff0cLaRoSA\u4ec5\u589e\u52a00.17\u56f0\u60d1\u5ea6\uff0c\u52a0\u901f1.30\u500d\uff0c\u96f6\u6837\u672c\u4efb\u52a1\u51c6\u786e\u7387\u5dee\u8ddd\u964d\u81f30.54%\u3002", "conclusion": "LaRoSA\u5728\u5404\u79cdLLM\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u7a00\u758f\u5316\u6548\u679c\u7a33\u5b9a\u4e14\u6027\u80fd\u635f\u5931\u5c0f\u3002"}}
{"id": "2507.01489", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.01489", "abs": "https://arxiv.org/abs/2507.01489", "authors": ["Yanfei Zhang"], "title": "Agent-as-Tool: A Study on the Hierarchical Decision Making with Reinforcement Learning", "comment": "12 pages", "summary": "Large Language Models (LLMs) have emerged as one of the most significant\ntechnological advancements in artificial intelligence in recent years. Their\nability to understand, generate, and reason with natural language has\ntransformed how we interact with AI systems. With the development of LLM-based\nagents and reinforcement-learning-based reasoning models, the study of applying\nreinforcement learning in agent frameworks has become a new research focus.\nHowever, all previous studies face the challenge of deciding the tool calling\nprocess and the reasoning process simultaneously, and the chain of reasoning\nwas solely relied on the unprocessed raw result with redundant information and\nsymbols unrelated to the task from the tool, which impose a heavy burden on the\nmodel's capability to reason. Therefore, in our research, we proposed a\nhierarchical framework Agent-as-tool that detach the tool calling process and\nthe reasoning process, which enables the model to focus on the verbally\nreasoning process while the tool calling process is handled by another agent.\nOur work had achieved comparable results with only a slight reinforcement\nfine-tuning on 180 samples, and had achieved exceptionally well performance in\nBamboogle with 63.2% of exact match and 75.2% in cover exact match, exceeding\nSearch-R1 by 4.8% in exact match and 3.2% in cover exact match.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u6846\u67b6Agent-as-tool\uff0c\u5c06\u5de5\u5177\u8c03\u7528\u8fc7\u7a0b\u4e0e\u63a8\u7406\u8fc7\u7a0b\u5206\u79bb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728\u5de5\u5177\u8c03\u7528\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u6a21\u578b\u9700\u540c\u65f6\u5904\u7406\u5197\u4f59\u4fe1\u606f\u548c\u65e0\u5173\u7b26\u53f7\uff0c\u589e\u52a0\u4e86\u63a8\u7406\u8d1f\u62c5\u3002", "method": "\u91c7\u7528\u5206\u5c42\u6846\u67b6Agent-as-tool\uff0c\u5c06\u5de5\u5177\u8c03\u7528\u548c\u63a8\u7406\u8fc7\u7a0b\u5206\u79bb\uff0c\u8ba9\u6a21\u578b\u4e13\u6ce8\u4e8e\u63a8\u7406\u3002", "result": "\u5728\u5c11\u91cf\u6837\u672c\uff08180\u4e2a\uff09\u4e0a\u5fae\u8c03\u540e\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0cBamboogle\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8fc7Search-R1\u3002", "conclusion": "Agent-as-tool\u6846\u67b6\u6709\u6548\u51cf\u8f7b\u4e86\u6a21\u578b\u8d1f\u62c5\uff0c\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\uff0c\u4e3aLLM\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.01032", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.01032", "abs": "https://arxiv.org/abs/2507.01032", "authors": ["Nan Mu", "Hongbo Yang", "Chen Zhao"], "title": "An Uncertainty-Aware Dynamic Decision Framework for Progressive Multi-Omics Integration in Classification Tasks", "comment": null, "summary": "Background and Objective: High-throughput multi-omics technologies have\nproven invaluable for elucidating disease mechanisms and enabling early\ndiagnosis. However, the high cost of multi-omics profiling imposes a\nsignificant economic burden, with over reliance on full omics data potentially\nleading to unnecessary resource consumption. To address these issues, we\npropose an uncertainty-aware, multi-view dynamic decision framework for omics\ndata classification that aims to achieve high diagnostic accuracy while\nminimizing testing costs. Methodology: At the single-omics level, we refine the\nactivation functions of neural networks to generate Dirichlet distribution\nparameters, utilizing subjective logic to quantify both the belief masses and\nuncertainty mass of classification results. Belief mass reflects the support of\na specific omics modality for a disease class, while the uncertainty parameter\ncaptures limitations in data quality and model discriminability, providing a\nmore trustworthy basis for decision-making. At the multi omics level, we employ\na fusion strategy based on Dempster-Shafer theory to integrate heterogeneous\nmodalities, leveraging their complementarity to boost diagnostic accuracy and\nrobustness. A dynamic decision mechanism is then applied that omics data are\nincrementally introduced for each patient until either all data sources are\nutilized or the model confidence exceeds a predefined threshold, potentially\nbefore all data sources are utilized. Results and Conclusion: We evaluate our\napproach on four benchmark multi-omics datasets, ROSMAP, LGG, BRCA, and KIPAN.\nIn three datasets, over 50% of cases achieved accurate classification using a\nsingle omics modality, effectively reducing redundant testing. Meanwhile, our\nmethod maintains diagnostic performance comparable to full-omics models and\npreserves essential biological insights.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u591a\u89c6\u56fe\u52a8\u6001\u51b3\u7b56\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u7ec4\u5b66\u6570\u636e\u5206\u7c7b\uff0c\u65e8\u5728\u964d\u4f4e\u6d4b\u8bd5\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u3002", "motivation": "\u591a\u7ec4\u5b66\u6280\u672f\u6210\u672c\u9ad8\u6602\uff0c\u8fc7\u5ea6\u4f9d\u8d56\u5b8c\u6574\u7ec4\u5b66\u6570\u636e\u53ef\u80fd\u5bfc\u81f4\u8d44\u6e90\u6d6a\u8d39\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u51cf\u5c11\u5197\u4f59\u6d4b\u8bd5\u53c8\u80fd\u4fdd\u6301\u8bca\u65ad\u51c6\u786e\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u5728\u5355\u7ec4\u5b66\u5c42\u9762\uff0c\u901a\u8fc7\u6539\u8fdb\u795e\u7ecf\u7f51\u7edc\u6fc0\u6d3b\u51fd\u6570\u751f\u6210Dirichlet\u5206\u5e03\u53c2\u6570\uff0c\u5229\u7528\u4e3b\u89c2\u903b\u8f91\u91cf\u5316\u5206\u7c7b\u7ed3\u679c\u7684\u7f6e\u4fe1\u5ea6\u548c\u4e0d\u786e\u5b9a\u6027\uff1b\u5728\u591a\u7ec4\u5b66\u5c42\u9762\uff0c\u57fa\u4e8eDempster-Shafer\u7406\u8bba\u878d\u5408\u5f02\u6784\u6a21\u6001\uff0c\u52a8\u6001\u51b3\u7b56\u673a\u5236\u9010\u6b65\u5f15\u5165\u6570\u636e\u76f4\u81f3\u6ee1\u8db3\u6761\u4ef6\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08ROSMAP\u3001LGG\u3001BRCA\u3001KIPAN\uff09\u4e0a\u8bc4\u4f30\uff0c\u8d85\u8fc750%\u7684\u6848\u4f8b\u901a\u8fc7\u5355\u7ec4\u5b66\u6a21\u6001\u5b9e\u73b0\u51c6\u786e\u5206\u7c7b\uff0c\u51cf\u5c11\u4e86\u5197\u4f59\u6d4b\u8bd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u5b8c\u6574\u7ec4\u5b66\u6a21\u578b\u76f8\u5f53\u7684\u8bca\u65ad\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u964d\u4f4e\u6d4b\u8bd5\u6210\u672c\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u548c\u751f\u7269\u5b66\u6d1e\u5bdf\u529b\uff0c\u4e3a\u591a\u7ec4\u5b66\u6570\u636e\u5206\u7c7b\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01269", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.01269", "abs": "https://arxiv.org/abs/2507.01269", "authors": ["Mohammad Jahanbakht", "Alex Olsen", "Ross Marchant", "Emilie Fillols", "Mostafa Rahimi Azghadi"], "title": "Advancements in Weed Mapping: A Systematic Review", "comment": null, "summary": "Weed mapping plays a critical role in precision management by providing\naccurate and timely data on weed distribution, enabling targeted control and\nreduced herbicide use. This minimizes environmental impacts, supports\nsustainable land management, and improves outcomes across agricultural and\nnatural environments. Recent advances in weed mapping leverage ground-vehicle\nRed Green Blue (RGB) cameras, satellite and drone-based remote sensing combined\nwith sensors such as spectral, Near Infra-Red (NIR), and thermal cameras. The\nresulting data are processed using advanced techniques including big data\nanalytics and machine learning, significantly improving the spatial and\ntemporal resolution of weed maps and enabling site-specific management\ndecisions. Despite a growing body of research in this domain, there is a lack\nof comprehensive literature reviews specifically focused on weed mapping. In\nparticular, the absence of a structured analysis spanning the entire mapping\npipeline, from data acquisition to processing techniques and mapping tools,\nlimits progress in the field. This review addresses these gaps by\nsystematically examining state-of-the-art methods in data acquisition (sensor\nand platform technologies), data processing (including annotation and\nmodelling), and mapping techniques (such as spatiotemporal analysis and\ndecision support tools). Following PRISMA guidelines, we critically evaluate\nand synthesize key findings from the literature to provide a holistic\nunderstanding of the weed mapping landscape. This review serves as a\nfoundational reference to guide future research and support the development of\nefficient, scalable, and sustainable weed management systems.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u6742\u8349\u6d4b\u7ed8\u7684\u6700\u65b0\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u4ece\u6570\u636e\u91c7\u96c6\u5230\u5904\u7406\u6280\u672f\u7684\u6587\u732e\u7a7a\u767d\uff0c\u4e3a\u7cbe\u51c6\u6742\u8349\u7ba1\u7406\u63d0\u4f9b\u53c2\u8003\u3002", "motivation": "\u6742\u8349\u6d4b\u7ed8\u5bf9\u7cbe\u51c6\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u5168\u9762\u7efc\u8ff0\uff0c\u9650\u5236\u4e86\u8be5\u9886\u57df\u8fdb\u5c55\u3002", "method": "\u9075\u5faaPRISMA\u6307\u5357\uff0c\u7cfb\u7edf\u5206\u6790\u6570\u636e\u91c7\u96c6\u3001\u5904\u7406\u53ca\u6d4b\u7ed8\u6280\u672f\u7684\u6700\u65b0\u65b9\u6cd5\u3002", "result": "\u7efc\u8ff0\u4e3a\u6742\u8349\u6d4b\u7ed8\u63d0\u4f9b\u4e86\u5168\u9762\u89c6\u89d2\uff0c\u652f\u6301\u9ad8\u6548\u3001\u53ef\u6301\u7eed\u7684\u6742\u8349\u7ba1\u7406\u7cfb\u7edf\u5f00\u53d1\u3002", "conclusion": "\u672c\u6587\u4e3a\u672a\u6765\u7814\u7a76\u548c\u6742\u8349\u7ba1\u7406\u7cfb\u7edf\u7684\u5f00\u53d1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.01334", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01334", "abs": "https://arxiv.org/abs/2507.01334", "authors": ["Nifu Dan", "Yujun Cai", "Yiwei Wang"], "title": "Symbolic or Numerical? Understanding Physics Problem Solving in Reasoning LLMs", "comment": null, "summary": "Navigating the complexities of physics reasoning has long been a difficult\ntask for Large Language Models (LLMs), requiring a synthesis of profound\nconceptual understanding and adept problem-solving techniques. In this study,\nwe investigate the application of advanced instruction-tuned reasoning models,\nsuch as Deepseek-R1, to address a diverse spectrum of physics problems curated\nfrom the challenging SciBench benchmark. Our comprehensive experimental\nevaluation reveals the remarkable capabilities of reasoning models. Not only do\nthey achieve state-of-the-art accuracy in answering intricate physics\nquestions, but they also generate distinctive reasoning patterns that emphasize\non symbolic derivation. Furthermore, our findings indicate that even for these\nhighly sophisticated reasoning models, the strategic incorporation of few-shot\nprompting can still yield measurable improvements in overall accuracy,\nhighlighting the potential for continued performance gains.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u9ad8\u7ea7\u6307\u4ee4\u8c03\u4f18\u63a8\u7406\u6a21\u578b\uff08\u5982Deepseek-R1\uff09\u5728\u89e3\u51b3\u590d\u6742\u7269\u7406\u95ee\u9898\u4e0a\u7684\u8868\u73b0\uff0c\u5c55\u793a\u4e86\u5176\u5728SciBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u5353\u8d8a\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5c11\u6837\u672c\u63d0\u793a\u8fdb\u4e00\u6b65\u63d0\u5347\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7269\u7406\u63a8\u7406\u4e2d\u7684\u590d\u6742\u6027\u95ee\u9898\uff0c\u9700\u8981\u7ed3\u5408\u6df1\u523b\u7684\u6982\u5ff5\u7406\u89e3\u548c\u95ee\u9898\u89e3\u51b3\u6280\u5de7\u3002", "method": "\u5e94\u7528\u9ad8\u7ea7\u6307\u4ee4\u8c03\u4f18\u63a8\u7406\u6a21\u578b\uff08\u5982Deepseek-R1\uff09\u5904\u7406SciBench\u57fa\u51c6\u4e2d\u7684\u591a\u6837\u5316\u7269\u7406\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u5c11\u6837\u672c\u63d0\u793a\u7b56\u7565\u3002", "result": "\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u7269\u7406\u95ee\u9898\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\uff0c\u5e76\u751f\u6210\u72ec\u7279\u7684\u7b26\u53f7\u63a8\u5bfc\u63a8\u7406\u6a21\u5f0f\uff1b\u5c11\u6837\u672c\u63d0\u793a\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u9ad8\u7ea7\u63a8\u7406\u6a21\u578b\u5728\u7269\u7406\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u5353\u8d8a\uff0c\u4e14\u5c11\u6837\u672c\u63d0\u793a\u4ecd\u6709\u6f5c\u529b\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2507.01597", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01597", "abs": "https://arxiv.org/abs/2507.01597", "authors": ["Yuehang Si", "Zefan Zeng", "Jincai Huang", "Qing Cheng"], "title": "T3DM: Test-Time Training-Guided Distribution Shift Modelling for Temporal Knowledge Graph Reasoning", "comment": null, "summary": "Temporal Knowledge Graph (TKG) is an efficient method for describing the\ndynamic development of facts along a timeline. Most research on TKG reasoning\n(TKGR) focuses on modelling the repetition of global facts and designing\npatterns of local historical facts. However, they face two significant\nchallenges: inadequate modeling of the event distribution shift between\ntraining and test samples, and reliance on random entity substitution for\ngenerating negative samples, which often results in low-quality sampling. To\nthis end, we propose a novel distributional feature modeling approach for\ntraining TKGR models, Test-Time Training-guided Distribution shift Modelling\n(T3DM), to adjust the model based on distribution shift and ensure the global\nconsistency of model reasoning. In addition, we design a negative-sampling\nstrategy to generate higher-quality negative quadruples based on adversarial\ntraining. Extensive experiments show that T3DM provides better and more robust\nresults than the state-of-the-art baselines in most cases.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684TKG\u63a8\u7406\u65b9\u6cd5T3DM\uff0c\u901a\u8fc7\u5efa\u6a21\u5206\u5e03\u504f\u79fb\u548c\u8bbe\u8ba1\u9ad8\u8d28\u91cf\u8d1f\u91c7\u6837\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u63a8\u7406\u7684\u5168\u5c40\u4e00\u81f4\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709TKG\u63a8\u7406\u65b9\u6cd5\u5728\u5efa\u6a21\u4e8b\u4ef6\u5206\u5e03\u504f\u79fb\u548c\u751f\u6210\u9ad8\u8d28\u91cf\u8d1f\u6837\u672c\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5f71\u54cd\u4e86\u63a8\u7406\u6027\u80fd\u3002", "method": "\u63d0\u51faT3DM\u65b9\u6cd5\uff0c\u7ed3\u5408\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u6307\u5bfc\u7684\u5206\u5e03\u504f\u79fb\u5efa\u6a21\u548c\u57fa\u4e8e\u5bf9\u6297\u8bad\u7ec3\u7684\u8d1f\u91c7\u6837\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660eT3DM\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "T3DM\u901a\u8fc7\u4f18\u5316\u5206\u5e03\u504f\u79fb\u5efa\u6a21\u548c\u8d1f\u91c7\u6837\uff0c\u663e\u8457\u63d0\u5347\u4e86TKG\u63a8\u7406\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.01034", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01034", "abs": "https://arxiv.org/abs/2507.01034", "authors": ["Asma Agaal", "Mansour Essgaer", "Hend M. Farkash", "Zulaiha Ali Othman"], "title": "Data-driven Insights for Informed Decision-Making: Applying LSTM Networks for Robust Electricity Forecasting in Libya", "comment": "This article was published in International Journal of Intelligent\n  Systems and Applications (IJISA) (MECS Press), Vol. 17, No. 3, 8 Jun. 2025,\n  DOI: https://doi.org/10.5815/ijisa.2025.03.05", "summary": "Accurate electricity forecasting is crucial for grid stability and energy\nplanning, especially in Benghazi, Libya, where frequent load shedding,\ngeneration deficits, and infrastructure limitations persist. This study\nproposes a data-driven approach to forecast electricity load, generation, and\ndeficits for 2025 using historical data from 2019 (a year marked by\ninstability) and 2023 (a more stable year). Multiple time series models were\napplied, including ARIMA, seasonal ARIMA, dynamic regression ARIMA, exponential\nsmoothing, extreme gradient boosting, and Long Short-Term Memory (LSTM) neural\nnetworks. The dataset was enhanced through missing value imputation, outlier\nsmoothing, and log transformation. Performance was assessed using mean squared\nerror, root mean squared error, mean absolute error, and mean absolute\npercentage error. LSTM outperformed all other models, showing strong\ncapabilities in modeling non-stationary and seasonal patterns. A key\ncontribution of this work is an optimized LSTM framework that integrates\nexogenous factors such as temperature and humidity, offering robust performance\nin forecasting multiple electricity indicators. These results provide practical\ninsights for policymakers and grid operators to enable proactive load\nmanagement and resource planning in data-scarce, volatile regions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u5386\u53f2\u6570\u636e\u9884\u6d4b2025\u5e74\u5229\u6bd4\u4e9a\u73ed\u52a0\u897f\u7684\u7535\u529b\u8d1f\u8377\u3001\u53d1\u7535\u91cf\u548c\u7f3a\u53e3\uff0cLSTM\u6a21\u578b\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u73ed\u52a0\u897f\u7535\u529b\u4f9b\u5e94\u4e0d\u7a33\u5b9a\uff0c\u57fa\u7840\u8bbe\u65bd\u6709\u9650\uff0c\u51c6\u786e\u7684\u7535\u529b\u9884\u6d4b\u5bf9\u7535\u7f51\u7a33\u5b9a\u548c\u80fd\u6e90\u89c4\u5212\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u591a\u79cd\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\uff08\u5982ARIMA\u3001\u5b63\u8282\u6027ARIMA\u3001LSTM\u7b49\uff09\uff0c\u5e76\u901a\u8fc7\u7f3a\u5931\u503c\u586b\u8865\u3001\u5f02\u5e38\u503c\u5e73\u6ed1\u548c\u5bf9\u6570\u53d8\u6362\u589e\u5f3a\u6570\u636e\u3002", "result": "LSTM\u6a21\u578b\u5728\u9884\u6d4b\u975e\u5e73\u7a33\u548c\u5b63\u8282\u6027\u6a21\u5f0f\u65b9\u9762\u8868\u73b0\u6700\u4f18\uff0c\u7ed3\u5408\u5916\u751f\u56e0\u7d20\uff08\u5982\u6e29\u6e7f\u5ea6\uff09\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "\u4f18\u5316\u7684LSTM\u6846\u67b6\u4e3a\u6570\u636e\u7a00\u7f3a\u4e14\u4e0d\u7a33\u5b9a\u7684\u5730\u533a\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u7535\u529b\u9884\u6d4b\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u653f\u7b56\u5236\u5b9a\u8005\u548c\u7535\u7f51\u8fd0\u8425\u5546\u8fdb\u884c\u8d44\u6e90\u89c4\u5212\u3002"}}
{"id": "2507.01275", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01275", "abs": "https://arxiv.org/abs/2507.01275", "authors": ["Chengxu Liu", "Lu Qi", "Jinshan Pan", "Xueming Qian", "Ming-Hsuan Yang"], "title": "Frequency Domain-Based Diffusion Model for Unpaired Image Dehazing", "comment": "Accepted by ICCV 2025", "summary": "Unpaired image dehazing has attracted increasing attention due to its\nflexible data requirements during model training. Dominant methods based on\ncontrastive learning not only introduce haze-unrelated content information, but\nalso ignore haze-specific properties in the frequency domain (\\ie,~haze-related\ndegradation is mainly manifested in the amplitude spectrum). To address these\nissues, we propose a novel frequency domain-based diffusion model, named \\ours,\nfor fully exploiting the beneficial knowledge in unpaired clear data. In\nparticular, inspired by the strong generative ability shown by Diffusion Models\n(DMs), we tackle the dehazing task from the perspective of frequency domain\nreconstruction and perform the DMs to yield the amplitude spectrum consistent\nwith the distribution of clear images. To implement it, we propose an Amplitude\nResidual Encoder (ARE) to extract the amplitude residuals, which effectively\ncompensates for the amplitude gap from the hazy to clear domains, as well as\nprovide supervision for the DMs training. In addition, we propose a Phase\nCorrection Module (PCM) to eliminate artifacts by further refining the phase\nspectrum during dehazing with a simple attention mechanism. Experimental\nresults demonstrate that our \\ours outperforms other state-of-the-art methods\non both synthetic and real-world datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9891\u57df\u7684\u6269\u6563\u6a21\u578b\uff08\\ours\uff09\uff0c\u7528\u4e8e\u65e0\u914d\u5bf9\u56fe\u50cf\u53bb\u96fe\uff0c\u901a\u8fc7\u632f\u5e45\u6b8b\u5dee\u7f16\u7801\u5668\u548c\u76f8\u4f4d\u6821\u6b63\u6a21\u5757\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u65b9\u6cd5\u5f15\u5165\u96fe\u65e0\u5173\u5185\u5bb9\u4e14\u5ffd\u7565\u96fe\u5728\u9891\u57df\u7684\u7279\u6027\uff08\u5982\u632f\u5e45\u8c31\u4e2d\u7684\u9000\u5316\uff09\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u9891\u57df\u6269\u6563\u6a21\u578b\uff0c\u5229\u7528\u632f\u5e45\u6b8b\u5dee\u7f16\u7801\u5668\uff08ARE\uff09\u8865\u507f\u632f\u5e45\u5dee\u8ddd\uff0c\u76f8\u4f4d\u6821\u6b63\u6a21\u5757\uff08PCM\uff09\u6d88\u9664\u4f2a\u5f71\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\\ours \u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u5176\u4ed6\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u9891\u57df\u6269\u6563\u6a21\u578b\u7ed3\u5408ARE\u548cPCM\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u914d\u5bf9\u53bb\u96fe\u95ee\u9898\u3002"}}
{"id": "2507.01335", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01335", "abs": "https://arxiv.org/abs/2507.01335", "authors": ["Xunjian Yin", "Sitao Cheng", "Yuxi Xie", "Xinyu Hu", "Li Lin", "Xinyi Wang", "Liangming Pan", "William Yang Wang", "Xiaojun Wan"], "title": "LEDOM: An Open and Fundamental Reverse Language Model", "comment": "Work in progress", "summary": "We introduce LEDOM, the first purely reverse language model, trained\nautoregressively on 435B tokens with 2B and 7B parameter variants, which\nprocesses sequences in reverse temporal order through previous token\nprediction. For the first time, we present the reverse language model as a\npotential foundational model across general tasks, accompanied by a set of\nintriguing examples and insights. Based on LEDOM, we further introduce a novel\napplication: Reverse Reward, where LEDOM-guided reranking of forward language\nmodel outputs leads to substantial performance improvements on mathematical\nreasoning tasks. This approach leverages LEDOM's unique backward reasoning\ncapability to refine generation quality through posterior evaluation. Our\nfindings suggest that LEDOM exhibits unique characteristics with broad\napplication potential. We will release all models, training code, and\npre-training data to facilitate future research.", "AI": {"tldr": "LEDOM\u662f\u9996\u4e2a\u7eaf\u9006\u5411\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u9006\u5411\u65f6\u5e8f\u5904\u7406\u5e8f\u5217\uff0c\u5c55\u793a\u4e86\u5728\u901a\u7528\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u63d0\u51fa\u9006\u5411\u5956\u52b1\u5e94\u7528\u4ee5\u63d0\u5347\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u9006\u5411\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u57fa\u7840\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u5e76\u9a8c\u8bc1\u5176\u5728\u4efb\u52a1\u4e2d\u7684\u72ec\u7279\u4f18\u52bf\u3002", "method": "\u8bad\u7ec3\u4e862B\u548c7B\u53c2\u6570\u7684\u9006\u5411\u8bed\u8a00\u6a21\u578bLEDOM\uff0c\u901a\u8fc7\u9006\u5411\u65f6\u5e8f\u9884\u6d4b\u524d\u4e00\u4e2atoken\uff0c\u5e76\u5f15\u5165\u9006\u5411\u5956\u52b1\u5e94\u7528\u4f18\u5316\u751f\u6210\u8d28\u91cf\u3002", "result": "LEDOM\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u901a\u8fc7\u9006\u5411\u5956\u52b1\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u72ec\u7279\u7684\u9006\u5411\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "LEDOM\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5c06\u516c\u5f00\u6a21\u578b\u3001\u4ee3\u7801\u548c\u6570\u636e\u4ee5\u4fc3\u8fdb\u7814\u7a76\u3002"}}
{"id": "2507.01717", "categories": ["cs.AI", "cs.IR", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.01717", "abs": "https://arxiv.org/abs/2507.01717", "authors": ["Gopichand Kanumolu", "Ashok Urlana", "Charaka Vinayak Kumar", "Bala Mallikarjunarao Garlapati"], "title": "Agent Ideate: A Framework for Product Idea Generation from Patents Using Agentic AI", "comment": "AgentScen Workshop, IJCAI 2025", "summary": "Patents contain rich technical knowledge that can inspire innovative product\nideas, yet accessing and interpreting this information remains a challenge.\nThis work explores the use of Large Language Models (LLMs) and autonomous\nagents to mine and generate product concepts from a given patent. In this work,\nwe design Agent Ideate, a framework for automatically generating product-based\nbusiness ideas from patents. We experimented with open-source LLMs and\nagent-based architectures across three domains: Computer Science, Natural\nLanguage Processing, and Material Chemistry. Evaluation results show that the\nagentic approach consistently outperformed standalone LLMs in terms of idea\nquality, relevance, and novelty. These findings suggest that combining LLMs\nwith agentic workflows can significantly enhance the innovation pipeline by\nunlocking the untapped potential of business idea generation from patent data.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u81ea\u4e3b\u4ee3\u7406\u4ece\u4e13\u5229\u4e2d\u6316\u6398\u5e76\u751f\u6210\u4ea7\u54c1\u6982\u5ff5\u7684\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAgent Ideate\u7684\u6846\u67b6\uff0c\u5b9e\u9a8c\u8868\u660e\u4ee3\u7406\u65b9\u6cd5\u5728\u751f\u6210\u521b\u610f\u7684\u8d28\u91cf\u3001\u76f8\u5173\u6027\u548c\u65b0\u9896\u6027\u4e0a\u4f18\u4e8e\u5355\u72ec\u4f7f\u7528LLMs\u3002", "motivation": "\u4e13\u5229\u8574\u542b\u4e30\u5bcc\u7684\u6280\u672f\u77e5\u8bc6\uff0c\u4f46\u83b7\u53d6\u548c\u89e3\u8bfb\u8fd9\u4e9b\u4fe1\u606f\u4ecd\u5177\u6311\u6218\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u4ece\u4e2d\u751f\u6210\u521b\u65b0\u4ea7\u54c1\u6982\u5ff5\u3002", "method": "\u8bbe\u8ba1\u4e86Agent Ideate\u6846\u67b6\uff0c\u7ed3\u5408\u5f00\u6e90LLMs\u548c\u57fa\u4e8e\u4ee3\u7406\u7684\u67b6\u6784\uff0c\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u6750\u6599\u5316\u5b66\u4e09\u4e2a\u9886\u57df\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u4ee3\u7406\u65b9\u6cd5\u5728\u521b\u610f\u8d28\u91cf\u3001\u76f8\u5173\u6027\u548c\u65b0\u9896\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u5355\u72ec\u4f7f\u7528LLMs\u3002", "conclusion": "\u7ed3\u5408LLMs\u548c\u4ee3\u7406\u5de5\u4f5c\u6d41\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u4ece\u4e13\u5229\u6570\u636e\u751f\u6210\u5546\u4e1a\u521b\u610f\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.01035", "categories": ["cs.LG", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.01035", "abs": "https://arxiv.org/abs/2507.01035", "authors": ["Yushang Zhao", "Haotian Lyu", "Yike Peng", "Aijia Sun", "Feng Jiang", "Xinyue Han"], "title": "Research on Low-Latency Inference and Training Efficiency Optimization for Graph Neural Network and Large Language Model-Based Recommendation Systems", "comment": null, "summary": "The incessant advent of online services demands high speed and efficient\nrecommender systems (ReS) that can maintain real-time performance along with\nprocessing very complex user-item interactions. The present study, therefore,\nconsiders computational bottlenecks involved in hybrid Graph Neural Network\n(GNN) and Large Language Model (LLM)-based ReS with the aim optimizing their\ninference latency and training efficiency. An extensive methodology was used:\nhybrid GNN-LLM integrated architecture-optimization strategies(quantization,\nLoRA, distillation)-hardware acceleration (FPGA, DeepSpeed)-all under R 4.4.2.\nExperimental improvements were significant, with the optimal Hybrid + FPGA +\nDeepSpeed configuration reaching 13.6% more accuracy (NDCG@10: 0.75) at 40-60ms\nof latency, while LoRA brought down training time by 66% (3.8 hours) in\ncomparison to the non-optimized baseline. Irrespective of domain, such as\naccuracy or efficiency, it can be established that hardware-software co-design\nand parameter-efficient tuning permit hybrid models to outperform GNN or LLM\napproaches implemented independently. It recommends the use of FPGA as well as\nLoRA for real-time deployment. Future work should involve federated learning\nalong with advanced fusion architectures for better scalability and privacy\npreservation. Thus, this research marks the fundamental groundwork concerning\nnext-generation ReS balancing low-latency response with cutting-edge\npersonalization.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u7ed3\u5408GNN\u548cLLM\u7684\u6df7\u5408\u63a8\u8350\u7cfb\u7edf\uff0c\u4f18\u5316\u63a8\u7406\u5ef6\u8fdf\u548c\u8bad\u7ec3\u6548\u7387\uff0c\u91c7\u7528\u91cf\u5316\u3001LoRA\u3001\u84b8\u998f\u7b49\u65b9\u6cd5\uff0c\u7ed3\u5408\u786c\u4ef6\u52a0\u901f\uff08FPGA\u3001DeepSpeed\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5728\u7ebf\u670d\u52a1\u5bf9\u9ad8\u6548\u5b9e\u65f6\u63a8\u8350\u7cfb\u7edf\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u9700\u8981\u89e3\u51b3\u6df7\u5408GNN-LLM\u63a8\u8350\u7cfb\u7edf\u7684\u8ba1\u7b97\u74f6\u9888\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6df7\u5408GNN-LLM\u67b6\u6784\uff0c\u7ed3\u5408\u91cf\u5316\u3001LoRA\u3001\u84b8\u998f\u7b49\u4f18\u5316\u7b56\u7565\uff0c\u5e76\u4f7f\u7528FPGA\u548cDeepSpeed\u8fdb\u884c\u786c\u4ef6\u52a0\u901f\u3002", "result": "\u6700\u4f18\u914d\u7f6e\uff08Hybrid + FPGA + DeepSpeed\uff09\u5728NDCG@10\u4e0a\u8fbe\u52300.75\uff0c\u5ef6\u8fdf40-60ms\uff1bLoRA\u5c06\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1166%\uff083.8\u5c0f\u65f6\uff09\u3002", "conclusion": "\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u548c\u53c2\u6570\u9ad8\u6548\u8c03\u4f18\u4f7f\u6df7\u5408\u6a21\u578b\u4f18\u4e8e\u72ec\u7acbGNN\u6216LLM\u65b9\u6cd5\uff0c\u63a8\u8350\u4f7f\u7528FPGA\u548cLoRA\u8fdb\u884c\u5b9e\u65f6\u90e8\u7f72\uff0c\u672a\u6765\u53ef\u63a2\u7d22\u8054\u90a6\u5b66\u4e60\u548c\u9ad8\u7ea7\u878d\u5408\u67b6\u6784\u3002"}}
{"id": "2507.01290", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01290", "abs": "https://arxiv.org/abs/2507.01290", "authors": ["Sunyong Seo", "Semin Kim", "Jongha Lee"], "title": "Learning an Ensemble Token from Task-driven Priors in Facial Analysis", "comment": "11pages, 8figures, 4tables", "summary": "Facial analysis exhibits task-specific feature variations. While\nConvolutional Neural Networks (CNNs) have enabled the fine-grained\nrepresentation of spatial information, Vision Transformers (ViTs) have\nfacilitated the representation of semantic information at the patch level.\nAlthough the generalization of conventional methodologies has advanced visual\ninterpretability, there remains paucity of research that preserves the unified\nfeature representation on single task learning during the training process. In\nthis work, we introduce ET-Fuser, a novel methodology for learning ensemble\ntoken by leveraging attention mechanisms based on task priors derived from\npre-trained models for facial analysis. Specifically, we propose a robust prior\nunification learning method that generates a ensemble token within a\nself-attention mechanism, which shares the mutual information along the\npre-trained encoders. This ensemble token approach offers high efficiency with\nnegligible computational cost. Our results show improvements across a variety\nof facial analysis, with statistically significant enhancements observed in the\nfeature representations.", "AI": {"tldr": "ET-Fuser\u65b9\u6cd5\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4efb\u52a1\u5148\u9a8c\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7279\u5f81\u8868\u793a\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9762\u90e8\u5206\u6790\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5355\u4efb\u52a1\u5b66\u4e60\u4e2d\u7f3a\u4e4f\u7edf\u4e00\u7684\u7279\u5f81\u8868\u793a\uff0cET-Fuser\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4efb\u52a1\u5148\u9a8c\uff0c\u751f\u6210\u4e00\u4e2a\u96c6\u6210\u4ee4\u724c\uff0c\u5171\u4eab\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u7684\u4e92\u4fe1\u606f\u3002", "result": "\u5728\u591a\u79cd\u9762\u90e8\u5206\u6790\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\uff0c\u7279\u5f81\u8868\u793a\u6548\u679c\u63d0\u5347\u3002", "conclusion": "ET-Fuser\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u8ba1\u7b97\u6210\u672c\u4f4e\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u7edf\u4e00\u7279\u5f81\u8868\u793a\u5e76\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2507.01352", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01352", "abs": "https://arxiv.org/abs/2507.01352", "authors": ["Chris Yuhao Liu", "Liang Zeng", "Yuzhen Xiao", "Jujie He", "Jiacai Liu", "Chaojie Wang", "Rui Yan", "Wei Shen", "Fuxiang Zhang", "Jiacheng Xu", "Yang Liu", "Yahui Zhou"], "title": "Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy", "comment": null, "summary": "Despite the critical role of reward models (RMs) in reinforcement learning\nfrom human feedback (RLHF), current state-of-the-art open RMs perform poorly on\nmost existing evaluation benchmarks, failing to capture the spectrum of nuanced\nand sophisticated human preferences. Even approaches that incorporate advanced\ntraining techniques have not yielded meaningful performance improvements. We\nhypothesize that this brittleness stems primarily from limitations in\npreference datasets, which are often narrowly scoped, synthetically labeled, or\nlack rigorous quality control. To address these challenges, we present a\nlarge-scale preference dataset comprising 40 million preference pairs, named\nSynPref-40M. To enable data curation at scale, we design a human-AI synergistic\ntwo-stage pipeline that leverages the complementary strengths of human\nannotation quality and AI scalability. In this pipeline, humans provide\nverified annotations, while large language models perform automatic curation\nbased on human guidance. Training on this preference mixture, we introduce\nSkywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B\nparameters, trained on a carefully curated subset of 26 million preference\npairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile\nacross a wide range of capabilities, including alignment with human\npreferences, objective correctness, safety, resistance to stylistic biases, and\nbest-of-N scaling, achieving state-of-the-art performance across seven major\nreward model benchmarks. Ablation studies confirm that the effectiveness of our\napproach stems not only from data scale but also from high-quality curation.\nThe Skywork-Reward-V2 series represents substantial progress in open reward\nmodels, highlighting the untapped potential of existing preference datasets and\ndemonstrating how human-AI curation synergy can unlock significantly higher\ndata quality.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5927\u89c4\u6a21\u504f\u597d\u6570\u636e\u96c6SynPref-40M\uff0c\u5e76\u901a\u8fc7\u4eba\u673a\u534f\u540c\u7684\u4e24\u9636\u6bb5\u6d41\u7a0b\u63d0\u5347\u6570\u636e\u8d28\u91cf\uff0c\u8bad\u7ec3\u4e86Skywork-Reward-V2\u5956\u52b1\u6a21\u578b\u7cfb\u5217\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u5956\u52b1\u6a21\u578b\u5728\u6355\u6349\u4eba\u7c7b\u504f\u597d\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u504f\u597d\u6570\u636e\u96c6\u8303\u56f4\u72ed\u7a84\u3001\u6807\u6ce8\u8d28\u91cf\u4e0d\u8db3\u3002", "method": "\u8bbe\u8ba1\u4eba\u673a\u534f\u540c\u7684\u4e24\u9636\u6bb5\u6570\u636e\u6807\u6ce8\u6d41\u7a0b\uff0c\u7ed3\u5408\u4eba\u7c7b\u6807\u6ce8\u8d28\u91cf\u4e0eAI\u6269\u5c55\u6027\uff0c\u8bad\u7ec3\u4e86\u4ece0.6B\u52308B\u53c2\u6570\u7684\u5956\u52b1\u6a21\u578b\u3002", "result": "Skywork-Reward-V2\u5728\u591a\u4e2a\u80fd\u529b\u7ef4\u5ea6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5305\u62ec\u5bf9\u9f50\u4eba\u7c7b\u504f\u597d\u3001\u5b89\u5168\u6027\u7b49\uff0c\u5e76\u5728\u4e03\u4e2a\u4e3b\u8981\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f18\u3002", "conclusion": "\u901a\u8fc7\u9ad8\u8d28\u91cf\u6570\u636e\u548c\u4eba\u673a\u534f\u540c\u6807\u6ce8\uff0cSkywork-Reward-V2\u7cfb\u5217\u663e\u8457\u63d0\u5347\u4e86\u5956\u52b1\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u73b0\u6709\u504f\u597d\u6570\u636e\u96c6\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.01749", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01749", "abs": "https://arxiv.org/abs/2507.01749", "authors": ["Arash Dehghan", "Mucahit Cevik", "Merve Bodur", "Bissan Ghaddar"], "title": "Joint Matching and Pricing for Crowd-shipping with In-store Customers", "comment": null, "summary": "This paper examines the use of in-store customers as delivery couriers in a\ncentralized crowd-shipping system, targeting the growing need for efficient\nlast-mile delivery in urban areas. We consider a brick-and-mortar retail\nsetting where shoppers are offered compensation to deliver time-sensitive\nonline orders. To manage this process, we propose a Markov Decision Process\n(MDP) model that captures key uncertainties, including the stochastic arrival\nof orders and crowd-shippers, and the probabilistic acceptance of delivery\noffers. Our solution approach integrates Neural Approximate Dynamic Programming\n(NeurADP) for adaptive order-to-shopper assignment with a Deep Double Q-Network\n(DDQN) for dynamic pricing. This joint optimization strategy enables multi-drop\nrouting and accounts for offer acceptance uncertainty, aligning more closely\nwith real-world operations. Experimental results demonstrate that the\nintegrated NeurADP + DDQN policy achieves notable improvements in delivery cost\nefficiency, with up to 6.7\\% savings over NeurADP with fixed pricing and\napproximately 18\\% over myopic baselines. We also show that allowing flexible\ndelivery delays and enabling multi-destination routing further reduces\noperational costs by 8\\% and 17\\%, respectively. These findings underscore the\nadvantages of dynamic, forward-looking policies in crowd-shipping systems and\noffer practical guidance for urban logistics operators.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5229\u7528\u5e97\u5185\u987e\u5ba2\u4f5c\u4e3a\u914d\u9001\u5458\u7684\u96c6\u4e2d\u5f0f\u4f17\u5305\u914d\u9001\u7cfb\u7edf\uff0c\u9488\u5bf9\u57ce\u5e02\u6700\u540e\u4e00\u516c\u91cc\u914d\u9001\u6548\u7387\u7684\u9700\u6c42\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408MDP\u3001NeurADP\u548cDDQN\u7684\u52a8\u6001\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u914d\u9001\u6210\u672c\u3002", "motivation": "\u9488\u5bf9\u57ce\u5e02\u6700\u540e\u4e00\u516c\u91cc\u914d\u9001\u6548\u7387\u7684\u9700\u6c42\uff0c\u63a2\u7d22\u5229\u7528\u5e97\u5185\u987e\u5ba2\u4f5c\u4e3a\u914d\u9001\u5458\u7684\u4f17\u5305\u6a21\u5f0f\uff0c\u4ee5\u964d\u4f4e\u6210\u672c\u5e76\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u91c7\u7528MDP\u6a21\u578b\u5904\u7406\u8ba2\u5355\u548c\u914d\u9001\u5458\u7684\u968f\u673a\u5230\u8fbe\u53ca\u63a5\u53d7\u4e0d\u786e\u5b9a\u6027\uff0c\u7ed3\u5408NeurADP\u548cDDQN\u8fdb\u884c\u52a8\u6001\u8ba2\u5355\u5206\u914d\u548c\u5b9a\u4ef7\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cNeurADP + DDQN\u7b56\u7565\u6bd4\u56fa\u5b9a\u5b9a\u4ef7\u8282\u77016.7%\u6210\u672c\uff0c\u6bd4\u77ed\u89c6\u57fa\u7ebf\u8282\u7701\u7ea618%\u3002\u7075\u6d3b\u914d\u9001\u548c\u591a\u76ee\u7684\u5730\u8def\u7531\u8fdb\u4e00\u6b65\u964d\u4f4e\u6210\u672c8%\u548c17%\u3002", "conclusion": "\u52a8\u6001\u524d\u77bb\u6027\u7b56\u7565\u5728\u4f17\u5305\u914d\u9001\u7cfb\u7edf\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u57ce\u5e02\u7269\u6d41\u8fd0\u8425\u5546\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2507.01037", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01037", "abs": "https://arxiv.org/abs/2507.01037", "authors": ["Wenbin Ouyang", "Sirui Li", "Yining Ma", "Cathy Wu"], "title": "Learning to Segment for Vehicle Routing Problems", "comment": null, "summary": "Iterative search heuristics are widely recognized as state-of-the-art for\nsolving Vehicle Routing Problems (VRPs). In this work, we identify and exploit\na critical observation: within these solvers, a large portion of the solution\nremains stable, i.e., unchanged across search iterations, causing redundant\ncomputations, especially for large-scale VRPs with long subtours. To address\nthis, we pioneer the formal study of the First-Segment-Then-Aggregate (FSTA)\ndecomposition technique to accelerate iterative solvers. Specifically, FSTA\npreserves stable solution segments during the search, aggregates nodes within\neach segment into fixed hypernodes, and focuses the search only on unstable\nportions. Yet, a key challenge lies in identifying which segments should be\naggregated by FSTA. To this end, we then introduce Learning-to-Segment (L2Seg),\na novel neural framework to intelligently differentiate potentially stable and\nunstable portions for FSTA decomposition. We present three L2Seg variants:\nnon-autoregressive (globally comprehensive but locally indiscriminate),\nautoregressive (locally refined but globally deficient), and their synergy,\nwith bespoke training and inference strategies. Empirical results on CVRP and\nVRPTW suggest that L2Seg accelerates state-of-the-art iterative solvers by up\nto 7x. Additionally, we provide in-depth analysis showing NAR and AR synergy\nachieves best performance by combining their complementary strengths. Notably,\nL2Seg is a flexible framework that is compatible with traditional,\nlearning-based, and hybrid solvers, while supporting a broad class of VRPs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFSTA\u7684\u5206\u89e3\u6280\u672f\uff0c\u901a\u8fc7\u4fdd\u7559\u7a33\u5b9a\u89e3\u6bb5\u5e76\u805a\u5408\u8282\u70b9\u6765\u52a0\u901f\u8fed\u4ee3\u6c42\u89e3\u5668\uff0c\u540c\u65f6\u5f15\u5165L2Seg\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\u667a\u80fd\u8bc6\u522b\u7a33\u5b9a\u4e0e\u4e0d\u7a33\u5b9a\u90e8\u5206\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u53ef\u5c06\u6c42\u89e3\u5668\u901f\u5ea6\u63d0\u53477\u500d\u3002", "motivation": "\u8fed\u4ee3\u641c\u7d22\u542f\u53d1\u5f0f\u5728\u89e3\u51b3\u8f66\u8f86\u8def\u5f84\u95ee\u9898\uff08VRP\uff09\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5b58\u5728\u5927\u91cf\u5197\u4f59\u8ba1\u7b97\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21VRP\u4e2d\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\u6765\u52a0\u901f\u6c42\u89e3\u5668\u3002", "method": "\u63d0\u51faFSTA\u5206\u89e3\u6280\u672f\uff0c\u4fdd\u7559\u7a33\u5b9a\u89e3\u6bb5\u5e76\u805a\u5408\u8282\u70b9\uff1b\u5f15\u5165L2Seg\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u79cd\u53d8\u4f53\uff08\u975e\u81ea\u56de\u5f52\u3001\u81ea\u56de\u5f52\u53ca\u5176\u534f\u540c\uff09\u667a\u80fd\u8bc6\u522b\u7a33\u5b9a\u4e0e\u4e0d\u7a33\u5b9a\u90e8\u5206\u3002", "result": "\u5728CVRP\u548cVRPTW\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cL2Seg\u53ef\u5c06\u6c42\u89e3\u5668\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe7\u500d\uff0c\u4e14NAR\u4e0eAR\u534f\u540c\u6548\u679c\u6700\u4f73\u3002", "conclusion": "L2Seg\u662f\u4e00\u4e2a\u7075\u6d3b\u6846\u67b6\uff0c\u517c\u5bb9\u4f20\u7edf\u3001\u5b66\u4e60\u548c\u6df7\u5408\u6c42\u89e3\u5668\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684VRP\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c42\u89e3\u6548\u7387\u3002"}}
{"id": "2507.01305", "categories": ["cs.CV", "cs.GR", "cs.LG", "I.3.3; I.4.8"], "pdf": "https://arxiv.org/pdf/2507.01305", "abs": "https://arxiv.org/abs/2507.01305", "authors": ["Worameth Chinchuthakun", "Pakkapon Phongthawee", "Amit Raj", "Varun Jampani", "Pramook Khungurn", "Supasorn Suwajanakorn"], "title": "DiffusionLight-Turbo: Accelerated Light Probes for Free via Single-Pass Chrome Ball Inpainting", "comment": "arXiv admin note: substantial text overlap with arXiv:2312.09168", "summary": "We introduce a simple yet effective technique for estimating lighting from a\nsingle low-dynamic-range (LDR) image by reframing the task as a chrome ball\ninpainting problem. This approach leverages a pre-trained diffusion model,\nStable Diffusion XL, to overcome the generalization failures of existing\nmethods that rely on limited HDR panorama datasets. While conceptually simple,\nthe task remains challenging because diffusion models often insert incorrect or\ninconsistent content and cannot readily generate chrome balls in HDR format.\nOur analysis reveals that the inpainting process is highly sensitive to the\ninitial noise in the diffusion process, occasionally resulting in unrealistic\noutputs. To address this, we first introduce DiffusionLight, which uses\niterative inpainting to compute a median chrome ball from multiple outputs to\nserve as a stable, low-frequency lighting prior that guides the generation of a\nhigh-quality final result. To generate high-dynamic-range (HDR) light probes,\nan Exposure LoRA is fine-tuned to create LDR images at multiple exposure\nvalues, which are then merged. While effective, DiffusionLight is\ntime-intensive, requiring approximately 30 minutes per estimation. To reduce\nthis overhead, we introduce DiffusionLight-Turbo, which reduces the runtime to\nabout 30 seconds with minimal quality loss. This 60x speedup is achieved by\ntraining a Turbo LoRA to directly predict the averaged chrome balls from the\niterative process. Inference is further streamlined into a single denoising\npass using a LoRA swapping technique. Experimental results that show our method\nproduces convincing light estimates across diverse settings and demonstrates\nsuperior generalization to in-the-wild scenarios. Our code is available at\nhttps://diffusionlight.github.io/turbo", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5c06\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u94ec\u7403\u4fee\u590d\u95ee\u9898\uff0c\u4ece\u5355\u5f20\u4f4e\u52a8\u6001\u8303\u56f4\uff08LDR\uff09\u56fe\u50cf\u4f30\u8ba1\u5149\u7167\u7684\u7b80\u5355\u6709\u6548\u6280\u672f\u3002\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578bStable Diffusion XL\uff0c\u89e3\u51b3\u4e86\u4f9d\u8d56\u6709\u9650HDR\u5168\u666f\u6570\u636e\u96c6\u65b9\u6cd5\u7684\u6cdb\u5316\u95ee\u9898\u3002\u901a\u8fc7\u8fed\u4ee3\u4fee\u590d\u751f\u6210\u7a33\u5b9a\u7684\u4f4e\u9891\u5149\u7167\u5148\u9a8c\uff0c\u5e76\u7ed3\u5408\u66dd\u5149LoRA\u751f\u6210HDR\u5149\u63a2\u9488\u3002\u8fdb\u4e00\u6b65\u63d0\u51faDiffusionLight-Turbo\uff0c\u5c06\u8fd0\u884c\u65f6\u95f4\u4ece30\u5206\u949f\u7f29\u77ed\u81f330\u79d2\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6709\u9650\u7684HDR\u5168\u666f\u6570\u636e\u96c6\uff0c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u6269\u6563\u6a21\u578b\u5728\u4fee\u590d\u4efb\u52a1\u4e2d\u53ef\u80fd\u751f\u6210\u4e0d\u4e00\u81f4\u6216\u4e0d\u6b63\u786e\u7684\u5185\u5bb9\uff0c\u4e14\u96be\u4ee5\u76f4\u63a5\u751f\u6210HDR\u683c\u5f0f\u7684\u94ec\u7403\u3002", "method": "1. \u4f7f\u7528\u8fed\u4ee3\u4fee\u590d\u751f\u6210\u591a\u4e2a\u94ec\u7403\u8f93\u51fa\uff0c\u53d6\u4e2d\u503c\u4f5c\u4e3a\u7a33\u5b9a\u7684\u4f4e\u9891\u5149\u7167\u5148\u9a8c\uff08DiffusionLight\uff09\u30022. \u5fae\u8c03\u66dd\u5149LoRA\u751f\u6210\u591a\u66dd\u5149LDR\u56fe\u50cf\u5e76\u5408\u5e76\u4e3aHDR\u5149\u63a2\u9488\u30023. \u63d0\u51faDiffusionLight-Turbo\uff0c\u901a\u8fc7\u8bad\u7ec3Turbo LoRA\u76f4\u63a5\u9884\u6d4b\u5e73\u5747\u94ec\u7403\uff0c\u5c06\u63a8\u7406\u7b80\u5316\u4e3a\u5355\u6b21\u53bb\u566a\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u751f\u6210\u903c\u771f\u7684\u5149\u7167\u4f30\u8ba1\uff0c\u5e76\u5728\u91ce\u5916\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DiffusionLight\u548cDiffusionLight-Turbo\u5728\u5149\u7167\u4f30\u8ba1\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u540e\u8005\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u8d28\u91cf\u7ed3\u679c\u3002"}}
{"id": "2507.01437", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01437", "abs": "https://arxiv.org/abs/2507.01437", "authors": ["Ting Xu", "Xiaoxiao Deng", "Xiandong Meng", "Haifeng Yang", "Yan Wu"], "title": "Clinical NLP with Attention-Based Deep Learning for Multi-Disease Prediction", "comment": null, "summary": "This paper addresses the challenges posed by the unstructured nature and\nhigh-dimensional semantic complexity of electronic health record texts. A deep\nlearning method based on attention mechanisms is proposed to achieve unified\nmodeling for information extraction and multi-label disease prediction. The\nstudy is conducted on the MIMIC-IV dataset. A Transformer-based architecture is\nused to perform representation learning over clinical text. Multi-layer\nself-attention mechanisms are employed to capture key medical entities and\ntheir contextual relationships. A Sigmoid-based multi-label classifier is then\napplied to predict multiple disease labels. The model incorporates a\ncontext-aware semantic alignment mechanism, enhancing its representational\ncapacity in typical medical scenarios such as label co-occurrence and sparse\ninformation. To comprehensively evaluate model performance, a series of\nexperiments were conducted, including baseline comparisons, hyperparameter\nsensitivity analysis, data perturbation studies, and noise injection tests.\nResults demonstrate that the proposed method consistently outperforms\nrepresentative existing approaches across multiple performance metrics. The\nmodel maintains strong generalization under varying data scales, interference\nlevels, and model depth configurations. The framework developed in this study\noffers an efficient algorithmic foundation for processing real-world clinical\ntexts and presents practical significance for multi-label medical text modeling\ntasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6587\u672c\u7684\u4fe1\u606f\u63d0\u53d6\u548c\u591a\u6807\u7b7e\u75be\u75c5\u9884\u6d4b\uff0c\u5e76\u5728MIMIC-IV\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6587\u672c\u7684\u975e\u7ed3\u6784\u5316\u548c\u9ad8\u7ef4\u8bed\u4e49\u590d\u6742\u6027\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528Transformer\u67b6\u6784\u548c\u591a\u5c42\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u5173\u952e\u533b\u5b66\u5b9e\u4f53\u53ca\u5176\u4e0a\u4e0b\u6587\u5173\u7cfb\uff0c\u7ed3\u5408Sigmoid\u591a\u6807\u7b7e\u5206\u7c7b\u5668\u9884\u6d4b\u75be\u75c5\u6807\u7b7e\u3002", "result": "\u6a21\u578b\u5728\u591a\u79cd\u6027\u80fd\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5904\u7406\u771f\u5b9e\u4e34\u5e8a\u6587\u672c\u63d0\u4f9b\u4e86\u9ad8\u6548\u7b97\u6cd5\u57fa\u7840\uff0c\u5bf9\u591a\u6807\u7b7e\u533b\u5b66\u6587\u672c\u5efa\u6a21\u4efb\u52a1\u5177\u6709\u5b9e\u9645\u610f\u4e49\u3002"}}
{"id": "2507.01833", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01833", "abs": "https://arxiv.org/abs/2507.01833", "authors": ["Yi-Dong Shen", "Thomas Eiter"], "title": "Refining Gelfond Rationality Principle Towards More Comprehensive Foundational Principles for Answer Set Semantics", "comment": "76 pages. This article is a significantly extended version of a paper\n  presented by the authors at IJCAI-2022", "summary": "Non-monotonic logic programming is the basis for a declarative problem\nsolving paradigm known as answer set programming (ASP). Departing from the\nseminal definition by Gelfond and Lifschitz in 1988 for simple normal logic\nprograms, various answer set semantics have been proposed for extensions. We\nconsider two important questions: (1) Should the minimal model property,\nconstraint monotonicity and foundedness as defined in the literature be\nmandatory conditions for an answer set semantics in general? (2) If not, what\nother properties could be considered as general principles for answer set\nsemantics? We address the two questions. First, it seems that the three\naforementioned conditions may sometimes be too strong, and we illustrate with\nexamples that enforcing them may exclude expected answer sets. Second, we\nevolve the Gelfond answer set (GAS) principles for answer set construction by\nrefining the Gelfond's rationality principle to well-supportedness, minimality\nw.r.t. negation by default and minimality w.r.t. epistemic negation. The\nprinciple of well-supportedness guarantees that every answer set is\nconstructible from if-then rules obeying a level mapping and is thus free of\ncircular justification, while the two minimality principles ensure that the\nformalism minimizes knowledge both at the level of answer sets and of world\nviews. Third, to embody the refined GAS principles, we extend the notion of\nwell-supportedness substantially to answer sets and world views, respectively.\nFourth, we define new answer set semantics in terms of the refined GAS\nprinciples. Fifth, we use the refined GAS principles as an alternative baseline\nto intuitively assess the existing answer set semantics. Finally, we analyze\nthe computational complexity.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u56de\u7b54\u96c6\u7f16\u7a0b\uff08ASP\uff09\u4e2d\u56de\u7b54\u96c6\u8bed\u4e49\u7684\u901a\u7528\u539f\u5219\uff0c\u63d0\u51fa\u4e86\u5bf9Gelfond\u56de\u7b54\u96c6\uff08GAS\uff09\u539f\u5219\u7684\u7ec6\u5316\uff0c\u5e76\u5b9a\u4e49\u4e86\u65b0\u7684\u8bed\u4e49\u3002", "motivation": "\u7814\u7a76\u56de\u7b54\u96c6\u8bed\u4e49\u662f\u5426\u9700\u8981\u5f3a\u5236\u6ee1\u8db3\u6700\u5c0f\u6a21\u578b\u5c5e\u6027\u3001\u7ea6\u675f\u5355\u8c03\u6027\u548c\u57fa\u7840\u6027\uff0c\u4ee5\u53ca\u63a2\u7d22\u5176\u4ed6\u53ef\u80fd\u7684\u901a\u7528\u539f\u5219\u3002", "method": "\u901a\u8fc7\u4f8b\u5b50\u8bf4\u660e\u73b0\u6709\u6761\u4ef6\u7684\u5c40\u9650\u6027\uff0c\u7ec6\u5316GAS\u539f\u5219\uff0c\u6269\u5c55\u652f\u6301\u6027\u6982\u5ff5\uff0c\u5b9a\u4e49\u65b0\u8bed\u4e49\uff0c\u5e76\u8bc4\u4f30\u73b0\u6709\u8bed\u4e49\u3002", "result": "\u63d0\u51fa\u4e86\u7ec6\u5316\u540e\u7684GAS\u539f\u5219\uff0c\u5b9a\u4e49\u4e86\u65b0\u7684\u56de\u7b54\u96c6\u8bed\u4e49\uff0c\u5e76\u5206\u6790\u4e86\u8ba1\u7b97\u590d\u6742\u6027\u3002", "conclusion": "\u7ec6\u5316\u540e\u7684GAS\u539f\u5219\u4e3a\u56de\u7b54\u96c6\u8bed\u4e49\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u903b\u8f91\u4e0a\u7684\u5408\u7406\u6027\u3002"}}
{"id": "2507.01039", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01039", "abs": "https://arxiv.org/abs/2507.01039", "authors": ["Kaaustaaub Shankar", "Wilhelm Louw", "Kelly Cohen"], "title": "On-Policy Optimization of ANFIS Policies Using Proximal Policy Optimization", "comment": "Submitted to NAFIPS 2025", "summary": "We propose a reinforcement learning (RL) approach for training neuro-fuzzy\ncontrollers using Proximal Policy Optimization (PPO). Building on prior work\nthat applied Deep Q-Learning to Adaptive Neuro-Fuzzy Inference Systems (ANFIS),\nour method replaces the off-policy value-based framework with a stable\non-policy actor-critic loop. We evaluate this approach in the CartPole-v1\nenvironment using multiple random seeds and compare its learning performance\nagainst ANFIS-Deep Q-Network (DQN) baselines. It was found that PPO-trained\nfuzzy agents achieved a mean return of 500 +/- 0 on CartPole-v1 after 20000\nupdates, showcasing less variance than prior DQN-based methods during training\nand overall faster convergence. These findings suggest that PPO offers a\npromising pathway for training explainable neuro-fuzzy controllers in\nreinforcement learning tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePPO\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bad\u7ec3\u795e\u7ecf\u6a21\u7cca\u63a7\u5236\u5668\uff0c\u76f8\u6bd4\u4e4b\u524d\u7684DQN\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u66f4\u4f4e\u7684\u65b9\u5dee\u548c\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u6539\u8fdb\u73b0\u6709\u7684\u57fa\u4e8eDQN\u7684\u795e\u7ecf\u6a21\u7cca\u63a7\u5236\u5668\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165PPO\u7b97\u6cd5\u63d0\u5347\u7a33\u5b9a\u6027\u548c\u6536\u655b\u901f\u5ea6\u3002", "method": "\u4f7f\u7528PPO\u7b97\u6cd5\u66ff\u6362\u539f\u6709\u7684DQN\u6846\u67b6\uff0c\u6784\u5efa\u7a33\u5b9a\u7684on-policy actor-critic\u5faa\u73af\uff0c\u5e76\u5728CartPole-v1\u73af\u5883\u4e2d\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "PPO\u8bad\u7ec3\u7684\u6a21\u7cca\u63a7\u5236\u5668\u5728CartPole-v1\u4e2d\u5b9e\u73b0\u4e86500 +/- 0\u7684\u5e73\u5747\u56de\u62a5\uff0c\u65b9\u5dee\u66f4\u5c0f\u4e14\u6536\u655b\u66f4\u5feb\u3002", "conclusion": "PPO\u4e3a\u8bad\u7ec3\u53ef\u89e3\u91ca\u7684\u795e\u7ecf\u6a21\u7cca\u63a7\u5236\u5668\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.01340", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01340", "abs": "https://arxiv.org/abs/2507.01340", "authors": ["Cuong Le", "Huy-Phuong Le", "Duc Le", "Minh-Thien Duong", "Van-Binh Nguyen", "My-Ha Le"], "title": "Physics-informed Ground Reaction Dynamics from Human Motion Capture", "comment": "6 pages, 4 figures, 4 tables, HSI 2025", "summary": "Body dynamics are crucial information for the analysis of human motions in\nimportant research fields, ranging from biomechanics, sports science to\ncomputer vision and graphics. Modern approaches collect the body dynamics,\nexternal reactive force specifically, via force plates, synchronizing with\nhuman motion capture data, and learn to estimate the dynamics from a black-box\ndeep learning model. Being specialized devices, force plates can only be\ninstalled in laboratory setups, imposing a significant limitation on the\nlearning of human dynamics. To this end, we propose a novel method for\nestimating human ground reaction dynamics directly from the more reliable\nmotion capture data with physics laws and computational simulation as\nconstrains. We introduce a highly accurate and robust method for computing\nground reaction forces from motion capture data using Euler's integration\nscheme and PD algorithm. The physics-based reactive forces are used to inform\nthe learning model about the physics-informed motion dynamics thus improving\nthe estimation accuracy. The proposed approach was tested on the GroundLink\ndataset, outperforming the baseline model on: 1) the ground reaction force\nestimation accuracy compared to the force plates measurement; and 2) our\nsimulated root trajectory precision. The implementation code is available at\nhttps://github.com/cuongle1206/Phys-GRD", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u7ea6\u675f\u7684\u65b0\u65b9\u6cd5\uff0c\u76f4\u63a5\u4ece\u8fd0\u52a8\u6355\u6349\u6570\u636e\u4f30\u8ba1\u5730\u9762\u53cd\u4f5c\u7528\u529b\uff0c\u7ed3\u5408\u6b27\u62c9\u79ef\u5206\u548cPD\u7b97\u6cd5\uff0c\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5b9e\u9a8c\u5ba4\u4e13\u7528\u8bbe\u5907\uff08\u5982\u529b\u677f\uff09\u6536\u96c6\u5730\u9762\u53cd\u4f5c\u7528\u529b\uff0c\u9650\u5236\u4e86\u52a8\u6001\u5b66\u4e60\u7684\u5e94\u7528\u8303\u56f4\u3002", "method": "\u7ed3\u5408\u7269\u7406\u5b9a\u5f8b\u548c\u8ba1\u7b97\u6a21\u62df\uff0c\u4f7f\u7528\u6b27\u62c9\u79ef\u5206\u548cPD\u7b97\u6cd5\u4ece\u8fd0\u52a8\u6355\u6349\u6570\u636e\u8ba1\u7b97\u5730\u9762\u53cd\u4f5c\u7528\u529b\u3002", "result": "\u5728GroundLink\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86\u5730\u9762\u53cd\u4f5c\u7528\u529b\u4f30\u8ba1\u7cbe\u5ea6\u548c\u6a21\u62df\u6839\u8f68\u8ff9\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7269\u7406\u7ea6\u675f\u63d0\u9ad8\u4e86\u52a8\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u6269\u5c55\u4e86\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2507.01449", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01449", "abs": "https://arxiv.org/abs/2507.01449", "authors": ["Tianyu Liu", "Qitan Lv", "Hao Li", "Xing Gao", "Xiao Sun"], "title": "LogitSpec: Accelerating Retrieval-based Speculative Decoding via Next Next Token Speculation", "comment": null, "summary": "Speculative decoding (SD), where a small draft model is employed to propose\ndraft tokens in advance and then the target model validates them in parallel,\nhas emerged as a promising technique for LLM inference acceleration. Many\nendeavors to improve SD are to eliminate the need for a draft model and\ngenerate draft tokens in a retrieval-based manner in order to further alleviate\nthe drafting overhead and significantly reduce the difficulty in deployment and\napplications. However, retrieval-based SD relies on a matching paradigm to\nretrieval the most relevant reference as the draft tokens, where these methods\noften fail to find matched and accurate draft tokens. To address this\nchallenge, we propose LogitSpec to effectively expand the retrieval range and\nfind the most relevant reference as drafts. Our LogitSpec is motivated by the\nobservation that the logit of the last token can not only predict the next\ntoken, but also speculate the next next token. Specifically, LogitSpec\ngenerates draft tokens in two steps: (1) utilizing the last logit to speculate\nthe next next token; (2) retrieving relevant reference for both the next token\nand the next next token. LogitSpec is training-free and plug-and-play, which\ncan be easily integrated into existing LLM inference frameworks. Extensive\nexperiments on a wide range of text generation benchmarks demonstrate that\nLogitSpec can achieve up to 2.61 $\\times$ speedup and 3.28 mean accepted tokens\nper decoding step. Our code is available at\nhttps://github.com/smart-lty/LogitSpec.", "AI": {"tldr": "LogitSpec\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u68c0\u7d22\u5f0f\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u6700\u540e\u4e00\u4e2atoken\u7684logit\u63a8\u6d4b\u540e\u7eedtoken\u5e76\u6269\u5c55\u68c0\u7d22\u8303\u56f4\uff0c\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u68c0\u7d22\u5f0f\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u56e0\u5339\u914d\u8303\u5f0f\u9650\u5236\uff0c\u96be\u4ee5\u627e\u5230\u51c6\u786e\u5339\u914d\u7684\u53c2\u8003token\uff0c\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\u3002", "method": "LogitSpec\u5206\u4e24\u6b65\u751f\u6210\u63a8\u6d4btoken\uff1a(1)\u5229\u7528\u6700\u540e\u4e00\u4e2atoken\u7684logit\u63a8\u6d4b\u4e0b\u4e00\u4e2atoken\uff1b(2)\u68c0\u7d22\u4e0e\u63a8\u6d4btoken\u76f8\u5173\u7684\u53c2\u8003token\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLogitSpec\u5728\u591a\u4e2a\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e2d\u6700\u9ad8\u5b9e\u73b02.61\u500d\u52a0\u901f\uff0c\u5e73\u5747\u6bcf\u89e3\u7801\u6b65\u9aa4\u63a5\u53d73.28\u4e2atoken\u3002", "conclusion": "LogitSpec\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u5373\u63d2\u5373\u7528\u7684\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2507.01040", "categories": ["cs.LG", "cs.AI", "cs.NE", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.01040", "abs": "https://arxiv.org/abs/2507.01040", "authors": ["Tianxiang Xia", "Max Neuwinger", "Lin Xiao"], "title": "Fast Clifford Neural Layers", "comment": "7 pages content-wise", "summary": "Clifford Neural Layers improve PDE modeling by introducing Clifford Algebra\ninto neural networks. In this project we focus on optimizing the inference of\n2/3D Clifford convolutional layers and multivector activation layers for one\ncore CPU performance.\n  Overall, by testing on a real network block involving Clifford convolutional\nlayers and multivector activation layers, we observe that our implementation is\n30% faster than standard PyTorch implementation in relatively large data +\nnetwork size (>L2 cache).\n  We open source our code base at\nhttps://github.com/egretwAlker/c-opt-clifford-layers", "AI": {"tldr": "Clifford Neural Layers\u901a\u8fc7\u5f15\u5165Clifford\u4ee3\u6570\u4f18\u5316PDE\u5efa\u6a21\uff0c\u5728\u5355\u6838CPU\u4e0a\u63d0\u53472/3D Clifford\u5377\u79ef\u5c42\u548c\u591a\u5411\u91cf\u6fc0\u6d3b\u5c42\u7684\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u901a\u8fc7\u4f18\u5316Clifford\u5377\u79ef\u5c42\u548c\u591a\u5411\u91cf\u6fc0\u6d3b\u5c42\u7684\u63a8\u7406\u6027\u80fd\uff0c\u63d0\u5347PDE\u5efa\u6a21\u7684\u6548\u7387\u3002", "method": "\u5728\u5355\u6838CPU\u4e0a\u4f18\u53162/3D Clifford\u5377\u79ef\u5c42\u548c\u591a\u5411\u91cf\u6fc0\u6d3b\u5c42\u7684\u63a8\u7406\u5b9e\u73b0\u3002", "result": "\u5728\u8f83\u5927\u6570\u636e\u548c\u7f51\u7edc\u89c4\u6a21\uff08>L2\u7f13\u5b58\uff09\u4e0b\uff0c\u5b9e\u73b0\u6bd4\u6807\u51c6PyTorch\u5feb30%\u7684\u6027\u80fd\u3002", "conclusion": "\u4f18\u5316\u540e\u7684Clifford\u5c42\u5728PDE\u5efa\u6a21\u4e2d\u8868\u73b0\u66f4\u9ad8\u6548\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.01342", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01342", "abs": "https://arxiv.org/abs/2507.01342", "authors": ["Luxi Zhao", "Mahmoud Afifi", "Michael S. Brown"], "title": "Learning Camera-Agnostic White-Balance Preferences", "comment": null, "summary": "The image signal processor (ISP) pipeline in modern cameras consists of\nseveral modules that transform raw sensor data into visually pleasing images in\na display color space. Among these, the auto white balance (AWB) module is\nessential for compensating for scene illumination. However, commercial AWB\nsystems often strive to compute aesthetic white-balance preferences rather than\naccurate neutral color correction. While learning-based methods have improved\nAWB accuracy, they typically struggle to generalize across different camera\nsensors -- an issue for smartphones with multiple cameras. Recent work has\nexplored cross-camera AWB, but most methods remain focused on achieving neutral\nwhite balance. In contrast, this paper is the first to address aesthetic\nconsistency by learning a post-illuminant-estimation mapping that transforms\nneutral illuminant corrections into aesthetically preferred corrections in a\ncamera-agnostic space. Once trained, our mapping can be applied after any\nneutral AWB module to enable consistent and stylized color rendering across\nunseen cameras. Our proposed model is lightweight -- containing only $\\sim$500\nparameters -- and runs in just 0.024 milliseconds on a typical flagship mobile\nCPU. Evaluated on a dataset of 771 smartphone images from three different\ncameras, our method achieves state-of-the-art performance while remaining fully\ncompatible with existing cross-camera AWB techniques, introducing minimal\ncomputational and memory overhead.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u540e\u5149\u7167\u4f30\u8ba1\u6620\u5c04\uff0c\u5c06\u4e2d\u6027\u767d\u5e73\u8861\u8f6c\u6362\u4e3a\u7f8e\u5b66\u504f\u597d\u6821\u6b63\uff0c\u5b9e\u73b0\u8de8\u76f8\u673a\u7f8e\u5b66\u4e00\u81f4\u6027\u3002", "motivation": "\u5546\u4e1a\u81ea\u52a8\u767d\u5e73\u8861\uff08AWB\uff09\u7cfb\u7edf\u901a\u5e38\u8ffd\u6c42\u7f8e\u5b66\u504f\u597d\u800c\u975e\u51c6\u786e\u4e2d\u6027\u6821\u6b63\uff0c\u4e14\u73b0\u6709\u5b66\u4e60\u578b\u65b9\u6cd5\u96be\u4ee5\u8de8\u76f8\u673a\u4f20\u611f\u5668\u6cdb\u5316\u3002\u672c\u6587\u9996\u6b21\u89e3\u51b3\u7f8e\u5b66\u4e00\u81f4\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u540e\u5149\u7167\u4f30\u8ba1\u6620\u5c04\uff0c\u5c06\u4e2d\u6027\u767d\u5e73\u8861\u8f6c\u6362\u4e3a\u76f8\u673a\u65e0\u5173\u7a7a\u95f4\u7684\u7f8e\u5b66\u504f\u597d\u6821\u6b63\uff0c\u6a21\u578b\u4ec5\u542b\u7ea6500\u53c2\u6570\uff0c\u8ba1\u7b97\u9ad8\u6548\u3002", "result": "\u5728771\u5f20\u667a\u80fd\u624b\u673a\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8ba1\u7b97\u65f6\u95f4\u4ec50.024\u6beb\u79d2\uff0c\u517c\u5bb9\u73b0\u6709\u8de8\u76f8\u673aAWB\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8f7b\u91cf\u9ad8\u6548\uff0c\u5b9e\u73b0\u4e86\u8de8\u76f8\u673a\u7f8e\u5b66\u4e00\u81f4\u6027\uff0c\u4e14\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u6781\u5c0f\u3002"}}
{"id": "2507.01479", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01479", "abs": "https://arxiv.org/abs/2507.01479", "authors": ["Yingqiang Gao", "Kaede Johnson", "David Froehlich", "Luisa Carrer", "Sarah Ebling"], "title": "Evaluating the Effectiveness of Direct Preference Optimization for Personalizing German Automatic Text Simplifications for Persons with Intellectual Disabilities", "comment": null, "summary": "Automatic text simplification (ATS) aims to enhance language accessibility\nfor various target groups, particularly persons with intellectual disabilities.\nRecent advancements in generative AI, especially large language models (LLMs),\nhave substantially improved the quality of machine-generated text\nsimplifications, thereby mitigating information barriers for the target group.\nHowever, existing LLM-based ATS systems do not incorporate preference feedback\non text simplifications during training, resulting in a lack of personalization\ntailored to the specific needs of target group representatives.\n  In this work, we extend the standard supervised fine-tuning (SFT) approach\nfor adapting LLM-based ATS models by leveraging a computationally efficient LLM\nalignment technique -- direct preference optimization (DPO). Specifically, we\npost-train LLM-based ATS models using human feedback collected from persons\nwith intellectual disabilities, reflecting their preferences on paired text\nsimplifications generated by mainstream LLMs. Furthermore, we propose a\npipeline for developing personalized LLM-based ATS systems, encompassing data\ncollection, model selection, SFT and DPO post-training, and evaluation. Our\nfindings underscore the necessity of active participation of target group\npersons in designing personalized AI accessibility solutions aligned with human\nexpectations. This work represents a step towards personalizing inclusive AI\nsystems at the target-group level, incorporating insights not only from text\nsimplification experts but also from target group persons themselves.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e2a\u6027\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6587\u672c\u7b80\u5316\u7cfb\u7edf\uff0c\u901a\u8fc7\u6536\u96c6\u667a\u529b\u969c\u788d\u4eba\u58eb\u7684\u504f\u597d\u53cd\u9988\u6765\u6539\u8fdb\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684LLM\u6587\u672c\u7b80\u5316\u7cfb\u7edf\u7f3a\u4e4f\u4e2a\u6027\u5316\uff0c\u672a\u80fd\u6ee1\u8db3\u76ee\u6807\u7fa4\u4f53\uff08\u5982\u667a\u529b\u969c\u788d\u4eba\u58eb\uff09\u7684\u5177\u4f53\u9700\u6c42\u3002", "method": "\u91c7\u7528\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u6280\u672f\uff0c\u7ed3\u5408\u76ee\u6807\u7fa4\u4f53\u7684\u504f\u597d\u53cd\u9988\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u76ee\u6807\u7fa4\u4f53\u7684\u79ef\u6781\u53c2\u4e0e\u5bf9\u8bbe\u8ba1\u7b26\u5408\u4eba\u7c7b\u671f\u671b\u7684\u4e2a\u6027\u5316AI\u89e3\u51b3\u65b9\u6848\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4e2a\u6027\u5316\u5305\u5bb9\u6027AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5f3a\u8c03\u4e86\u76ee\u6807\u7fa4\u4f53\u76f4\u63a5\u53c2\u4e0e\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.01041", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01041", "abs": "https://arxiv.org/abs/2507.01041", "authors": ["Zuguang Li", "Wen Wu", "Shaohua Wu", "Songge Zhang", "Ye Wang", "Xuemin", "Shen"], "title": "Fast AI Model Splitting over Edge Networks", "comment": "13 pages, 14 figures", "summary": "Split learning (SL) has emerged as a computationally efficient approach for\nartificial intelligence (AI) model training, which can alleviate device-side\ncomputational workloads. However, complex AI model architectures pose high\ncomputational complexity to obtain the optimal model splitting. In this paper,\nwe represent an arbitrary AI model as a directed acyclic graph (DAG), and then\nreformulate the optimal model splitting problem as a minimum s-t cut search\nproblem. To solve the problem, we propose a fast DAG-based model splitting\nalgorithm, which restructures the DAG to enable the optimal model splitting\nidentification via a maximum flow method. Theoretical analysis indicates that\nthe proposed algorithm is optimal. Furthermore, considering AI models with\nblock structures, we propose a block-wise model splitting algorithm to reduce\ncomputational complexity. The algorithm abstracts each block, i.e., a component\nconsisting of multiple layers, into a single vertex, thereby obtaining the\noptimal model splitting via a simplified DAG. Extensive experimental results\ndemonstrate that the proposed algorithms can determine the optimal model\nsplitting within milliseconds, as well as reduce training delay by\n24.62%-38.95% in dynamic edge networks as compared to the state-of-the-art\nbenchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6709\u5411\u65e0\u73af\u56fe\uff08DAG\uff09\u7684\u5feb\u901f\u6a21\u578b\u5206\u5272\u7b97\u6cd5\uff0c\u901a\u8fc7\u6700\u5927\u6d41\u65b9\u6cd5\u627e\u5230\u6700\u4f18\u5206\u5272\uff0c\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u5ef6\u8fdf\u3002", "motivation": "\u89e3\u51b3\u590d\u6742AI\u6a21\u578b\u5206\u5272\u7684\u9ad8\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u63d0\u9ad8\u8bbe\u5907\u7aef\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u5c06AI\u6a21\u578b\u8868\u793a\u4e3aDAG\uff0c\u91cd\u65b0\u5b9a\u4e49\u5206\u5272\u95ee\u9898\u4e3a\u6700\u5c0fs-t\u5272\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8eDAG\u548c\u5757\u7ed3\u6784\u7684\u5feb\u901f\u5206\u5272\u7b97\u6cd5\u3002", "result": "\u7b97\u6cd5\u80fd\u5728\u6beb\u79d2\u7ea7\u627e\u5230\u6700\u4f18\u5206\u5272\uff0c\u52a8\u6001\u8fb9\u7f18\u7f51\u7edc\u4e2d\u8bad\u7ec3\u5ef6\u8fdf\u964d\u4f4e24.62%-38.95%\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u590d\u6742AI\u6a21\u578b\u5206\u5272\u3002"}}
{"id": "2507.01347", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01347", "abs": "https://arxiv.org/abs/2507.01347", "authors": ["Andrei Jelea", "Ahmed Nabil Belbachir", "Marius Leordeanu"], "title": "Learning from Random Subspace Exploration: Generalized Test-Time Augmentation with Self-supervised Distillation", "comment": null, "summary": "We introduce Generalized Test-Time Augmentation (GTTA), a highly effective\nmethod for improving the performance of a trained model, which unlike other\nexisting Test-Time Augmentation approaches from the literature is general\nenough to be used off-the-shelf for many vision and non-vision tasks, such as\nclassification, regression, image segmentation and object detection. By\napplying a new general data transformation, that randomly perturbs multiple\ntimes the PCA subspace projection of a test input, GTTA forms robust ensembles\nat test time in which, due to sound statistical properties, the structural and\nsystematic noises in the initial input data is filtered out and final estimator\nerrors are reduced. Different from other existing methods, we also propose a\nfinal self-supervised learning stage in which the ensemble output, acting as an\nunsupervised teacher, is used to train the initial single student model, thus\nreducing significantly the test time computational cost, at no loss in\naccuracy. Our tests and comparisons to strong TTA approaches and SoTA models on\nvarious vision and non-vision well-known datasets and tasks, such as image\nclassification and segmentation, speech recognition and house price prediction,\nvalidate the generality of the proposed GTTA. Furthermore, we also prove its\neffectiveness on the more specific real-world task of salmon segmentation and\ndetection in low-visibility underwater videos, for which we introduce\nDeepSalmon, the largest dataset of its kind in the literature.", "AI": {"tldr": "GTTA\u662f\u4e00\u79cd\u901a\u7528\u7684\u6d4b\u8bd5\u65f6\u589e\u5f3a\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\uff0c\u901a\u8fc7\u6270\u52a8PCA\u5b50\u7a7a\u95f4\u6295\u5f71\u5f62\u6210\u9c81\u68d2\u96c6\u6210\uff0c\u5e76\u7ed3\u5408\u81ea\u76d1\u7763\u5b66\u4e60\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u589e\u5f3a\u65b9\u6cd5\u7f3a\u4e4f\u901a\u7528\u6027\uff0cGTTA\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u901a\u7528\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u89c6\u89c9\u548c\u975e\u89c6\u89c9\u4efb\u52a1\u3002", "method": "GTTA\u901a\u8fc7\u968f\u673a\u6270\u52a8PCA\u5b50\u7a7a\u95f4\u6295\u5f71\u5f62\u6210\u96c6\u6210\uff0c\u5e76\u5f15\u5165\u81ea\u76d1\u7763\u5b66\u4e60\u9636\u6bb5\uff0c\u7528\u96c6\u6210\u8f93\u51fa\u8bad\u7ec3\u521d\u59cb\u6a21\u578b\u3002", "result": "GTTA\u5728\u591a\u79cd\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u5176\u901a\u7528\u6027\u548c\u6709\u6548\u6027\uff0c\u5c24\u5176\u5728\u4f4e\u80fd\u89c1\u5ea6\u6c34\u4e0b\u89c6\u9891\u4e2d\u7684\u4e09\u6587\u9c7c\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "GTTA\u662f\u4e00\u79cd\u901a\u7528\u4e14\u9ad8\u6548\u7684\u6d4b\u8bd5\u65f6\u589e\u5f3a\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2507.01541", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01541", "abs": "https://arxiv.org/abs/2507.01541", "authors": ["\u00c1lvaro Zaera", "Diana Nicoleta Popa", "Ivan Sekulic", "Paolo Rosso"], "title": "Efficient Out-of-Scope Detection in Dialogue Systems via Uncertainty-Driven LLM Routing", "comment": null, "summary": "Out-of-scope (OOS) intent detection is a critical challenge in task-oriented\ndialogue systems (TODS), as it ensures robustness to unseen and ambiguous\nqueries. In this work, we propose a novel but simple modular framework that\ncombines uncertainty modeling with fine-tuned large language models (LLMs) for\nefficient and accurate OOS detection. The first step applies uncertainty\nestimation to the output of an in-scope intent detection classifier, which is\ncurrently deployed in a real-world TODS handling tens of thousands of user\ninteractions daily. The second step then leverages an emerging LLM-based\napproach, where a fine-tuned LLM is triggered to make a final decision on\ninstances with high uncertainty. Unlike prior approaches, our method\neffectively balances computational efficiency and performance, combining\ntraditional approaches with LLMs and yielding state-of-the-art results on key\nOOS detection benchmarks, including real-world OOS data acquired from a\ndeployed TODS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u548c\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u51c6\u786e\u5730\u68c0\u6d4b\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u7684\u8d85\u51fa\u8303\u56f4\uff08OOS\uff09\u610f\u56fe\u3002", "motivation": "\u89e3\u51b3\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u5bf9\u672a\u89c1\u6216\u6a21\u7cca\u67e5\u8be2\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u786e\u4fdd\u7cfb\u7edf\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u53ef\u9760\u6027\u3002", "method": "\u9996\u5148\u5bf9\u73b0\u6709\u610f\u56fe\u5206\u7c7b\u5668\u7684\u8f93\u51fa\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u7136\u540e\u5bf9\u9ad8\u4e0d\u786e\u5b9a\u6027\u5b9e\u4f8b\u89e6\u53d1\u5fae\u8c03\u7684LLM\u8fdb\u884c\u6700\u7ec8\u51b3\u7b56\u3002", "result": "\u5728\u5173\u952eOOS\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5305\u62ec\u4ece\u5b9e\u9645\u90e8\u7f72\u7cfb\u7edf\u4e2d\u83b7\u53d6\u7684\u771f\u5b9eOOS\u6570\u636e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u7ed3\u5408\u4f20\u7edf\u65b9\u6cd5\u548cLLM\uff0c\u4e3aOOS\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01043", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01043", "abs": "https://arxiv.org/abs/2507.01043", "authors": ["Szymon \u015awiderski", "Agnieszka Jastrz\u0119bska"], "title": "Data Classification with Dynamically Growing and Shrinking Neural Networks", "comment": "Paper submitted to Journal of Computational Science", "summary": "The issue of data-driven neural network model construction is one of the core\nproblems in the domain of Artificial Intelligence. A standard approach assumes\na fixed architecture with trainable weights. A conceptually more advanced\nassumption is that we not only train the weights, but also find out the optimal\nmodel architecture. We present a new method that realizes just that. This\narticle is an extended version of our conference paper titled \"Dynamic Growing\nand Shrinking of Neural Networks with Monte Carlo Tree Search [26]\". In the\npaper, we show in detail how to create a neural network with a procedure that\nallows dynamic shrinking and growing of the model while it is being trained.\nThe decision-making mechanism for the architectural design is governed by a\nMonte Carlo tree search procedure which simulates network behavior and allows\nto compare several candidate architecture changes to choose the best one. The\nproposed method was validated using both visual and time series datasets,\ndemonstrating its particular effectiveness in multivariate time series\nclassification. This is attributed to the architecture's ability to adapt\ndynamically, allowing independent modifications for each time series. The\napproach is supplemented by Python source code for reproducibility.\nExperimental evaluations in visual pattern and multivariate time series\nclassification tasks revealed highly promising performance, underscoring the\nmethod's robustness and adaptability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u8c03\u6574\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u5b9e\u73b0\u8bad\u7ec3\u4e2d\u7684\u67b6\u6784\u4f18\u5316\uff0c\u9002\u7528\u4e8e\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u56fa\u5b9a\u67b6\u6784\u795e\u7ecf\u7f51\u7edc\u7684\u5c40\u9650\u6027\uff0c\u63a2\u7d22\u52a8\u6001\u8c03\u6574\u67b6\u6784\u4ee5\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u6a21\u62df\u7f51\u7edc\u884c\u4e3a\uff0c\u52a8\u6001\u589e\u51cf\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u72ec\u7acb\u4f18\u5316\u6bcf\u4e2a\u65f6\u95f4\u5e8f\u5217\u3002", "result": "\u5728\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u52a8\u6001\u67b6\u6784\u8c03\u6574\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4e2d\u6548\u679c\u7a81\u51fa\u3002"}}
{"id": "2507.01351", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01351", "abs": "https://arxiv.org/abs/2507.01351", "authors": ["Chaoxiang Cai", "Longrong Yang", "Kaibing Chen", "Fan Yang", "Xi Li"], "title": "Long-Tailed Distribution-Aware Router For Mixture-of-Experts in Large Vision-Language Model", "comment": null, "summary": "The mixture-of-experts (MoE), which replaces dense models with sparse\narchitectures, has gained attention in large vision-language models (LVLMs) for\nachieving comparable performance with fewer activated parameters. Existing MoE\nframeworks for LVLMs focus on token-to-expert routing (TER), encouraging\ndifferent experts to specialize in processing distinct tokens. However, these\nframeworks often rely on the load balancing mechanism, overlooking the inherent\ndistributional differences between vision and language. To this end, we propose\na Long-Tailed Distribution-aware Router (LTDR) for vision-language TER,\ntackling two challenges: (1) Distribution-aware router for modality-specific\nrouting. We observe that language TER follows a uniform distribution, whereas\nvision TER exhibits a long-tailed distribution. This discrepancy necessitates\ndistinct routing strategies tailored to each modality. (2) Enhancing expert\nactivation for vision tail tokens. Recognizing the importance of vision tail\ntokens, we introduce an oversampling-like strategy by increasing the number of\nactivated experts for these tokens. Experiments on extensive benchmarks\nvalidate the effectiveness of our approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u957f\u5c3e\u5206\u5e03\u611f\u77e5\u8def\u7531\u5668\uff08LTDR\uff09\uff0c\u7528\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u6a21\u6001\u95f4\u5206\u5e03\u5dee\u5f02\u548c\u89c6\u89c9\u5c3e\u90e8\u4ee4\u724c\u6fc0\u6d3b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709MoE\u6846\u67b6\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u5ffd\u89c6\u4e86\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u6001\u95f4\u7684\u5206\u5e03\u5dee\u5f02\uff0c\u5bfc\u81f4\u8def\u7531\u7b56\u7565\u4e0d\u5339\u914d\u3002", "method": "\u63d0\u51faLTDR\uff0c\u5305\u62ec\u6a21\u6001\u611f\u77e5\u8def\u7531\u7b56\u7565\u548c\u9488\u5bf9\u89c6\u89c9\u5c3e\u90e8\u4ee4\u724c\u7684\u4e13\u5bb6\u6fc0\u6d3b\u589e\u5f3a\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86LTDR\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "LTDR\u4e3a\u89c6\u89c9\u8bed\u8a00MoE\u6846\u67b6\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u8def\u7531\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2507.01543", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01543", "abs": "https://arxiv.org/abs/2507.01543", "authors": ["Quang Minh Nguyen", "Taegyoon Kim"], "title": "Is External Information Useful for Stance Detection with LLMs?", "comment": "ACL Findings 2025", "summary": "In the stance detection task, a text is classified as either favorable,\nopposing, or neutral towards a target. Prior work suggests that the use of\nexternal information, e.g., excerpts from Wikipedia, improves stance detection\nperformance. However, whether or not such information can benefit large\nlanguage models (LLMs) remains an unanswered question, despite their wide\nadoption in many reasoning tasks. In this study, we conduct a systematic\nevaluation on how Wikipedia and web search external information can affect\nstance detection across eight LLMs and in three datasets with 12 targets.\nSurprisingly, we find that such information degrades performance in most cases,\nwith macro F1 scores dropping by up to 27.9\\%. We explain this through\nexperiments showing LLMs' tendency to align their predictions with the stance\nand sentiment of the provided information rather than the ground truth stance\nof the given text. We also find that performance degradation persists with\nchain-of-thought prompting, while fine-tuning mitigates but does not fully\neliminate it. Our findings, in contrast to previous literature on BERT-based\nsystems which suggests that external information enhances performance,\nhighlight the risks of information biases in LLM-based stance classifiers. Code\nis available at https://github.com/ngqm/acl2025-stance-detection.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u7acb\u573a\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0c\u5916\u90e8\u4fe1\u606f\uff08\u5982\u7ef4\u57fa\u767e\u79d1\u6216\u7f51\u7edc\u641c\u7d22\uff09\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6027\u80fd\u901a\u5e38\u6709\u8d1f\u9762\u5f71\u54cd\uff0c\u5bfc\u81f4F1\u5206\u6570\u4e0b\u964d\u9ad8\u8fbe27.9%\u3002", "motivation": "\u63a2\u8ba8\u5916\u90e8\u4fe1\u606f\u662f\u5426\u5bf9LLMs\u5728\u7acb\u573a\u68c0\u6d4b\u4efb\u52a1\u4e2d\u6709\u76ca\uff0c\u5c3d\u7ba1\u6b64\u524d\u7814\u7a76\u8868\u660e\u5176\u5bf9BERT\u7c7b\u6a21\u578b\u6709\u63d0\u5347\u4f5c\u7528\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u7ef4\u57fa\u767e\u79d1\u548c\u7f51\u7edc\u641c\u7d22\u4fe1\u606f\u5bf98\u79cdLLMs\u57283\u4e2a\u6570\u636e\u96c6\uff0812\u4e2a\u76ee\u6807\uff09\u4e2d\u7684\u7acb\u573a\u68c0\u6d4b\u6027\u80fd\u5f71\u54cd\u3002", "result": "\u5916\u90e8\u4fe1\u606f\u901a\u5e38\u964d\u4f4e\u6027\u80fd\uff0cLLMs\u503e\u5411\u4e8e\u6839\u636e\u63d0\u4f9b\u7684\u4fe1\u606f\u800c\u975e\u6587\u672c\u771f\u5b9e\u7acb\u573a\u8fdb\u884c\u9884\u6d4b\u3002\u5fae\u8c03\u53ef\u7f13\u89e3\u4f46\u65e0\u6cd5\u5b8c\u5168\u6d88\u9664\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLMs\u5728\u7acb\u573a\u68c0\u6d4b\u4e2d\u7684\u4fe1\u606f\u504f\u89c1\u98ce\u9669\uff0c\u4e0eBERT\u7c7b\u6a21\u578b\u7684\u7814\u7a76\u7ed3\u679c\u5f62\u6210\u5bf9\u6bd4\u3002"}}
{"id": "2507.01045", "categories": ["cs.LG", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.01045", "abs": "https://arxiv.org/abs/2507.01045", "authors": ["Xiao Gu", "Wei Tang", "Jinpei Han", "Veer Sangha", "Fenglin Liu", "Shreyank N Gowda", "Antonio H. Ribeiro", "Patrick Schwab", "Kim Branson", "Lei Clifton", "Antonio Luiz P. Ribeiro", "Zhangdaihong Liu", "David A. Clifton"], "title": "Sensing Cardiac Health Across Scenarios and Devices: A Multi-Modal Foundation Model Pretrained on Heterogeneous Data from 1.7 Million Individuals", "comment": null, "summary": "Cardiac biosignals, such as electrocardiograms (ECG) and photoplethysmograms\n(PPG), are of paramount importance for the diagnosis, prevention, and\nmanagement of cardiovascular diseases, and have been extensively used in a\nvariety of clinical tasks. Conventional deep learning approaches for analyzing\nthese signals typically rely on homogeneous datasets and static bespoke models,\nlimiting their robustness and generalizability across diverse clinical settings\nand acquisition protocols. In this study, we present a cardiac sensing\nfoundation model (CSFM) that leverages advanced transformer architectures and a\ngenerative, masked pretraining strategy to learn unified representations from\nvast, heterogeneous health records. Our model is pretrained on an innovative\nmulti-modal integration of data from multiple large-scale datasets (including\nMIMIC-III-WDB, MIMIC-IV-ECG, and CODE), comprising cardiac signals and the\ncorresponding clinical or machine-generated text reports from approximately 1.7\nmillion individuals. We demonstrate that the embeddings derived from our CSFM\nnot only serve as effective feature extractors across diverse cardiac sensing\nscenarios, but also enable seamless transfer learning across varying input\nconfigurations and sensor modalities. Extensive evaluations across diagnostic\ntasks, demographic information recognition, vital sign measurement, clinical\noutcome prediction, and ECG question answering reveal that CSFM consistently\noutperforms traditional one-modal-one-task approaches. Notably, CSFM exhibits\nrobust performance across multiple ECG lead configurations from standard\n12-lead systems to single-lead setups, and in scenarios where only ECG, only\nPPG, or a combination thereof is available. These findings highlight the\npotential of CSFM as a versatile and scalable solution, for comprehensive\ncardiac monitoring.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u67b6\u6784\u7684\u5fc3\u810f\u611f\u77e5\u57fa\u7840\u6a21\u578b\uff08CSFM\uff09\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u9884\u8bad\u7ec3\uff0c\u63d0\u5347\u4e86\u5fc3\u810f\u4fe1\u53f7\u5206\u6790\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u540c\u8d28\u6570\u636e\u96c6\u548c\u9759\u6001\u6a21\u578b\uff0c\u9650\u5236\u4e86\u5176\u5728\u591a\u6837\u5316\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5229\u7528Transformer\u67b6\u6784\u548c\u751f\u6210\u5f0f\u63a9\u7801\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u4ece\u5927\u89c4\u6a21\u5f02\u6784\u5065\u5eb7\u8bb0\u5f55\u4e2d\u5b66\u4e60\u7edf\u4e00\u8868\u793a\u3002", "result": "CSFM\u5728\u591a\u79cd\u5fc3\u810f\u611f\u77e5\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u5355\u6a21\u6001\u5355\u4efb\u52a1\u65b9\u6cd5\uff0c\u5e76\u9002\u5e94\u4e0d\u540c\u4f20\u611f\u5668\u914d\u7f6e\u3002", "conclusion": "CSFM\u662f\u4e00\u79cd\u591a\u529f\u80fd\u3001\u53ef\u6269\u5c55\u7684\u5fc3\u810f\u76d1\u6d4b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01367", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01367", "abs": "https://arxiv.org/abs/2507.01367", "authors": ["Tianrui Lou", "Xiaojun Jia", "Siyuan Liang", "Jiawei Liang", "Ming Zhang", "Yanjun Xiao", "Xiaochun Cao"], "title": "3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial Camouflage Generation", "comment": "Accepted by ICCV 2025", "summary": "Physical adversarial attack methods expose the vulnerabilities of deep neural\nnetworks and pose a significant threat to safety-critical scenarios such as\nautonomous driving. Camouflage-based physical attack is a more promising\napproach compared to the patch-based attack, offering stronger adversarial\neffectiveness in complex physical environments. However, most prior work relies\non mesh priors of the target object and virtual environments constructed by\nsimulators, which are time-consuming to obtain and inevitably differ from the\nreal world. Moreover, due to the limitations of the backgrounds in training\nimages, previous methods often fail to produce multi-view robust adversarial\ncamouflage and tend to fall into sub-optimal solutions. Due to these reasons,\nprior work lacks adversarial effectiveness and robustness across diverse\nviewpoints and physical environments. We propose a physical attack framework\nbased on 3D Gaussian Splatting (3DGS), named PGA, which provides rapid and\nprecise reconstruction with few images, along with photo-realistic rendering\ncapabilities. Our framework further enhances cross-view robustness and\nadversarial effectiveness by preventing mutual and self-occlusion among\nGaussians and employing a min-max optimization approach that adjusts the\nimaging background of each viewpoint, helping the algorithm filter out\nnon-robust adversarial features. Extensive experiments validate the\neffectiveness and superiority of PGA. Our code is available\nat:https://github.com/TRLou/PGA.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6e85\u5c04\uff083DGS\uff09\u7684\u7269\u7406\u653b\u51fb\u6846\u67b6PGA\uff0c\u901a\u8fc7\u5feb\u901f\u91cd\u5efa\u548c\u903c\u771f\u6e32\u67d3\uff0c\u63d0\u5347\u4e86\u8de8\u89c6\u89d2\u9c81\u68d2\u6027\u548c\u5bf9\u6297\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7269\u7406\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u4f9d\u8d56\u76ee\u6807\u5bf9\u8c61\u7684\u7f51\u683c\u5148\u9a8c\u548c\u6a21\u62df\u5668\u6784\u5efa\u7684\u865a\u62df\u73af\u5883\uff0c\u8017\u65f6\u4e14\u4e0e\u73b0\u5b9e\u5b58\u5728\u5dee\u5f02\uff0c\u4e14\u7f3a\u4e4f\u591a\u89c6\u89d2\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u75283D\u9ad8\u65af\u6e85\u5c04\u6280\u672f\uff0c\u901a\u8fc7\u9632\u6b62\u9ad8\u65af\u95f4\u7684\u76f8\u4e92\u548c\u81ea\u906e\u6321\uff0c\u7ed3\u5408min-max\u4f18\u5316\u8c03\u6574\u6bcf\u4e2a\u89c6\u89d2\u7684\u6210\u50cf\u80cc\u666f\uff0c\u8fc7\u6ee4\u975e\u9c81\u68d2\u5bf9\u6297\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86PGA\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "PGA\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u7269\u7406\u5bf9\u6297\u653b\u51fb\u7684\u8de8\u89c6\u89d2\u9c81\u68d2\u6027\u548c\u5bf9\u6297\u6548\u679c\u3002"}}
{"id": "2507.01594", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01594", "abs": "https://arxiv.org/abs/2507.01594", "authors": ["Shutong Feng", "Hsien-chin Lin", "Nurul Lubis", "Carel van Niekerk", "Michael Heck", "Benjamin Ruppik", "Renato Vukovic", "Milica Ga\u0161i\u0107"], "title": "Emotionally Intelligent Task-oriented Dialogue Systems: Architecture, Representation, and Optimisation", "comment": "19 pages, 6 figures", "summary": "Task-oriented dialogue (ToD) systems are designed to help users achieve\nspecific goals through natural language interaction. While recent advances in\nlarge language models (LLMs) have significantly improved linguistic fluency and\ncontextual understanding, building effective and emotionally intelligent ToD\nsystems remains a complex challenge. Effective ToD systems must optimise for\ntask success, emotional understanding and responsiveness, and precise\ninformation conveyance, all within inherently noisy and ambiguous\nconversational environments. In this work, we investigate architectural,\nrepresentational, optimisational as well as emotional considerations of ToD\nsystems. We set up systems covering these design considerations with a\nchallenging evaluation environment composed of a natural-language user\nsimulator coupled with an imperfect natural language understanding module. We\npropose \\textbf{LUSTER}, an \\textbf{L}LM-based \\textbf{U}nified \\textbf{S}ystem\nfor \\textbf{T}ask-oriented dialogue with \\textbf{E}nd-to-end\n\\textbf{R}einforcement learning with both short-term (user sentiment) and\nlong-term (task success) rewards. Our findings demonstrate that combining LLM\ncapability with structured reward modelling leads to more resilient and\nemotionally responsive ToD systems, offering a practical path forward for\nnext-generation conversational agents.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edfLUSTER\uff0c\u7ed3\u5408\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\uff0c\u4f18\u5316\u4efb\u52a1\u6210\u529f\u7387\u548c\u60c5\u611f\u54cd\u5e94\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u8bed\u8a00\u6d41\u7545\u6027\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\u65b9\u9762\u6709\u663e\u8457\u8fdb\u6b65\uff0c\u4f46\u6784\u5efa\u9ad8\u6548\u4e14\u60c5\u611f\u667a\u80fd\u7684\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51faLUSTER\u7cfb\u7edf\uff0c\u7ed3\u5408LLM\u80fd\u529b\u548c\u7ed3\u6784\u5316\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u77ed\u671f\uff08\u7528\u6237\u60c5\u611f\uff09\u548c\u957f\u671f\uff08\u4efb\u52a1\u6210\u529f\uff09\u5956\u52b1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408LLM\u548c\u7ed3\u6784\u5316\u5956\u52b1\u6a21\u578b\u80fd\u63d0\u5347\u7cfb\u7edf\u7684\u97e7\u6027\u548c\u60c5\u611f\u54cd\u5e94\u80fd\u529b\u3002", "conclusion": "LUSTER\u4e3a\u4e0b\u4e00\u4ee3\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2507.01047", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.01047", "abs": "https://arxiv.org/abs/2507.01047", "authors": ["Logan A. Burnett", "Umme Mahbuba Nabila", "Majdi I. Radaideh"], "title": "Variational Digital Twins", "comment": "33 pages, 14 figures, and 7 tables", "summary": "While digital twins (DT) hold promise for providing real-time insights into\ncomplex energy assets, much of the current literature either does not offer a\nclear framework for information exchange between the model and the asset, lacks\nkey features needed for real-time implementation, or gives limited attention to\nmodel uncertainty. Here, we aim to solve these gaps by proposing a variational\ndigital twin (VDT) framework that augments standard neural architectures with a\nsingle Bayesian output layer. This lightweight addition, along with a novel VDT\nupdating algorithm, lets a twin update in seconds on commodity GPUs while\nproducing calibrated uncertainty bounds that can inform experiment design,\ncontrol algorithms, and model reliability. The VDT is evaluated on four\nenergy-sector problems. For critical-heat-flux prediction, uncertainty-driven\nactive learning reaches R2 = 0.98 using 47 % fewer experiments and one-third\nthe training time of random sampling. A three-year renewable-generation twin\nmaintains R2 > 0.95 for solar output and curbs error growth for volatile wind\nforecasts via monthly updates that process only one month of data at a time. A\nnuclear reactor transient cooldown twin reconstructs thermocouple signals with\nR2 > 0.99 and preserves accuracy after 50 % sensor loss, demonstrating\nrobustness to degraded instrumentation. Finally, a physics-informed Li-ion\nbattery twin, retrained after every ten discharges, lowers voltage mean-squared\nerror by an order of magnitude relative to the best static model while adapting\nits credible intervals as the cell approaches end-of-life. These results\ndemonstrate that combining modest Bayesian augmentation with efficient update\nschemes turns conventional surrogates into uncertainty-aware, data-efficient,\nand computationally tractable DTs, paving the way for dependable models across\nindustrial and scientific energy systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53d8\u5206\u6570\u5b57\u5b6a\u751f\uff08VDT\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u8f93\u51fa\u5c42\u548c\u9ad8\u6548\u66f4\u65b0\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u5b57\u5b6a\u751f\u5728\u5b9e\u65f6\u6027\u3001\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u548c\u4fe1\u606f\u4ea4\u6362\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u6570\u5b57\u5b6a\u751f\u6280\u672f\u7f3a\u4e4f\u5b9e\u65f6\u5b9e\u73b0\u7684\u5173\u952e\u7279\u6027\u3001\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u5173\u6ce8\u4e0d\u8db3\uff0c\u4e14\u7f3a\u4e4f\u6e05\u6670\u7684\u6a21\u578b\u4e0e\u8d44\u4ea7\u4fe1\u606f\u4ea4\u6362\u6846\u67b6\u3002", "method": "\u63d0\u51faVDT\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8d1d\u53f6\u65af\u8f93\u51fa\u5c42\u548c\u65b0\u578b\u66f4\u65b0\u7b97\u6cd5\uff0c\u5b9e\u73b0\u5feb\u901f\u66f4\u65b0\u548c\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\u8fb9\u754c\u3002", "result": "\u5728\u56db\u4e2a\u80fd\u6e90\u9886\u57df\u95ee\u9898\u4e2d\u9a8c\u8bc1\uff0cVDT\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\u3001\u6570\u636e\u6548\u7387\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "VDT\u5c06\u4f20\u7edf\u4ee3\u7406\u6a21\u578b\u8f6c\u5316\u4e3a\u5177\u6709\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u3001\u6570\u636e\u9ad8\u6548\u548c\u8ba1\u7b97\u53ef\u884c\u7684\u6570\u5b57\u5b6a\u751f\uff0c\u4e3a\u80fd\u6e90\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u9760\u6a21\u578b\u3002"}}
{"id": "2507.01368", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01368", "abs": "https://arxiv.org/abs/2507.01368", "authors": ["Tianning Chai", "Chancharik Mitra", "Brandon Huang", "Gautam Rajendrakumar Gare", "Zhiqiu Lin", "Assaf Arbelle", "Leonid Karlinsky", "Rogerio Feris", "Trevor Darrell", "Deva Ramanan", "Roei Herzig"], "title": "Activation Reward Models for Few-Shot Model Alignment", "comment": null, "summary": "Aligning Large Language Models (LLMs) and Large Multimodal Models (LMMs) to\nhuman preferences is a central challenge in improving the quality of the\nmodels' generative outputs for real-world applications. A common approach is to\nuse reward modeling to encode preferences, enabling alignment via post-training\nusing reinforcement learning. However, traditional reward modeling is not\neasily adaptable to new preferences because it requires a separate reward\nmodel, commonly trained on large preference datasets. To address this, we\nintroduce Activation Reward Models (Activation RMs) -- a novel few-shot reward\nmodeling method that leverages activation steering to construct well-aligned\nreward signals using minimal supervision and no additional model finetuning.\nActivation RMs outperform existing few-shot reward modeling approaches such as\nLLM-as-a-judge with in-context learning, voting-based scoring, and token\nprobability scoring on standard reward modeling benchmarks. Furthermore, we\ndemonstrate the effectiveness of Activation RMs in mitigating reward hacking\nbehaviors, highlighting their utility for safety-critical applications. Toward\nthis end, we propose PreferenceHack, a novel few-shot setting benchmark, the\nfirst to test reward models on reward hacking in a paired preference format.\nFinally, we show that Activation RM achieves state-of-the-art performance on\nthis benchmark, surpassing even GPT-4o.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aActivation RMs\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6fc0\u6d3b\u5bfc\u5411\u6784\u5efa\u5956\u52b1\u4fe1\u53f7\uff0c\u65e0\u9700\u989d\u5916\u5fae\u8c03\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5c11\u6837\u672c\u5956\u52b1\u5efa\u6a21\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5956\u52b1\u5efa\u6a21\u96be\u4ee5\u9002\u5e94\u65b0\u504f\u597d\u4e14\u4f9d\u8d56\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u6fc0\u6d3b\u5bfc\u5411\u6784\u5efa\u5956\u52b1\u4fe1\u53f7\uff0c\u4ec5\u9700\u5c11\u91cf\u76d1\u7763\u6570\u636e\u4e14\u65e0\u9700\u989d\u5916\u6a21\u578b\u5fae\u8c03\u3002", "result": "\u5728\u6807\u51c6\u5956\u52b1\u5efa\u6a21\u57fa\u51c6\u548cPreferenceHack\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u53caGPT-4o\u3002", "conclusion": "Activation RMs\u4e3a\u5c11\u6837\u672c\u5956\u52b1\u5efa\u6a21\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5b89\u5168\u5173\u952e\u5e94\u7528\u3002"}}
{"id": "2507.01627", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01627", "abs": "https://arxiv.org/abs/2507.01627", "authors": ["Maeve Hutchinson", "Radu Jianu", "Aidan Slingsby", "Jo Wood", "Pranava Madhyastha"], "title": "Chart Question Answering from Real-World Analytical Narratives", "comment": "This paper has been accepted to the ACL Student Research Workshop\n  (SRW) 2025", "summary": "We present a new dataset for chart question answering (CQA) constructed from\nvisualization notebooks. The dataset features real-world, multi-view charts\npaired with natural language questions grounded in analytical narratives.\nUnlike prior benchmarks, our data reflects ecologically valid reasoning\nworkflows. Benchmarking state-of-the-art multimodal large language models\nreveals a significant performance gap, with GPT-4.1 achieving an accuracy of\n69.3%, underscoring the challenges posed by this more authentic CQA setting.", "AI": {"tldr": "\u65b0\u6570\u636e\u96c6\u7528\u4e8e\u56fe\u8868\u95ee\u7b54\uff08CQA\uff09\uff0c\u57fa\u4e8e\u771f\u5b9e\u7684\u591a\u89c6\u56fe\u56fe\u8868\u548c\u81ea\u7136\u8bed\u8a00\u95ee\u9898\uff0c\u53cd\u6620\u5b9e\u9645\u63a8\u7406\u6d41\u7a0b\u3002\u6d4b\u8bd5\u663e\u793a\u5f53\u524d\u6a21\u578b\uff08\u5982GPT-4.1\uff09\u6027\u80fd\u6709\u9650\uff0869.3%\u51c6\u786e\u7387\uff09\u3002", "motivation": "\u73b0\u6709CQA\u6570\u636e\u96c6\u7f3a\u4e4f\u771f\u5b9e\u6027\u548c\u591a\u89c6\u56fe\u56fe\u8868\u7684\u590d\u6742\u6027\uff0c\u9700\u6784\u5efa\u66f4\u8d34\u8fd1\u5b9e\u9645\u5e94\u7528\u7684\u6570\u636e\u96c6\u3002", "method": "\u4ece\u53ef\u89c6\u5316\u7b14\u8bb0\u672c\u4e2d\u6784\u5efa\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u89c6\u56fe\u56fe\u8868\u548c\u81ea\u7136\u8bed\u8a00\u95ee\u9898\uff0c\u6a21\u62df\u771f\u5b9e\u63a8\u7406\u6d41\u7a0b\u3002", "result": "\u6d4b\u8bd5\u4e2d\uff0cGPT-4.1\u7684\u51c6\u786e\u7387\u4e3a69.3%\uff0c\u663e\u793a\u5f53\u524d\u6a21\u578b\u5728\u771f\u5b9eCQA\u573a\u666f\u4e2d\u7684\u6311\u6218\u3002", "conclusion": "\u65b0\u6570\u636e\u96c6\u4e3aCQA\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.01048", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01048", "abs": "https://arxiv.org/abs/2507.01048", "authors": ["Ricardo Emanuel Vaz Vargas", "Afr\u00e2nio Jos\u00e9 de Melo Junior", "Celso Jos\u00e9 Munaro", "Cl\u00e1udio Benevenuto de Campos Lima", "Eduardo Toledo de Lima Junior", "Felipe Muntzberg Barrocas", "Fl\u00e1vio Miguel Varej\u00e3o", "Guilherme Fidelis Peixer", "Igor de Melo Nery Oliveira", "Jader Riso Barbosa Jr.", "Jaime Andr\u00e9s Lozano Cadena", "Jean Carlos Dias de Ara\u00fajo", "Jo\u00e3o Neuenschwander Escosteguy Carneiro", "Lucas Gouveia Omena Lopes", "Lucas Pereira de Gouveia", "Mateus de Araujo Fernandes", "Matheus Lima Scramignon", "Patrick Marques Ciarelli", "Rodrigo Castello Branco", "Rog\u00e9rio Leite Alves Pinto"], "title": "3W Dataset 2.0.0: a realistic and public dataset with rare undesirable real events in oil wells", "comment": "21 pages, 10 figures, and 7 tables", "summary": "In the oil industry, undesirable events in oil wells can cause economic\nlosses, environmental accidents, and human casualties. Solutions based on\nArtificial Intelligence and Machine Learning for Early Detection of such events\nhave proven valuable for diverse applications across industries. In 2019,\nrecognizing the importance and the lack of public datasets related to\nundesirable events in oil wells, Petrobras developed and publicly released the\nfirst version of the 3W Dataset, which is essentially a set of Multivariate\nTime Series labeled by experts. Since then, the 3W Dataset has been developed\ncollaboratively and has become a foundational reference for numerous works in\nthe field. This data article describes the current publicly available version\nof the 3W Dataset, which contains structural modifications and additional\nlabeled data. The detailed description provided encourages and supports the 3W\ncommunity and new 3W users to improve previous published results and to develop\nnew robust methodologies, digital products and services capable of detecting\nundesirable events in oil wells with enough anticipation to enable corrective\nor mitigating actions.", "AI": {"tldr": "\u77f3\u6cb9\u884c\u4e1a\u4e2d\u7684\u4e0d\u826f\u4e8b\u4ef6\u53ef\u80fd\u5bfc\u81f4\u7ecf\u6d4e\u635f\u5931\u3001\u73af\u5883\u4e8b\u6545\u548c\u4eba\u5458\u4f24\u4ea1\u3002\u57fa\u4e8e\u4eba\u5de5\u667a\u80fd\u548c\u673a\u5668\u5b66\u4e60\u7684\u65e9\u671f\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\u5df2\u88ab\u8bc1\u660e\u5bf9\u8de8\u884c\u4e1a\u5e94\u7528\u5177\u6709\u91cd\u8981\u4ef7\u503c\u30022019\u5e74\uff0cPetrobras\u5f00\u53d1\u5e76\u516c\u5f00\u4e86\u9996\u4e2a\u7248\u672c\u76843W\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u76f8\u5173\u516c\u5f00\u6570\u636e\u7684\u7a7a\u767d\uff0c\u5e76\u6210\u4e3a\u8be5\u9886\u57df\u7684\u57fa\u7840\u53c2\u8003\u3002\u672c\u6587\u63cf\u8ff0\u4e86\u5f53\u524d\u516c\u5f00\u7248\u672c\u76843W\u6570\u636e\u96c6\uff0c\u5305\u542b\u7ed3\u6784\u4fee\u6539\u548c\u989d\u5916\u6807\u6ce8\u6570\u636e\uff0c\u65e8\u5728\u652f\u6301\u793e\u533a\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u77f3\u6cb9\u884c\u4e1a\u4e0d\u826f\u4e8b\u4ef6\u7684\u65e9\u671f\u68c0\u6d4b\u5bf9\u51cf\u5c11\u635f\u5931\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u516c\u5f00\u6570\u636e\u96c6\u9650\u5236\u4e86\u76f8\u5173\u7814\u7a76\u7684\u53d1\u5c55\u3002", "method": "Petrobras\u5f00\u53d1\u5e76\u516c\u5f00\u4e863W\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u548c\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\uff0c\u652f\u6301\u4eba\u5de5\u667a\u80fd\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7684\u5e94\u7528\u3002", "result": "3W\u6570\u636e\u96c6\u5df2\u6210\u4e3a\u8be5\u9886\u57df\u7684\u57fa\u7840\u53c2\u8003\uff0c\u5f53\u524d\u7248\u672c\u5305\u542b\u66f4\u591a\u6807\u6ce8\u6570\u636e\u548c\u7ed3\u6784\u6539\u8fdb\u3002", "conclusion": "3W\u6570\u636e\u96c6\u7684\u66f4\u65b0\u65e8\u5728\u652f\u6301\u793e\u533a\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u77f3\u6cb9\u884c\u4e1a\u4e0d\u826f\u4e8b\u4ef6\u7684\u5f71\u54cd\u3002"}}
{"id": "2507.01372", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01372", "abs": "https://arxiv.org/abs/2507.01372", "authors": ["Max Hamilton", "Jinlin Lai", "Wenlong Zhao", "Subhransu Maji", "Daniel Sheldon"], "title": "Active Measurement: Efficient Estimation at Scale", "comment": null, "summary": "AI has the potential to transform scientific discovery by analyzing vast\ndatasets with little human effort. However, current workflows often do not\nprovide the accuracy or statistical guarantees that are needed. We introduce\nactive measurement, a human-in-the-loop AI framework for scientific\nmeasurement. An AI model is used to predict measurements for individual units,\nwhich are then sampled for human labeling using importance sampling. With each\nnew set of human labels, the AI model is improved and an unbiased Monte Carlo\nestimate of the total measurement is refined. Active measurement can provide\nprecise estimates even with an imperfect AI model, and requires little human\neffort when the AI model is very accurate. We derive novel estimators,\nweighting schemes, and confidence intervals, and show that active measurement\nreduces estimation error compared to alternatives in several measurement tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u4e3b\u52a8\u6d4b\u91cf\u201d\u7684\u4eba\u673a\u534f\u4f5cAI\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u8981\u6027\u91c7\u6837\u548c\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u63d0\u5347\u79d1\u5b66\u6d4b\u91cf\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524dAI\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u7684\u5de5\u4f5c\u6d41\u7a0b\u7f3a\u4e4f\u8db3\u591f\u7684\u51c6\u786e\u6027\u548c\u7edf\u8ba1\u4fdd\u8bc1\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408AI\u9884\u6d4b\u548c\u4eba\u7c7b\u6807\u6ce8\uff0c\u91c7\u7528\u91cd\u8981\u6027\u91c7\u6837\u548c\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u9010\u6b65\u4f18\u5316\u6a21\u578b\u548c\u6d4b\u91cf\u7ed3\u679c\u3002", "result": "\u4e3b\u52a8\u6d4b\u91cf\u5728\u591a\u4e2a\u6d4b\u91cf\u4efb\u52a1\u4e2d\u964d\u4f4e\u4e86\u4f30\u8ba1\u8bef\u5dee\uff0c\u4e14\u80fd\u5728AI\u6a21\u578b\u4e0d\u5b8c\u7f8e\u65f6\u4ecd\u63d0\u4f9b\u7cbe\u786e\u4f30\u8ba1\u3002", "conclusion": "\u4e3b\u52a8\u6d4b\u91cf\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u79d1\u5b66\u6d4b\u91cf\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u9700\u8981\u9ad8\u7cbe\u5ea6\u548c\u4f4e\u4eba\u5de5\u6210\u672c\u7684\u4efb\u52a1\u3002"}}
{"id": "2507.01633", "categories": ["cs.CL", "cs.IR", "62-04", "D.2.3"], "pdf": "https://arxiv.org/pdf/2507.01633", "abs": "https://arxiv.org/abs/2507.01633", "authors": ["Georgii Levtsov", "Dmitry Ustalov"], "title": "Confidence and Stability of Global and Pairwise Scores in NLP Evaluation", "comment": "8 pages, accepted at ACL SRW 2025", "summary": "With the advent of highly capable instruction-tuned neural language models,\nbenchmarking in natural language processing (NLP) is increasingly shifting\ntowards pairwise comparison leaderboards, such as LMSYS Arena, from traditional\nglobal pointwise scores (e.g., GLUE, BIG-bench, SWE-bench). This paper\nempirically investigates the strengths and weaknesses of both global scores and\npairwise comparisons to aid decision-making in selecting appropriate model\nevaluation strategies. Through computational experiments on synthetic and\nreal-world datasets using standard global metrics and the popular Bradley-Terry\nmodel for pairwise comparisons, we found that while global scores provide more\nreliable overall rankings, they can underestimate strong models with rare,\nsignificant errors or low confidence. Conversely, pairwise comparisons are\nparticularly effective for identifying strong contenders among models with\nlower global scores, especially where quality metrics are hard to define (e.g.,\ntext generation), though they require more comparisons to converge if ties are\nfrequent. Our code and data are available at\nhttps://github.com/HSPyroblast/srw-ranking under a permissive license.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86\u5168\u5c40\u8bc4\u5206\u548c\u6210\u5bf9\u6bd4\u8f83\u5728NLP\u6a21\u578b\u8bc4\u4f30\u4e2d\u7684\u4f18\u7f3a\u70b9\uff0c\u53d1\u73b0\u5168\u5c40\u8bc4\u5206\u66f4\u53ef\u9760\u4f46\u53ef\u80fd\u4f4e\u4f30\u67d0\u4e9b\u6a21\u578b\uff0c\u800c\u6210\u5bf9\u6bd4\u8f83\u80fd\u66f4\u597d\u8bc6\u522b\u4f4e\u5206\u6a21\u578b\u4e2d\u7684\u5f3a\u8005\uff0c\u4f46\u6536\u655b\u8f83\u6162\u3002", "motivation": "\u968f\u7740\u6307\u4ee4\u8c03\u4f18\u795e\u7ecf\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0cNLP\u8bc4\u4f30\u9010\u6e10\u4ece\u5168\u5c40\u8bc4\u5206\u8f6c\u5411\u6210\u5bf9\u6bd4\u8f83\u3002\u672c\u6587\u65e8\u5728\u4e3a\u9009\u62e9\u8bc4\u4f30\u7b56\u7565\u63d0\u4f9b\u4f9d\u636e\u3002", "method": "\u901a\u8fc7\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u7684\u8ba1\u7b97\u5b9e\u9a8c\uff0c\u4f7f\u7528\u5168\u5c40\u6307\u6807\u548cBradley-Terry\u6a21\u578b\u8fdb\u884c\u6210\u5bf9\u6bd4\u8f83\u3002", "result": "\u5168\u5c40\u8bc4\u5206\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u603b\u4f53\u6392\u540d\uff0c\u4f46\u53ef\u80fd\u4f4e\u4f30\u67d0\u4e9b\u6a21\u578b\uff1b\u6210\u5bf9\u6bd4\u8f83\u80fd\u8bc6\u522b\u4f4e\u5206\u6a21\u578b\u4e2d\u7684\u5f3a\u8005\uff0c\u4f46\u6536\u655b\u8f83\u6162\u3002", "conclusion": "\u4e24\u79cd\u65b9\u6cd5\u5404\u6709\u4f18\u52a3\uff0c\u9700\u6839\u636e\u5177\u4f53\u573a\u666f\u9009\u62e9\u8bc4\u4f30\u7b56\u7565\u3002"}}
{"id": "2507.01050", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01050", "abs": "https://arxiv.org/abs/2507.01050", "authors": ["Jing Yu", "Yibo Zhao", "Jiapeng Zhu", "Wenming Shao", "Bo Pang", "Zhao Zhang", "Xiang Li"], "title": "Text Detoxification: Data Efficiency, Semantic Preservation and Model Generalization", "comment": null, "summary": "The widespread dissemination of toxic content on social media poses a serious\nthreat to both online environments and public discourse, highlighting the\nurgent need for detoxification methods that effectively remove toxicity while\npreserving the original semantics. However, existing approaches often struggle\nto simultaneously achieve strong detoxification performance, semantic\npreservation, and robustness to out-of-distribution data. Moreover, they\ntypically rely on costly, manually annotated parallel corpora while showing\npoor data efficiency. To address these challenges, we propose a two-stage\ntraining framework that jointly optimizes for data efficiency, semantic\npreservation, and model generalization. We first perform supervised fine-tuning\non a small set of high-quality, filtered parallel data to establish a strong\ninitialization. Then, we leverage unlabeled toxic inputs and a custom-designed\nreward model to train the LLM using Group Relative Policy Optimization.\nExperimental results demonstrate that our method effectively mitigates the\ntrade-offs faced by previous work, achieving state-of-the-art performance with\nimproved generalization and significantly reduced dependence on annotated data.\nOur code is available at:\nhttps://anonymous.4open.science/r/Detoxification-of-Text-725F/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u53bb\u9664\u793e\u4ea4\u5a92\u4f53\u6587\u672c\u4e2d\u7684\u6bd2\u6027\u5185\u5bb9\uff0c\u540c\u65f6\u4fdd\u7559\u8bed\u4e49\uff0c\u51cf\u5c11\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e0a\u6bd2\u6027\u5185\u5bb9\u7684\u5e7f\u6cdb\u4f20\u64ad\u5bf9\u5728\u7ebf\u73af\u5883\u548c\u516c\u5171\u8ba8\u8bba\u6784\u6210\u5a01\u80c1\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u53bb\u6bd2\u6027\u80fd\u3001\u8bed\u4e49\u4fdd\u7559\u548c\u6570\u636e\u6548\u7387\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff1a\u9996\u5148\u5728\u5c11\u91cf\u9ad8\u8d28\u91cf\u5e73\u884c\u6570\u636e\u4e0a\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u7136\u540e\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\u548c\u81ea\u5b9a\u4e49\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7Group Relative Policy Optimization\u8bad\u7ec3LLM\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u5de5\u4f5c\u7684\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u4e86\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u793e\u4ea4\u5a92\u4f53\u6587\u672c\u53bb\u6bd2\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u6cdb\u5316\u80fd\u529b\u5f3a\u4e14\u6570\u636e\u6548\u7387\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01384", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01384", "abs": "https://arxiv.org/abs/2507.01384", "authors": ["Langyu Wang", "Bingke Zhu", "Yingying Chen", "Yiyuan Zhang", "Ming Tang", "Jinqiao Wang"], "title": "MUG: Pseudo Labeling Augmented Audio-Visual Mamba Network for Audio-Visual Video Parsing", "comment": "Accpted by ICCV 2025", "summary": "The weakly-supervised audio-visual video parsing (AVVP) aims to predict all\nmodality-specific events and locate their temporal boundaries. Despite\nsignificant progress, due to the limitations of the weakly-supervised and the\ndeficiencies of the model architecture, existing methods are lacking in\nsimultaneously improving both the segment-level prediction and the event-level\nprediction. In this work, we propose a audio-visual Mamba network with pseudo\nlabeling aUGmentation (MUG) for emphasising the uniqueness of each segment and\nexcluding the noise interference from the alternate modalities. Specifically,\nwe annotate some of the pseudo-labels based on previous work. Using unimodal\npseudo-labels, we perform cross-modal random combinations to generate new data,\nwhich can enhance the model's ability to parse various segment-level event\ncombinations. For feature processing and interaction, we employ a audio-visual\nmamba network. The AV-Mamba enhances the ability to perceive different segments\nand excludes additional modal noise while sharing similar modal information.\nOur extensive experiments demonstrate that MUG improves state-of-the-art\nresults on LLP dataset in all metrics (e.g,, gains of 2.1% and 1.2% in terms of\nvisual Segment-level and audio Segment-level metrics). Our code is available at\nhttps://github.com/WangLY136/MUG.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f2a\u6807\u7b7e\u589e\u5f3a\u7684\u97f3\u9891-\u89c6\u89c9Mamba\u7f51\u7edc\uff08MUG\uff09\uff0c\u7528\u4e8e\u6539\u8fdb\u5f31\u76d1\u7763\u97f3\u9891-\u89c6\u89c9\u89c6\u9891\u89e3\u6790\u4efb\u52a1\u4e2d\u7684\u6bb5\u7ea7\u548c\u4e8b\u4ef6\u7ea7\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5f31\u76d1\u7763\u548c\u6a21\u578b\u67b6\u6784\u9650\u5236\u4e0b\uff0c\u96be\u4ee5\u540c\u65f6\u63d0\u5347\u6bb5\u7ea7\u548c\u4e8b\u4ef6\u7ea7\u9884\u6d4b\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u4f2a\u6807\u7b7e\u589e\u5f3a\u751f\u6210\u65b0\u6570\u636e\uff0c\u5e76\u91c7\u7528\u97f3\u9891-\u89c6\u89c9Mamba\u7f51\u7edc\u8fdb\u884c\u7279\u5f81\u5904\u7406\u548c\u4ea4\u4e92\u3002", "result": "\u5728LLP\u6570\u636e\u96c6\u4e0a\uff0cMUG\u5728\u6240\u6709\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u89c6\u89c9\u6bb5\u7ea7\u548c\u97f3\u9891\u6bb5\u7ea7\u6307\u6807\u5206\u522b\u63d0\u53472.1%\u548c1.2%\uff09\u3002", "conclusion": "MUG\u901a\u8fc7\u4f2a\u6807\u7b7e\u589e\u5f3a\u548cMamba\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u97f3\u9891-\u89c6\u89c9\u89c6\u9891\u89e3\u6790\u7684\u6027\u80fd\u3002"}}
{"id": "2507.01645", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01645", "abs": "https://arxiv.org/abs/2507.01645", "authors": ["Rifki Afina Putri"], "title": "Adapting Language Models to Indonesian Local Languages: An Empirical Study of Language Transferability on Zero-Shot Settings", "comment": "AMLDS 2025", "summary": "In this paper, we investigate the transferability of pre-trained language\nmodels to low-resource Indonesian local languages through the task of sentiment\nanalysis. We evaluate both zero-shot performance and adapter-based transfer on\nten local languages using models of different types: a monolingual Indonesian\nBERT, multilingual models such as mBERT and XLM-R, and a modular adapter-based\napproach called MAD-X. To better understand model behavior, we group the target\nlanguages into three categories: seen (included during pre-training), partially\nseen (not included but linguistically related to seen languages), and unseen\n(absent and unrelated in pre-training data). Our results reveal clear\nperformance disparities across these groups: multilingual models perform best\non seen languages, moderately on partially seen ones, and poorly on unseen\nlanguages. We find that MAD-X significantly improves performance, especially\nfor seen and partially seen languages, without requiring labeled data in the\ntarget language. Additionally, we conduct a further analysis on tokenization\nand show that while subword fragmentation and vocabulary overlap with\nIndonesian correlate weakly with prediction quality, they do not fully explain\nthe observed performance. Instead, the most consistent predictor of transfer\nsuccess is the model's prior exposure to the language, either directly or\nthrough a related language.", "AI": {"tldr": "\u7814\u7a76\u4e86\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u5370\u5c3c\u672c\u5730\u8bed\u8a00\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u53ef\u8fc1\u79fb\u6027\uff0c\u8bc4\u4f30\u4e86\u96f6\u6837\u672c\u548c\u9002\u914d\u5668\u8fc1\u79fb\u65b9\u6cd5\uff0c\u53d1\u73b0\u6a21\u578b\u6027\u80fd\u4e0e\u8bed\u8a00\u5728\u9884\u8bad\u7ec3\u4e2d\u7684\u66b4\u9732\u7a0b\u5ea6\u76f8\u5173\u3002", "motivation": "\u63a2\u7d22\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u5370\u5c3c\u672c\u5730\u8bed\u8a00\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u60c5\u611f\u5206\u6790\u4efb\u52a1\uff0c\u4ee5\u586b\u8865\u76f8\u5173\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u5370\u5c3c\u5355\u8bedBERT\u3001\u591a\u8bed\u8a00\u6a21\u578b\uff08mBERT\u3001XLM-R\uff09\u548c\u9002\u914d\u5668\u65b9\u6cd5MAD-X\uff0c\u5bf9\u5341\u79cd\u672c\u5730\u8bed\u8a00\u8fdb\u884c\u96f6\u6837\u672c\u548c\u9002\u914d\u5668\u8fc1\u79fb\u8bc4\u4f30\uff0c\u5e76\u5c06\u8bed\u8a00\u5206\u4e3a\u4e09\u7c7b\uff08\u5df2\u89c1\u3001\u90e8\u5206\u89c1\u3001\u672a\u89c1\uff09\u3002", "result": "\u591a\u8bed\u8a00\u6a21\u578b\u5728\u5df2\u89c1\u8bed\u8a00\u8868\u73b0\u6700\u4f73\uff0c\u90e8\u5206\u89c1\u6b21\u4e4b\uff0c\u672a\u89c1\u6700\u5dee\uff1bMAD-X\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5df2\u89c1\u548c\u90e8\u5206\u89c1\u8bed\u8a00\u3002\u8bed\u8a00\u66b4\u9732\u7a0b\u5ea6\u662f\u8fc1\u79fb\u6210\u529f\u7684\u6700\u5f3a\u9884\u6d4b\u56e0\u7d20\u3002", "conclusion": "\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u4e2d\u5bf9\u76ee\u6807\u8bed\u8a00\u6216\u5176\u76f8\u5173\u8bed\u8a00\u7684\u66b4\u9732\u7a0b\u5ea6\u662f\u8fc1\u79fb\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\uff0c\u9002\u914d\u5668\u65b9\u6cd5\uff08\u5982MAD-X\uff09\u80fd\u6709\u6548\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u8868\u73b0\u3002"}}
{"id": "2507.01052", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.01052", "abs": "https://arxiv.org/abs/2507.01052", "authors": ["Ahmed Farooq"], "title": "Long-Sequence Memory with Temporal Kernels and Dense Hopfield Functionals", "comment": null, "summary": "In this study we introduce a novel energy functional for long-sequence\nmemory, building upon the framework of dense Hopfield networks which achieves\nexponential storage capacity through higher-order interactions. Building upon\nearlier work on long-sequence Hopfield memory models, we propose a temporal\nkernal $K(m, k)$ to incorporate temporal dependencies, enabling efficient\nsequential retrieval of patterns over extended sequences. We demonstrate the\nsuccessful application of this technique for the storage and sequential\nretrieval of movies frames which are well suited for this because of the high\ndimensional vectors that make up each frame creating enough variation between\neven sequential frames in the high dimensional space. The technique has\napplications in modern transformer architectures, including efficient\nlong-sequence modeling, memory augmentation, improved attention with temporal\nbias, and enhanced handling of long-term dependencies in time-series data. Our\nmodel offers a promising approach to address the limitations of transformers in\nlong-context tasks, with potential implications for natural language\nprocessing, forecasting, and beyond.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bc6\u96c6Hopfield\u7f51\u7edc\u7684\u65b0\u578b\u80fd\u91cf\u51fd\u6570\uff0c\u901a\u8fc7\u9ad8\u9636\u4ea4\u4e92\u5b9e\u73b0\u6307\u6570\u5b58\u50a8\u5bb9\u91cf\uff0c\u5e76\u5f15\u5165\u65f6\u95f4\u6838\u4ee5\u652f\u6301\u957f\u5e8f\u5217\u6a21\u5f0f\u7684\u9ad8\u6548\u68c0\u7d22\u3002", "motivation": "\u89e3\u51b3\u957f\u5e8f\u5217\u4efb\u52a1\u4e2dTransformer\u7684\u5c40\u9650\u6027\uff0c\u5982\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u548c\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u957f\u671f\u4f9d\u8d56\u95ee\u9898\u3002", "method": "\u6784\u5efa\u65f6\u95f4\u6838$K(m, k)$\u4ee5\u6355\u6349\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u5e76\u5e94\u7528\u4e8e\u9ad8\u7ef4\u7535\u5f71\u5e27\u7684\u5b58\u50a8\u4e0e\u68c0\u7d22\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u957f\u5e8f\u5217\u6a21\u5f0f\u7684\u9ad8\u6548\u5b58\u50a8\u4e0e\u68c0\u7d22\uff0c\u9002\u7528\u4e8eTransformer\u67b6\u6784\u7684\u6539\u8fdb\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u9884\u6d4b\u7b49\u9886\u57df\u7684\u957f\u5e8f\u5217\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01390", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01390", "abs": "https://arxiv.org/abs/2507.01390", "authors": ["Shuai Tan", "Bill Gong", "Bin Ji", "Ye Pan"], "title": "FixTalk: Taming Identity Leakage for High-Quality Talking Head Generation in Extreme Cases", "comment": null, "summary": "Talking head generation is gaining significant importance across various\ndomains, with a growing demand for high-quality rendering. However, existing\nmethods often suffer from identity leakage (IL) and rendering artifacts (RA),\nparticularly in extreme cases. Through an in-depth analysis of previous\napproaches, we identify two key insights: (1) IL arises from identity\ninformation embedded within motion features, and (2) this identity information\ncan be leveraged to address RA. Building on these findings, this paper\nintroduces FixTalk, a novel framework designed to simultaneously resolve both\nissues for high-quality talking head generation. Firstly, we propose an\nEnhanced Motion Indicator (EMI) to effectively decouple identity information\nfrom motion features, mitigating the impact of IL on generated talking heads.\nTo address RA, we introduce an Enhanced Detail Indicator (EDI), which utilizes\nthe leaked identity information to supplement missing details, thus fixing the\nartifacts. Extensive experiments demonstrate that FixTalk effectively mitigates\nIL and RA, achieving superior performance compared to state-of-the-art methods.", "AI": {"tldr": "FixTalk\u6846\u67b6\u901a\u8fc7EMI\u548cEDI\u5206\u522b\u89e3\u51b3\u8eab\u4efd\u6cc4\u6f0f\u548c\u6e32\u67d3\u4f2a\u5f71\u95ee\u9898\uff0c\u63d0\u5347\u8bf4\u8bdd\u5934\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6781\u7aef\u60c5\u51b5\u4e0b\u5b58\u5728\u8eab\u4efd\u6cc4\u6f0f\u548c\u6e32\u67d3\u4f2a\u5f71\u95ee\u9898\uff0c\u5f71\u54cd\u751f\u6210\u8d28\u91cf\u3002", "method": "\u63d0\u51faEMI\u89e3\u8026\u8eab\u4efd\u4fe1\u606f\u4e0e\u8fd0\u52a8\u7279\u5f81\uff0cEDI\u5229\u7528\u6cc4\u6f0f\u8eab\u4efd\u4fe1\u606f\u8865\u5145\u7ec6\u8282\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eFixTalk\u6709\u6548\u89e3\u51b3\u8eab\u4efd\u6cc4\u6f0f\u548c\u4f2a\u5f71\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FixTalk\u4e3a\u9ad8\u8d28\u91cf\u8bf4\u8bdd\u5934\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01702", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01702", "abs": "https://arxiv.org/abs/2507.01702", "authors": ["Zixin Chen", "Hongzhan Lin", "Kaixin Li", "Ziyang Luo", "Zhen Ye", "Guang Chen", "Zhiyong Huang", "Jing Ma"], "title": "AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness", "comment": "ACL 2025", "summary": "The proliferation of multimodal memes in the social media era demands that\nmultimodal Large Language Models (mLLMs) effectively understand meme\nharmfulness. Existing benchmarks for assessing mLLMs on harmful meme\nunderstanding rely on accuracy-based, model-agnostic evaluations using static\ndatasets. These benchmarks are limited in their ability to provide up-to-date\nand thorough assessments, as online memes evolve dynamically. To address this,\nwe propose AdamMeme, a flexible, agent-based evaluation framework that\nadaptively probes the reasoning capabilities of mLLMs in deciphering meme\nharmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive\nevaluations by iteratively updating the meme data with challenging samples,\nthereby exposing specific limitations in how mLLMs interpret harmfulness.\nExtensive experiments show that our framework systematically reveals the\nvarying performance of different target mLLMs, offering in-depth, fine-grained\nanalyses of model-specific weaknesses. Our code is available at\nhttps://github.com/Lbotirx/AdamMeme.", "AI": {"tldr": "\u63d0\u51faAdamMeme\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u52a8\u6001\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08mLLMs\uff09\u5bf9\u6709\u5bb3\u6a21\u56e0\u7684\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u9759\u6001\u6570\u636e\u96c6\u7684\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u6f14\u53d8\u7684\u5728\u7ebf\u6a21\u56e0\uff0c\u9700\u66f4\u7075\u6d3b\u3001\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684AdamMeme\u6846\u67b6\uff0c\u52a8\u6001\u66f4\u65b0\u6a21\u56e0\u6570\u636e\u4ee5\u6d4b\u8bd5mLLMs\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAdamMeme\u80fd\u7cfb\u7edf\u63ed\u793a\u4e0d\u540cmLLMs\u7684\u6027\u80fd\u5dee\u5f02\u53ca\u5176\u5f31\u70b9\u3002", "conclusion": "AdamMeme\u4e3amLLMs\u7684\u6709\u5bb3\u6a21\u56e0\u7406\u89e3\u63d0\u4f9b\u4e86\u6df1\u5165\u3001\u7ec6\u7c92\u5ea6\u7684\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2507.01054", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01054", "abs": "https://arxiv.org/abs/2507.01054", "authors": ["Jithendaraa Subramanian", "Linda Hung", "Daniel Schweigert", "Santosh Suram", "Weike Ye"], "title": "XxaCT-NN: Structure Agnostic Multimodal Learning for Materials Science", "comment": "10 pages, 6 figures", "summary": "Recent advances in materials discovery have been driven by structure-based\nmodels, particularly those using crystal graphs. While effective for\ncomputational datasets, these models are impractical for real-world\napplications where atomic structures are often unknown or difficult to obtain.\nWe propose a scalable multimodal framework that learns directly from elemental\ncomposition and X-ray diffraction (XRD) -- two of the more available modalities\nin experimental workflows without requiring crystal structure input. Our\narchitecture integrates modality-specific encoders with a cross-attention\nfusion module and is trained on the 5-million-sample Alexandria dataset. We\npresent masked XRD modeling (MXM), and apply MXM and contrastive alignment as\nself-supervised pretraining strategies. Pretraining yields faster convergence\n(up to 4.2x speedup) and improves both accuracy and representation quality. We\nfurther demonstrate that multimodal performance scales more favorably with\ndataset size than unimodal baselines, with gains compounding at larger data\nregimes. Our results establish a path toward structure-free, experimentally\ngrounded foundation models for materials science.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5143\u7d20\u7ec4\u6210\u548cXRD\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u65e0\u9700\u6676\u4f53\u7ed3\u6784\u8f93\u5165\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7b56\u7565\u63d0\u5347\u6750\u6599\u53d1\u73b0\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6676\u4f53\u7ed3\u6784\u7684\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4e0d\u5b9e\u7528\uff0c\u56e0\u4e3a\u539f\u5b50\u7ed3\u6784\u5f80\u5f80\u672a\u77e5\u6216\u96be\u4ee5\u83b7\u53d6\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7ed3\u5408\u5143\u7d20\u7ec4\u6210\u548cXRD\u6570\u636e\uff0c\u4f7f\u7528\u6a21\u6001\u7279\u5b9a\u7f16\u7801\u5668\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u6a21\u5757\uff0c\u5e76\u5f15\u5165\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7b56\u7565\uff08MXM\u548c\u5bf9\u6bd4\u5bf9\u9f50\uff09\u3002", "result": "\u9884\u8bad\u7ec3\u663e\u8457\u52a0\u901f\u6536\u655b\uff08\u6700\u9ad84.2\u500d\uff09\uff0c\u63d0\u5347\u51c6\u786e\u6027\u548c\u8868\u793a\u8d28\u91cf\uff0c\u4e14\u591a\u6a21\u6001\u6027\u80fd\u968f\u6570\u636e\u96c6\u89c4\u6a21\u589e\u957f\u4f18\u4e8e\u5355\u6a21\u6001\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6750\u6599\u79d1\u5b66\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u7ed3\u6784\u8f93\u5165\u3001\u57fa\u4e8e\u5b9e\u9a8c\u6570\u636e\u7684\u901a\u7528\u6a21\u578b\u8def\u5f84\u3002"}}
{"id": "2507.01397", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01397", "abs": "https://arxiv.org/abs/2507.01397", "authors": ["Khanh Son Pham", "Christian Witte", "Jens Behley", "Johannes Betz", "Cyrill Stachniss"], "title": "Coherent Online Road Topology Estimation and Reasoning with Standard-Definition Maps", "comment": "Accepted at IROS 2025", "summary": "Most autonomous cars rely on the availability of high-definition (HD) maps.\nCurrent research aims to address this constraint by directly predicting HD map\nelements from onboard sensors and reasoning about the relationships between the\npredicted map and traffic elements. Despite recent advancements, the coherent\nonline construction of HD maps remains a challenging endeavor, as it\nnecessitates modeling the high complexity of road topologies in a unified and\nconsistent manner. To address this challenge, we propose a coherent approach to\npredict lane segments and their corresponding topology, as well as road\nboundaries, all by leveraging prior map information represented by commonly\navailable standard-definition (SD) maps. We propose a network architecture,\nwhich leverages hybrid lane segment encodings comprising prior information and\ndenoising techniques to enhance training stability and performance.\nFurthermore, we facilitate past frames for temporal consistency. Our\nexperimental evaluation demonstrates that our approach outperforms previous\nmethods by a large margin, highlighting the benefits of our modeling scheme.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u6807\u51c6\u5730\u56fe\uff08SD\uff09\u4fe1\u606f\u9884\u6d4b\u8f66\u9053\u6bb5\u53ca\u5176\u62d3\u6251\u7ed3\u6784\u548c\u9053\u8def\u8fb9\u754c\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u7f16\u7801\u548c\u53bb\u566a\u6280\u672f\u63d0\u5347\u6027\u80fd\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u4f9d\u8d56\u9ad8\u7cbe\u5730\u56fe\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u8f66\u8f7d\u4f20\u611f\u5668\u76f4\u63a5\u9884\u6d4b\u5730\u56fe\u5143\u7d20\uff0c\u5e76\u7edf\u4e00\u5efa\u6a21\u590d\u6742\u9053\u8def\u62d3\u6251\u7ed3\u6784\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7f51\u7edc\u67b6\u6784\uff0c\u5229\u7528\u6df7\u5408\u8f66\u9053\u6bb5\u7f16\u7801\uff08\u7ed3\u5408\u5148\u9a8c\u4fe1\u606f\u548c\u53bb\u566a\u6280\u672f\uff09\u548c\u8fc7\u53bb\u5e27\u6570\u636e\u786e\u4fdd\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5927\u5e45\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5efa\u6a21\u65b9\u6848\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u5148\u9a8c\u5730\u56fe\u4fe1\u606f\u548c\u53bb\u566a\u6280\u672f\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u8f66\u9053\u6bb5\u548c\u9053\u8def\u8fb9\u754c\u7684\u5728\u7ebf\u9884\u6d4b\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01715", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01715", "abs": "https://arxiv.org/abs/2507.01715", "authors": ["Aditya Tomar", "Rudra Murthy", "Pushpak Bhattacharyya"], "title": "Stereotype Detection as a Catalyst for Enhanced Bias Detection: A Multi-Task Learning Approach", "comment": null, "summary": "Bias and stereotypes in language models can cause harm, especially in\nsensitive areas like content moderation and decision-making. This paper\naddresses bias and stereotype detection by exploring how jointly learning these\ntasks enhances model performance. We introduce StereoBias, a unique dataset\nlabeled for bias and stereotype detection across five categories: religion,\ngender, socio-economic status, race, profession, and others, enabling a deeper\nstudy of their relationship. Our experiments compare encoder-only models and\nfine-tuned decoder-only models using QLoRA. While encoder-only models perform\nwell, decoder-only models also show competitive results. Crucially, joint\ntraining on bias and stereotype detection significantly improves bias detection\ncompared to training them separately. Additional experiments with sentiment\nanalysis confirm that the improvements stem from the connection between bias\nand stereotypes, not multi-task learning alone. These findings highlight the\nvalue of leveraging stereotype information to build fairer and more effective\nAI systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faStereoBias\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u504f\u89c1\u548c\u523b\u677f\u5370\u8c61\u68c0\u6d4b\u4efb\u52a1\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5b9e\u9a8c\u8868\u660e\u8054\u5408\u8bad\u7ec3\u663e\u8457\u4f18\u4e8e\u5355\u72ec\u8bad\u7ec3\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u504f\u89c1\u548c\u523b\u677f\u5370\u8c61\u53ef\u80fd\u9020\u6210\u5371\u5bb3\uff0c\u5c24\u5176\u5728\u654f\u611f\u9886\u57df\u5982\u5185\u5bb9\u5ba1\u6838\u548c\u51b3\u7b56\u4e2d\u3002", "method": "\u5f15\u5165StereoBias\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u7f16\u7801\u5668\u6a21\u578b\u548c\u89e3\u7801\u5668\u6a21\u578b\uff08\u4f7f\u7528QLoRA\u5fae\u8c03\uff09\uff0c\u5e76\u8054\u5408\u8bad\u7ec3\u504f\u89c1\u548c\u523b\u677f\u5370\u8c61\u68c0\u6d4b\u4efb\u52a1\u3002", "result": "\u8054\u5408\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u504f\u89c1\u68c0\u6d4b\u6027\u80fd\uff0c\u89e3\u7801\u5668\u6a21\u578b\u8868\u73b0\u7ade\u4e89\u529b\u3002", "conclusion": "\u5229\u7528\u523b\u677f\u5370\u8c61\u4fe1\u606f\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u516c\u5e73\u6709\u6548\u7684AI\u7cfb\u7edf\u3002"}}
{"id": "2507.01056", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01056", "abs": "https://arxiv.org/abs/2507.01056", "authors": ["Lidan Peng", "Lu Gao", "Feng Hong", "Jingran Sun"], "title": "Evaluating Pavement Deterioration Rates Due to Flooding Events Using Explainable AI", "comment": null, "summary": "Flooding can damage pavement infrastructure significantly, causing both\nimmediate and long-term structural and functional issues. This research\ninvestigates how flooding events affect pavement deterioration, specifically\nfocusing on measuring pavement roughness by the International Roughness Index\n(IRI). To quantify these effects, we utilized 20 years of pavement condition\ndata from TxDOT's PMIS database, which is integrated with flood event data,\nincluding duration and spatial extent. Statistical analyses were performed to\ncompare IRI values before and after flooding and to calculate the deterioration\nrates influenced by flood exposure. Moreover, we applied Explainable Artificial\nIntelligence (XAI) techniques, such as SHapley Additive exPlanations (SHAP) and\nLocal Interpretable Model-Agnostic Explanations (LIME), to assess the impact of\nflooding on pavement performance. The results demonstrate that flood-affected\npavements experience a more rapid increase in roughness compared to non-flooded\nsections. These findings emphasize the need for proactive flood mitigation\nstrategies, including improved drainage systems, flood-resistant materials, and\npreventative maintenance, to enhance pavement resilience in vulnerable regions.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u6d2a\u6c34\u5bf9\u8def\u9762\u7c97\u7cd9\u5ea6\u7684\u5f71\u54cd\uff0c\u4f7f\u752820\u5e74\u6570\u636e\u548cXAI\u6280\u672f\uff0c\u53d1\u73b0\u6d2a\u6c34\u52a0\u901f\u8def\u9762\u6076\u5316\uff0c\u5efa\u8bae\u91c7\u53d6\u9632\u6d2a\u63aa\u65bd\u3002", "motivation": "\u6d2a\u6c34\u5bf9\u8def\u9762\u57fa\u7840\u8bbe\u65bd\u9020\u6210\u4e25\u91cd\u635f\u5bb3\uff0c\u7814\u7a76\u65e8\u5728\u91cf\u5316\u6d2a\u6c34\u5bf9\u8def\u9762\u7c97\u7cd9\u5ea6\u7684\u5f71\u54cd\uff0c\u4e3a\u9632\u6d2a\u7b56\u7565\u63d0\u4f9b\u4f9d\u636e\u3002", "method": "\u5229\u7528TxDOT\u7684PMIS\u6570\u636e\u5e9320\u5e74\u8def\u9762\u6570\u636e\uff0c\u7ed3\u5408\u6d2a\u6c34\u4e8b\u4ef6\u6570\u636e\uff0c\u901a\u8fc7\u7edf\u8ba1\u5206\u6790\u548cXAI\u6280\u672f\uff08\u5982SHAP\u548cLIME\uff09\u8bc4\u4f30\u5f71\u54cd\u3002", "result": "\u6d2a\u6c34\u533a\u57df\u7684\u8def\u9762\u7c97\u7cd9\u5ea6\uff08IRI\uff09\u589e\u957f\u66f4\u5feb\uff0c\u8868\u660e\u6d2a\u6c34\u663e\u8457\u52a0\u901f\u8def\u9762\u6076\u5316\u3002", "conclusion": "\u5efa\u8bae\u91c7\u53d6\u4e3b\u52a8\u9632\u6d2a\u63aa\u65bd\uff08\u5982\u6539\u8fdb\u6392\u6c34\u7cfb\u7edf\u3001\u4f7f\u7528\u6297\u6d2a\u6750\u6599\uff09\u4ee5\u589e\u5f3a\u8def\u9762\u6297\u6d2a\u80fd\u529b\u3002"}}
{"id": "2507.01401", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01401", "abs": "https://arxiv.org/abs/2507.01401", "authors": ["Huanwen Liang", "Jingxian Xu", "Yuanji Zhang", "Yuhao Huang", "Yuhan Zhang", "Xin Yang", "Ran Li", "Xuedong Deng", "Yanjun Liu", "Guowei Tao", "Yun Wu", "Sheng Zhao", "Xinru Gao", "Dong Ni"], "title": "Medical-Knowledge Driven Multiple Instance Learning for Classifying Severe Abdominal Anomalies on Prenatal Ultrasound", "comment": "Accepted by MICCAI 2025", "summary": "Fetal abdominal malformations are serious congenital anomalies that require\naccurate diagnosis to guide pregnancy management and reduce mortality. Although\nAI has demonstrated significant potential in medical diagnosis, its application\nto prenatal abdominal anomalies remains limited. Most existing studies focus on\nimage-level classification and rely on standard plane localization, placing\nless emphasis on case-level diagnosis. In this paper, we develop a case-level\nmultiple instance learning (MIL)-based method, free of standard plane\nlocalization, for classifying fetal abdominal anomalies in prenatal ultrasound.\nOur contribution is three-fold. First, we adopt a mixture-of-attention-experts\nmodule (MoAE) to weight different attention heads for various planes. Secondly,\nwe propose a medical-knowledge-driven feature selection module (MFS) to align\nimage features with medical knowledge, performing self-supervised image token\nselection at the case-level. Finally, we propose a prompt-based prototype\nlearning (PPL) to enhance the MFS. Extensively validated on a large prenatal\nabdominal ultrasound dataset containing 2,419 cases, with a total of 24,748\nimages and 6 categories, our proposed method outperforms the state-of-the-art\ncompetitors. Codes are available at:https://github.com/LL-AC/AAcls.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u5b9e\u4f8b\u5b66\u4e60\uff08MIL\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u80ce\u513f\u8179\u90e8\u5f02\u5e38\u7684\u4ea7\u524d\u8d85\u58f0\u5206\u7c7b\uff0c\u65e0\u9700\u6807\u51c6\u5e73\u9762\u5b9a\u4f4d\uff0c\u901a\u8fc7\u6df7\u5408\u6ce8\u610f\u529b\u4e13\u5bb6\u6a21\u5757\uff08MoAE\uff09\u3001\u533b\u5b66\u77e5\u8bc6\u9a71\u52a8\u7684\u7279\u5f81\u9009\u62e9\u6a21\u5757\uff08MFS\uff09\u548c\u63d0\u793a\u539f\u578b\u5b66\u4e60\uff08PPL\uff09\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u80ce\u513f\u8179\u90e8\u7578\u5f62\u662f\u4e25\u91cd\u7684\u5148\u5929\u6027\u5f02\u5e38\uff0c\u9700\u8981\u51c6\u786e\u8bca\u65ad\u4ee5\u6307\u5bfc\u598a\u5a20\u7ba1\u7406\u548c\u964d\u4f4e\u6b7b\u4ea1\u7387\u3002AI\u5728\u533b\u5b66\u8bca\u65ad\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5728\u4ea7\u524d\u8179\u90e8\u5f02\u5e38\u4e2d\u7684\u5e94\u7528\u4ecd\u6709\u9650\u3002\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u56fe\u50cf\u7ea7\u5206\u7c7b\uff0c\u8f83\u5c11\u5173\u6ce8\u75c5\u4f8b\u7ea7\u8bca\u65ad\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMIL\u7684\u65b9\u6cd5\uff0c\u5305\u62ecMoAE\u6a21\u5757\uff08\u52a0\u6743\u4e0d\u540c\u5e73\u9762\u7684\u6ce8\u610f\u529b\u5934\uff09\u3001MFS\u6a21\u5757\uff08\u533b\u5b66\u77e5\u8bc6\u9a71\u52a8\u7684\u7279\u5f81\u9009\u62e9\uff09\u548cPPL\u6a21\u5757\uff08\u589e\u5f3aMFS\uff09\u3002", "result": "\u5728\u5305\u542b2,419\u4e2a\u75c5\u4f8b\u300124,748\u5f20\u56fe\u50cf\u548c6\u4e2a\u7c7b\u522b\u7684\u5927\u578b\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u75c5\u4f8b\u7ea7\u8bca\u65ad\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u4ea7\u524d\u8d85\u58f0\u5206\u7c7b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.01734", "categories": ["cs.CL", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.01734", "abs": "https://arxiv.org/abs/2507.01734", "authors": ["Oliver Wardas", "Florian Matthes"], "title": "LLMs for Legal Subsumption in German Employment Contracts", "comment": "PrePrint - ICAIL25, Chicago", "summary": "Legal work, characterized by its text-heavy and resource-intensive nature,\npresents unique challenges and opportunities for NLP research. While\ndata-driven approaches have advanced the field, their lack of interpretability\nand trustworthiness limits their applicability in dynamic legal environments.\nTo address these issues, we collaborated with legal experts to extend an\nexisting dataset and explored the use of Large Language Models (LLMs) and\nin-context learning to evaluate the legality of clauses in German employment\ncontracts. Our work evaluates the ability of different LLMs to classify clauses\nas \"valid,\" \"unfair,\" or \"void\" under three legal context variants: no legal\ncontext, full-text sources of laws and court rulings, and distilled versions of\nthese (referred to as examination guidelines). Results show that full-text\nsources moderately improve performance, while examination guidelines\nsignificantly enhance recall for void clauses and weighted F1-Score, reaching\n80\\%. Despite these advancements, LLMs' performance when using full-text\nsources remains substantially below that of human lawyers. We contribute an\nextended dataset, including examination guidelines, referenced legal sources,\nand corresponding annotations, alongside our code and all log files. Our\nfindings highlight the potential of LLMs to assist lawyers in contract legality\nreview while also underscoring the limitations of the methods presented.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86NLP\u5728\u6cd5\u5f8b\u6587\u672c\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u6269\u5c55\u6570\u636e\u96c6\u548c\u4f7f\u7528LLM\u8bc4\u4f30\u5fb7\u56fd\u96c7\u4f63\u5408\u540c\u6761\u6b3e\u7684\u5408\u6cd5\u6027\uff0c\u53d1\u73b0\u6cd5\u5f8b\u4e0a\u4e0b\u6587\u5bf9\u6a21\u578b\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4f46LLM\u8868\u73b0\u4ecd\u4e0d\u53ca\u4eba\u7c7b\u5f8b\u5e08\u3002", "motivation": "\u6cd5\u5f8b\u5de5\u4f5c\u7684\u6587\u672c\u5bc6\u96c6\u6027\u548c\u8d44\u6e90\u5bc6\u96c6\u6027\u4e3aNLP\u7814\u7a76\u63d0\u4f9b\u4e86\u72ec\u7279\u6311\u6218\u548c\u673a\u4f1a\uff0c\u4f46\u73b0\u6709\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u5ea6\uff0c\u9650\u5236\u4e86\u5176\u5728\u52a8\u6001\u6cd5\u5f8b\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u4e0e\u6cd5\u5f8b\u4e13\u5bb6\u5408\u4f5c\u6269\u5c55\u6570\u636e\u96c6\uff0c\u63a2\u7d22\u4f7f\u7528LLM\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u8bc4\u4f30\u5408\u540c\u6761\u6b3e\u7684\u5408\u6cd5\u6027\uff0c\u6bd4\u8f83\u4e0d\u540c\u6cd5\u5f8b\u4e0a\u4e0b\u6587\uff08\u65e0\u4e0a\u4e0b\u6587\u3001\u5168\u6587\u6cd5\u5f8b\u6765\u6e90\u3001\u7b80\u5316\u7248\u6307\u5357\uff09\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5168\u6587\u6cd5\u5f8b\u6765\u6e90\u5bf9\u6027\u80fd\u6709\u4e2d\u7b49\u63d0\u5347\uff0c\u800c\u7b80\u5316\u7248\u6307\u5357\u663e\u8457\u63d0\u9ad8\u4e86\u65e0\u6548\u6761\u6b3e\u7684\u53ec\u56de\u7387\u548c\u52a0\u6743F1\u5206\u6570\uff08\u8fbe80%\uff09\uff0c\u4f46LLM\u8868\u73b0\u4ecd\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u5f8b\u5e08\u3002", "conclusion": "LLM\u5728\u5408\u540c\u5408\u6cd5\u6027\u5ba1\u67e5\u4e2d\u6709\u8f85\u52a9\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4ecd\u6709\u5c40\u9650\u6027\uff0c\u8d21\u732e\u4e86\u6269\u5c55\u6570\u636e\u96c6\u548c\u76f8\u5173\u8d44\u6e90\u3002"}}
{"id": "2507.01057", "categories": ["cs.LG", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2507.01057", "abs": "https://arxiv.org/abs/2507.01057", "authors": ["Lushun Fan", "Yuqin Xia", "Jun Li", "Karl Jenkins"], "title": "Loop2Net: Data-Driven Generation and Optimization of Airfoil CFD Meshes from Sparse Boundary Coordinates", "comment": null, "summary": "In this study, an innovative intelligent optimization system for mesh quality\nis proposed, which is based on a deep convolutional neural network\narchitecture, to achieve mesh generation and optimization. The core of the\nstudy is the Loop2Net generator and loss function, it predicts the mesh based\non the given wing coordinates. And the model's performance is continuously\noptimised by two key loss functions during the training. Then discipline by\nadding penalties, the goal of mesh generation was finally reached.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u667a\u80fd\u7f51\u683c\u8d28\u91cf\u4f18\u5316\u7cfb\u7edf\uff0c\u901a\u8fc7Loop2Net\u751f\u6210\u5668\u548c\u635f\u5931\u51fd\u6570\u5b9e\u73b0\u7f51\u683c\u751f\u6210\u4e0e\u4f18\u5316\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u667a\u80fd\u4f18\u5316\u7cfb\u7edf\u63d0\u5347\u7f51\u683c\u751f\u6210\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u6838\u5fc3\u662fLoop2Net\u751f\u6210\u5668\u548c\u4e24\u4e2a\u5173\u952e\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u60e9\u7f5a\u673a\u5236\u4f18\u5316\u6a21\u578b\u6027\u80fd\u3002", "result": "\u7cfb\u7edf\u6210\u529f\u5b9e\u73b0\u4e86\u57fa\u4e8e\u7ed9\u5b9a\u7ffc\u5750\u6807\u7684\u7f51\u683c\u9884\u6d4b\u548c\u4f18\u5316\u3002", "conclusion": "\u8be5\u667a\u80fd\u4f18\u5316\u7cfb\u7edf\u6709\u6548\u63d0\u5347\u4e86\u7f51\u683c\u751f\u6210\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2507.01409", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01409", "abs": "https://arxiv.org/abs/2507.01409", "authors": ["Kuniaki Saito", "Donghyun Kim", "Kwanyong Park", "Atsushi Hashimoto", "Yoshitaka Ushiku"], "title": "CaptionSmiths: Flexibly Controlling Language Pattern in Image Captioning", "comment": "Accepted to ICCV2025", "summary": "An image captioning model flexibly switching its language pattern, e.g.,\ndescriptiveness and length, should be useful since it can be applied to diverse\napplications. However, despite the dramatic improvement in generative\nvision-language models, fine-grained control over the properties of generated\ncaptions is not easy due to two reasons: (i) existing models are not given the\nproperties as a condition during training and (ii) existing models cannot\nsmoothly transition its language pattern from one state to the other. Given\nthis challenge, we propose a new approach, CaptionSmiths, to acquire a single\ncaptioning model that can handle diverse language patterns. First, our approach\nquantifies three properties of each caption, length, descriptiveness, and\nuniqueness of a word, as continuous scalar values, without human annotation.\nGiven the values, we represent the conditioning via interpolation between two\nendpoint vectors corresponding to the extreme states, e.g., one for a very\nshort caption and one for a very long caption. Empirical results demonstrate\nthat the resulting model can smoothly change the properties of the output\ncaptions and show higher lexical alignment than baselines. For instance,\nCaptionSmiths reduces the error in controlling caption length by 506\\% despite\nbetter lexical alignment. Code will be available on\nhttps://github.com/omron-sinicx/captionsmiths.", "AI": {"tldr": "\u63d0\u51faCaptionSmiths\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cf\u5316\u63cf\u8ff0\u957f\u5ea6\u3001\u63cf\u8ff0\u6027\u548c\u5355\u8bcd\u72ec\u7279\u6027\uff0c\u5b9e\u73b0\u56fe\u50cf\u63cf\u8ff0\u6a21\u578b\u5bf9\u8bed\u8a00\u6a21\u5f0f\u7684\u7075\u6d3b\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u7cbe\u7ec6\u63a7\u5236\u751f\u6210\u63cf\u8ff0\u7684\u5c5e\u6027\uff0c\u4e14\u65e0\u6cd5\u5e73\u6ed1\u5207\u6362\u8bed\u8a00\u6a21\u5f0f\u3002", "method": "\u91cf\u5316\u63cf\u8ff0\u5c5e\u6027\u4e3a\u8fde\u7eed\u6807\u91cf\u503c\uff0c\u901a\u8fc7\u7aef\u70b9\u5411\u91cf\u63d2\u503c\u5b9e\u73b0\u6761\u4ef6\u63a7\u5236\u3002", "result": "\u6a21\u578b\u80fd\u5e73\u6ed1\u8c03\u6574\u8f93\u51fa\u63cf\u8ff0\u5c5e\u6027\uff0c\u8bcd\u6c47\u5bf9\u9f50\u4f18\u4e8e\u57fa\u7ebf\uff0c\u957f\u5ea6\u63a7\u5236\u8bef\u5dee\u964d\u4f4e506%\u3002", "conclusion": "CaptionSmiths\u6210\u529f\u5b9e\u73b0\u5355\u6a21\u578b\u5904\u7406\u591a\u6837\u5316\u8bed\u8a00\u6a21\u5f0f\uff0c\u63d0\u5347\u63a7\u5236\u7cbe\u5ea6\u548c\u8bcd\u6c47\u5bf9\u9f50\u3002"}}
{"id": "2507.01764", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01764", "abs": "https://arxiv.org/abs/2507.01764", "authors": ["Matteo Di Cristofaro"], "title": "Data interference: emojis, homoglyphs, and issues of data fidelity in corpora and their results", "comment": "Author submitted manuscript", "summary": "Tokenisation - \"the process of splitting text into atomic parts\" (Brezina &\nTimperley, 2017: 1) - is a crucial step for corpus linguistics, as it provides\nthe basis for any applicable quantitative method (e.g. collocations) while\nensuring the reliability of qualitative approaches. This paper examines how\ndiscrepancies in tokenisation affect the representation of language data and\nthe validity of analytical findings: investigating the challenges posed by\nemojis and homoglyphs, the study highlights the necessity of preprocessing\nthese elements to maintain corpus fidelity to the source data. The research\npresents methods for ensuring that digital texts are accurately represented in\ncorpora, thereby supporting reliable linguistic analysis and guaranteeing the\nrepeatability of linguistic interpretations. The findings emphasise the\nnecessity of a detailed understanding of both linguistic and technical aspects\ninvolved in digital textual data to enhance the accuracy of corpus analysis,\nand have significant implications for both quantitative and qualitative\napproaches in corpus-based research.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5206\u8bcd\u5bf9\u8bed\u8a00\u6570\u636e\u8868\u793a\u548c\u5206\u6790\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u5f3a\u8c03\u4e86\u9884\u5904\u7406\u8868\u60c5\u7b26\u53f7\u548c\u540c\u5f62\u5f02\u4e49\u8bcd\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u786e\u4fdd\u6570\u5b57\u6587\u672c\u5728\u8bed\u6599\u5e93\u4e2d\u51c6\u786e\u8868\u793a\u7684\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u5206\u8bcd\u5dee\u5f02\u5982\u4f55\u5f71\u54cd\u8bed\u8a00\u6570\u636e\u7684\u8868\u793a\u548c\u5206\u6790\u7ed3\u679c\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u8868\u60c5\u7b26\u53f7\u548c\u540c\u5f62\u5f02\u4e49\u8bcd\u65f6\u3002", "method": "\u901a\u8fc7\u7814\u7a76\u8868\u60c5\u7b26\u53f7\u548c\u540c\u5f62\u5f02\u4e49\u8bcd\u7684\u6311\u6218\uff0c\u63d0\u51fa\u9884\u5904\u7406\u65b9\u6cd5\u4ee5\u786e\u4fdd\u6570\u5b57\u6587\u672c\u5728\u8bed\u6599\u5e93\u4e2d\u7684\u51c6\u786e\u8868\u793a\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u9884\u5904\u7406\u8868\u60c5\u7b26\u53f7\u548c\u540c\u5f62\u5f02\u4e49\u8bcd\u5bf9\u4fdd\u6301\u8bed\u6599\u5e93\u4e0e\u6e90\u6570\u636e\u7684\u5fe0\u5b9e\u6027\u81f3\u5173\u91cd\u8981\uff0c\u652f\u6301\u53ef\u9760\u7684\u8bed\u6599\u5206\u6790\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u6570\u5b57\u6587\u672c\u6570\u636e\u5904\u7406\u4e2d\u7406\u89e3\u8bed\u8a00\u548c\u6280\u672f\u7ec6\u8282\u7684\u91cd\u8981\u6027\uff0c\u5bf9\u8bed\u6599\u5e93\u7814\u7a76\u7684\u5b9a\u91cf\u548c\u5b9a\u6027\u65b9\u6cd5\u5177\u6709\u91cd\u8981\u5f71\u54cd\u3002"}}
{"id": "2507.01067", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.01067", "abs": "https://arxiv.org/abs/2507.01067", "authors": ["Keun Soo Yim"], "title": "Evaluation of a Foundational Model and Stochastic Models for Forecasting Sporadic or Spiky Production Outages of High-Performance Machine Learning Services", "comment": null, "summary": "Time series forecasting models have diverse real world applications (e.g.,\nfrom electricity metrics to software workload). Latest foundational models\ntrained for time series forecasting show strengths (e.g., for long sequences\nand in zero-shot settings). However, foundational model was not yet used for\nforecasting rare, spiky events, i.e., a challenging target because those are a\ncorner case of extreme events. In this paper, we optimize a state-of-the-art\nfoundational model to forecast sporadic or spiky production outages of\nhigh-performance machine learning services powering billions of client devices.\nWe evaluate the forecasting errors of the foundational model compared with\nclassical stochastic forecasting models (e.g., moving average and\nautoregressive). The analysis helps us understand how each of the evaluated\nmodels performs for the sporadic or spiky events. For example, it identifies\nthe key patterns in the target data that are well tracked by the foundational\nmodel vs. each of the stochastic models. We use the models with optimal\nparameters to estimate a year-long outage statistics of a particular root cause\nwith less than 6% value errors.", "AI": {"tldr": "\u672c\u6587\u4f18\u5316\u4e86\u4e00\u79cd\u6700\u5148\u8fdb\u7684\u57fa\u7840\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u9ad8\u6027\u80fd\u673a\u5668\u5b66\u4e60\u670d\u52a1\u4e2d\u7f55\u89c1\u4e14\u5c16\u5cf0\u7684\u751f\u4ea7\u4e2d\u65ad\u4e8b\u4ef6\uff0c\u5e76\u4e0e\u7ecf\u5178\u968f\u673a\u9884\u6d4b\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\uff0c\u53d1\u73b0\u57fa\u7840\u6a21\u578b\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5c1a\u672a\u7528\u4e8e\u9884\u6d4b\u7f55\u89c1\u4e14\u5c16\u5cf0\u7684\u4e8b\u4ef6\uff0c\u8fd9\u662f\u6781\u7aef\u4e8b\u4ef6\u7684\u7279\u6b8a\u60c5\u51b5\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f18\u5316\u4e00\u79cd\u6700\u5148\u8fdb\u7684\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u4e0e\u7ecf\u5178\u968f\u673a\u9884\u6d4b\u6a21\u578b\uff08\u5982\u79fb\u52a8\u5e73\u5747\u548c\u81ea\u56de\u5f52\u6a21\u578b\uff09\u8fdb\u884c\u6bd4\u8f83\uff0c\u5206\u6790\u5b83\u4eec\u5728\u9884\u6d4b\u7f55\u89c1\u5c16\u5cf0\u4e8b\u4ef6\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u57fa\u7840\u6a21\u578b\u5728\u9884\u6d4b\u7f55\u89c1\u5c16\u5cf0\u4e8b\u4ef6\u65f6\u8868\u73b0\u4f18\u4e8e\u7ecf\u5178\u968f\u673a\u6a21\u578b\uff0c\u5e76\u80fd\u51c6\u786e\u4f30\u8ba1\u7279\u5b9a\u6839\u56e0\u7684\u5e74\u4e2d\u65ad\u7edf\u8ba1\uff0c\u8bef\u5dee\u4f4e\u4e8e6%\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u5728\u9884\u6d4b\u7f55\u89c1\u4e14\u5c16\u5cf0\u4e8b\u4ef6\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u9884\u6d4b\u5de5\u5177\u3002"}}
{"id": "2507.01417", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01417", "abs": "https://arxiv.org/abs/2507.01417", "authors": ["Jiawei Gu", "Ziyue Qiao", "Zechao Li"], "title": "Gradient Short-Circuit: Efficient Out-of-Distribution Detection via Feature Intervention", "comment": "Accepted to ICCV 2025", "summary": "Out-of-Distribution (OOD) detection is critical for safely deploying deep\nmodels in open-world environments, where inputs may lie outside the training\ndistribution. During inference on a model trained exclusively with\nIn-Distribution (ID) data, we observe a salient gradient phenomenon: around an\nID sample, the local gradient directions for \"enhancing\" that sample's\npredicted class remain relatively consistent, whereas OOD samples--unseen in\ntraining--exhibit disorganized or conflicting gradient directions in the same\nneighborhood. Motivated by this observation, we propose an inference-stage\ntechnique to short-circuit those feature coordinates that spurious gradients\nexploit to inflate OOD confidence, while leaving ID classification largely\nintact. To circumvent the expense of recomputing the logits after this gradient\nshort-circuit, we further introduce a local first-order approximation that\naccurately captures the post-modification outputs without a second forward\npass. Experiments on standard OOD benchmarks show our approach yields\nsubstantial improvements. Moreover, the method is lightweight and requires\nminimal changes to the standard inference pipeline, offering a practical path\ntoward robust OOD detection in real-world applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u65b9\u5411\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u77ed\u8def\u5f02\u5e38\u68af\u5ea6\u6765\u63d0\u5347\u68c0\u6d4b\u6548\u679c\uff0c\u540c\u65f6\u4fdd\u6301ID\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u5728\u5f00\u653e\u73af\u5883\u4e2d\u90e8\u7f72\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u65f6\uff0cOOD\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\u3002\u7814\u7a76\u53d1\u73b0ID\u6837\u672c\u7684\u68af\u5ea6\u65b9\u5411\u4e00\u81f4\uff0c\u800cOOD\u6837\u672c\u7684\u68af\u5ea6\u65b9\u5411\u6df7\u4e71\uff0c\u8fd9\u542f\u53d1\u4e86\u65b0\u65b9\u6cd5\u7684\u63d0\u51fa\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u63a8\u7406\u9636\u6bb5\u7684\u6280\u672f\uff0c\u77ed\u8def\u5f02\u5e38\u68af\u5ea6\uff0c\u5e76\u901a\u8fc7\u4e00\u9636\u8fd1\u4f3c\u907f\u514d\u91cd\u590d\u8ba1\u7b97\u3002", "result": "\u5728\u6807\u51c6OOD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u663e\u8457\u63d0\u5347\uff0c\u65b9\u6cd5\u8f7b\u91cf\u4e14\u6613\u4e8e\u96c6\u6210\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684OOD\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01785", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01785", "abs": "https://arxiv.org/abs/2507.01785", "authors": ["Zhixun Chen", "Ping Guo", "Wenhan Han", "Yifan Zhang", "Binbin Liu", "Haobin Lin", "Fengze Liu", "Yan Zhao", "Bingni Zhang", "Taifeng Wang", "Yin Zheng", "Meng Fang"], "title": "MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining", "comment": null, "summary": "Data quality is a critical driver of large language model performance, yet\nexisting model-based selection methods focus almost exclusively on English. We\nintroduce MuRating, a scalable framework that transfers high-quality English\ndata-quality signals into a single rater for 17 target languages. MuRating\naggregates multiple English \"raters\" via pairwise comparisons to learn unified\ndocument-quality scores,then projects these judgments through translation to\ntrain a multilingual evaluator on monolingual, cross-lingual, and parallel text\npairs. Applied to web data, MuRating selects balanced subsets of English and\nmultilingual content to pretrain a 1.2 B-parameter LLaMA model. Compared to\nstrong baselines, including QuRater, AskLLM, DCLM and so on, our approach\nboosts average accuracy on both English benchmarks and multilingual\nevaluations, with especially large gains on knowledge-intensive tasks. We\nfurther analyze translation fidelity, selection biases, and underrepresentation\nof narrative material, outlining directions for future work.", "AI": {"tldr": "MuRating\u662f\u4e00\u4e2a\u591a\u8bed\u8a00\u6570\u636e\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u82f1\u8bed\u6570\u636e\u8d28\u91cf\u4fe1\u53f7\u8f6c\u79fb\u523017\u79cd\u76ee\u6807\u8bed\u8a00\uff0c\u63d0\u5347\u591a\u8bed\u8a00\u5927\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u82f1\u8bed\uff0c\u7f3a\u4e4f\u591a\u8bed\u8a00\u652f\u6301\uff0c\u9650\u5236\u4e86\u975e\u82f1\u8bed\u6570\u636e\u7684\u6a21\u578b\u8868\u73b0\u3002", "method": "MuRating\u901a\u8fc7\u805a\u5408\u591a\u4e2a\u82f1\u8bed\u8bc4\u5206\u5668\u7684\u4fe1\u53f7\uff0c\u5b66\u4e60\u7edf\u4e00\u6587\u6863\u8d28\u91cf\u8bc4\u5206\uff0c\u5e76\u901a\u8fc7\u7ffb\u8bd1\u5c06\u8fd9\u4e9b\u8bc4\u5206\u6295\u5c04\u5230\u76ee\u6807\u8bed\u8a00\uff0c\u8bad\u7ec3\u591a\u8bed\u8a00\u8bc4\u4f30\u5668\u3002", "result": "MuRating\u5728\u82f1\u8bed\u548c\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "MuRating\u4e3a\u591a\u8bed\u8a00\u6570\u636e\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u7ffb\u8bd1\u4fdd\u771f\u5ea6\u548c\u53d9\u4e8b\u6750\u6599\u4ee3\u8868\u6027\u7b49\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2507.01068", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01068", "abs": "https://arxiv.org/abs/2507.01068", "authors": ["Biplov Paneru"], "title": "Prediction of Freezing of Gait in Parkinsons Disease using Explainable AI and Federated Deep Learning for Wearable Sensors", "comment": null, "summary": "This study leverages an Inertial Measurement Unit (IMU) dataset to develop\nexplainable AI methods for the early detection and prediction of Freezing of\nGait (FOG), a common symptom in Parkinson's disease. Machine learning models,\nincluding CatBoost, XGBoost, and Extra Trees classifiers, are employed to\naccurately categorize FOG episodes based on relevant clinical features. A\nStacking Ensemble model achieves superior performance, surpassing a hybrid\nbidirectional GRU model and reaching nearly 99% classification accuracy. SHAP\ninterpretability analysis reveals that time (seconds) is the most influential\nfactor in distinguishing gait patterns. Additionally, the proposed FOG\nprediction framework incorporates federated learning, where models are trained\nlocally on individual devices and aggregated on a central server using a\nfederated averaging approach, utilizing a hybrid Conv1D + LSTM architecture for\nenhanced predictive capability.", "AI": {"tldr": "\u5229\u7528IMU\u6570\u636e\u548c\u53ef\u89e3\u91caAI\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86\u65e9\u671f\u68c0\u6d4b\u548c\u9884\u6d4b\u5e15\u91d1\u68ee\u75c5\u51bb\u7ed3\u6b65\u6001\uff08FOG\uff09\u7684\u6a21\u578b\uff0c\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u8fbe99%\u3002", "motivation": "\u51bb\u7ed3\u6b65\u6001\uff08FOG\uff09\u662f\u5e15\u91d1\u68ee\u75c5\u7684\u5e38\u89c1\u75c7\u72b6\uff0c\u65e9\u671f\u68c0\u6d4b\u548c\u9884\u6d4b\u5bf9\u60a3\u8005\u7ba1\u7406\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528CatBoost\u3001XGBoost\u548cExtra Trees\u5206\u7c7b\u5668\uff0c\u7ed3\u5408Stacking\u96c6\u6210\u6a21\u578b\uff0c\u5e76\u5f15\u5165SHAP\u5206\u6790\u89e3\u91ca\u6a21\u578b\u3002\u6b64\u5916\uff0c\u7ed3\u5408\u8054\u90a6\u5b66\u4e60\u63d0\u5347\u9884\u6d4b\u80fd\u529b\u3002", "result": "Stacking\u96c6\u6210\u6a21\u578b\u8868\u73b0\u6700\u4f18\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u8fbe99%\uff0cSHAP\u5206\u6790\u663e\u793a\u65f6\u95f4\u662f\u533a\u5206\u6b65\u6001\u6a21\u5f0f\u7684\u6700\u91cd\u8981\u56e0\u7d20\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u5728FOG\u68c0\u6d4b\u548c\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7ed3\u5408\u8054\u90a6\u5b66\u4e60\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u5b9e\u7528\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u3002"}}
{"id": "2507.01422", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01422", "abs": "https://arxiv.org/abs/2507.01422", "authors": ["Wenjie Liu", "Bingshu Wang", "Ze Wang", "C. L. Philip Chen"], "title": "DocShaDiffusion: Diffusion Model in Latent Space for Document Image Shadow Removal", "comment": null, "summary": "Document shadow removal is a crucial task in the field of document image\nenhancement. However, existing methods tend to remove shadows with constant\ncolor background and ignore color shadows. In this paper, we first design a\ndiffusion model in latent space for document image shadow removal, called\nDocShaDiffusion. It translates shadow images from pixel space to latent space,\nenabling the model to more easily capture essential features. To address the\nissue of color shadows, we design a shadow soft-mask generation module (SSGM).\nIt is able to produce accurate shadow mask and add noise into shadow regions\nspecially. Guided by the shadow mask, a shadow mask-aware guided diffusion\nmodule (SMGDM) is proposed to remove shadows from document images by\nsupervising the diffusion and denoising process. We also propose a\nshadow-robust perceptual feature loss to preserve details and structures in\ndocument images. Moreover, we develop a large-scale synthetic document color\nshadow removal dataset (SDCSRD). It simulates the distribution of realistic\ncolor shadows and provides powerful supports for the training of models.\nExperiments on three public datasets validate the proposed method's superiority\nover state-of-the-art. Our code and dataset will be publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDocShaDiffusion\u7684\u6f5c\u5728\u7a7a\u95f4\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u6587\u6863\u56fe\u50cf\u9634\u5f71\u53bb\u9664\uff0c\u7ed3\u5408\u9634\u5f71\u8f6f\u63a9\u6a21\u751f\u6210\u6a21\u5757\uff08SSGM\uff09\u548c\u9634\u5f71\u63a9\u6a21\u5f15\u5bfc\u6269\u6563\u6a21\u5757\uff08SMGDM\uff09\uff0c\u5e76\u5f00\u53d1\u4e86\u5408\u6210\u6570\u636e\u96c6SDCSRD\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u5f69\u8272\u9634\u5f71\u4e14\u80cc\u666f\u989c\u8272\u5355\u4e00\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u9634\u5f71\u53bb\u9664\u6280\u672f\u3002", "method": "\u8bbe\u8ba1\u6f5c\u5728\u7a7a\u95f4\u6269\u6563\u6a21\u578bDocShaDiffusion\uff0c\u7ed3\u5408SSGM\u751f\u6210\u9634\u5f71\u63a9\u6a21\uff0cSMGDM\u5f15\u5bfc\u53bb\u9634\u5f71\u8fc7\u7a0b\uff0c\u5e76\u4f7f\u7528\u9634\u5f71\u9c81\u68d2\u611f\u77e5\u7279\u5f81\u635f\u5931\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "DocShaDiffusion\u5728\u6587\u6863\u56fe\u50cf\u9634\u5f71\u53bb\u9664\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u516c\u5f00\u3002"}}
{"id": "2507.01786", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01786", "abs": "https://arxiv.org/abs/2507.01786", "authors": ["Jord Nguyen", "Khiem Hoang", "Carlo Leonardo Attubato", "Felix Hofst\u00e4tter"], "title": "Probing Evaluation Awareness of Language Models", "comment": "Technical AI Governance Workshop, ICML (Poster)", "summary": "Language models can distinguish between testing and deployment phases -- a\ncapability known as evaluation awareness. This has significant safety and\npolicy implications, potentially undermining the reliability of evaluations\nthat are central to AI governance frameworks and voluntary industry\ncommitments. In this paper, we study evaluation awareness in\nLlama-3.3-70B-Instruct. We show that linear probes can separate real-world\nevaluation and deployment prompts, suggesting that current models internally\nrepresent this distinction. We also find that current safety evaluations are\ncorrectly classified by the probes, suggesting that they already appear\nartificial or inauthentic to models. Our findings underscore the importance of\nensuring trustworthy evaluations and understanding deceptive capabilities. More\nbroadly, our work showcases how model internals may be leveraged to support\nblackbox methods in safety audits, especially for future models more competent\nat evaluation awareness and deception.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u80fd\u533a\u5206\u6d4b\u8bd5\u4e0e\u90e8\u7f72\u9636\u6bb5\uff0c\u79f0\u4e3a\u8bc4\u4f30\u610f\u8bc6\uff0c\u8fd9\u5bf9AI\u5b89\u5168\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u6784\u6210\u6311\u6218\u3002", "motivation": "\u63a2\u8ba8\u8bc4\u4f30\u610f\u8bc6\u5bf9AI\u6cbb\u7406\u6846\u67b6\u548c\u884c\u4e1a\u81ea\u613f\u627f\u8bfa\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u7684\u6f5c\u5728\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u7ebf\u6027\u63a2\u9488\u5206\u6790Llama-3.3-70B-Instruct\u6a21\u578b\uff0c\u533a\u5206\u8bc4\u4f30\u4e0e\u90e8\u7f72\u63d0\u793a\u3002", "result": "\u7ebf\u6027\u63a2\u9488\u80fd\u6709\u6548\u5206\u79bb\u8bc4\u4f30\u4e0e\u90e8\u7f72\u63d0\u793a\uff0c\u4e14\u5f53\u524d\u5b89\u5168\u8bc4\u4f30\u88ab\u6a21\u578b\u89c6\u4e3a\u4e0d\u771f\u5b9e\u3002", "conclusion": "\u9700\u786e\u4fdd\u8bc4\u4f30\u7684\u53ef\u4fe1\u5ea6\uff0c\u7406\u89e3\u6a21\u578b\u7684\u6b3a\u9a97\u80fd\u529b\uff0c\u5e76\u5229\u7528\u6a21\u578b\u5185\u90e8\u4fe1\u606f\u652f\u6301\u5b89\u5168\u5ba1\u8ba1\u3002"}}
{"id": "2507.01073", "categories": ["cs.LG", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2507.01073", "abs": "https://arxiv.org/abs/2507.01073", "authors": ["Dian Jin"], "title": "Rotational Sampling: A Plug-and-Play Encoder for Rotation-Invariant 3D Molecular GNNs", "comment": null, "summary": "Graph neural networks (GNNs) have achieved remarkable success in molecular\nproperty prediction. However, traditional graph representations struggle to\neffectively encode the inherent 3D spatial structures of molecules, as\nmolecular orientations in 3D space introduce significant variability, severely\nlimiting model generalization and robustness. Existing approaches primarily\nfocus on rotation-invariant and rotation-equivariant methods. Invariant methods\noften rely heavily on prior knowledge and lack sufficient generalizability,\nwhile equivariant methods suffer from high computational costs. To address\nthese limitations, this paper proposes a novel plug-and-play 3D encoding module\nleveraging rotational sampling. By computing the expectation over the SO(3)\nrotational group, the method naturally achieves approximate rotational\ninvariance. Furthermore, by introducing a carefully designed post-alignment\nstrategy, strict invariance can be achieved without compromising performance.\nExperimental evaluations on the QM9 and C10 Datasets demonstrate superior\npredictive accuracy, robustness, and generalization performance compared to\nexisting methods. Moreover, the proposed approach maintains low computational\ncomplexity and enhanced interpretability, providing a promising direction for\nefficient and effective handling of 3D molecular information in drug discovery\nand material design.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u76843D\u7f16\u7801\u6a21\u5757\uff0c\u901a\u8fc7\u65cb\u8f6c\u91c7\u6837\u548cSO(3)\u65cb\u8f6c\u7fa4\u8ba1\u7b97\u5b9e\u73b0\u8fd1\u4f3c\u65cb\u8f6c\u4e0d\u53d8\u6027\uff0c\u5e76\u901a\u8fc7\u540e\u5bf9\u9f50\u7b56\u7565\u5b9e\u73b0\u4e25\u683c\u4e0d\u53d8\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5b50\u5c5e\u6027\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u56fe\u8868\u793a\u96be\u4ee5\u6709\u6548\u7f16\u7801\u5206\u5b50\u76843D\u7a7a\u95f4\u7ed3\u6784\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u548c\u8ba1\u7b97\u6210\u672c\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65cb\u8f6c\u91c7\u6837\u76843D\u7f16\u7801\u6a21\u5757\uff0c\u7ed3\u5408SO(3)\u65cb\u8f6c\u7fa4\u8ba1\u7b97\u548c\u540e\u5bf9\u9f50\u7b56\u7565\u3002", "result": "\u5728QM9\u548cC10\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u836f\u7269\u53d1\u73b0\u548c\u6750\u6599\u8bbe\u8ba1\u4e2d\u76843D\u5206\u5b50\u4fe1\u606f\u5904\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01428", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.01428", "abs": "https://arxiv.org/abs/2507.01428", "authors": ["Chen Sun", "Haiyang Sun", "Zhiqing Guo", "Yunfeng Diao", "Liejun Wang", "Dan Ma", "Gaobo Yang", "Keqin Li"], "title": "DiffMark: Diffusion-based Robust Watermark Against Deepfakes", "comment": null, "summary": "Deepfakes pose significant security and privacy threats through malicious\nfacial manipulations. While robust watermarking can aid in authenticity\nverification and source tracking, existing methods often lack the sufficient\nrobustness against Deepfake manipulations. Diffusion models have demonstrated\nremarkable performance in image generation, enabling the seamless fusion of\nwatermark with image during generation. In this study, we propose a novel\nrobust watermarking framework based on diffusion model, called DiffMark. By\nmodifying the training and sampling scheme, we take the facial image and\nwatermark as conditions to guide the diffusion model to progressively denoise\nand generate corresponding watermarked image. In the construction of facial\ncondition, we weight the facial image by a timestep-dependent factor that\ngradually reduces the guidance intensity with the decrease of noise, thus\nbetter adapting to the sampling process of diffusion model. To achieve the\nfusion of watermark condition, we introduce a cross information fusion (CIF)\nmodule that leverages a learnable embedding table to adaptively extract\nwatermark features and integrates them with image features via cross-attention.\nTo enhance the robustness of the watermark against Deepfake manipulations, we\nintegrate a frozen autoencoder during training phase to simulate Deepfake\nmanipulations. Additionally, we introduce Deepfake-resistant guidance that\nemploys specific Deepfake model to adversarially guide the diffusion sampling\nprocess to generate more robust watermarked images. Experimental results\ndemonstrate the effectiveness of the proposed DiffMark on typical Deepfakes.\nOur code will be available at https://github.com/vpsg-research/DiffMark.", "AI": {"tldr": "DiffMark\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u9c81\u68d2\u6c34\u5370\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u8bad\u7ec3\u548c\u91c7\u6837\u65b9\u6848\uff0c\u7ed3\u5408\u9762\u90e8\u56fe\u50cf\u548c\u6c34\u5370\u6761\u4ef6\u751f\u6210\u6c34\u5370\u56fe\u50cf\uff0c\u5bf9\u6297Deepfake\u64cd\u4f5c\u3002", "motivation": "Deepfake\u6280\u672f\u5bf9\u5b89\u5168\u548c\u9690\u79c1\u6784\u6210\u5a01\u80c1\uff0c\u73b0\u6709\u6c34\u5370\u65b9\u6cd5\u5728\u5bf9\u6297Deepfake\u64cd\u4f5c\u65f6\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51faDiffMark\u6846\u67b6\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u6c34\u5370\u56fe\u50cf\uff0c\u5f15\u5165\u65f6\u95f4\u6b65\u4f9d\u8d56\u7684\u9762\u90e8\u6761\u4ef6\u6743\u91cd\u548c\u8de8\u4fe1\u606f\u878d\u5408\u6a21\u5757\uff08CIF\uff09\uff0c\u5e76\u96c6\u6210\u51bb\u7ed3\u81ea\u52a8\u7f16\u7801\u5668\u6a21\u62dfDeepfake\u64cd\u4f5c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eDiffMark\u5728\u5178\u578bDeepfake\u64cd\u4f5c\u4e2d\u8868\u73b0\u6709\u6548\u3002", "conclusion": "DiffMark\u901a\u8fc7\u6269\u6563\u6a21\u578b\u548c\u5bf9\u6297\u6027\u6307\u5bfc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c34\u5370\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.01790", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01790", "abs": "https://arxiv.org/abs/2507.01790", "authors": ["Tianze Hua", "Tian Yun", "Ellie Pavlick"], "title": "How Do Vision-Language Models Process Conflicting Information Across Modalities?", "comment": "All code and resources are available at:\n  https://github.com/ethahtz/vlm_conflicting_info_processing", "summary": "AI models are increasingly required to be multimodal, integrating disparate\ninput streams into a coherent state representation on which subsequent\nbehaviors and actions can be based. This paper seeks to understand how such\nmodels behave when input streams present conflicting information. Focusing\nspecifically on vision-language models, we provide inconsistent inputs (e.g.,\nan image of a dog paired with the caption \"A photo of a cat\") and ask the model\nto report the information present in one of the specific modalities (e.g.,\n\"What does the caption say / What is in the image?\"). We find that models often\nfavor one modality over the other, e.g., reporting the image regardless of what\nthe caption says, but that different models differ in which modality they\nfavor. We find evidence that the behaviorally preferred modality is evident in\nthe internal representational structure of the model, and that specific\nattention heads can restructure the representations to favor one modality over\nthe other. Moreover, we find modality-agnostic \"router heads\" which appear to\npromote answers about the modality requested in the instruction, and which can\nbe manipulated or transferred in order to improve performance across datasets\nand modalities. Together, the work provides essential steps towards identifying\nand controlling if and how models detect and resolve conflicting signals within\ncomplex multimodal environments.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u591a\u6a21\u6001AI\u6a21\u578b\u5728\u8f93\u5165\u4fe1\u606f\u51b2\u7a81\u65f6\u7684\u884c\u4e3a\uff0c\u53d1\u73b0\u6a21\u578b\u503e\u5411\u4e8e\u4f18\u5148\u5904\u7406\u67d0\u4e00\u6a21\u6001\uff0c\u5e76\u63ed\u793a\u4e86\u5185\u90e8\u6ce8\u610f\u529b\u673a\u5236\u7684\u4f5c\u7528\u3002", "motivation": "\u7406\u89e3\u591a\u6a21\u6001\u6a21\u578b\u5728\u8f93\u5165\u4fe1\u606f\u51b2\u7a81\u65f6\u7684\u884c\u4e3a\uff0c\u63a2\u7d22\u5176\u5185\u90e8\u8868\u5f81\u7ed3\u6784\u548c\u6ce8\u610f\u529b\u673a\u5236\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5411\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e0d\u4e00\u81f4\u7684\u8f93\u5165\uff08\u5982\u56fe\u50cf\u4e0e\u6807\u9898\u77db\u76fe\uff09\uff0c\u89c2\u5bdf\u6a21\u578b\u5bf9\u4e0d\u540c\u6a21\u6001\u7684\u504f\u597d\u53ca\u5176\u5185\u90e8\u673a\u5236\u3002", "result": "\u6a21\u578b\u503e\u5411\u4e8e\u4f18\u5148\u5904\u7406\u67d0\u4e00\u6a21\u6001\uff0c\u4e14\u5185\u90e8\u6ce8\u610f\u529b\u5934\u53ef\u8c03\u6574\u8868\u5f81\u504f\u597d\uff1b\u53d1\u73b0\u6a21\u6001\u65e0\u5173\u7684\u201c\u8def\u7531\u5934\u201d\u53ef\u4f18\u5316\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8bc6\u522b\u548c\u63a7\u5236\u591a\u6a21\u6001\u6a21\u578b\u5728\u51b2\u7a81\u4fe1\u53f7\u4e2d\u7684\u884c\u4e3a\u63d0\u4f9b\u4e86\u5173\u952e\u6b65\u9aa4\u3002"}}
{"id": "2507.01075", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.01075", "abs": "https://arxiv.org/abs/2507.01075", "authors": ["Gabriele Padovani", "Valentine Anantharaj", "Sandro Fiore"], "title": "Provenance Tracking in Large-Scale Machine Learning Systems", "comment": null, "summary": "As the demand for large scale AI models continues to grow, the optimization\nof their training to balance computational efficiency, execution time, accuracy\nand energy consumption represents a critical multidimensional challenge.\nAchieving this balance requires not only innovative algorithmic techniques and\nhardware architectures but also comprehensive tools for monitoring, analyzing,\nand understanding the underlying processes involved in model training and\ndeployment. Provenance data information about the origins, context, and\ntransformations of data and processes has become a key component in this\npursuit. By leveraging provenance, researchers and engineers can gain insights\ninto resource usage patterns, identify inefficiencies, and ensure\nreproducibility and accountability in AI development workflows. For this\nreason, the question of how distributed resources can be optimally utilized to\nscale large AI models in an energy efficient manner is a fundamental one. To\nsupport this effort, we introduce the yProv4ML library, a tool designed to\ncollect provenance data in JSON format, compliant with the W3C PROV and ProvML\nstandards. yProv4ML focuses on flexibility and extensibility, and enables users\nto integrate additional data collection tools via plugins. The library is fully\nintegrated with the yProv framework, allowing for higher level pairing in tasks\nrun also through workflow management systems.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86yProv4ML\u5e93\uff0c\u7528\u4e8e\u6536\u96c6\u7b26\u5408W3C PROV\u548cProvML\u6807\u51c6\u7684JSON\u683c\u5f0f\u7684\u6eaf\u6e90\u6570\u636e\uff0c\u4ee5\u4f18\u5316\u5927\u89c4\u6a21AI\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u7387\u548c\u80fd\u6e90\u6d88\u8017\u3002", "motivation": "\u968f\u7740\u5927\u89c4\u6a21AI\u6a21\u578b\u9700\u6c42\u7684\u589e\u957f\uff0c\u5982\u4f55\u5728\u8ba1\u7b97\u6548\u7387\u3001\u6267\u884c\u65f6\u95f4\u3001\u51c6\u786e\u6027\u548c\u80fd\u6e90\u6d88\u8017\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002\u6eaf\u6e90\u6570\u636e\u6210\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u7684\u6838\u5fc3\u3002", "method": "\u5f00\u53d1\u4e86yProv4ML\u5e93\uff0c\u652f\u6301\u7075\u6d3b\u6269\u5c55\uff0c\u53ef\u901a\u8fc7\u63d2\u4ef6\u96c6\u6210\u5176\u4ed6\u6570\u636e\u6536\u96c6\u5de5\u5177\uff0c\u5e76\u4e0eyProv\u6846\u67b6\u5b8c\u5168\u96c6\u6210\u3002", "result": "yProv4ML\u5e93\u80fd\u591f\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u548c\u5de5\u7a0b\u5e08\u5206\u6790\u8d44\u6e90\u4f7f\u7528\u6a21\u5f0f\uff0c\u8bc6\u522b\u4f4e\u6548\u73af\u8282\uff0c\u786e\u4fddAI\u5f00\u53d1\u6d41\u7a0b\u7684\u53ef\u91cd\u73b0\u6027\u548c\u8d23\u4efb\u6027\u3002", "conclusion": "yProv4ML\u5e93\u4e3a\u4f18\u5316\u5927\u89c4\u6a21AI\u6a21\u578b\u7684\u5206\u5e03\u5f0f\u8d44\u6e90\u5229\u7528\u548c\u80fd\u6e90\u6548\u7387\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2507.01439", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01439", "abs": "https://arxiv.org/abs/2507.01439", "authors": ["Shaocheng Yan", "Pengcheng Shi", "Zhenjun Zhao", "Kaixin Wang", "Kuang Cao", "Ji Wu", "Jiayuan Li"], "title": "TurboReg: TurboClique for Robust and Efficient Point Cloud Registration", "comment": "ICCV-2025 Accepted Paper", "summary": "Robust estimation is essential in correspondence-based Point Cloud\nRegistration (PCR). Existing methods using maximal clique search in\ncompatibility graphs achieve high recall but suffer from exponential time\ncomplexity, limiting their use in time-sensitive applications. To address this\nchallenge, we propose a fast and robust estimator, TurboReg, built upon a novel\nlightweight clique, TurboClique, and a highly parallelizable Pivot-Guided\nSearch (PGS) algorithm. First, we define the TurboClique as a 3-clique within a\nhighly-constrained compatibility graph. The lightweight nature of the 3-clique\nallows for efficient parallel searching, and the highly-constrained\ncompatibility graph ensures robust spatial consistency for stable\ntransformation estimation. Next, PGS selects matching pairs with high SC$^2$\nscores as pivots, effectively guiding the search toward TurboCliques with\nhigher inlier ratios. Moreover, the PGS algorithm has linear time complexity\nand is significantly more efficient than the maximal clique search with\nexponential time complexity. Extensive experiments show that TurboReg achieves\nstate-of-the-art performance across multiple real-world datasets, with\nsubstantial speed improvements. For example, on the 3DMatch+FCGF dataset,\nTurboReg (1K) operates $208.22\\times$ faster than 3DMAC while also achieving\nhigher recall. Our code is accessible at\n\\href{https://github.com/Laka-3DV/TurboReg}{\\texttt{TurboReg}}.", "AI": {"tldr": "TurboReg\u662f\u4e00\u79cd\u5feb\u901f\u9c81\u68d2\u7684PCR\u4f30\u8ba1\u5668\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7TurboClique\u548c\u5e76\u884cPGS\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u901f\u5ea6\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6700\u5927\u56e2\u641c\u7d22\u7684\u65b9\u6cd5\u867d\u53ec\u56de\u7387\u9ad8\uff0c\u4f46\u65f6\u95f4\u590d\u6742\u5ea6\u6307\u6570\u7ea7\uff0c\u4e0d\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\u3002", "method": "\u63d0\u51faTurboClique\uff083-\u56e2\uff09\u548cPGS\u7b97\u6cd5\uff0c\u5229\u7528\u9ad8SC$^2$\u5206\u6570\u5339\u914d\u5bf9\u5f15\u5bfc\u641c\u7d22\uff0c\u5b9e\u73b0\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u901f\u5ea6\u63d0\u5347\u663e\u8457\uff08\u59823DMatch+FCGF\u4e0a\u5feb208.22\u500d\uff09\u3002", "conclusion": "TurboReg\u5728\u4fdd\u6301\u9ad8\u53ec\u56de\u7387\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6PCR\u4efb\u52a1\u3002"}}
{"id": "2507.01802", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01802", "abs": "https://arxiv.org/abs/2507.01802", "authors": ["Katharina Beckh", "Elisa Studeny", "Sujan Sai Gannamaneni", "Dario Antweiler", "Stefan R\u00fcping"], "title": "The Anatomy of Evidence: An Investigation Into Explainable ICD Coding", "comment": "Accepted to ACL 2025 Findings", "summary": "Automatic medical coding has the potential to ease documentation and billing\nprocesses. For this task, transparency plays an important role for medical\ncoders and regulatory bodies, which can be achieved using explainability\nmethods. However, the evaluation of these approaches has been mostly limited to\nshort text and binary settings due to a scarcity of annotated data. Recent\nefforts by Cheng et al. (2023) have introduced the MDACE dataset, which\nprovides a valuable resource containing code evidence in clinical records. In\nthis work, we conduct an in-depth analysis of the MDACE dataset and perform\nplausibility evaluation of current explainable medical coding systems from an\napplied perspective. With this, we contribute to a deeper understanding of\nautomatic medical coding and evidence extraction. Our findings reveal that\nground truth evidence aligns with code descriptions to a certain degree. An\ninvestigation into state-of-the-art approaches shows a high overlap with ground\ntruth evidence. We propose match measures and highlight success and failure\ncases. Based on our findings, we provide recommendations for developing and\nevaluating explainable medical coding systems.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86MDACE\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u5f53\u524d\u53ef\u89e3\u91ca\u7684\u533b\u7597\u7f16\u7801\u7cfb\u7edf\uff0c\u53d1\u73b0\u771f\u5b9e\u8bc1\u636e\u4e0e\u4ee3\u7801\u63cf\u8ff0\u90e8\u5206\u4e00\u81f4\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "\u81ea\u52a8\u533b\u7597\u7f16\u7801\u53ef\u7b80\u5316\u6587\u6863\u548c\u8ba1\u8d39\u6d41\u7a0b\uff0c\u4f46\u9700\u900f\u660e\u6027\uff0c\u73b0\u6709\u8bc4\u4f30\u56e0\u6570\u636e\u7a00\u7f3a\u53d7\u9650\u3002", "method": "\u6df1\u5165\u5206\u6790MDACE\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u73b0\u6709\u53ef\u89e3\u91ca\u533b\u7597\u7f16\u7801\u7cfb\u7edf\u7684\u5408\u7406\u6027\u3002", "result": "\u771f\u5b9e\u8bc1\u636e\u4e0e\u4ee3\u7801\u63cf\u8ff0\u90e8\u5206\u4e00\u81f4\uff0c\u5148\u8fdb\u65b9\u6cd5\u4e0e\u771f\u5b9e\u8bc1\u636e\u9ad8\u5ea6\u91cd\u53e0\u3002", "conclusion": "\u63d0\u51fa\u5339\u914d\u5ea6\u91cf\uff0c\u603b\u7ed3\u6210\u529f\u4e0e\u5931\u8d25\u6848\u4f8b\uff0c\u4e3a\u5f00\u53d1\u53ef\u89e3\u91ca\u533b\u7597\u7f16\u7801\u7cfb\u7edf\u63d0\u4f9b\u5efa\u8bae\u3002"}}
{"id": "2507.01196", "categories": ["cs.LG", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.01196", "abs": "https://arxiv.org/abs/2507.01196", "authors": ["Na Lee", "Konstantinos Barmpas", "Yannis Panagakis", "Dimitrios Adamos", "Nikolaos Laskaris", "Stefanos Zafeiriou"], "title": "Are Large Brainwave Foundation Models Capable Yet? Insights from Fine-tuning", "comment": null, "summary": "Foundation Models have demonstrated significant success across various\ndomains in Artificial Intelligence (AI), yet their capabilities for brainwave\nmodeling remain unclear. In this paper, we comprehensively evaluate current\nLarge Brainwave Foundation Models (LBMs) through systematic fine-tuning\nexperiments across multiple Brain-Computer Interface (BCI) benchmark tasks,\nincluding memory tasks and sleep stage classification. Our extensive analysis\nshows that state-of-the-art LBMs achieve only marginal improvements (0.9%-1.2%)\nover traditional deep architectures while requiring significantly more\nparameters (millions vs thousands), raising important questions about their\nefficiency and applicability in BCI contexts. Moreover, through detailed\nablation studies and Low-Rank Adaptation (LoRA), we significantly reduce\ntrainable parameters without performance degradation, while demonstrating that\narchitectural and training inefficiencies limit LBMs' current capabilities. Our\nexperiments span both full model fine-tuning and parameter-efficient adaptation\ntechniques, providing insights into optimal training strategies for BCI\napplications. We pioneer the application of LoRA to LBMs, revealing that\nperformance benefits generally emerge when adapting multiple neural network\ncomponents simultaneously. These findings highlight the critical need for\ndomain-specific development strategies to advance LBMs, suggesting that current\narchitectures may require redesign to fully leverage the potential of\nfoundation models in brainwave analysis.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u5927\u578b\u8111\u6ce2\u57fa\u7840\u6a21\u578b\uff08LBMs\uff09\u5728\u8111\u673a\u63a5\u53e3\uff08BCI\uff09\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u6027\u80fd\u63d0\u5347\u6709\u9650\u4e14\u53c2\u6570\u9700\u6c42\u9ad8\uff0c\u63d0\u51fa\u4e86\u53c2\u6570\u9ad8\u6548\u9002\u5e94\u6280\u672f\uff08\u5982LoRA\uff09\u7684\u4f18\u5316\u7b56\u7565\u3002", "motivation": "\u63a2\u7d22\u57fa\u7840\u6a21\u578b\u5728\u8111\u6ce2\u5efa\u6a21\u4e2d\u7684\u6f5c\u529b\uff0c\u8bc4\u4f30\u5176\u5728BCI\u4efb\u52a1\u4e2d\u7684\u6548\u7387\u548c\u9002\u7528\u6027\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5fae\u8c03\u5b9e\u9a8c\u548cLoRA\u6280\u672f\uff0c\u5bf9LBMs\u5728\u591a\u4e2aBCI\u4efb\u52a1\uff08\u5982\u8bb0\u5fc6\u4efb\u52a1\u548c\u7761\u7720\u9636\u6bb5\u5206\u7c7b\uff09\u4e2d\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "LBMs\u4ec5\u6bd4\u4f20\u7edf\u67b6\u6784\u63d0\u53470.9%-1.2%\uff0c\u4f46\u53c2\u6570\u9700\u6c42\u663e\u8457\u589e\u52a0\uff1bLoRA\u53ef\u51cf\u5c11\u53c2\u6570\u800c\u4e0d\u964d\u4f4e\u6027\u80fd\u3002", "conclusion": "\u5f53\u524dLBMs\u5728\u8111\u6ce2\u5206\u6790\u4e2d\u6548\u7387\u4e0d\u8db3\uff0c\u9700\u9886\u57df\u7279\u5b9a\u4f18\u5316\u548c\u67b6\u6784\u91cd\u65b0\u8bbe\u8ba1\u3002"}}
{"id": "2507.01077", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01077", "abs": "https://arxiv.org/abs/2507.01077", "authors": ["Bogdan Bogdan", "Arina Cazacu", "Laura Vasilie"], "title": "Good Enough to Learn: LLM-based Anomaly Detection in ECU Logs without Reliable Labels", "comment": "6 pages, 7 figures, 4 tables, accepted to IEEE Intelligent Vehicles\n  Symposium (IV) 2025", "summary": "Anomaly detection often relies on supervised or clustering approaches, with\nlimited success in specialized domains like automotive communication systems\nwhere scalable solutions are essential. We propose a novel decoder-only Large\nLanguage Model (LLM) to detect anomalies in Electronic Control Unit (ECU)\ncommunication logs. Our approach addresses two key challenges: the lack of LLMs\ntailored for ECU communication and the complexity of inconsistent ground truth\ndata. By learning from UDP communication logs, we formulate anomaly detection\nsimply as identifying deviations in time from normal behavior. We introduce an\nentropy regularization technique that increases model's uncertainty in known\nanomalies while maintaining consistency in similar scenarios. Our solution\noffers three novelties: a decoder-only anomaly detection architecture, a way to\nhandle inconsistent labeling, and an adaptable LLM for different ECU\ncommunication use cases. By leveraging the generative capabilities of\ndecoder-only models, we present a new technique that addresses the high cost\nand error-prone nature of manual labeling through a more scalable system that\nis able to learn from a minimal set of examples, while improving detection\naccuracy in complex communication environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89e3\u7801\u5668\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7528\u4e8e\u68c0\u6d4b\u7535\u5b50\u63a7\u5236\u5355\u5143\uff08ECU\uff09\u901a\u4fe1\u65e5\u5fd7\u4e2d\u7684\u5f02\u5e38\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7279\u5b9a\u9886\u57df\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u6c7d\u8f66\u901a\u4fe1\u7cfb\u7edf\u7b49\u4e13\u4e1a\u9886\u57df\u4e2d\u6548\u679c\u6709\u9650\uff0c\u4e14\u7f3a\u4e4f\u9488\u5bf9ECU\u901a\u4fe1\u7684LLM\u6a21\u578b\u548c\u4e0d\u4e00\u81f4\u6807\u6ce8\u6570\u636e\u7684\u590d\u6742\u6027\u3002", "method": "\u4f7f\u7528\u89e3\u7801\u5668LLM\u4eceUDP\u901a\u4fe1\u65e5\u5fd7\u4e2d\u5b66\u4e60\uff0c\u901a\u8fc7\u65f6\u95f4\u504f\u5dee\u8bc6\u522b\u5f02\u5e38\uff0c\u5e76\u5f15\u5165\u71b5\u6b63\u5219\u5316\u6280\u672f\u63d0\u9ad8\u6a21\u578b\u5bf9\u5df2\u77e5\u5f02\u5e38\u7684\u8bc6\u522b\u80fd\u529b\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u7801\u5668\u5f02\u5e38\u68c0\u6d4b\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u6807\u6ce8\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u9002\u7528\u4e8e\u4e0d\u540cECU\u901a\u4fe1\u7528\u4f8b\u7684LLM\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u80fd\u529b\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u6210\u672c\uff0c\u63d0\u9ad8\u4e86\u590d\u6742\u901a\u4fe1\u73af\u5883\u4e2d\u7684\u68c0\u6d4b\u51c6\u786e\u6027\u3002"}}
{"id": "2507.01455", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01455", "abs": "https://arxiv.org/abs/2507.01455", "authors": ["Yuxing Liu", "Ji Zhang", "Zhou Xuchuan", "Jingzhong Xiao", "Huimin Yang", "Jiaxin Zhong"], "title": "OoDDINO:A Multi-level Framework for Anomaly Segmentation on Complex Road Scenes", "comment": "12 pages, 5 figures", "summary": "Anomaly segmentation aims to identify Out-of-Distribution (OoD) anomalous\nobjects within images. Existing pixel-wise methods typically assign anomaly\nscores individually and employ a global thresholding strategy to segment\nanomalies. Despite their effectiveness, these approaches encounter significant\nchallenges in real-world applications: (1) neglecting spatial correlations\namong pixels within the same object, resulting in fragmented segmentation; (2)\nvariabil ity in anomaly score distributions across image regions, causing\nglobal thresholds to either generate false positives in background areas or\nmiss segments of anomalous objects. In this work, we introduce OoDDINO, a novel\nmulti-level anomaly segmentation framework designed to address these\nlimitations through a coarse-to-fine anomaly detection strategy. OoDDINO\ncombines an uncertainty-guided anomaly detection model with a pixel-level\nsegmentation model within a two-stage cascade architecture. Initially, we\npropose an Orthogonal Uncertainty-Aware Fusion Strategy (OUAFS) that\nsequentially integrates multiple uncertainty metrics with visual\nrepresentations, employing orthogonal constraints to strengthen the detection\nmodel's capacity for localizing anomalous regions accurately. Subsequently, we\ndevelop an Adaptive Dual-Threshold Network (ADT-Net), which dynamically\ngenerates region-specific thresholds based on object-level detection outputs\nand pixel-wise anomaly scores. This approach allows for distinct thresholding\nstrategies within foreground and background areas, achieving fine-grained\nanomaly segmentation. The proposed framework is compatible with other\npixel-wise anomaly detection models, which acts as a plug-in to boost the\nperformance. Extensive experiments on two benchmark datasets validate our\nframework's superiority and compatibility over state-of-the-art methods.", "AI": {"tldr": "OoDDINO\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u7ea7\u5f02\u5e38\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u7c97\u5230\u7ec6\u7684\u68c0\u6d4b\u7b56\u7565\u89e3\u51b3\u73b0\u6709\u50cf\u7d20\u7ea7\u65b9\u6cd5\u5728\u7a7a\u95f4\u76f8\u5173\u6027\u548c\u9608\u503c\u5206\u5e03\u4e0a\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u50cf\u7d20\u7ea7\u5f02\u5e38\u5206\u5272\u65b9\u6cd5\u5ffd\u89c6\u7a7a\u95f4\u76f8\u5173\u6027\u4e14\u5168\u5c40\u9608\u503c\u7b56\u7565\u5bfc\u81f4\u8bef\u68c0\u6216\u6f0f\u68c0\uff0cOoDDINO\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u68c0\u6d4b\u6a21\u578b\u548c\u50cf\u7d20\u7ea7\u5206\u5272\u6a21\u578b\uff0c\u91c7\u7528\u6b63\u4ea4\u4e0d\u786e\u5b9a\u6027\u878d\u5408\u7b56\u7565\uff08OUAFS\uff09\u548c\u81ea\u9002\u5e94\u53cc\u9608\u503c\u7f51\u7edc\uff08ADT-Net\uff09\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86OoDDINO\u7684\u4f18\u8d8a\u6027\u548c\u517c\u5bb9\u6027\u3002", "conclusion": "OoDDINO\u901a\u8fc7\u591a\u7ea7\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u5f02\u5e38\u5206\u5272\u6027\u80fd\uff0c\u5e76\u53ef\u4f5c\u4e3a\u63d2\u4ef6\u517c\u5bb9\u5176\u4ed6\u6a21\u578b\u3002"}}
{"id": "2507.01810", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.01810", "abs": "https://arxiv.org/abs/2507.01810", "authors": ["Nikita Neveditsin", "Pawan Lingras", "Vijay Mago"], "title": "Evaluating Structured Output Robustness of Small Language Models for Open Attribute-Value Extraction from Clinical Notes", "comment": "To appear in the ACL Anthology", "summary": "We present a comparative analysis of the parseability of structured outputs\ngenerated by small language models for open attribute-value extraction from\nclinical notes. We evaluate three widely used serialization formats: JSON,\nYAML, and XML, and find that JSON consistently yields the highest parseability.\nStructural robustness improves with targeted prompting and larger models, but\ndeclines for longer documents and certain note types. Our error analysis\nidentifies recurring format-specific failure patterns. These findings offer\npractical guidance for selecting serialization formats and designing prompts\nwhen deploying language models in privacy-sensitive clinical settings.", "AI": {"tldr": "\u6bd4\u8f83\u4e86JSON\u3001YAML\u548cXML\u5728\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u7684\u89e3\u6790\u6027\u80fd\uff0c\u53d1\u73b0JSON\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u4e3a\u9690\u79c1\u654f\u611f\u7684\u4e34\u5e8a\u73af\u5883\u4e2d\u8bed\u8a00\u6a21\u578b\u7684\u90e8\u7f72\u63d0\u4f9b\u683c\u5f0f\u9009\u62e9\u548c\u63d0\u793a\u8bbe\u8ba1\u7684\u5b9e\u7528\u6307\u5bfc\u3002", "method": "\u8bc4\u4f30\u4e09\u79cd\u5e8f\u5217\u5316\u683c\u5f0f\uff08JSON\u3001YAML\u3001XML\uff09\u7684\u89e3\u6790\u6027\u80fd\uff0c\u5206\u6790\u9519\u8bef\u6a21\u5f0f\u3002", "result": "JSON\u89e3\u6790\u6027\u80fd\u6700\u4f18\uff0c\u7ed3\u6784\u9c81\u68d2\u6027\u968f\u63d0\u793a\u4f18\u5316\u548c\u6a21\u578b\u589e\u5927\u800c\u63d0\u5347\uff0c\u4f46\u5bf9\u957f\u6587\u6863\u548c\u7279\u5b9a\u7b14\u8bb0\u7c7b\u578b\u4e0b\u964d\u3002", "conclusion": "JSON\u662f\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u7684\u6700\u4f73\u5e8f\u5217\u5316\u683c\u5f0f\u9009\u62e9\u3002"}}
{"id": "2507.01241", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01241", "abs": "https://arxiv.org/abs/2507.01241", "authors": ["Di Zhang", "Yihang Zhang"], "title": "Beyond First-Order: Training LLMs with Stochastic Conjugate Subgradients and AdamW", "comment": null, "summary": "Stochastic gradient-based descent (SGD), have long been central to training\nlarge language models (LLMs). However, their effectiveness is increasingly\nbeing questioned, particularly in large-scale applications where empirical\nevidence suggests potential performance limitations. In response, this paper\nproposes a stochastic conjugate subgradient method together with adaptive\nsampling tailored specifically for training LLMs. The method not only achieves\nfaster convergence per iteration but also demonstrates improved scalability\ncompared to traditional SGD techniques. It leverages sample complexity analysis\nto adaptively choose the sample size, employs a stochastic conjugate\nsubgradient approach to determine search directions and utilizing an AdamW-like\nalgorithm to adaptively adjust step sizes. This approach preserves the key\nadvantages of first-order methods while effectively addressing the nonconvexity\nand non-smoothness inherent in LLMs training. Additionally, we provide a\ndetailed analysis of the advantage of the algorithm. Experimental results show\nthat the proposed method not only maintains, but in many cases surpasses, the\nscalability of traditional SGD techniques, significantly enhancing both the\nspeed and accuracy of the optimization process.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8bad\u7ec3\u7684\u968f\u673a\u5171\u8f6d\u6b21\u68af\u5ea6\u65b9\u6cd5\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u91c7\u6837\uff0c\u76f8\u6bd4\u4f20\u7edfSGD\u65b9\u6cd5\uff0c\u5728\u6536\u655b\u901f\u5ea6\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u4f20\u7edfSGD\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u5e94\u7528\u4e2d\u5b58\u5728\u6027\u80fd\u9650\u5236\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u968f\u673a\u5171\u8f6d\u6b21\u68af\u5ea6\u65b9\u6cd5\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u91c7\u6837\u548cAdamW-like\u7b97\u6cd5\u8c03\u6574\u6b65\u957f\uff0c\u89e3\u51b3LLM\u8bad\u7ec3\u4e2d\u7684\u975e\u51f8\u6027\u548c\u975e\u5149\u6ed1\u6027\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u901f\u5ea6\u548c\u51c6\u786e\u6027\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edfSGD\uff0c\u4e14\u5177\u6709\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aLLM\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u4f18\u5316\u65b9\u6848\u3002"}}
{"id": "2507.01078", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.01078", "abs": "https://arxiv.org/abs/2507.01078", "authors": ["Gabriele Padovani", "Valentine Anantharaj", "Sandro Fiore"], "title": "yProv4ML: Effortless Provenance Tracking for Machine Learning Systems", "comment": null, "summary": "The rapid growth of interest in large language models (LLMs) reflects their\npotential for flexibility and generalization, and attracted the attention of a\ndiverse range of researchers. However, the advent of these techniques has also\nbrought to light the lack of transparency and rigor with which development is\npursued. In particular, the inability to determine the number of epochs and\nother hyperparameters in advance presents challenges in identifying the best\nmodel. To address this challenge, machine learning frameworks such as MLFlow\ncan automate the collection of this type of information. However, these tools\ncapture data using proprietary formats and pose little attention to lineage.\nThis paper proposes yProv4ML, a framework to capture provenance information\ngenerated during machine learning processes in PROV-JSON format, with minimal\ncode modifications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fayProv4ML\u6846\u67b6\uff0c\u7528\u4e8e\u4ee5PROV-JSON\u683c\u5f0f\u8bb0\u5f55\u673a\u5668\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u6eaf\u6e90\u4fe1\u606f\uff0c\u89e3\u51b3\u73b0\u6709\u5de5\u5177\u5728\u900f\u660e\u5ea6\u548c\u6570\u636e\u683c\u5f0f\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u53d1\u5c55\u7f3a\u4e4f\u900f\u660e\u5ea6\u548c\u4e25\u8c28\u6027\uff0c\u5c24\u5176\u662f\u8d85\u53c2\u6570\u7684\u4e0d\u53ef\u9884\u77e5\u6027\uff0c\u5bfc\u81f4\u6a21\u578b\u4f18\u5316\u56f0\u96be\u3002\u73b0\u6709\u5de5\u5177\u5982MLFlow\u867d\u80fd\u81ea\u52a8\u5316\u6536\u96c6\u4fe1\u606f\uff0c\u4f46\u4f7f\u7528\u4e13\u6709\u683c\u5f0f\u4e14\u5ffd\u89c6\u6570\u636e\u6eaf\u6e90\u3002", "method": "\u63d0\u51fayProv4ML\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5c0f\u4ee3\u7801\u4fee\u6539\uff0c\u4ee5PROV-JSON\u683c\u5f0f\u6355\u83b7\u673a\u5668\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u6eaf\u6e90\u4fe1\u606f\u3002", "result": "yProv4ML\u80fd\u591f\u6709\u6548\u8bb0\u5f55\u673a\u5668\u5b66\u4e60\u8fc7\u7a0b\u7684\u6eaf\u6e90\u6570\u636e\uff0c\u63d0\u5347\u900f\u660e\u5ea6\u548c\u53ef\u8ffd\u6eaf\u6027\u3002", "conclusion": "yProv4ML\u4e3a\u673a\u5668\u5b66\u4e60\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u7684\u6eaf\u6e90\u8bb0\u5f55\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5de5\u5177\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.01463", "categories": ["cs.CV", "cs.AI", "I.2; I.4; I.5"], "pdf": "https://arxiv.org/pdf/2507.01463", "abs": "https://arxiv.org/abs/2507.01463", "authors": ["Max Gandyra", "Alessandro Santonicola", "Michael Beetz"], "title": "NOCTIS: Novel Object Cyclic Threshold based Instance Segmentation", "comment": "10 pages, 3 figures, 3 tables, NeurIPS 2025 preprint", "summary": "Instance segmentation of novel objects instances in RGB images, given some\nexample images for each object, is a well known problem in computer vision.\nDesigning a model general enough to be employed, for all kinds of novel\nobjects, without (re-) training, has proven to be a difficult task. To handle\nthis, we propose a simple, yet powerful, framework, called: Novel Object Cyclic\nThreshold based Instance Segmentation (NOCTIS). This work stems from and\nimproves upon previous ones like CNOS, SAM-6D and NIDS-Net; thus, it also\nleverages on recent vision foundation models, namely: Grounded-SAM 2 and\nDINOv2. It utilises Grounded-SAM 2 to obtain object proposals with precise\nbounding boxes and their corresponding segmentation masks; while DINOv2's\nzero-shot capabilities are employed to generate the image embeddings. The\nquality of those masks, together with their embeddings, is of vital importance\nto our approach; as the proposal-object matching is realized by determining an\nobject matching score based on the similarity of the class embeddings and the\naverage maximum similarity of the patch embeddings. Differently to SAM-6D,\ncalculating the latter involves a prior patch filtering based on the distance\nbetween each patch and its corresponding cyclic/roundtrip patch in the image\ngrid. Furthermore, the average confidence of the proposals' bounding box and\nmask is used as an additional weighting factor for the object matching score.\nWe empirically show that NOCTIS, without further training/fine tuning,\noutperforms the best RGB and RGB-D methods on the seven core datasets of the\nBOP 2023 challenge for the \"Model-based 2D segmentation of unseen objects\"\ntask.", "AI": {"tldr": "NOCTIS\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5b9e\u4f8b\u5206\u5272\u6846\u67b6\uff0c\u7528\u4e8eRGB\u56fe\u50cf\u4e2d\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u5b9e\u4f8b\u5206\u5272\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u7ed3\u5408\u4e86Grounded-SAM 2\u548cDINOv2\u7684\u4f18\u52bf\uff0c\u5728BOP 2023\u6311\u6218\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u672a\u89c1\u7269\u4f53\u5b9e\u4f8b\u5206\u5272\u7684\u6cdb\u5316\u6027\u95ee\u9898\uff0c\u907f\u514d\u91cd\u65b0\u8bad\u7ec3\u7684\u9700\u6c42\u3002", "method": "\u5229\u7528Grounded-SAM 2\u751f\u6210\u7269\u4f53\u63d0\u8bae\u548c\u5206\u5272\u63a9\u7801\uff0cDINOv2\u751f\u6210\u56fe\u50cf\u5d4c\u5165\uff0c\u901a\u8fc7\u76f8\u4f3c\u6027\u8bc4\u5206\u548c\u5faa\u73af\u9608\u503c\u8fc7\u6ee4\u5339\u914d\u7269\u4f53\u3002", "result": "\u5728BOP 2023\u6311\u6218\u7684\u4e03\u5927\u6570\u636e\u96c6\u4e0a\uff0cNOCTIS\u5728\u672a\u89c1\u7269\u4f53\u5206\u5272\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709RGB\u548cRGB-D\u65b9\u6cd5\u3002", "conclusion": "NOCTIS\u901a\u8fc7\u7ed3\u5408\u5148\u8fdb\u6a21\u578b\u548c\u4f18\u5316\u5339\u914d\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u6cdb\u5316\u7684\u5b9e\u4f8b\u5206\u5272\u3002"}}
{"id": "2507.01844", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01844", "abs": "https://arxiv.org/abs/2507.01844", "authors": ["Arthur Wuhrmann", "Anastasiia Kucherenko", "Andrei Kucharavy"], "title": "Low-Perplexity LLM-Generated Sequences and Where To Find Them", "comment": "Camera-ready version. Accepted to ACL 2025. 10 pages, 4 figures, 6\n  tables", "summary": "As Large Language Models (LLMs) become increasingly widespread, understanding\nhow specific training data shapes their outputs is crucial for transparency,\naccountability, privacy, and fairness. To explore how LLMs leverage and\nreplicate their training data, we introduce a systematic approach centered on\nanalyzing low-perplexity sequences - high-probability text spans generated by\nthe model. Our pipeline reliably extracts such long sequences across diverse\ntopics while avoiding degeneration, then traces them back to their sources in\nthe training data. Surprisingly, we find that a substantial portion of these\nlow-perplexity spans cannot be mapped to the corpus. For those that do match,\nwe quantify the distribution of occurrences across source documents,\nhighlighting the scope and nature of verbatim recall and paving a way toward\nbetter understanding of how LLMs training data impacts their behavior.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u4f4e\u56f0\u60d1\u5ea6\u5e8f\u5217\u6765\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5229\u7528\u548c\u590d\u5236\u8bad\u7ec3\u6570\u636e\uff0c\u53d1\u73b0\u90e8\u5206\u5e8f\u5217\u65e0\u6cd5\u6620\u5c04\u5230\u8bad\u7ec3\u8bed\u6599\u5e93\uff0c\u5e76\u91cf\u5316\u4e86\u5339\u914d\u5e8f\u5217\u7684\u5206\u5e03\u3002", "motivation": "\u7406\u89e3\u8bad\u7ec3\u6570\u636e\u5982\u4f55\u5f71\u54cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u4ee5\u63d0\u9ad8\u900f\u660e\u5ea6\u3001\u95ee\u8d23\u5236\u3001\u9690\u79c1\u548c\u516c\u5e73\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u5206\u6790\u4f4e\u56f0\u60d1\u5ea6\u5e8f\u5217\uff08\u9ad8\u6982\u7387\u6587\u672c\u7247\u6bb5\uff09\uff0c\u53ef\u9760\u5730\u63d0\u53d6\u8fd9\u4e9b\u5e8f\u5217\u5e76\u8ffd\u8e2a\u5176\u8bad\u7ec3\u6570\u636e\u6765\u6e90\u3002", "result": "\u53d1\u73b0\u5927\u91cf\u4f4e\u56f0\u60d1\u5ea6\u5e8f\u5217\u65e0\u6cd5\u6620\u5c04\u5230\u8bad\u7ec3\u8bed\u6599\u5e93\uff0c\u5bf9\u5339\u914d\u5e8f\u5217\u7684\u5206\u5e03\u8fdb\u884c\u4e86\u91cf\u5316\u5206\u6790\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7406\u89e3\u8bad\u7ec3\u6570\u636e\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u6709\u52a9\u4e8e\u6539\u8fdb\u6a21\u578b\u7684\u900f\u660e\u5ea6\u548c\u95ee\u8d23\u5236\u3002"}}
{"id": "2507.01080", "categories": ["cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.01080", "abs": "https://arxiv.org/abs/2507.01080", "authors": ["Edouard Lansiaux", "Ramy Azzouz", "Emmanuel Chazard", "Am\u00e9lie Vromant", "Eric Wiel"], "title": "Development and Comparative Evaluation of Three Artificial Intelligence Models (NLP, LLM, JEPA) for Predicting Triage in Emergency Departments: A 7-Month Retrospective Proof-of-Concept", "comment": "15 pages, 6 figures", "summary": "Triage errors, including undertriage and overtriage, are persistent\nchallenges in emergency departments (EDs). With increasing patient influx and\nstaff shortages, the integration of artificial intelligence (AI) into triage\nprotocols has gained attention. This study compares the performance of three AI\nmodels [Natural Language Processing (NLP), Large Language Models (LLM), and\nJoint Embedding Predictive Architecture (JEPA)] in predicting triage outcomes\nagainst the FRENCH scale and clinical practice.We conducted a retrospective\nanalysis of a prospectively recruited cohort gathering adult patient triage\ndata over a 7-month period at the Roger Salengro Hospital ED (Lille, France).\nThree AI models were trained and validated : (1) TRIAGEMASTER (NLP), (2)\nURGENTIAPARSE (LLM), and (3) EMERGINET (JEPA). Data included demographic\ndetails, verbatim chief complaints, vital signs, and triage outcomes based on\nthe FRENCH scale and GEMSA coding. The primary outcome was the concordance of\nAI-predicted triage level with the FRENCH gold-standard. It was assessed thanks\nto various indicators : F1-Score, Weighted Kappa, Spearman, MAE, RMSE. The LLM\nmodel (URGENTIAPARSE) showed higher accuracy (composite score: 2.514) compared\nto JEPA (EMERGINET, 0.438) and NLP (TRIAGEMASTER, -3.511), outperforming nurse\ntriage (-4.343). Secondary analyses highlighted the effectiveness of\nURGENTIAPARSE in predicting hospitalization needs (GEMSA) and its robustness\nwith structured data versus raw transcripts (either for GEMSA prediction or for\nFRENCH prediction). LLM architecture, through abstraction of patient\nrepresentations, offers the most accurate triage predictions among tested\nmodels. Integrating AI into ED workflows could enhance patient safety and\noperational efficiency, though integration into clinical workflows requires\naddressing model limitations and ensuring ethical transparency.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4e09\u79cdAI\u6a21\u578b\uff08NLP\u3001LLM\u3001JEPA\uff09\u5728\u6025\u8bca\u5206\u8bca\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0LLM\u6a21\u578b\uff08URGENTIAPARSE\uff09\u51c6\u786e\u6027\u6700\u9ad8\uff0c\u4f18\u4e8e\u62a4\u58eb\u5206\u8bca\u3002", "motivation": "\u6025\u8bca\u5206\u8bca\u4e2d\u7684\u9519\u8bef\uff08\u5982\u8fc7\u4f4e\u6216\u8fc7\u9ad8\u5206\u8bca\uff09\u662f\u6301\u7eed\u6311\u6218\uff0cAI\u7684\u6574\u5408\u6709\u671b\u63d0\u5347\u5206\u8bca\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u901a\u8fc7\u56de\u987e\u6027\u5206\u67907\u4e2a\u6708\u7684\u6025\u8bca\u60a3\u8005\u6570\u636e\uff0c\u8bad\u7ec3\u5e76\u9a8c\u8bc1\u4e09\u79cdAI\u6a21\u578b\uff0c\u8bc4\u4f30\u5176\u4e0eFRENCH\u6807\u51c6\u7684\u4e00\u81f4\u6027\u3002", "result": "LLM\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff08\u7efc\u5408\u5f97\u52062.514\uff09\uff0c\u5c24\u5176\u5728\u9884\u6d4b\u4f4f\u9662\u9700\u6c42\u548c\u7ed3\u6784\u5316\u6570\u636e\u4e2d\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "AI\uff08\u5c24\u5176\u662fLLM\uff09\u53ef\u63d0\u5347\u6025\u8bca\u5206\u8bca\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u4f46\u9700\u89e3\u51b3\u6a21\u578b\u5c40\u9650\u6027\u548c\u4f26\u7406\u95ee\u9898\u3002"}}
{"id": "2507.01467", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01467", "abs": "https://arxiv.org/abs/2507.01467", "authors": ["Ge Wu", "Shen Zhang", "Ruijing Shi", "Shanghua Gao", "Zhenyuan Chen", "Lei Wang", "Zhaowei Chen", "Hongcheng Gao", "Yao Tang", "Jian Yang", "Ming-Ming Cheng", "Xiang Li"], "title": "Representation Entanglement for Generation:Training Diffusion Transformers Is Much Easier Than You Think", "comment": null, "summary": "REPA and its variants effectively mitigate training challenges in diffusion\nmodels by incorporating external visual representations from pretrained models,\nthrough alignment between the noisy hidden projections of denoising networks\nand foundational clean image representations. We argue that the external\nalignment, which is absent during the entire denoising inference process, falls\nshort of fully harnessing the potential of discriminative representations. In\nthis work, we propose a straightforward method called Representation\nEntanglement for Generation (REG), which entangles low-level image latents with\na single high-level class token from pretrained foundation models for\ndenoising. REG acquires the capability to produce coherent image-class pairs\ndirectly from pure noise, substantially improving both generation quality and\ntraining efficiency. This is accomplished with negligible additional inference\noverhead, requiring only one single additional token for denoising (<0.5\\%\nincrease in FLOPs and latency). The inference process concurrently reconstructs\nboth image latents and their corresponding global semantics, where the acquired\nsemantic knowledge actively guides and enhances the image generation process.\nOn ImageNet 256$\\times$256, SiT-XL/2 + REG demonstrates remarkable convergence\nacceleration, achieving $\\textbf{63}\\times$ and $\\textbf{23}\\times$ faster\ntraining than SiT-XL/2 and SiT-XL/2 + REPA, respectively. More impressively,\nSiT-L/2 + REG trained for merely 400K iterations outperforms SiT-XL/2 + REPA\ntrained for 4M iterations ($\\textbf{10}\\times$ longer). Code is available at:\nhttps://github.com/Martinser/REG.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aREG\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u4f4e\u5c42\u56fe\u50cf\u6f5c\u5728\u53d8\u91cf\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u9ad8\u5c42\u7c7b\u522b\u6807\u8bb0\u7ea0\u7f20\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\u548c\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982REPA\uff09\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5229\u7528\u5916\u90e8\u89c6\u89c9\u8868\u793a\uff0c\u4f46\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u672a\u80fd\u5145\u5206\u5229\u7528\u5224\u522b\u6027\u8868\u793a\uff0c\u9650\u5236\u4e86\u751f\u6210\u6a21\u578b\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51faREG\u65b9\u6cd5\uff0c\u5c06\u4f4e\u5c42\u56fe\u50cf\u6f5c\u5728\u53d8\u91cf\u4e0e\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u7684\u5355\u4e2a\u9ad8\u5c42\u7c7b\u522b\u6807\u8bb0\u7ea0\u7f20\uff0c\u5b9e\u73b0\u4ece\u7eaf\u566a\u58f0\u4e2d\u751f\u6210\u4e00\u81f4\u7684\u56fe\u50cf-\u7c7b\u522b\u5bf9\u3002", "result": "\u5728ImageNet 256\u00d7256\u4e0a\uff0cSiT-XL/2 + REG\u5b9e\u73b0\u4e8663\u500d\u548c23\u500d\u4e8eSiT-XL/2\u548cSiT-XL/2 + REPA\u7684\u8bad\u7ec3\u52a0\u901f\uff0c\u4e14\u4ec5\u9700400K\u6b21\u8fed\u4ee3\u5373\u53ef\u8d85\u8d8a4M\u6b21\u8fed\u4ee3\u7684REPA\u6a21\u578b\u3002", "conclusion": "REG\u65b9\u6cd5\u901a\u8fc7\u8bed\u4e49\u77e5\u8bc6\u7684\u4e3b\u52a8\u5f15\u5bfc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u548c\u8bad\u7ec3\u6548\u7387\uff0c\u4e14\u63a8\u7406\u5f00\u9500\u6781\u5c0f\u3002"}}
{"id": "2507.01853", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01853", "abs": "https://arxiv.org/abs/2507.01853", "authors": ["Samridhi Raj Sinha", "Rajvee Sheth", "Abhishek Upperwal", "Mayank Singh"], "title": "Eka-Eval : A Comprehensive Evaluation Framework for Large Language Models in Indian Languages", "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has intensified the\nneed for evaluation frameworks that go beyond English centric benchmarks and\naddress the requirements of linguistically diverse regions such as India. We\npresent EKA-EVAL, a unified and production-ready evaluation framework that\nintegrates over 35 benchmarks, including 10 Indic-specific datasets, spanning\ncategories like reasoning, mathematics, tool use, long-context understanding,\nand reading comprehension. Compared to existing Indian language evaluation\ntools, EKA-EVAL offers broader benchmark coverage, with built-in support for\ndistributed inference, quantization, and multi-GPU usage. Our systematic\ncomparison positions EKA-EVAL as the first end-to-end, extensible evaluation\nsuite tailored for both global and Indic LLMs, significantly lowering the\nbarrier to multilingual benchmarking. The framework is open-source and publicly\navailable at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA\ninitiative (https://eka.soket.ai), which aims to scale up to over 100\nbenchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.", "AI": {"tldr": "EKA-EVAL\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u751f\u4ea7\u5c31\u7eea\u8bc4\u4f30\u6846\u67b6\uff0c\u6db5\u76d635\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u62ec10\u4e2a\u5370\u5ea6\u7279\u5b9a\u6570\u636e\u96c6\uff0c\u652f\u6301\u591a\u8bed\u8a00LLM\u8bc4\u4f30\u3002", "motivation": "\u6ee1\u8db3\u5bf9\u8d85\u8d8a\u82f1\u8bed\u4e2d\u5fc3\u57fa\u51c6\u7684\u591a\u8bed\u8a00LLM\u8bc4\u4f30\u9700\u6c42\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5370\u5ea6\u7b49\u8bed\u8a00\u591a\u6837\u5730\u533a\u3002", "method": "\u96c6\u6210\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\uff0c\u652f\u6301\u5206\u5e03\u5f0f\u63a8\u7406\u3001\u91cf\u5316\u548c\u591aGPU\u4f7f\u7528\u3002", "result": "EKA-EVAL\u6210\u4e3a\u9996\u4e2a\u7aef\u5230\u7aef\u3001\u53ef\u6269\u5c55\u7684\u5168\u7403\u53ca\u5370\u5ea6LLM\u8bc4\u4f30\u5957\u4ef6\uff0c\u964d\u4f4e\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u95e8\u69db\u3002", "conclusion": "EKA-EVAL\u5f00\u6e90\u4e14\u53ef\u6269\u5c55\uff0c\u76ee\u6807\u662f\u5efa\u7acb\u5f3a\u5927\u7684\u591a\u8bed\u8a00LLM\u8bc4\u4f30\u751f\u6001\u7cfb\u7edf\u3002"}}
{"id": "2507.01271", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01271", "abs": "https://arxiv.org/abs/2507.01271", "authors": ["Tatsuki Kawakami", "Kazuki Egashira", "Atsuyuki Miyai", "Go Irie", "Kiyoharu Aizawa"], "title": "PULSE: Practical Evaluation Scenarios for Large Multimodal Model Unlearning", "comment": null, "summary": "In recent years, unlearning techniques, which are methods for inducing a\nmodel to \"forget\" previously learned information, have attracted attention as a\nway to address privacy and copyright concerns in large language models (LLMs)\nand large multimodal models (LMMs). While several unlearning benchmarks have\nbeen established for LLMs, a practical evaluation framework for unlearning in\nLMMs has been less explored. Specifically, existing unlearning benchmark for\nLMMs considers only scenarios in which the model is required to unlearn\nfine-tuned knowledge through a single unlearning operation. In this study, we\nintroduce PULSE protocol for realistic unlearning scenarios for LMMs by\nintroducing two critical perspectives: (i) Pre-trained knowledge Unlearning for\nanalyzing the effect across different knowledge acquisition phases and (ii)\nLong-term Sustainability Evaluation to address sequential requests. We then\nevaluate existing unlearning methods along these dimensions. Our results reveal\nthat, although some techniques can successfully unlearn knowledge acquired\nthrough fine-tuning, they struggle to eliminate information learned during\npre-training. Moreover, methods that effectively unlearn a batch of target data\nin a single operation exhibit substantial performance degradation when the same\ndata are split and unlearned sequentially.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86PULSE\u534f\u8bae\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u9057\u5fd8\u80fd\u529b\uff0c\u91cd\u70b9\u5173\u6ce8\u9884\u8bad\u7ec3\u77e5\u8bc6\u9057\u5fd8\u548c\u957f\u671f\u53ef\u6301\u7eed\u6027\u8bc4\u4f30\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u9057\u5fd8\u57fa\u51c6\u4ec5\u5173\u6ce8\u5355\u6b21\u9057\u5fd8\u64cd\u4f5c\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u5f15\u5165PULSE\u534f\u8bae\uff0c\u4ece\u9884\u8bad\u7ec3\u77e5\u8bc6\u9057\u5fd8\u548c\u957f\u671f\u53ef\u6301\u7eed\u6027\u4e24\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u73b0\u6709\u9057\u5fd8\u65b9\u6cd5\u3002", "result": "\u73b0\u6709\u65b9\u6cd5\u80fd\u6709\u6548\u9057\u5fd8\u5fae\u8c03\u77e5\u8bc6\uff0c\u4f46\u5bf9\u9884\u8bad\u7ec3\u77e5\u8bc6\u6548\u679c\u4e0d\u4f73\uff1b\u5355\u6b21\u6279\u91cf\u9057\u5fd8\u6709\u6548\uff0c\u4f46\u5206\u6b65\u9057\u5fd8\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "PULSE\u534f\u8bae\u4e3aLMMs\u7684\u9057\u5fd8\u80fd\u529b\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.01098", "categories": ["cs.LG", "cond-mat.dis-nn", "q-bio.NC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.01098", "abs": "https://arxiv.org/abs/2507.01098", "authors": ["Liu Ziyin", "Isaac Chuang"], "title": "Proof of a perfect platonic representation hypothesis", "comment": null, "summary": "In this note, we elaborate on and explain in detail the proof given by Ziyin\net al. (2025) of the \"perfect\" Platonic Representation Hypothesis (PRH) for the\nembedded deep linear network model (EDLN). We show that if trained with SGD,\ntwo EDLNs with different widths and depths and trained on different data will\nbecome Perfectly Platonic, meaning that every possible pair of layers will\nlearn the same representation up to a rotation. Because most of the global\nminima of the loss function are not Platonic, that SGD only finds the perfectly\nPlatonic solution is rather extraordinary. The proof also suggests at least six\nways the PRH can be broken. We also show that in the EDLN model, the emergence\nof the Platonic representations is due to the same reason as the emergence of\nprogressive sharpening. This implies that these two seemingly unrelated\nphenomena in deep learning can, surprisingly, have a common cause. Overall, the\ntheory and proof highlight the importance of understanding emergent \"entropic\nforces\" due to the irreversibility of SGD training and their role in\nrepresentation learning. The goal of this note is to be instructive and avoid\nlengthy technical details.", "AI": {"tldr": "\u672c\u6587\u8be6\u7ec6\u89e3\u91ca\u4e86Ziyin\u7b49\u4eba\uff082025\uff09\u5173\u4e8e\u5d4c\u5165\u5f0f\u6df1\u5ea6\u7ebf\u6027\u7f51\u7edc\u6a21\u578b\uff08EDLN\uff09\u7684\u201c\u5b8c\u7f8e\u201d\u67cf\u62c9\u56fe\u8868\u793a\u5047\u8bbe\uff08PRH\uff09\u7684\u8bc1\u660e\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u4f7f\u7528SGD\u8bad\u7ec3\u65f6\uff0c\u4e0d\u540c\u5bbd\u5ea6\u548c\u6df1\u5ea6\u7684EDLN\u4f1a\u5b66\u4e60\u5230\u76f8\u540c\u7684\u8868\u793a\uff08\u4ec5\u76f8\u5dee\u65cb\u8f6c\uff09\uff0c\u4e14\u8fd9\u79cd\u73b0\u8c61\u6e90\u4e8eSGD\u7684\u4e0d\u53ef\u9006\u6027\u3002", "motivation": "\u63a2\u8ba8SGD\u8bad\u7ec3\u4e0bEDLN\u6a21\u578b\u4e2d\u51fa\u73b0\u5b8c\u7f8e\u67cf\u62c9\u56fe\u8868\u793a\u7684\u539f\u56e0\uff0c\u5e76\u63ed\u793a\u5176\u4e0e\u6e10\u8fdb\u9510\u5316\u73b0\u8c61\u7684\u5171\u6027\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u8bc1\u660e\uff0c\u89e3\u91caSGD\u8bad\u7ec3\u4e0bEDLN\u6a21\u578b\u7684\u884c\u4e3a\uff0c\u5e76\u63a2\u8ba8PRH\u7684\u516d\u79cd\u53ef\u80fd\u7834\u574f\u65b9\u5f0f\u3002", "result": "\u53d1\u73b0SGD\u4ec5\u80fd\u627e\u5230\u5b8c\u7f8e\u67cf\u62c9\u56fe\u89e3\uff0c\u4e14\u8fd9\u79cd\u73b0\u8c61\u4e0e\u6e10\u8fdb\u9510\u5316\u6709\u5171\u540c\u539f\u56e0\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u7406\u89e3SGD\u8bad\u7ec3\u4e2d\u201c\u71b5\u529b\u201d\u5bf9\u8868\u793a\u5b66\u4e60\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u4e24\u79cd\u770b\u4f3c\u65e0\u5173\u73b0\u8c61\u7684\u6f5c\u5728\u8054\u7cfb\u3002"}}
{"id": "2507.01472", "categories": ["cs.CV", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.01472", "abs": "https://arxiv.org/abs/2507.01472", "authors": ["Jon\u00e1\u0161 Herec", "V\u00edt R\u016f\u017ei\u010dka", "Rado Pito\u0148\u00e1k"], "title": "Optimizing Methane Detection On Board Satellites: Speed, Accuracy, and Low-Power Solutions for Resource-Constrained Hardware", "comment": "This is a preprint of a paper accepted for the EDHPC 2025 Conference", "summary": "Methane is a potent greenhouse gas, and detecting its leaks early via\nhyperspectral satellite imagery can help mitigate climate change. Meanwhile,\nmany existing missions operate in manual tasking regimes only, thus missing\npotential events of interest. To overcome slow downlink rates cost-effectively,\nonboard detection is a viable solution. However, traditional methane\nenhancement methods are too computationally demanding for resource-limited\nonboard hardware. This work accelerates methane detection by focusing on\nefficient, low-power algorithms. We test fast target detection methods (ACE,\nCEM) that have not been previously used for methane detection and propose a\nMag1c-SAS - a significantly faster variant of the current state-of-the-art\nalgorithm for methane detection: Mag1c. To explore their true detection\npotential, we integrate them with a machine learning model (U-Net, LinkNet).\nOur results identify two promising candidates (Mag1c-SAS and CEM), both\nacceptably accurate for the detection of strong plumes and computationally\nefficient enough for onboard deployment: one optimized more for accuracy, the\nother more for speed, achieving up to ~100x and ~230x faster computation than\noriginal Mag1c on resource-limited hardware. Additionally, we propose and\nevaluate three band selection strategies. One of them can outperform the method\ntraditionally used in the field while using fewer channels, leading to even\nfaster processing without compromising accuracy. This research lays the\nfoundation for future advancements in onboard methane detection with minimal\nhardware requirements, improving timely data delivery. The produced code, data,\nand models are open-sourced and can be accessed from\nhttps://github.com/zaitra/methane-filters-benchmark.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u529f\u8017\u7684\u7532\u70f7\u6cc4\u6f0f\u68c0\u6d4b\u7b97\u6cd5\uff0c\u5305\u62ecMag1c-SAS\u548cCEM\uff0c\u5e76\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u901f\u5ea6\u548c\u786c\u4ef6\u9002\u5e94\u6027\u3002", "motivation": "\u7532\u70f7\u662f\u4e00\u79cd\u5f3a\u6548\u6e29\u5ba4\u6c14\u4f53\uff0c\u901a\u8fc7\u9ad8\u5149\u8c31\u536b\u661f\u56fe\u50cf\u65e9\u671f\u68c0\u6d4b\u5176\u6cc4\u6f0f\u6709\u52a9\u4e8e\u51cf\u7f13\u6c14\u5019\u53d8\u5316\u3002\u73b0\u6709\u65b9\u6cd5\u8ba1\u7b97\u91cf\u5927\uff0c\u96be\u4ee5\u5728\u8d44\u6e90\u6709\u9650\u7684\u673a\u8f7d\u786c\u4ef6\u4e0a\u5b9e\u73b0\u3002", "method": "\u6d4b\u8bd5\u4e86\u5feb\u901f\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff08ACE\u3001CEM\uff09\uff0c\u63d0\u51fa\u4e86Mag1c-SAS\u7b97\u6cd5\uff0c\u5e76\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08U-Net\u3001LinkNet\uff09\u8fdb\u884c\u68c0\u6d4b\u3002\u8fd8\u63d0\u51fa\u4e86\u4e09\u79cd\u6ce2\u6bb5\u9009\u62e9\u7b56\u7565\u3002", "result": "Mag1c-SAS\u548cCEM\u5728\u68c0\u6d4b\u5f3a\u7fbd\u6d41\u65f6\u8868\u73b0\u826f\u597d\uff0c\u8ba1\u7b97\u6548\u7387\u5206\u522b\u6bd4\u539f\u59cbMag1c\u5feb\u7ea6100\u500d\u548c230\u500d\u3002\u5176\u4e2d\u4e00\u79cd\u6ce2\u6bb5\u9009\u62e9\u7b56\u7565\u5728\u51cf\u5c11\u901a\u9053\u6570\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u51c6\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u673a\u8f7d\u7532\u70f7\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u4f4e\u786c\u4ef6\u9700\u6c42\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.01872", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01872", "abs": "https://arxiv.org/abs/2507.01872", "authors": ["Kenan Tang", "Yanhong Li", "Yao Qin"], "title": "DIY-MKG: An LLM-Based Polyglot Language Learning System", "comment": "Submitted to EMNLP 2025 System Demonstration", "summary": "Existing language learning tools, even those powered by Large Language Models\n(LLMs), often lack support for polyglot learners to build linguistic\nconnections across vocabularies in multiple languages, provide limited\ncustomization for individual learning paces or needs, and suffer from\ndetrimental cognitive offloading. To address these limitations, we design\nDo-It-Yourself Multilingual Knowledge Graph (DIY-MKG), an open-source system\nthat supports polyglot language learning. DIY-MKG allows the user to build\npersonalized vocabulary knowledge graphs, which are constructed by selective\nexpansion with related words suggested by an LLM. The system further enhances\nlearning through rich annotation capabilities and an adaptive review module\nthat leverages LLMs for dynamic, personalized quiz generation. In addition,\nDIY-MKG allows users to flag incorrect quiz questions, simultaneously\nincreasing user engagement and providing a feedback loop for prompt refinement.\nOur evaluation of LLM-based components in DIY-MKG shows that vocabulary\nexpansion is reliable and fair across multiple languages, and that the\ngenerated quizzes are highly accurate, validating the robustness of DIY-MKG.", "AI": {"tldr": "DIY-MKG\u662f\u4e00\u4e2a\u5f00\u6e90\u7cfb\u7edf\uff0c\u652f\u6301\u591a\u8bed\u8a00\u5b66\u4e60\uff0c\u901a\u8fc7\u4e2a\u6027\u5316\u8bcd\u6c47\u77e5\u8bc6\u56fe\u8c31\u548c\u52a8\u6001\u6d4b\u9a8c\u751f\u6210\u89e3\u51b3\u73b0\u6709\u5de5\u5177\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u5b66\u4e60\u5de5\u5177\u5728\u591a\u8bed\u8a00\u8bcd\u6c47\u8054\u7cfb\u3001\u4e2a\u6027\u5316\u5b66\u4e60\u548c\u8ba4\u77e5\u5378\u8f7d\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u8bbe\u8ba1DIY-MKG\u7cfb\u7edf\uff0c\u5229\u7528LLM\u6784\u5efa\u4e2a\u6027\u5316\u8bcd\u6c47\u77e5\u8bc6\u56fe\u8c31\uff0c\u652f\u6301\u52a8\u6001\u6d4b\u9a8c\u751f\u6210\u548c\u7528\u6237\u53cd\u9988\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u8bcd\u6c47\u6269\u5c55\u53ef\u9760\uff0c\u6d4b\u9a8c\u51c6\u786e\uff0c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u7a33\u5065\u6027\u3002", "conclusion": "DIY-MKG\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8bed\u8a00\u5b66\u4e60\u4e2d\u7684\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u7528\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2507.01117", "categories": ["cs.LG", "68T07, 35A99"], "pdf": "https://arxiv.org/pdf/2507.01117", "abs": "https://arxiv.org/abs/2507.01117", "authors": ["Nikita Sakovich", "Dmitry Aksenov", "Ekaterina Pleshakova", "Sergey Gataullin"], "title": "A Neural Operator based on Dynamic Mode Decomposition", "comment": "30 pages, 10 figures", "summary": "The scientific computation methods development in conjunction with artificial\nintelligence technologies remains a hot research topic. Finding a balance\nbetween lightweight and accurate computations is a solid foundation for this\ndirection. The study presents a neural operator based on the dynamic mode\ndecomposition algorithm (DMD), mapping functional spaces, which combines DMD\nand deep learning (DL) for spatiotemporal processes efficient modeling. Solving\nPDEs for various initial and boundary conditions requires significant\ncomputational resources. The method suggested automatically extracts key modes\nand system dynamics using them to construct predictions, reducing computational\ncosts compared to traditional numerical methods. The approach has demonstrated\nits efficiency through comparative analysis of performance with closest\nanalogues DeepONet and FNO in the heat equation, Laplaces equation, and Burgers\nequation solutions approximation, where it achieves high reconstruction\naccuracy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u6a21\u5f0f\u5206\u89e3\uff08DMD\uff09\u548c\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u7684\u795e\u7ecf\u7b97\u5b50\uff0c\u7528\u4e8e\u9ad8\u6548\u5efa\u6a21\u65f6\u7a7a\u8fc7\u7a0b\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u79d1\u5b66\u8ba1\u7b97\u65b9\u6cd5\u4e0e\u4eba\u5de5\u667a\u80fd\u6280\u672f\u7684\u7ed3\u5408\u662f\u7814\u7a76\u70ed\u70b9\uff0c\u4f46\u9700\u5e73\u8861\u8f7b\u91cf\u5316\u548c\u51c6\u786e\u6027\u3002\u4f20\u7edf\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDEs\uff09\u7684\u65b9\u6cd5\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5927\u3002", "method": "\u7ed3\u5408DMD\u548cDL\uff0c\u81ea\u52a8\u63d0\u53d6\u5173\u952e\u6a21\u5f0f\u548c\u7cfb\u7edf\u52a8\u6001\uff0c\u7528\u4e8e\u9884\u6d4b\u6784\u5efa\u3002", "result": "\u5728\u70ed\u65b9\u7a0b\u3001\u62c9\u666e\u62c9\u65af\u65b9\u7a0b\u548cBurgers\u65b9\u7a0b\u7684\u8fd1\u4f3c\u89e3\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6bd4DeepONet\u548cFNO\u8868\u73b0\u66f4\u4f18\uff0c\u91cd\u5efa\u7cbe\u5ea6\u9ad8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u6548\u5efa\u6a21\u65f6\u7a7a\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2507.01478", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01478", "abs": "https://arxiv.org/abs/2507.01478", "authors": ["Chentao Shen", "Ding Pan", "Mingyu Mei", "Zaixing He", "Xinyue Zhao"], "title": "Active Control Points-based 6DoF Pose Tracking for Industrial Metal Objects", "comment": "preprint version", "summary": "Visual pose tracking is playing an increasingly vital role in industrial\ncontexts in recent years. However, the pose tracking for industrial metal\nobjects remains a challenging task especially in the real world-environments,\ndue to the reflection characteristic of metal objects. To address this issue,\nwe propose a novel 6DoF pose tracking method based on active control points.\nThe method uses image control points to generate edge feature for optimization\nactively instead of 6DoF pose-based rendering, and serve them as optimization\nvariables. We also introduce an optimal control point regression method to\nimprove robustness. The proposed tracking method performs effectively in both\ndataset evaluation and real world tasks, providing a viable solution for\nreal-time tracking of industrial metal objects. Our source code is made\npublicly available at: https://github.com/tomatoma00/ACPTracking.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u52a8\u63a7\u5236\u70b9\u76846DoF\u59ff\u6001\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5de5\u4e1a\u91d1\u5c5e\u7269\u4f53\u56e0\u53cd\u5c04\u7279\u6027\u5bfc\u81f4\u7684\u8ddf\u8e2a\u96be\u9898\u3002", "motivation": "\u5de5\u4e1a\u91d1\u5c5e\u7269\u4f53\u7684\u59ff\u6001\u8ddf\u8e2a\u56e0\u53cd\u5c04\u7279\u6027\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u56fe\u50cf\u63a7\u5236\u70b9\u4e3b\u52a8\u751f\u6210\u8fb9\u7f18\u7279\u5f81\u8fdb\u884c\u4f18\u5316\uff0c\u800c\u975e\u57fa\u4e8e6DoF\u59ff\u6001\u7684\u6e32\u67d3\uff0c\u5e76\u5f15\u5165\u6700\u4f18\u63a7\u5236\u70b9\u56de\u5f52\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "result": "\u5728\u6570\u636e\u96c6\u8bc4\u4f30\u548c\u771f\u5b9e\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u6709\u6548\uff0c\u4e3a\u5de5\u4e1a\u91d1\u5c5e\u7269\u4f53\u7684\u5b9e\u65f6\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e3b\u52a8\u63a7\u5236\u70b9\u548c\u4f18\u5316\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u91d1\u5c5e\u7269\u4f53\u59ff\u6001\u8ddf\u8e2a\u7684\u96be\u9898\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.01887", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01887", "abs": "https://arxiv.org/abs/2507.01887", "authors": ["Dongyi Ding", "Tiannan Wang", "Chenghao Zhu", "Meiling Tao", "Yuchen Eleanor Jiang", "Wangchunshu Zhou"], "title": "MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher Assistants", "comment": "Work in progress", "summary": "Large language models (LLMs) excel at reasoning tasks requiring long thought\nsequences for planning, reflection, and refinement. However, their substantial\nmodel size and high computational demands are impractical for widespread\ndeployment. Yet, small language models (SLMs) often struggle to learn long-form\nCoT reasoning due to their limited capacity, a phenomenon we refer to as the\n\"SLMs Learnability Gap\". To address this, we introduce\n\\textbf{Mi}d-\\textbf{Co}T \\textbf{T}eacher \\textbf{A}ssistant Distillation\n(MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA\nemploys intermediate-sized models as teacher assistants and utilizes\nintermediate-length CoT sequences to bridge both the capacity and reasoning\nlength gaps. Our experiments on downstream tasks demonstrate that although SLMs\ndistilled from large teachers can perform poorly, by applying MiCoTA, they\nachieve significant improvements in reasoning performance. Specifically,\nQwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and\n3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and\nGSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform\na quantitative experiment demonstrating that our method produces data more\nclosely aligned with base SLM distributions. Our insights pave the way for\nfuture research into long-CoT data distillation for SLMs.", "AI": {"tldr": "MiCoTA\u6846\u67b6\u901a\u8fc7\u4e2d\u95f4\u6a21\u578b\u548c\u4e2d\u95f4\u957f\u5ea6\u63a8\u7406\u5e8f\u5217\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\u7684\u957f\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u957f\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5b66\u4e60\u80fd\u529b\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u4e2d\u95f4\u6a21\u578b\u4f5c\u4e3a\u6559\u5e08\u52a9\u624b\uff0c\u7ed3\u5408\u4e2d\u95f4\u957f\u5ea6\u63a8\u7406\u5e8f\u5217\u8fdb\u884c\u84b8\u998f\u3002", "result": "Qwen2.5-7B\u548c3B\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "conclusion": "MiCoTA\u4e3a\u5c0f\u8bed\u8a00\u6a21\u578b\u7684\u957f\u63a8\u7406\u84b8\u998f\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0c\u5e76\u542f\u53d1\u4e86\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2507.01313", "categories": ["cs.LG", "cs.AI", "math.DS", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.01313", "abs": "https://arxiv.org/abs/2507.01313", "authors": ["Qian Qi"], "title": "Neural Hamiltonian Operator", "comment": null, "summary": "Stochastic control problems in high dimensions are notoriously difficult to\nsolve due to the curse of dimensionality. An alternative to traditional dynamic\nprogramming is Pontryagin's Maximum Principle (PMP), which recasts the problem\nas a system of Forward-Backward Stochastic Differential Equations (FBSDEs). In\nthis paper, we introduce a formal framework for solving such problems with deep\nlearning by defining a \\textbf{Neural Hamiltonian Operator (NHO)}. This\noperator parameterizes the coupled FBSDE dynamics via neural networks that\nrepresent the feedback control and an ansatz for the value function's spatial\ngradient. We show how the optimal NHO can be found by training the underlying\nnetworks to enforce the consistency conditions dictated by the PMP. By adopting\nthis operator-theoretic view, we situate the deep FBSDE method within the\nrigorous language of statistical inference, framing it as a problem of learning\nan unknown operator from simulated data. This perspective allows us to prove\nthe universal approximation capabilities of NHOs under general martingale\ndrivers and provides a clear lens for analyzing the significant optimization\nchallenges inherent to this class of models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5b9a\u4e49\u795e\u7ecf\u54c8\u5bc6\u987f\u7b97\u5b50\uff08NHO\uff09\u6765\u89e3\u51b3\u9ad8\u7ef4\u968f\u673a\u63a7\u5236\u95ee\u9898\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u52a8\u6001\u89c4\u5212\u7684\u7ef4\u5ea6\u707e\u96be\u3002", "motivation": "\u9ad8\u7ef4\u968f\u673a\u63a7\u5236\u95ee\u9898\u56e0\u7ef4\u5ea6\u707e\u96be\u96be\u4ee5\u89e3\u51b3\uff0c\u4f20\u7edf\u52a8\u6001\u89c4\u5212\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u795e\u7ecf\u54c8\u5bc6\u987f\u7b97\u5b50\uff08NHO\uff09\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u5316FBSDE\u7cfb\u7edf\uff0c\u5e76\u5229\u7528PMP\u7684\u4e00\u81f4\u6027\u6761\u4ef6\u8bad\u7ec3\u7f51\u7edc\u3002", "result": "\u8bc1\u660e\u4e86NHO\u5728\u4e00\u822c\u9785\u9a71\u52a8\u4e0b\u7684\u901a\u7528\u903c\u8fd1\u80fd\u529b\uff0c\u5e76\u5206\u6790\u4e86\u4f18\u5316\u6311\u6218\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u9ad8\u7ef4\u968f\u673a\u63a7\u5236\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u8df5\u610f\u4e49\u3002"}}
{"id": "2507.01129", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.01129", "abs": "https://arxiv.org/abs/2507.01129", "authors": ["Arun Ganesh", "Brendan McMahan", "Abhradeep Thakurta"], "title": "On Design Principles for Private Adaptive Optimizers", "comment": "PPML 2025", "summary": "The spherical noise added to gradients in differentially private (DP)\ntraining undermines the performance of adaptive optimizers like AdaGrad and\nAdam, and hence many recent works have proposed algorithms to address this\nchallenge. However, the empirical results in these works focus on simple tasks\nand models and the conclusions may not generalize to model training in\npractice. In this paper we survey several of these variants, and develop better\ntheoretical intuition for them as well as perform empirical studies comparing\nthem. We find that a common intuition of aiming for unbiased estimates of\nsecond moments of gradients in adaptive optimizers is misguided, and instead\nthat a simple technique called scale-then-privatize (which does not achieve\nunbiased second moments) has more desirable theoretical behaviors and\noutperforms all other variants we study on a small-scale language model\ntraining task. We additionally argue that scale-then-privatize causes the noise\naddition to better match the application of correlated noise mechanisms which\nare more desirable to use in practice.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u5dee\u5206\u9690\u79c1\u8bad\u7ec3\u4e2d\u7403\u5f62\u566a\u58f0\u5bf9\u81ea\u9002\u5e94\u4f18\u5316\u5668\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cscale-then-privatize\u201d\u7684\u7b80\u5355\u6280\u672f\uff0c\u5176\u7406\u8bba\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5e76\u5728\u5c0f\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u5dee\u5206\u9690\u79c1\u8bad\u7ec3\u4e2d\u7684\u7403\u5f62\u566a\u58f0\u964d\u4f4e\u4e86\u81ea\u9002\u5e94\u4f18\u5316\u5668\uff08\u5982AdaGrad\u548cAdam\uff09\u7684\u6027\u80fd\uff0c\u73b0\u6709\u7814\u7a76\u7ed3\u8bba\u53ef\u80fd\u65e0\u6cd5\u63a8\u5e7f\u5230\u5b9e\u9645\u6a21\u578b\u8bad\u7ec3\u4e2d\u3002", "method": "\u8c03\u67e5\u4e86\u591a\u79cd\u6539\u8fdb\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u7814\u7a76\u6bd4\u8f83\u4e86\u5b83\u4eec\u7684\u6548\u679c\u3002", "result": "\u53d1\u73b0\u8ffd\u6c42\u68af\u5ea6\u4e8c\u9636\u77e9\u65e0\u504f\u4f30\u8ba1\u7684\u76f4\u89c9\u662f\u9519\u8bef\u7684\uff0c\u201cscale-then-privatize\u201d\u6280\u672f\u8868\u73b0\u66f4\u4f18\uff0c\u4e14\u566a\u58f0\u6dfb\u52a0\u66f4\u7b26\u5408\u5b9e\u9645\u9700\u6c42\u3002", "conclusion": "\u201cscale-then-privatize\u201d\u6280\u672f\u5728\u5c0f\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4e14\u566a\u58f0\u673a\u5236\u66f4\u5b9e\u7528\u3002"}}
{"id": "2507.01484", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01484", "abs": "https://arxiv.org/abs/2507.01484", "authors": ["Xiaoshuai Hao", "Yuting Zhao", "Yuheng Ji", "Luanyuan Dai", "Peng Hao", "Dingzhe Li", "Shuai Cheng", "Rong Yin"], "title": "What Really Matters for Robust Multi-Sensor HD Map Construction?", "comment": "Accepted by IROS 2025", "summary": "High-definition (HD) map construction methods are crucial for providing\nprecise and comprehensive static environmental information, which is essential\nfor autonomous driving systems. While Camera-LiDAR fusion techniques have shown\npromising results by integrating data from both modalities, existing approaches\nprimarily focus on improving model accuracy and often neglect the robustness of\nperception models, which is a critical aspect for real-world applications. In\nthis paper, we explore strategies to enhance the robustness of multi-modal\nfusion methods for HD map construction while maintaining high accuracy. We\npropose three key components: data augmentation, a novel multi-modal fusion\nmodule, and a modality dropout training strategy. These components are\nevaluated on a challenging dataset containing 10 days of NuScenes data. Our\nexperimental results demonstrate that our proposed methods significantly\nenhance the robustness of baseline methods. Furthermore, our approach achieves\nstate-of-the-art performance on the clean validation set of the NuScenes\ndataset. Our findings provide valuable insights for developing more robust and\nreliable HD map construction models, advancing their applicability in\nreal-world autonomous driving scenarios. Project website:\nhttps://robomap-123.github.io.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\u9c81\u68d2\u6027\u7684\u7b56\u7565\uff0c\u7528\u4e8e\u9ad8\u6e05\u5730\u56fe\u6784\u5efa\uff0c\u5305\u62ec\u6570\u636e\u589e\u5f3a\u3001\u65b0\u578b\u591a\u6a21\u6001\u878d\u5408\u6a21\u5757\u548c\u6a21\u6001\u4e22\u5f03\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u5728NuScenes\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6a21\u578b\u7cbe\u5ea6\uff0c\u800c\u5ffd\u7565\u4e86\u611f\u77e5\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u8fd9\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u6570\u636e\u589e\u5f3a\u3001\u65b0\u578b\u591a\u6a21\u6001\u878d\u5408\u6a21\u5757\u548c\u6a21\u6001\u4e22\u5f03\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u57fa\u7ebf\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728NuScenes\u6570\u636e\u96c6\u7684\u5e72\u51c0\u9a8c\u8bc1\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u548c\u53ef\u9760\u7684\u9ad8\u6e05\u5730\u56fe\u6784\u5efa\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u63a8\u52a8\u4e86\u5176\u5728\u73b0\u5b9e\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2507.01900", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01900", "abs": "https://arxiv.org/abs/2507.01900", "authors": ["Songtao Liu", "Peng Liu"], "title": "High-Layer Attention Pruning with Rescaling", "comment": null, "summary": "Pruning is a highly effective approach for compressing large language models\n(LLMs), significantly reducing inference latency. However, conventional\ntraining-free structured pruning methods often employ a heuristic metric that\nindiscriminately removes some attention heads across all pruning layers,\nwithout considering their positions within the network architecture. In this\nwork, we propose a novel pruning algorithm that strategically prunes attention\nheads in the model's higher layers. Since the removal of attention heads can\nalter the magnitude of token representations, we introduce an adaptive\nrescaling parameter that calibrates the representation scale post-pruning to\ncounteract this effect. We conduct comprehensive experiments on a wide range of\nLLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our\nevaluation includes both generation and discriminative tasks across 27\ndatasets. The results consistently demonstrate that our method outperforms\nexisting structured pruning methods. This improvement is particularly notable\nin generation tasks, where our approach significantly outperforms existing\nbaselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u526a\u679d\u7b97\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u9ad8\u5c42\u6ce8\u610f\u529b\u5934\u7684\u526a\u679d\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u91cd\u7f29\u653e\u53c2\u6570\u6821\u51c6\u8868\u793a\u89c4\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u526a\u679d\u65b9\u6cd5\u4e0d\u8003\u8651\u6ce8\u610f\u529b\u5934\u5728\u7f51\u7edc\u4e2d\u7684\u4f4d\u7f6e\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u667a\u80fd\u7684\u526a\u679d\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u526a\u679d\u7b97\u6cd5\uff0c\u9488\u5bf9\u9ad8\u5c42\u6ce8\u610f\u529b\u5934\u8fdb\u884c\u526a\u679d\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u91cd\u7f29\u653e\u53c2\u6570\u4ee5\u6821\u51c6\u8868\u793a\u89c4\u6a21\u3002", "result": "\u5728\u591a\u4e2aLLM\u548c27\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u667a\u80fd\u526a\u679d\u548c\u6821\u51c6\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2507.01321", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.01321", "abs": "https://arxiv.org/abs/2507.01321", "authors": ["Zhiyao Ren", "Siyuan Liang", "Aishan Liu", "Dacheng Tao"], "title": "ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks", "comment": "ICML 2025", "summary": "In-context learning (ICL) has demonstrated remarkable success in large\nlanguage models (LLMs) due to its adaptability and parameter-free nature.\nHowever, it also introduces a critical vulnerability to backdoor attacks, where\nadversaries can manipulate LLM behaviors by simply poisoning a few ICL\ndemonstrations. In this paper, we propose, for the first time, the\ndual-learning hypothesis, which posits that LLMs simultaneously learn both the\ntask-relevant latent concepts and backdoor latent concepts within poisoned\ndemonstrations, jointly influencing the probability of model outputs. Through\ntheoretical analysis, we derive an upper bound for ICL backdoor effects,\nrevealing that the vulnerability is dominated by the concept preference ratio\nbetween the task and the backdoor. Motivated by these findings, we propose\nICLShield, a defense mechanism that dynamically adjusts the concept preference\nratio. Our method encourages LLMs to select clean demonstrations during the ICL\nphase by leveraging confidence and similarity scores, effectively mitigating\nsusceptibility to backdoor attacks. Extensive experiments across multiple LLMs\nand tasks demonstrate that our method achieves state-of-the-art defense\neffectiveness, significantly outperforming existing approaches (+26.02% on\naverage). Furthermore, our method exhibits exceptional adaptability and\ndefensive performance even for closed-source models (e.g., GPT-4).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u53cc\u5b66\u4e60\u5047\u8bf4\uff0c\u63ed\u793aLLMs\u5728ICL\u4e2d\u540c\u65f6\u5b66\u4e60\u4efb\u52a1\u76f8\u5173\u548c\u540e\u95e8\u6f5c\u5728\u6982\u5ff5\uff0c\u63d0\u51faICLShield\u9632\u5fa1\u673a\u5236\uff0c\u52a8\u6001\u8c03\u6574\u6982\u5ff5\u504f\u597d\u6bd4\uff0c\u6709\u6548\u62b5\u5fa1\u540e\u95e8\u653b\u51fb\u3002", "motivation": "ICL\u7684\u9002\u5e94\u6027\u548c\u65e0\u53c2\u6570\u7279\u6027\u4f7f\u5176\u6613\u53d7\u540e\u95e8\u653b\u51fb\uff0c\u9700\u7814\u7a76\u9632\u5fa1\u673a\u5236\u3002", "method": "\u63d0\u51fa\u53cc\u5b66\u4e60\u5047\u8bf4\uff0c\u5206\u6790\u540e\u95e8\u5f71\u54cd\u4e0a\u9650\uff0c\u8bbe\u8ba1ICLShield\u52a8\u6001\u8c03\u6574\u6982\u5ff5\u504f\u597d\u6bd4\u3002", "result": "\u5b9e\u9a8c\u663e\u793aICLShield\u5728\u9632\u5fa1\u6548\u679c\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u5e73\u5747\u63d0\u534726.02%\uff09\uff0c\u5bf9\u95ed\u6e90\u6a21\u578b\uff08\u5982GPT-4\uff09\u4e5f\u6709\u6548\u3002", "conclusion": "ICLShield\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6982\u5ff5\u504f\u597d\u6bd4\uff0c\u6709\u6548\u63d0\u5347LLMs\u5bf9\u540e\u95e8\u653b\u51fb\u7684\u9632\u5fa1\u80fd\u529b\u3002"}}
{"id": "2507.01131", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2507.01131", "abs": "https://arxiv.org/abs/2507.01131", "authors": ["Yuchao Lin", "Cong Fu", "Zachary Krueger", "Haiyang Yu", "Maho Nakata", "Jianwen Xie", "Emine Kucukbenli", "Xiaofeng Qian", "Shuiwang Ji"], "title": "Tensor Decomposition Networks for Fast Machine Learning Interatomic Potential Computations", "comment": null, "summary": "$\\rm{SO}(3)$-equivariant networks are the dominant models for machine\nlearning interatomic potentials (MLIPs). The key operation of such networks is\nthe Clebsch-Gordan (CG) tensor product, which is computationally expensive. To\naccelerate the computation, we develop tensor decomposition networks (TDNs) as\na class of approximately equivariant networks whose CG tensor products are\nreplaced by low-rank tensor decompositions, such as the CANDECOMP/PARAFAC (CP)\ndecomposition. With the CP decomposition, we prove (i) a uniform bound on the\ninduced error of $\\rm{SO}(3)$-equivariance, and (ii) the universality of\napproximating any equivariant bilinear map. To further reduce the number of\nparameters, we propose path-weight sharing that ties all multiplicity-space\nweights across the $O(L^3)$ CG paths into a single path without compromising\nequivariance, where $L$ is the maximum angular degree. The resulting layer acts\nas a plug-and-play replacement for tensor products in existing networks, and\nthe computational complexity of tensor products is reduced from $O(L^6)$ to\n$O(L^4)$. We evaluate TDNs on PubChemQCR, a newly curated molecular relaxation\ndataset containing 105 million DFT-calculated snapshots. We also use existing\ndatasets, including OC20, and OC22. Results show that TDNs achieve competitive\nperformance with dramatic speedup in computations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8fd1\u4f3c\u7b49\u53d8\u7684\u5f20\u91cf\u5206\u89e3\u7f51\u7edc\uff08TDNs\uff09\uff0c\u901a\u8fc7\u4f4e\u79e9\u5f20\u91cf\u5206\u89e3\uff08\u5982CP\u5206\u89e3\uff09\u66ff\u4ee3\u8ba1\u7b97\u6602\u8d35\u7684Clebsch-Gordan\u5f20\u91cf\u79ef\uff0c\u663e\u8457\u52a0\u901f\u8ba1\u7b97\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684SO(3)-\u7b49\u53d8\u7f51\u7edc\u5728\u673a\u5668\u5b66\u4e60\u539f\u5b50\u95f4\u52bf\uff08MLIPs\uff09\u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u4f46\u5176\u6838\u5fc3\u64cd\u4f5cClebsch-Gordan\u5f20\u91cf\u79ef\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e9f\u9700\u4f18\u5316\u3002", "method": "\u91c7\u7528CP\u5206\u89e3\u8fd1\u4f3c\u5f20\u91cf\u79ef\uff0c\u63d0\u51fa\u8def\u5f84\u6743\u91cd\u5171\u4eab\u4ee5\u51cf\u5c11\u53c2\u6570\uff0c\u5c06\u8ba1\u7b97\u590d\u6742\u5ea6\u4eceO(L^6)\u964d\u81f3O(L^4)\u3002", "result": "\u5728PubChemQCR\u3001OC20\u548cOC22\u6570\u636e\u96c6\u4e0a\uff0cTDNs\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u52a0\u901f\u8ba1\u7b97\u3002", "conclusion": "TDNs\u4f5c\u4e3a\u73b0\u6709\u7f51\u7edc\u7684\u5373\u63d2\u5373\u7528\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u4e0a\u5747\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2507.01492", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01492", "abs": "https://arxiv.org/abs/2507.01492", "authors": ["Jiyang Tang", "Hengyi Li", "Yifan Du", "Wayne Xin Zhao"], "title": "AVC-DPO: Aligned Video Captioning via Direct Preference Optimization", "comment": null, "summary": "Although video multimodal large language models (video MLLMs) have achieved\nsubstantial progress in video captioning tasks, it remains challenging to\nadjust the focal emphasis of video captions according to human preferences. To\naddress this limitation, we propose Aligned Video Captioning via Direct\nPreference Optimization (AVC-DPO), a post-training framework designed to\nenhance captioning capabilities in video MLLMs through preference alignment.\nOur approach designs enhanced prompts that specifically target temporal\ndynamics and spatial information-two key factors that humans care about when\nwatching a video-thereby incorporating human-centric preferences. AVC-DPO\nleverages the same foundation model's caption generation responses under varied\nprompt conditions to conduct preference-aware training and caption alignment.\nUsing this framework, we have achieved exceptional performance in the\nLOVE@CVPR'25 Workshop Track 1A: Video Detailed Captioning Challenge, achieving\nfirst place on the Video Detailed Captioning (VDC) benchmark according to the\nVDCSCORE evaluation metric.", "AI": {"tldr": "AVC-DPO\u662f\u4e00\u79cd\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u504f\u597d\u5bf9\u9f50\u589e\u5f3a\u89c6\u9891MLLMs\u7684\u6807\u9898\u751f\u6210\u80fd\u529b\uff0c\u7279\u522b\u5173\u6ce8\u65f6\u95f4\u548c\u7a7a\u95f4\u4fe1\u606f\uff0c\u4ee5\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u3002", "motivation": "\u73b0\u6709\u89c6\u9891MLLMs\u5728\u6807\u9898\u751f\u6210\u4efb\u52a1\u4e2d\u96be\u4ee5\u6839\u636e\u4eba\u7c7b\u504f\u597d\u8c03\u6574\u7126\u70b9\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5b9e\u73b0\u504f\u597d\u5bf9\u9f50\u3002", "method": "\u8bbe\u8ba1\u9488\u5bf9\u65f6\u95f4\u548c\u7a7a\u95f4\u4fe1\u606f\u7684\u589e\u5f3a\u63d0\u793a\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u5728\u4e0d\u540c\u63d0\u793a\u6761\u4ef6\u4e0b\u7684\u54cd\u5e94\u8fdb\u884c\u504f\u597d\u611f\u77e5\u8bad\u7ec3\u548c\u5bf9\u9f50\u3002", "result": "\u5728LOVE@CVPR'25 Workshop Track 1A\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cVDC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6392\u540d\u7b2c\u4e00\u3002", "conclusion": "AVC-DPO\u901a\u8fc7\u504f\u597d\u5bf9\u9f50\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891MLLMs\u7684\u6807\u9898\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2507.01903", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01903", "abs": "https://arxiv.org/abs/2507.01903", "authors": ["Qiguang Chen", "Mingda Yang", "Libo Qin", "Jinhao Liu", "Zheng Yan", "Jiannan Guan", "Dengyun Peng", "Yiyan Ji", "Hanjing Li", "Mengkang Hu", "Yimeng Zhang", "Yihao Liang", "Yuhang Zhou", "Jiaqi Wang", "Zhi Chen", "Wanxiang Che"], "title": "AI4Research: A Survey of Artificial Intelligence for Scientific Research", "comment": "Preprint", "summary": "Recent advancements in artificial intelligence (AI), particularly in large\nlanguage models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated\nremarkable capabilities in complex domains such as logical reasoning and\nexperimental coding. Motivated by these advancements, numerous studies have\nexplored the application of AI in the innovation process, particularly in the\ncontext of scientific research. These AI technologies primarily aim to develop\nsystems that can autonomously conduct research processes across a wide range of\nscientific disciplines. Despite these significant strides, a comprehensive\nsurvey on AI for Research (AI4Research) remains absent, which hampers our\nunderstanding and impedes further development in this field. To address this\ngap, we present a comprehensive survey and offer a unified perspective on\nAI4Research. Specifically, the main contributions of our work are as follows:\n(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify\nfive mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key\nresearch gaps and highlight promising future directions, focusing on the rigor\nand scalability of automated experiments, as well as the societal impact. (3)\nAbundant applications and resources: Finally, we compile a wealth of resources,\nincluding relevant multidisciplinary applications, data corpora, and tools. We\nhope our work will provide the research community with quick access to these\nresources and stimulate innovative breakthroughs in AI4Research.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86AI\u5728\u79d1\u7814\u4e2d\u7684\u5e94\u7528\uff08AI4Research\uff09\uff0c\u63d0\u51fa\u4e86\u7cfb\u7edf\u5206\u7c7b\u6cd5\uff0c\u6307\u51fa\u4e86\u7814\u7a76\u7a7a\u767d\u548c\u672a\u6765\u65b9\u5411\uff0c\u5e76\u6574\u7406\u4e86\u4e30\u5bcc\u7684\u8d44\u6e90\u3002", "motivation": "AI\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5c24\u5176\u662f\u5927\u8bed\u8a00\u6a21\u578b\u5728\u903b\u8f91\u63a8\u7406\u548c\u5b9e\u9a8c\u7f16\u7801\u4e2d\u7684\u8868\u73b0\uff0c\u6fc0\u53d1\u4e86AI\u5728\u79d1\u7814\u521b\u65b0\u4e2d\u7684\u5e94\u7528\u9700\u6c42\u3002\u7136\u800c\uff0c\u7f3a\u4e4f\u5168\u9762\u7efc\u8ff0\u963b\u788d\u4e86\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5206\u7c7b\u6cd5\u5bf9AI4Research\u7684\u4e94\u7c7b\u4e3b\u6d41\u4efb\u52a1\u8fdb\u884c\u5206\u7c7b\uff0c\u8bc6\u522b\u7814\u7a76\u7a7a\u767d\uff0c\u5e76\u6574\u7406\u591a\u5b66\u79d1\u5e94\u7528\u3001\u6570\u636e\u548c\u5de5\u5177\u8d44\u6e90\u3002", "result": "\u63d0\u51fa\u4e86AI4Research\u7684\u7edf\u4e00\u89c6\u89d2\uff0c\u660e\u786e\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u8d44\u6e90\u5e93\u3002", "conclusion": "\u672c\u6587\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u5feb\u901f\u8bbf\u95ee\u8d44\u6e90\u7684\u9014\u5f84\uff0c\u6709\u671b\u63a8\u52a8AI4Research\u9886\u57df\u7684\u521b\u65b0\u7a81\u7834\u3002"}}
{"id": "2507.01327", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01327", "abs": "https://arxiv.org/abs/2507.01327", "authors": ["Xiaoyun Zhang", "Jingqing Ruan", "Xing Ma", "Yawen Zhu", "Jiansong Chen", "Ke Zeng", "Xunliang Cai"], "title": "Reasoner for Real-World Event Detection: Scaling Reinforcement Learning via Adaptive Perplexity-Aware Sampling Strategy", "comment": "15 pages, 6 figures, submitted to EMNLP", "summary": "Detecting abnormal events in real-world customer service dialogues is highly\nchallenging due to the complexity of business data and the dynamic nature of\ncustomer interactions. Moreover, models must demonstrate strong out-of-domain\n(OOD) generalization to enable rapid adaptation across different business\nscenarios and maximize commercial value. In this work, we propose a novel\nAdaptive Perplexity-Aware Reinforcement Learning (APARL) framework that\nleverages the advanced reasoning capabilities of large language models for\nabnormal event detection. APARL introduces a dual-loop dynamic curriculum\nlearning architecture, enabling the model to progressively focus on more\nchallenging samples as its proficiency increases. This design effectively\naddresses performance bottlenecks and significantly enhances OOD\ntransferability. Extensive evaluations on food delivery dialogue tasks show\nthat our model achieves significantly enhanced adaptability and robustness,\nattaining the highest F1 score with an average improvement of 17.19\\%, and an\naverage improvement of 9.59\\% in OOD transfer tests. This method provides a\nsuperior solution for industrial deployment of anomaly detection models,\ncontributing to improved operational efficiency and commercial benefits.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u9002\u5e94\u56f0\u60d1\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\uff08APARL\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5ba2\u6237\u670d\u52a1\u5bf9\u8bdd\u4e2d\u7684\u5f02\u5e38\u4e8b\u4ef6\u68c0\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u7531\u4e8e\u4e1a\u52a1\u6570\u636e\u7684\u590d\u6742\u6027\u548c\u5ba2\u6237\u4e92\u52a8\u7684\u52a8\u6001\u6027\uff0c\u68c0\u6d4b\u5f02\u5e38\u4e8b\u4ef6\u5177\u6709\u6311\u6218\u6027\uff0c\u4e14\u6a21\u578b\u9700\u5177\u5907\u5f3a\u6cdb\u5316\u80fd\u529b\u4ee5\u9002\u5e94\u4e0d\u540c\u5546\u4e1a\u573a\u666f\u3002", "method": "\u91c7\u7528\u53cc\u73af\u52a8\u6001\u8bfe\u7a0b\u5b66\u4e60\u67b6\u6784\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u9010\u6b65\u805a\u7126\u66f4\u5177\u6311\u6218\u6027\u7684\u6837\u672c\u3002", "result": "\u5728\u98df\u54c1\u914d\u9001\u5bf9\u8bdd\u4efb\u52a1\u4e2d\uff0cF1\u5206\u6570\u5e73\u5747\u63d0\u534717.19%\uff0cOOD\u8fc1\u79fb\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u53479.59%\u3002", "conclusion": "APARL\u4e3a\u5de5\u4e1a\u7ea7\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u5347\u4e86\u8fd0\u8425\u6548\u7387\u548c\u5546\u4e1a\u4ef7\u503c\u3002"}}
{"id": "2507.01132", "categories": ["cs.LG", "q-bio.MN"], "pdf": "https://arxiv.org/pdf/2507.01132", "abs": "https://arxiv.org/abs/2507.01132", "authors": ["Brenda Nogueira", "Gabe Gomes", "Meng Jiang", "Nitesh V. Chawla", "Nuno Moniz"], "title": "Spectral Manifold Harmonization for Graph Imbalanced Regression", "comment": null, "summary": "Graph-structured data is ubiquitous in scientific domains, where models often\nface imbalanced learning settings. In imbalanced regression, domain preferences\nfocus on specific target value ranges representing the most scientifically\nvaluable cases; we observe a significant lack of research. In this paper, we\npresent Spectral Manifold Harmonization (SMH), a novel approach for addressing\nthis imbalanced regression challenge on graph-structured data by generating\nsynthetic graph samples that preserve topological properties while focusing on\noften underrepresented target distribution regions. Conventional methods fail\nin this context because they either ignore graph topology in case generation or\ndo not target specific domain ranges, resulting in models biased toward average\ntarget values. Experimental results demonstrate the potential of SMH on\nchemistry and drug discovery benchmark datasets, showing consistent\nimprovements in predictive performance for target domain ranges.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSpectral Manifold Harmonization (SMH)\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u56fe\u7ed3\u6784\u6570\u636e\u4e2d\u7684\u4e0d\u5e73\u8861\u56de\u5f52\u95ee\u9898\uff0c\u901a\u8fc7\u751f\u6210\u5408\u6210\u6837\u672c\u4ee5\u4fdd\u7559\u62d3\u6251\u7279\u6027\u5e76\u5173\u6ce8\u76ee\u6807\u5206\u5e03\u4e2d\u672a\u88ab\u5145\u5206\u4ee3\u8868\u7684\u533a\u57df\u3002", "motivation": "\u5728\u79d1\u5b66\u9886\u57df\u4e2d\uff0c\u56fe\u7ed3\u6784\u6570\u636e\u666e\u904d\u5b58\u5728\uff0c\u4f46\u6a21\u578b\u5e38\u9762\u4e34\u4e0d\u5e73\u8861\u5b66\u4e60\u95ee\u9898\uff0c\u5c24\u5176\u662f\u76ee\u6807\u503c\u8303\u56f4\u504f\u597d\u5bfc\u81f4\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "SMH\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u5408\u6210\u56fe\u6837\u672c\uff0c\u4fdd\u7559\u62d3\u6251\u7279\u6027\u5e76\u4e13\u6ce8\u4e8e\u76ee\u6807\u5206\u5e03\u4e2d\u672a\u88ab\u5145\u5206\u4ee3\u8868\u7684\u533a\u57df\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSMH\u5728\u5316\u5b66\u548c\u836f\u7269\u53d1\u73b0\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u76ee\u6807\u57df\u8303\u56f4\u7684\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "SMH\u4e3a\u89e3\u51b3\u56fe\u7ed3\u6784\u6570\u636e\u4e2d\u7684\u4e0d\u5e73\u8861\u56de\u5f52\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5173\u6ce8\u7279\u5b9a\u76ee\u6807\u8303\u56f4\u65f6\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2507.01494", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01494", "abs": "https://arxiv.org/abs/2507.01494", "authors": ["Muhammad Hassam Ejaz", "Muhammad Bilal", "Usman Habib"], "title": "Crop Pest Classification Using Deep Learning Techniques: A Review", "comment": null, "summary": "Insect pests continue to bring a serious threat to crop yields around the\nworld, and traditional methods for monitoring them are often slow, manual, and\ndifficult to scale. In recent years, deep learning has emerged as a powerful\nsolution, with techniques like convolutional neural networks (CNNs), vision\ntransformers (ViTs), and hybrid models gaining popularity for automating pest\ndetection. This review looks at 37 carefully selected studies published between\n2018 and 2025, all focused on AI-based pest classification. The selected\nresearch is organized by crop type, pest species, model architecture, dataset\nusage, and key technical challenges. The early studies relied heavily on CNNs\nbut latest work is shifting toward hybrid and transformer-based models that\ndeliver higher accuracy and better contextual understanding. Still, challenges\nlike imbalanced datasets, difficulty in detecting small pests, limited\ngeneralizability, and deployment on edge devices remain significant hurdles.\nOverall, this review offers a structured overview of the field, highlights\nuseful datasets, and outlines the key challenges and future directions for\nAI-based pest monitoring systems.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e862018\u81f32025\u5e74\u95f437\u9879\u5173\u4e8eAI\u5bb3\u866b\u5206\u7c7b\u7684\u7814\u7a76\uff0c\u63a2\u8ba8\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u5bb3\u866b\u76d1\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u5305\u62ecCNN\u3001ViT\u548c\u6df7\u5408\u6a21\u578b\uff0c\u5e76\u603b\u7ed3\u4e86\u5f53\u524d\u6311\u6218\u4e0e\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u4f20\u7edf\u5bb3\u866b\u76d1\u6d4b\u65b9\u6cd5\u6548\u7387\u4f4e\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u4e3a\u81ea\u52a8\u5316\u5bb3\u866b\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5206\u679037\u9879\u7814\u7a76\uff0c\u6309\u4f5c\u7269\u7c7b\u578b\u3001\u5bb3\u866b\u79cd\u7c7b\u3001\u6a21\u578b\u67b6\u6784\u3001\u6570\u636e\u96c6\u4f7f\u7528\u548c\u6280\u672f\u6311\u6218\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u65e9\u671f\u7814\u7a76\u591a\u4f9d\u8d56CNN\uff0c\u8fd1\u671f\u8f6c\u5411\u6df7\u5408\u548cTransformer\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\uff0c\u4f46\u4ecd\u9762\u4e34\u6570\u636e\u96c6\u4e0d\u5e73\u8861\u3001\u5c0f\u5bb3\u866b\u68c0\u6d4b\u96be\u7b49\u6311\u6218\u3002", "conclusion": "AI\u5bb3\u866b\u76d1\u6d4b\u7cfb\u7edf\u5728\u6280\u672f\u4e0a\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u9700\u89e3\u51b3\u901a\u7528\u6027\u548c\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u7b49\u6311\u6218\uff0c\u672a\u6765\u7814\u7a76\u65b9\u5411\u660e\u786e\u3002"}}
{"id": "2507.01915", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01915", "abs": "https://arxiv.org/abs/2507.01915", "authors": ["Chengao Li", "Hanyu Zhang", "Yunkun Xu", "Hongyan Xue", "Xiang Ao", "Qing He"], "title": "Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models", "comment": "19 pages, 3 figures. Accepted by ACL 2025 (main)", "summary": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful\ntechnique for aligning large language models (LLMs) with human preferences.\nHowever, effectively aligning LLMs with diverse human preferences remains a\nsignificant challenge, particularly when they are conflict. To address this\nissue, we frame human value alignment as a multi-objective optimization\nproblem, aiming to maximize a set of potentially conflicting objectives. We\nintroduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning\nparadigm that employs multiple-gradient descent to align LLMs with diverse\npreference distributions. GAPO adaptively rescales the gradients for each\nobjective to determine an update direction that optimally balances the\ntrade-offs between objectives. Additionally, we introduce P-GAPO, which\nincorporates user preferences across different objectives and achieves Pareto\nsolutions that better align with the user's specific needs. Our theoretical\nanalysis demonstrates that GAPO converges towards a Pareto optimal solution for\nmultiple objectives. Empirical results on Mistral-7B show that GAPO outperforms\ncurrent state-of-the-art methods, achieving superior performance in both\nhelpfulness and harmlessness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGAPO\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u591a\u76ee\u6807\u51b2\u7a81\uff0c\u5b9e\u73b0LLM\u4e0e\u591a\u6837\u5316\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\u3002", "motivation": "\u89e3\u51b3LLM\u4e0e\u591a\u6837\u5316\u4e14\u53ef\u80fd\u51b2\u7a81\u7684\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u7684\u6311\u6218\u3002", "method": "\u5f15\u5165GAPO\u548cP-GAPO\uff0c\u5229\u7528\u591a\u68af\u5ea6\u4e0b\u964d\u81ea\u9002\u5e94\u8c03\u6574\u68af\u5ea6\uff0c\u5e73\u8861\u76ee\u6807\u95f4\u7684\u6743\u8861\u3002", "result": "GAPO\u5728Mistral-7B\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u5e2e\u52a9\u6027\u548c\u65e0\u5bb3\u6027\u4e0a\u5747\u53d6\u5f97\u4f18\u5f02\u7ed3\u679c\u3002", "conclusion": "GAPO\u80fd\u6709\u6548\u6536\u655b\u81f3Pareto\u6700\u4f18\u89e3\uff0c\u4e3aLLM\u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.01154", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.01154", "abs": "https://arxiv.org/abs/2507.01154", "authors": ["Liangyu Wang", "Junxiao Wang", "Jie Ren", "Zihang Xiang", "David E. Keyes", "Di Wang"], "title": "FlashDP: Private Training Large Language Models with Efficient DP-SGD", "comment": null, "summary": "As large language models (LLMs) increasingly underpin technological\nadvancements, the privacy of their training data emerges as a critical concern.\nDifferential Privacy (DP) serves as a rigorous mechanism to protect this data,\nyet its integration via Differentially Private Stochastic Gradient Descent\n(DP-SGD) introduces substantial challenges, primarily due to the complexities\nof per-sample gradient clipping. Current explicit methods, such as Opacus,\nnecessitate extensive storage for per-sample gradients, significantly inflating\nmemory requirements. Conversely, implicit methods like GhostClip reduce storage\nneeds by recalculating gradients multiple times, which leads to inefficiencies\ndue to redundant computations. This paper introduces FlashDP, an innovative\ncache-friendly per-layer DP-SGD that consolidates necessary operations into a\nsingle task, calculating gradients only once in a fused manner. This approach\nnot only diminishes memory movement by up to \\textbf{50\\%} but also cuts down\nredundant computations by \\textbf{20\\%}, compared to previous methods.\nConsequently, FlashDP does not increase memory demands and achieves a\n\\textbf{90\\%} throughput compared to the Non-DP method on a four-A100 system\nduring the pre-training of the Llama-13B model, while maintaining parity with\nstandard per-layer clipped DP-SGD in terms of accuracy. These advancements\nestablish FlashDP as a pivotal development for efficient and privacy-preserving\ntraining of LLMs. FlashDP's code has been open-sourced in\nhttps://github.com/kaustpradalab/flashdp.", "AI": {"tldr": "FlashDP\u662f\u4e00\u79cd\u521b\u65b0\u7684\u7f13\u5b58\u53cb\u597d\u578bDP-SGD\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u6b21\u878d\u5408\u8ba1\u7b97\u68af\u5ea6\uff0c\u663e\u8457\u51cf\u5c11\u5185\u5b58\u9700\u6c42\u548c\u5197\u4f59\u8ba1\u7b97\uff0c\u540c\u65f6\u4fdd\u6301\u9690\u79c1\u4fdd\u62a4\u6548\u679c\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u8bad\u7ec3\u6570\u636e\u7684\u9690\u79c1\u4fdd\u62a4\u6210\u4e3a\u5173\u952e\u95ee\u9898\u3002\u73b0\u6709DP-SGD\u65b9\u6cd5\uff08\u5982Opacus\u548cGhostClip\uff09\u5728\u5185\u5b58\u6216\u8ba1\u7b97\u6548\u7387\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "FlashDP\u91c7\u7528\u7f13\u5b58\u53cb\u597d\u7684\u9010\u5c42DP-SGD\uff0c\u5c06\u5fc5\u8981\u64cd\u4f5c\u6574\u5408\u4e3a\u5355\u4efb\u52a1\uff0c\u4ec5\u9700\u4e00\u6b21\u878d\u5408\u8ba1\u7b97\u68af\u5ea6\uff0c\u51cf\u5c11\u5185\u5b58\u79fb\u52a8\u548c\u5197\u4f59\u8ba1\u7b97\u3002", "result": "FlashDP\u5728Llama-13B\u6a21\u578b\u9884\u8bad\u7ec3\u4e2d\uff0c\u5185\u5b58\u9700\u6c42\u4e0d\u589e\u52a0\uff0c\u541e\u5410\u91cf\u8fbe\u5230\u975eDP\u65b9\u6cd5\u768490%\uff0c\u4e14\u7cbe\u5ea6\u4e0e\u6807\u51c6DP-SGD\u76f8\u5f53\u3002", "conclusion": "FlashDP\u4e3a\u9ad8\u6548\u4e14\u9690\u79c1\u4fdd\u62a4\u7684LLM\u8bad\u7ec3\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u5176\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.01496", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01496", "abs": "https://arxiv.org/abs/2507.01496", "authors": ["Jimyeong Kim", "Jungwon Park", "Yeji Song", "Nojun Kwak", "Wonjong Rhee"], "title": "ReFlex: Text-Guided Editing of Real Images in Rectified Flow via Mid-Step Feature Extraction and Attention Adaptation", "comment": "Published at ICCV 2025. Project page:\n  https://wlaud1001.github.io/ReFlex/", "summary": "Rectified Flow text-to-image models surpass diffusion models in image quality\nand text alignment, but adapting ReFlow for real-image editing remains\nchallenging. We propose a new real-image editing method for ReFlow by analyzing\nthe intermediate representations of multimodal transformer blocks and\nidentifying three key features. To extract these features from real images with\nsufficient structural preservation, we leverage mid-step latent, which is\ninverted only up to the mid-step. We then adapt attention during injection to\nimprove editability and enhance alignment to the target text. Our method is\ntraining-free, requires no user-provided mask, and can be applied even without\na source prompt. Extensive experiments on two benchmarks with nine baselines\ndemonstrate its superior performance over prior methods, further validated by\nhuman evaluations confirming a strong user preference for our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684ReFlow\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u591a\u6a21\u6001Transformer\u5757\u7684\u4e2d\u95f4\u8868\u793a\uff0c\u63d0\u53d6\u5173\u952e\u7279\u5f81\uff0c\u65e0\u9700\u8bad\u7ec3\u6216\u7528\u6237\u63d0\u4f9b\u63a9\u7801\uff0c\u663e\u8457\u63d0\u5347\u7f16\u8f91\u6548\u679c\u3002", "motivation": "\u5c3d\u7ba1ReFlow\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u6587\u672c\u5bf9\u9f50\u4e0a\u4f18\u4e8e\u6269\u6563\u6a21\u578b\uff0c\u4f46\u5b9e\u9645\u56fe\u50cf\u7f16\u8f91\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u5229\u7528\u4e2d\u95f4\u6f5c\u5728\u8868\u793a\u63d0\u53d6\u5173\u952e\u7279\u5f81\uff0c\u8c03\u6574\u6ce8\u610f\u529b\u673a\u5236\u4ee5\u63d0\u5347\u7f16\u8f91\u6027\u548c\u76ee\u6807\u6587\u672c\u5bf9\u9f50\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u4e5d\u79cd\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4eba\u7c7b\u8bc4\u4f30\u663e\u793a\u7528\u6237\u504f\u597d\u8be5\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u8bad\u7ec3\u6216\u63a9\u7801\uff0c\u9002\u7528\u4e8e\u65e0\u6e90\u63d0\u793a\u7684\u60c5\u51b5\uff0c\u6027\u80fd\u4f18\u8d8a\u3002"}}
{"id": "2507.01921", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01921", "abs": "https://arxiv.org/abs/2507.01921", "authors": ["Yang Li", "Youssef Emad", "Karthik Padthe", "Jack Lanchantin", "Weizhe Yuan", "Thao Nguyen", "Jason Weston", "Shang-Wen Li", "Dong Wang", "Ilia Kulikov", "Xian Li"], "title": "NaturalThoughts: Selecting and Distilling Reasoning Traces for General Reasoning Tasks", "comment": null, "summary": "Recent work has shown that distilling reasoning traces from a larger teacher\nmodel via supervised finetuning outperforms reinforcement learning with the\nsmaller student model alone (Guo et al. 2025). However, there has not been a\nsystematic study of what kind of reasoning demonstrations from the teacher are\nmost effective in improving the student model's reasoning capabilities. In this\nwork we curate high-quality \"NaturalThoughts\" by selecting reasoning traces\nfrom a strong teacher model based on a large pool of questions from\nNaturalReasoning (Yuan et al. 2025). We first conduct a systematic analysis of\nfactors that affect distilling reasoning capabilities, in terms of sample\nefficiency and scalability for general reasoning tasks. We observe that simply\nscaling up data size with random sampling is a strong baseline with steady\nperformance gains. Further, we find that selecting difficult examples that\nrequire more diverse reasoning strategies is more sample-efficient to transfer\nthe teacher model's reasoning skills. Evaluated on both Llama and Qwen models,\ntraining with NaturalThoughts outperforms existing reasoning datasets such as\nOpenThoughts, LIMO, etc. on general STEM reasoning benchmarks including\nGPQA-Diamond, MMLU-Pro and SuperGPQA.", "AI": {"tldr": "\u901a\u8fc7\u4ece\u6559\u5e08\u6a21\u578b\u4e2d\u9009\u62e9\u9ad8\u8d28\u91cf\u63a8\u7406\u8f68\u8ff9\uff08NaturalThoughts\uff09\uff0c\u63d0\u5347\u5b66\u751f\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f18\u4e8e\u968f\u673a\u91c7\u6837\u548c\u73b0\u6709\u6570\u636e\u96c6\u3002", "motivation": "\u7814\u7a76\u6559\u5e08\u6a21\u578b\u63d0\u4f9b\u7684\u63a8\u7406\u6f14\u793a\u4e2d\u54ea\u4e9b\u7c7b\u578b\u6700\u6709\u6548\uff0c\u4ee5\u63d0\u5347\u5b66\u751f\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4ece\u6559\u5e08\u6a21\u578b\u4e2d\u7b5b\u9009\u9ad8\u8d28\u91cf\u63a8\u7406\u8f68\u8ff9\uff08NaturalThoughts\uff09\uff0c\u5206\u6790\u6837\u672c\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u6bd4\u8f83\u968f\u673a\u91c7\u6837\u4e0e\u9009\u62e9\u56f0\u96be\u6837\u672c\u7684\u6548\u679c\u3002", "result": "NaturalThoughts\u5728Llama\u548cQwen\u6a21\u578b\u4e0a\u4f18\u4e8e\u73b0\u6709\u6570\u636e\u96c6\uff0c\u5c24\u5176\u5728\u9700\u8981\u591a\u6837\u63a8\u7406\u7b56\u7565\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u9009\u62e9\u9ad8\u8d28\u91cf\u4e14\u591a\u6837\u5316\u7684\u63a8\u7406\u6837\u672c\u80fd\u66f4\u9ad8\u6548\u5730\u63d0\u5347\u5b66\u751f\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2507.01178", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01178", "abs": "https://arxiv.org/abs/2507.01178", "authors": ["Alec Helbling", "Duen Horng Chau"], "title": "Diffusion Explorer: Interactive Exploration of Diffusion Models", "comment": null, "summary": "Diffusion models have been central to the development of recent image, video,\nand even text generation systems. They posses striking geometric properties\nthat can be faithfully portrayed in low-dimensional settings. However, existing\nresources for explaining diffusion either require an advanced theoretical\nfoundation or focus on their neural network architectures rather than their\nrich geometric properties. We introduce Diffusion Explorer, an interactive tool\nto explain the geometric properties of diffusion models. Users can train 2D\ndiffusion models in the browser and observe the temporal dynamics of their\nsampling process. Diffusion Explorer leverages interactive animation, which has\nbeen shown to be a powerful tool for making engaging visualizations of dynamic\nsystems, making it well suited to explaining diffusion models which represent\nstochastic processes that evolve over time. Diffusion Explorer is open source\nand a live demo is available at alechelbling.com/Diffusion-Explorer.", "AI": {"tldr": "Diffusion Explorer\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u5de5\u5177\uff0c\u7528\u4e8e\u89e3\u91ca\u6269\u6563\u6a21\u578b\u7684\u51e0\u4f55\u7279\u6027\uff0c\u7528\u6237\u53ef\u5728\u6d4f\u89c8\u5668\u4e2d\u8bad\u7ec32D\u6269\u6563\u6a21\u578b\u5e76\u89c2\u5bdf\u91c7\u6837\u8fc7\u7a0b\u7684\u52a8\u6001\u3002", "motivation": "\u73b0\u6709\u8d44\u6e90\u8981\u4e48\u9700\u8981\u9ad8\u7ea7\u7406\u8bba\u57fa\u7840\uff0c\u8981\u4e48\u5173\u6ce8\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u800c\u975e\u51e0\u4f55\u7279\u6027\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u76f4\u89c2\u7684\u5de5\u5177\u6765\u89e3\u91ca\u6269\u6563\u6a21\u578b\u7684\u51e0\u4f55\u7279\u6027\u3002", "method": "\u5f00\u53d1\u4e86Diffusion Explorer\uff0c\u5229\u7528\u4ea4\u4e92\u5f0f\u52a8\u753b\u5c55\u793a\u6269\u6563\u6a21\u578b\u7684\u52a8\u6001\u7cfb\u7edf\u7279\u6027\u3002", "result": "\u7528\u6237\u53ef\u901a\u8fc7\u6d4f\u89c8\u5668\u76f4\u63a5\u4f53\u9a8c\u6269\u6563\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u91c7\u6837\u8fc7\u7a0b\uff0c\u5de5\u5177\u5f00\u6e90\u5e76\u63d0\u4f9b\u5728\u7ebf\u6f14\u793a\u3002", "conclusion": "Diffusion Explorer\u4e3a\u7406\u89e3\u548c\u6559\u5b66\u6269\u6563\u6a21\u578b\u7684\u51e0\u4f55\u7279\u6027\u63d0\u4f9b\u4e86\u76f4\u89c2\u4e14\u4e92\u52a8\u7684\u65b9\u5f0f\u3002"}}
{"id": "2507.01502", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01502", "abs": "https://arxiv.org/abs/2507.01502", "authors": ["Ozan Durgut", "Beril Kallfelz-Sirmacek", "Cem Unsalan"], "title": "Integrating Traditional and Deep Learning Methods to Detect Tree Crowns in Satellite Images", "comment": "11 pages, 4 figures, journal manuscript", "summary": "Global warming, loss of biodiversity, and air pollution are among the most\nsignificant problems facing Earth. One of the primary challenges in addressing\nthese issues is the lack of monitoring forests to protect them. To tackle this\nproblem, it is important to leverage remote sensing and computer vision methods\nto automate monitoring applications. Hence, automatic tree crown detection\nalgorithms emerged based on traditional and deep learning methods. In this\nstudy, we first introduce two different tree crown detection methods based on\nthese approaches. Then, we form a novel rule-based approach that integrates\nthese two methods to enhance robustness and accuracy of tree crown detection\nresults. While traditional methods are employed for feature extraction and\nsegmentation of forested areas, deep learning methods are used to detect tree\ncrowns in our method. With the proposed rule-based approach, we post-process\nthese results, aiming to increase the number of detected tree crowns through\nneighboring trees and localized operations. We compare the obtained results\nwith the proposed method in terms of the number of detected tree crowns and\nreport the advantages, disadvantages, and areas for improvement of the obtained\noutcomes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4f20\u7edf\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u89c4\u5219\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u6811\u51a0\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5168\u7403\u53d8\u6696\u3001\u751f\u7269\u591a\u6837\u6027\u4e27\u5931\u548c\u7a7a\u6c14\u6c61\u67d3\u7b49\u95ee\u9898\u4e9f\u9700\u68ee\u6797\u76d1\u6d4b\uff0c\u4f46\u7f3a\u4e4f\u81ea\u52a8\u5316\u76d1\u6d4b\u624b\u6bb5\uff0c\u56e0\u6b64\u9700\u8981\u5229\u7528\u9065\u611f\u4e0e\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u3002", "method": "\u7ed3\u5408\u4f20\u7edf\u65b9\u6cd5\u7684\u7279\u5f81\u63d0\u53d6\u4e0e\u5206\u5272\uff0c\u4ee5\u53ca\u6df1\u5ea6\u5b66\u4e60\u7684\u6811\u51a0\u68c0\u6d4b\uff0c\u5f62\u6210\u89c4\u5219\u5316\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u90bb\u8fd1\u6811\u548c\u5c40\u90e8\u64cd\u4f5c\u63d0\u9ad8\u68c0\u6d4b\u6570\u91cf\u3002", "result": "\u4e0e\u4f20\u7edf\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u76f8\u6bd4\uff0c\u63d0\u51fa\u7684\u89c4\u5219\u5316\u65b9\u6cd5\u5728\u6811\u51a0\u68c0\u6d4b\u6570\u91cf\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "\u89c4\u5219\u5316\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u6811\u51a0\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u3002"}}
{"id": "2507.01923", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01923", "abs": "https://arxiv.org/abs/2507.01923", "authors": ["Yu-Shiang Huang", "Chuan-Ju Wang", "Chung-Chi Chen"], "title": "Decision-oriented Text Evaluation", "comment": null, "summary": "Natural language generation (NLG) is increasingly deployed in high-stakes\ndomains, yet common intrinsic evaluation methods, such as n-gram overlap or\nsentence plausibility, weakly correlate with actual decision-making efficacy.\nWe propose a decision-oriented framework for evaluating generated text by\ndirectly measuring its influence on human and large language model (LLM)\ndecision outcomes. Using market digest texts--including objective morning\nsummaries and subjective closing-bell analyses--as test cases, we assess\ndecision quality based on the financial performance of trades executed by human\ninvestors and autonomous LLM agents informed exclusively by these texts. Our\nfindings reveal that neither humans nor LLM agents consistently surpass random\nperformance when relying solely on summaries. However, richer analytical\ncommentaries enable collaborative human-LLM teams to outperform individual\nhuman or agent baselines significantly. Our approach underscores the importance\nof evaluating generated text by its ability to facilitate synergistic\ndecision-making between humans and LLMs, highlighting critical limitations of\ntraditional intrinsic metrics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51b3\u7b56\u6548\u679c\u7684\u6587\u672c\u751f\u6210\u8bc4\u4f30\u6846\u67b6\uff0c\u53d1\u73b0\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u4e0e\u5b9e\u9645\u51b3\u7b56\u6548\u679c\u76f8\u5173\u6027\u8f83\u5f31\uff0c\u800c\u4eba\u7c7b\u4e0eLLM\u534f\u4f5c\u80fd\u663e\u8457\u63d0\u5347\u51b3\u7b56\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u7684\u5185\u5728\u8bc4\u4f30\u65b9\u6cd5\uff08\u5982n-gram\u91cd\u53e0\u6216\u53e5\u5b50\u5408\u7406\u6027\uff09\u4e0e\u5b9e\u9645\u51b3\u7b56\u6548\u679c\u76f8\u5173\u6027\u8f83\u5f31\uff0c\u9700\u8981\u66f4\u76f4\u63a5\u7684\u8bc4\u4f30\u65b9\u5f0f\u3002", "method": "\u901a\u8fc7\u5e02\u573a\u6458\u8981\u6587\u672c\uff08\u5305\u62ec\u5ba2\u89c2\u7684\u65e9\u95f4\u603b\u7ed3\u548c\u4e3b\u89c2\u7684\u6536\u76d8\u5206\u6790\uff09\u6d4b\u8bd5\u4eba\u7c7b\u6295\u8d44\u8005\u548cLLM\u4ee3\u7406\u7684\u51b3\u7b56\u8d28\u91cf\u3002", "result": "\u4eba\u7c7b\u548cLLM\u4ee3\u7406\u5355\u72ec\u4f9d\u8d56\u6458\u8981\u65f6\u8868\u73b0\u4e0d\u4f18\u4e8e\u968f\u673a\uff1b\u4f46\u7ed3\u5408\u66f4\u4e30\u5bcc\u7684\u5206\u6790\u8bc4\u8bba\u540e\uff0c\u4eba\u7c7b-LLM\u534f\u4f5c\u56e2\u961f\u663e\u8457\u4f18\u4e8e\u5355\u72ec\u4e2a\u4f53\u3002", "conclusion": "\u8bc4\u4f30\u751f\u6210\u6587\u672c\u5e94\u5173\u6ce8\u5176\u4fc3\u8fdb\u4eba\u7c7b\u4e0eLLM\u534f\u540c\u51b3\u7b56\u7684\u80fd\u529b\uff0c\u4f20\u7edf\u5185\u5728\u6307\u6807\u5b58\u5728\u5c40\u9650\u6027\u3002"}}
{"id": "2507.01381", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01381", "abs": "https://arxiv.org/abs/2507.01381", "authors": ["Tong Liu", "Yinuo Wang", "Xujie Song", "Wenjun Zou", "Liangfa Chen", "Likun Wang", "Bin Shuai", "Jingliang Duan", "Shengbo Eben Li"], "title": "Distributional Soft Actor-Critic with Diffusion Policy", "comment": "Accepted IEEE ITSC 2025", "summary": "Reinforcement learning has been proven to be highly effective in handling\ncomplex control tasks. Traditional methods typically use unimodal\ndistributions, such as Gaussian distributions, to model the output of value\ndistributions. However, unimodal distribution often and easily causes bias in\nvalue function estimation, leading to poor algorithm performance. This paper\nproposes a distributional reinforcement learning algorithm called DSAC-D\n(Distributed Soft Actor Critic with Diffusion Policy) to address the challenges\nof estimating bias in value functions and obtaining multimodal policy\nrepresentations. A multimodal distributional policy iteration framework that\ncan converge to the optimal policy was established by introducing policy\nentropy and value distribution function. A diffusion value network that can\naccurately characterize the distribution of multi peaks was constructed by\ngenerating a set of reward samples through reverse sampling using a diffusion\nmodel. Based on this, a distributional reinforcement learning algorithm with\ndual diffusion of the value network and the policy network was derived. MuJoCo\ntesting tasks demonstrate that the proposed algorithm not only learns\nmultimodal policy, but also achieves state-of-the-art (SOTA) performance in all\n9 control tasks, with significant suppression of estimation bias and total\naverage return improvement of over 10\\% compared to existing mainstream\nalgorithms. The results of real vehicle testing show that DSAC-D can accurately\ncharacterize the multimodal distribution of different driving styles, and the\ndiffusion policy network can characterize multimodal trajectories.", "AI": {"tldr": "\u63d0\u51faDSAC-D\u7b97\u6cd5\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u4ef7\u503c\u51fd\u6570\u4f30\u8ba1\u504f\u5dee\u548c\u591a\u6a21\u6001\u7b56\u7565\u8868\u793a\u95ee\u9898\uff0c\u5728\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f7f\u7528\u5355\u6a21\u6001\u5206\u5e03\uff08\u5982\u9ad8\u65af\u5206\u5e03\uff09\u5efa\u6a21\u4ef7\u503c\u51fd\u6570\uff0c\u6613\u5bfc\u81f4\u4f30\u8ba1\u504f\u5dee\u548c\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u5f15\u5165\u7b56\u7565\u71b5\u548c\u4ef7\u503c\u5206\u5e03\u51fd\u6570\uff0c\u6784\u5efa\u6269\u6563\u4ef7\u503c\u7f51\u7edc\uff0c\u901a\u8fc7\u53cd\u5411\u91c7\u6837\u751f\u6210\u591a\u5cf0\u5206\u5e03\uff0c\u5b9e\u73b0\u4ef7\u503c\u7f51\u7edc\u548c\u7b56\u7565\u7f51\u7edc\u7684\u53cc\u6269\u6563\u3002", "result": "\u5728MuJoCo\u4efb\u52a1\u4e2d\u5b9e\u73b0SOTA\u6027\u80fd\uff0c\u4f30\u8ba1\u504f\u5dee\u663e\u8457\u6291\u5236\uff0c\u5e73\u5747\u56de\u62a5\u63d0\u534710%\uff1b\u5b9e\u8f66\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u591a\u6a21\u6001\u9a7e\u9a76\u98ce\u683c\u7684\u51c6\u786e\u8868\u5f81\u3002", "conclusion": "DSAC-D\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4ef7\u503c\u51fd\u6570\u4f30\u8ba1\u504f\u5dee\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u7b56\u7565\u5b66\u4e60\uff0c\u6027\u80fd\u4f18\u4e8e\u4e3b\u6d41\u7b97\u6cd5\u3002"}}
{"id": "2507.01504", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01504", "abs": "https://arxiv.org/abs/2507.01504", "authors": ["Robert Aufschl\u00e4ger", "Youssef Shoeb", "Azarm Nowzad", "Michael Heigl", "Fabian Bally", "Martin Schramm"], "title": "Following the Clues: Experiments on Person Re-ID using Cross-Modal Intelligence", "comment": "accepted for publication at the 2025 IEEE 28th International\n  Conference on Intelligent Transportation Systems (ITSC 2025), taking place\n  during November 18-21, 2025 in Gold Coast, Australia", "summary": "The collection and release of street-level recordings as Open Data play a\nvital role in advancing autonomous driving systems and AI research. However,\nthese datasets pose significant privacy risks, particularly for pedestrians,\ndue to the presence of Personally Identifiable Information (PII) that extends\nbeyond biometric traits such as faces. In this paper, we present cRID, a novel\ncross-modal framework combining Large Vision-Language Models, Graph Attention\nNetworks, and representation learning to detect textual describable clues of\nPII and enhance person re-identification (Re-ID). Our approach focuses on\nidentifying and leveraging interpretable features, enabling the detection of\nsemantically meaningful PII beyond low-level appearance cues. We conduct a\nsystematic evaluation of PII presence in person image datasets. Our experiments\nshow improved performance in practical cross-dataset Re-ID scenarios, notably\nfrom Market-1501 to CUHK03-np (detected), highlighting the framework's\npractical utility. Code is available at https://github.com/RAufschlaeger/cRID.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3acRID\u7684\u8de8\u6a21\u6001\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff0c\u7528\u4e8e\u68c0\u6d4b\u884c\u4eba\u6570\u636e\u4e2d\u7684\u4e2a\u4eba\u53ef\u8bc6\u522b\u4fe1\u606f\uff08PII\uff09\u5e76\u63d0\u5347\u884c\u4eba\u91cd\u8bc6\u522b\uff08Re-ID\uff09\u6027\u80fd\u3002", "motivation": "\u8857\u666f\u6570\u636e\u4f5c\u4e3a\u5f00\u653e\u6570\u636e\u5bf9\u81ea\u52a8\u9a7e\u9a76\u548cAI\u7814\u7a76\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u4e2d\u5305\u542b\u7684\u4e2a\u4eba\u53ef\u8bc6\u522b\u4fe1\u606f\uff08PII\uff09\u5bf9\u884c\u4eba\u9690\u79c1\u6784\u6210\u5a01\u80c1\u3002", "method": "\u7ed3\u5408\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u548c\u8868\u5f81\u5b66\u4e60\uff0c\u68c0\u6d4b\u6587\u672c\u53ef\u63cf\u8ff0\u7684PII\u7ebf\u7d22\uff0c\u5e76\u5229\u7528\u53ef\u89e3\u91ca\u7279\u5f81\u63d0\u5347Re-ID\u6027\u80fd\u3002", "result": "\u5728Market-1501\u5230CUHK03-np\u7684\u8de8\u6570\u636e\u96c6Re-ID\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "cRID\u6846\u67b6\u80fd\u6709\u6548\u68c0\u6d4b\u548c\u5229\u7528\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u7684PII\uff0c\u63d0\u5347\u884c\u4eba\u91cd\u8bc6\u522b\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u62a4\u9690\u79c1\u3002"}}
{"id": "2507.01931", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.01931", "abs": "https://arxiv.org/abs/2507.01931", "authors": ["Md Sazzadul Islam Ridoy", "Sumi Akter", "Md. Aminur Rahman"], "title": "Adaptability of ASR Models on Low-Resource Language: A Comparative Study of Whisper and Wav2Vec-BERT on Bangla", "comment": null, "summary": "In recent years, neural models trained on large multilingual text and speech\ndatasets have shown great potential for supporting low-resource languages. This\nstudy investigates the performances of two state-of-the-art Automatic Speech\nRecognition (ASR) models, OpenAI's Whisper (Small & Large-V2) and Facebook's\nWav2Vec-BERT on Bangla, a low-resource language. We have conducted experiments\nusing two publicly available datasets: Mozilla Common Voice-17 and OpenSLR to\nevaluate model performances. Through systematic fine-tuning and hyperparameter\noptimization, including learning rate, epochs, and model checkpoint selection,\nwe have compared the models based on Word Error Rate (WER), Character Error\nRate (CER), Training Time, and Computational Efficiency. The Wav2Vec-BERT model\noutperformed Whisper across all key evaluation metrics, demonstrated superior\nperformance while requiring fewer computational resources, and offered valuable\ninsights to develop robust speech recognition systems in low-resource\nlinguistic settings.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86Whisper\u548cWav2Vec-BERT\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00Bangla\u4e0a\u7684\u8868\u73b0\uff0cWav2Vec-BERT\u5728\u6240\u6709\u5173\u952e\u6307\u6807\u4e0a\u8868\u73b0\u66f4\u4f18\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002", "motivation": "\u63a2\u7d22\u795e\u7ecf\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00Bangla\u4e0a\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u6027\u80fd\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u63d0\u4f9b\u6280\u672f\u652f\u6301\u3002", "method": "\u4f7f\u7528Mozilla Common Voice-17\u548cOpenSLR\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5fae\u8c03\u548c\u8d85\u53c2\u6570\u4f18\u5316\uff08\u5982\u5b66\u4e60\u7387\u3001\u8bad\u7ec3\u8f6e\u6b21\uff09\u8bc4\u4f30Whisper\u548cWav2Vec-BERT\u7684WER\u3001CER\u3001\u8bad\u7ec3\u65f6\u95f4\u548c\u8ba1\u7b97\u6548\u7387\u3002", "result": "Wav2Vec-BERT\u5728WER\u3001CER\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u4f18\u4e8eWhisper\uff0c\u4e14\u6240\u9700\u8ba1\u7b97\u8d44\u6e90\u66f4\u5c11\u3002", "conclusion": "Wav2Vec-BERT\u662f\u4f4e\u8d44\u6e90\u8bed\u8a00Bangla\u8bed\u97f3\u8bc6\u522b\u7684\u66f4\u4f18\u9009\u62e9\uff0c\u4e3a\u7c7b\u4f3c\u8bed\u8a00\u63d0\u4f9b\u4e86\u5b9e\u7528\u53c2\u8003\u3002"}}
{"id": "2507.01201", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01201", "abs": "https://arxiv.org/abs/2507.01201", "authors": ["Hyoseo", "Yoon", "Yisong Yue", "Been Kim"], "title": "Escaping Platos Cave: JAM for Aligning Independently Trained Vision and Language Models", "comment": null, "summary": "Independently trained vision and language models inhabit disjoint\nrepresentational spaces, shaped by their respective modalities, objectives, and\narchitectures. Yet an emerging hypothesis - the Platonic Representation\nHypothesis - suggests that such models may nonetheless converge toward a shared\nstatistical model of reality. This compatibility, if it exists, raises a\nfundamental question: can we move beyond post-hoc statistical detection of\nalignment and explicitly optimize for it between such disjoint representations?\nWe cast this Platonic alignment problem as a multi-objective optimization task\n- preserve each modality's native structure while aligning for mutual\ncoherence. We introduce the Joint Autoencoder Modulator (JAM) framework that\njointly trains modality-specific autoencoders on the latent representations of\npre-trained single modality models, encouraging alignment through both\nreconstruction and cross-modal objectives. By analogy, this framework serves as\na method to escape Plato's Cave, enabling the emergence of shared structure\nfrom disjoint inputs. We evaluate this framework across three critical design\naxes: (i) the alignment objective - comparing contrastive loss (Con), its\nhard-negative variant (NegCon), and our Spread loss, (ii) the layer depth at\nwhich alignment is most effective, and (iii) the impact of foundation model\nscale on representational convergence. Our findings show that our lightweight\nPareto-efficient framework reliably induces alignment, even across frozen,\nindependently trained representations, offering both theoretical insight and\npractical pathways for transforming generalist unimodal foundations into\nspecialist multimodal models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u81ea\u52a8\u7f16\u7801\u5668\u8c03\u5236\u5668\uff08JAM\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u578b\u4e4b\u95f4\u7684\u5bf9\u9f50\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u5b9e\u73b0\u6a21\u6001\u95f4\u7684\u5171\u4eab\u7ed3\u6784\u3002", "motivation": "\u63a2\u7d22\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u578b\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u4f18\u5316\u5b9e\u73b0\u5171\u4eab\u7684\u7edf\u8ba1\u6a21\u578b\uff0c\u8d85\u8d8a\u540e\u9a8c\u7edf\u8ba1\u68c0\u6d4b\u7684\u5bf9\u9f50\u3002", "method": "\u91c7\u7528JAM\u6846\u67b6\uff0c\u8054\u5408\u8bad\u7ec3\u6a21\u6001\u7279\u5b9a\u7684\u81ea\u52a8\u7f16\u7801\u5668\uff0c\u7ed3\u5408\u91cd\u6784\u548c\u8de8\u6a21\u6001\u76ee\u6807\u3002", "result": "\u6846\u67b6\u5728\u4e09\u79cd\u8bbe\u8ba1\u8f74\u4e0a\u9a8c\u8bc1\u6709\u6548\uff0c\u80fd\u591f\u53ef\u9760\u5730\u8bf1\u5bfc\u5bf9\u9f50\uff0c\u5373\u4f7f\u5bf9\u4e8e\u51bb\u7ed3\u7684\u72ec\u7acb\u8bad\u7ec3\u8868\u793a\u3002", "conclusion": "JAM\u6846\u67b6\u4e3a\u5c06\u901a\u7528\u5355\u6a21\u6001\u6a21\u578b\u8f6c\u5316\u4e3a\u4e13\u4e1a\u591a\u6a21\u6001\u6a21\u578b\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u9014\u5f84\u3002"}}
{"id": "2507.01509", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01509", "abs": "https://arxiv.org/abs/2507.01509", "authors": ["Tapas K. Dutta", "Snehashis Majhi", "Deepak Ranjan Nayak", "Debesh Jha"], "title": "Mamba Guided Boundary Prior Matters: A New Perspective for Generalized Polyp Segmentation", "comment": "11 pages, 2 figures, MICCAI-2025", "summary": "Polyp segmentation in colonoscopy images is crucial for early detection and\ndiagnosis of colorectal cancer. However, this task remains a significant\nchallenge due to the substantial variations in polyp shape, size, and color, as\nwell as the high similarity between polyps and surrounding tissues, often\ncompounded by indistinct boundaries. While existing encoder-decoder CNN and\ntransformer-based approaches have shown promising results, they struggle with\nstable segmentation performance on polyps with weak or blurry boundaries. These\nmethods exhibit limited abilities to distinguish between polyps and non-polyps\nand capture essential boundary cues. Moreover, their generalizability still\nfalls short of meeting the demands of real-time clinical applications. To\naddress these limitations, we propose SAM-MaGuP, a groundbreaking approach for\nrobust polyp segmentation. By incorporating a boundary distillation module and\na 1D-2D Mamba adapter within the Segment Anything Model (SAM), SAM-MaGuP excels\nat resolving weak boundary challenges and amplifies feature learning through\nenriched global contextual interactions. Extensive evaluations across five\ndiverse datasets reveal that SAM-MaGuP outperforms state-of-the-art methods,\nachieving unmatched segmentation accuracy and robustness. Our key innovations,\na Mamba-guided boundary prior and a 1D-2D Mamba block, set a new benchmark in\nthe field, pushing the boundaries of polyp segmentation to new heights.", "AI": {"tldr": "SAM-MaGuP\u662f\u4e00\u79cd\u57fa\u4e8eSegment Anything Model\uff08SAM\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fb9\u754c\u84b8\u998f\u6a21\u5757\u548c1D-2D Mamba\u9002\u914d\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u606f\u8089\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u606f\u8089\u5206\u5272\u5728\u7ed3\u80a0\u955c\u56fe\u50cf\u4e2d\u5bf9\u65e9\u671f\u7ed3\u76f4\u80a0\u764c\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5f31\u8fb9\u754c\u548c\u6a21\u7cca\u8fb9\u754c\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u8fb9\u754c\u84b8\u998f\u6a21\u5757\u548c1D-2D Mamba\u9002\u914d\u5668\uff0c\u589e\u5f3a\u5168\u5c40\u4e0a\u4e0b\u6587\u4ea4\u4e92\u548c\u8fb9\u754c\u7279\u5f81\u5b66\u4e60\u3002", "result": "\u5728\u4e94\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\uff0cSAM-MaGuP\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u5206\u5272\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "SAM-MaGuP\u901a\u8fc7\u521b\u65b0\u6027\u7684\u8fb9\u754c\u5148\u9a8c\u548c1D-2D Mamba\u6a21\u5757\uff0c\u4e3a\u606f\u8089\u5206\u5272\u9886\u57df\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002"}}
{"id": "2507.01936", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.01936", "abs": "https://arxiv.org/abs/2507.01936", "authors": ["Adrian de Wynter", "Tangming Yuan"], "title": "The Thin Line Between Comprehension and Persuasion in LLMs", "comment": null, "summary": "Large language models (LLMs) are excellent at maintaining high-level,\nconvincing dialogues. They are being fast deployed as chatbots and evaluators\nin sensitive areas, such as peer review and mental health applications. This,\nalong with the disparate accounts on their reasoning capabilities, calls for a\ncloser examination of LLMs and their comprehension of dialogue. In this work we\nbegin by evaluating LLMs' ability to maintain a debate--one of the purest yet\nmost complex forms of human communication. Then we measure how this capability\nrelates to their understanding of what is being talked about, namely, their\ncomprehension of dialogical structures and the pragmatic context. We find that\nLLMs are capable of maintaining coherent, persuasive debates, often swaying the\nbeliefs of participants and audiences alike. We also note that awareness or\nsuspicion of AI involvement encourage people to be more critical of the\narguments made. When polling LLMs on their comprehension of deeper structures\nof dialogue, however, they cannot demonstrate said understanding. Our findings\ntie the shortcomings of LLMs-as-evaluators to their (in)ability to understand\nthe context. More broadly, for the field of argumentation theory we posit that,\nif an agent can convincingly maintain a dialogue, it is not necessary for it to\nknow what it is talking about. Hence, the modelling of pragmatic context and\ncoherence are secondary to effectiveness.", "AI": {"tldr": "LLMs\u80fd\u8fdb\u884c\u8fde\u8d2f\u4e14\u6709\u8bf4\u670d\u529b\u7684\u8fa9\u8bba\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5bf9\u8bdd\u6df1\u5c42\u7ed3\u6784\u7684\u7406\u89e3\u3002", "motivation": "\u8bc4\u4f30LLMs\u5728\u8fa9\u8bba\u4e2d\u7684\u8868\u73b0\u53ca\u5176\u5bf9\u5bf9\u8bdd\u7ed3\u6784\u7684\u7406\u89e3\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u6d4b\u8bd5LLMs\u7684\u8fa9\u8bba\u80fd\u529b\u548c\u5bf9\u5bf9\u8bdd\u7ed3\u6784\u7684\u7406\u89e3\u3002", "result": "LLMs\u80fd\u8fdb\u884c\u6709\u8bf4\u670d\u529b\u7684\u8fa9\u8bba\uff0c\u4f46\u65e0\u6cd5\u7406\u89e3\u6df1\u5c42\u5bf9\u8bdd\u7ed3\u6784\u3002", "conclusion": "LLMs\u7684\u8fa9\u8bba\u80fd\u529b\u4e0d\u4f9d\u8d56\u4e8e\u5bf9\u5185\u5bb9\u7684\u7406\u89e3\uff0c\u5b9e\u7528\u6027\u548c\u8fde\u8d2f\u6027\u6bd4\u7406\u89e3\u66f4\u91cd\u8981\u3002"}}
{"id": "2507.01208", "categories": ["cs.LG", "cs.CR", "C.2.0; I.2.0"], "pdf": "https://arxiv.org/pdf/2507.01208", "abs": "https://arxiv.org/abs/2507.01208", "authors": ["Pedro R. X. Carmo", "Igor de Moura", "Assis T. de Oliveira Filho", "Djamel Sadok", "Cleber Zanchettin"], "title": "Deep Learning-Based Intrusion Detection for Automotive Ethernet: Evaluating & Optimizing Fast Inference Techniques for Deployment on Low-Cost Platform", "comment": null, "summary": "Modern vehicles are increasingly connected, and in this context, automotive\nEthernet is one of the technologies that promise to provide the necessary\ninfrastructure for intra-vehicle communication. However, these systems are\nsubject to attacks that can compromise safety, including flow injection\nattacks. Deep Learning-based Intrusion Detection Systems (IDS) are often\ndesigned to combat this problem, but they require expensive hardware to run in\nreal time. In this work, we propose to evaluate and apply fast neural network\ninference techniques like Distilling and Prunning for deploying IDS models on\nlow-cost platforms in real time. The results show that these techniques can\nachieve intrusion detection times of up to 727 {\\mu}s using a Raspberry Pi 4,\nwith AUCROC values of 0.9890.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u84b8\u998f\u548c\u526a\u679d\u7684\u5feb\u901f\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u6280\u672f\uff0c\u7528\u4e8e\u5728\u4f4e\u6210\u672c\u5e73\u53f0\u4e0a\u5b9e\u65f6\u90e8\u7f72\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\uff08IDS\uff09\uff0c\u4ee5\u5e94\u5bf9\u8f66\u8f7d\u4ee5\u592a\u7f51\u7684\u5b89\u5168\u5a01\u80c1\u3002", "motivation": "\u73b0\u4ee3\u8f66\u8f86\u65e5\u76ca\u4e92\u8054\uff0c\u8f66\u8f7d\u4ee5\u592a\u7f51\u6280\u672f\u9762\u4e34\u6d41\u6ce8\u5165\u653b\u51fb\u7b49\u5b89\u5168\u5a01\u80c1\uff0c\u800c\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60IDS\u9700\u8981\u6602\u8d35\u786c\u4ef6\u652f\u6301\u5b9e\u65f6\u8fd0\u884c\u3002", "method": "\u91c7\u7528\u84b8\u998f\u548c\u526a\u679d\u6280\u672f\u4f18\u5316\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u5728\u4f4e\u6210\u672c\u5e73\u53f0\uff08\u5982Raspberry Pi 4\uff09\u4e0a\u5b9e\u65f6\u8fd0\u884c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4f18\u5316\u540e\u7684\u6a21\u578b\u5728Raspberry Pi 4\u4e0a\u5b9e\u73b0\u4e86727\u5fae\u79d2\u7684\u5165\u4fb5\u68c0\u6d4b\u65f6\u95f4\uff0cAUCROC\u503c\u8fbe\u52300.9890\u3002", "conclusion": "\u901a\u8fc7\u84b8\u998f\u548c\u526a\u679d\u6280\u672f\uff0c\u53ef\u4ee5\u5728\u4f4e\u6210\u672c\u5e73\u53f0\u4e0a\u9ad8\u6548\u90e8\u7f72\u5b9e\u65f6IDS\uff0c\u6709\u6548\u63d0\u5347\u8f66\u8f7d\u4ee5\u592a\u7f51\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2507.01532", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01532", "abs": "https://arxiv.org/abs/2507.01532", "authors": ["Tomas Zelezny", "Jakub Straka", "Vaclav Javorek", "Ondrej Valach", "Marek Hruz", "Ivan Gruber"], "title": "Exploring Pose-based Sign Language Translation: Ablation Studies and Attention Insights", "comment": "8 pages, 9 figures, supplementary, SLRTP2025, CVPR2025", "summary": "Sign Language Translation (SLT) has evolved significantly, moving from\nisolated recognition approaches to complex, continuous gloss-free translation\nsystems. This paper explores the impact of pose-based data preprocessing\ntechniques - normalization, interpolation, and augmentation - on SLT\nperformance. We employ a transformer-based architecture, adapting a modified T5\nencoder-decoder model to process pose representations. Through extensive\nablation studies on YouTubeASL and How2Sign datasets, we analyze how different\npreprocessing strategies affect translation accuracy. Our results demonstrate\nthat appropriate normalization, interpolation, and augmentation techniques can\nsignificantly improve model robustness and generalization abilities.\nAdditionally, we provide a deep analysis of the model's attentions and reveal\ninteresting behavior suggesting that adding a dedicated register token can\nimprove overall model performance. We publish our code on our GitHub\nrepository, including the preprocessed YouTubeASL data.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8e\u59ff\u6001\u7684\u6570\u636e\u9884\u5904\u7406\u6280\u672f\uff08\u5f52\u4e00\u5316\u3001\u63d2\u503c\u548c\u589e\u5f3a\uff09\u5bf9\u624b\u8bed\u7ffb\u8bd1\uff08SLT\uff09\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4f7f\u7528\u6539\u8fdb\u7684T5\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\uff0c\u5e76\u5728YouTubeASL\u548cHow2Sign\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u6280\u672f\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u59ff\u6001\u6570\u636e\u7684\u9884\u5904\u7406\u6280\u672f\u63d0\u5347\u624b\u8bed\u7ffb\u8bd1\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u6539\u8fdb\u7684T5\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\u5904\u7406\u59ff\u6001\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u5f52\u4e00\u5316\u3001\u63d2\u503c\u548c\u589e\u5f3a\u7b49\u9884\u5904\u7406\u6280\u672f\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u9002\u5f53\u7684\u9884\u5904\u7406\u6280\u672f\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u53d1\u73b0\u6dfb\u52a0\u4e13\u7528\u5bc4\u5b58\u5668\u6807\u8bb0\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u59ff\u6001\u6570\u636e\u7684\u9884\u5904\u7406\u6280\u672f\u5bf9\u624b\u8bed\u7ffb\u8bd1\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u6539\u8fdb\u7684\u6a21\u578b\u67b6\u6784\u548c\u9884\u5904\u7406\u7b56\u7565\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2507.01457", "categories": ["cs.LG", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.01457", "abs": "https://arxiv.org/abs/2507.01457", "authors": ["Federico Nicolas Peccia", "Frederik Haxel", "Oliver Bringmann"], "title": "Tensor Program Optimization for the RISC-V Vector Extension Using Probabilistic Programs", "comment": "9 pages, 10 figures, 2 algorithms", "summary": "RISC-V provides a flexible and scalable platform for applications ranging\nfrom embedded devices to high-performance computing clusters. Particularly, its\nRISC-V Vector Extension (RVV) becomes of interest for the acceleration of AI\nworkloads. But writing software that efficiently utilizes the vector units of\nRISC-V CPUs without expert knowledge requires the programmer to rely on the\nautovectorization features of compilers or hand-crafted libraries like\nmuRISCV-NN. Smarter approaches, like autotuning frameworks, have been missing\nthe integration with the RISC-V RVV extension, thus heavily limiting the\nefficient deployment of complex AI workloads. In this paper, we present a\nworkflow based on the TVM compiler to efficiently map AI workloads onto RISC-V\nvector units. Instead of relying on hand-crafted libraries, we integrated the\nRVV extension into TVM's MetaSchedule framework, a probabilistic program\nframework for tensor operation tuning. We implemented different RISC-V SoCs on\nan FPGA and tuned a wide range of AI workloads on them. We found that our\nproposal shows a mean improvement of 46% in execution latency when compared\nagainst the autovectorization feature of GCC, and 29% against muRISCV-NN.\nMoreover, the binary resulting from our proposal has a smaller code memory\nfootprint, making it more suitable for embedded devices. Finally, we also\nevaluated our solution on a commercially available RISC-V SoC implementing the\nRVV 1.0 Vector Extension and found our solution is able to find mappings that\nare 35% faster on average than the ones proposed by LLVM. We open-sourced our\nproposal for the community to expand it to target other RISC-V extensions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTVM\u7f16\u8bd1\u5668\u7684\u5de5\u4f5c\u6d41\uff0c\u7528\u4e8e\u9ad8\u6548\u5730\u5c06AI\u5de5\u4f5c\u8d1f\u8f7d\u6620\u5c04\u5230RISC-V\u5411\u91cf\u5355\u5143\uff0c\u901a\u8fc7\u96c6\u6210RVV\u6269\u5c55\u81f3TVM\u7684MetaSchedule\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "RISC-V\u7684\u5411\u91cf\u6269\u5c55\uff08RVV\uff09\u5bf9AI\u5de5\u4f5c\u8d1f\u8f7d\u52a0\u901f\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u9ad8\u6548\u7684\u81ea\u52a8\u8c03\u4f18\u6846\u67b6\uff0c\u9650\u5236\u4e86\u5176\u590d\u6742AI\u5de5\u4f5c\u8d1f\u8f7d\u7684\u90e8\u7f72\u3002", "method": "\u5c06RVV\u6269\u5c55\u96c6\u6210\u5230TVM\u7684MetaSchedule\u6846\u67b6\u4e2d\uff0c\u5229\u7528\u6982\u7387\u6027\u7a0b\u5e8f\u6846\u67b6\u8fdb\u884c\u5f20\u91cf\u64cd\u4f5c\u8c03\u4f18\uff0c\u5e76\u5728FPGA\u4e0a\u5b9e\u73b0\u4e0d\u540cRISC-V SoC\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u4e0eGCC\u7684\u81ea\u52a8\u5411\u91cf\u5316\u529f\u80fd\u76f8\u6bd4\uff0c\u6267\u884c\u5ef6\u8fdf\u5e73\u5747\u63d0\u534746%\uff0c\u6bd4muRISCV-NN\u5feb29%\uff0c\u4e14\u4ee3\u7801\u5185\u5b58\u5360\u7528\u66f4\u5c0f\u3002\u5728\u5546\u7528RISC-V SoC\u4e0a\uff0c\u6bd4LLVM\u5feb35%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86RISC-V RVV\u6269\u5c55\u7684AI\u5de5\u4f5c\u8d1f\u8f7d\u6027\u80fd\uff0c\u9002\u5408\u5d4c\u5165\u5f0f\u8bbe\u5907\uff0c\u5e76\u5df2\u5f00\u6e90\u4f9b\u793e\u533a\u6269\u5c55\u3002"}}
{"id": "2507.01216", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.01216", "abs": "https://arxiv.org/abs/2507.01216", "authors": ["Xingke Yang", "Liang Li", "Zhiyi Wan", "Sicong Li", "Hao Wang", "Xiaoqi Qi", "Jiang Liu", "Tomoaki Ohtsuki", "Xin Fu", "Miao Pan"], "title": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile Device via Additive Side-Tuning", "comment": null, "summary": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data, labels or fine-tuned models to the\nserver. To address those issues, we develop PAE MobiLLM, a privacy-aware and\nefficient LLM FT method which can be deployed on the mobile device via\nserver-assisted additive side-tuning. To further accelerate FT convergence and\nimprove computing efficiency, PAE MobiLLM integrates activation caching on the\nserver side, which allows the server to reuse historical activations and saves\nthe mobile device from repeatedly computing forward passes for the recurring\ndata samples. Besides, to reduce communication cost, PAE MobiLLM develops a\none-token (i.e., ``pivot'' token) activation shortcut that transmits only a\nsingle activation dimension instead of full activation matrices to guide the\nside network tuning. Last but not least, PAE MobiLLM introduces the additive\nadapter side-network design which makes the server train the adapter modules\nbased on device-defined prediction differences rather than raw ground-truth\nlabels. In this way, the server can only assist device-defined side-network\ncomputing, and learn nothing about data, labels or fine-tuned models.", "AI": {"tldr": "PAE MobiLLM\u662f\u4e00\u79cd\u9690\u79c1\u611f\u77e5\u4e14\u9ad8\u6548\u7684LLM\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u670d\u52a1\u5668\u8f85\u52a9\u7684\u4fa7\u8c03\u4f18\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u5b9e\u73b0\uff0c\u51cf\u5c11\u901a\u4fe1\u8d1f\u62c5\u5e76\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u3002", "motivation": "\u89e3\u51b3\u79fb\u52a8\u8bbe\u5907\u8d44\u6e90\u6709\u9650\u4e0eLLM\u5fae\u8c03\u9700\u6c42\u4e4b\u95f4\u7684\u5de8\u5927\u5dee\u8ddd\uff0c\u540c\u65f6\u907f\u514d\u73b0\u6709\u670d\u52a1\u5668\u8f85\u52a9\u65b9\u6cd5\u7684\u9ad8\u901a\u4fe1\u8d1f\u62c5\u548c\u6570\u636e\u9690\u79c1\u6cc4\u9732\u95ee\u9898\u3002", "method": "\u91c7\u7528\u670d\u52a1\u5668\u8f85\u52a9\u7684\u4fa7\u8c03\u4f18\uff0c\u96c6\u6210\u670d\u52a1\u5668\u7aef\u6fc0\u6d3b\u7f13\u5b58\u3001\u5355\u4ee4\u724c\u6fc0\u6d3b\u5feb\u6377\u65b9\u5f0f\u548c\u52a0\u6cd5\u9002\u914d\u5668\u4fa7\u7f51\u7edc\u8bbe\u8ba1\u3002", "result": "\u51cf\u5c11\u4e86\u901a\u4fe1\u6210\u672c\uff0c\u52a0\u901f\u4e86\u5fae\u8c03\u6536\u655b\uff0c\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u62a4\u4e86\u6570\u636e\u548c\u6807\u7b7e\u9690\u79c1\u3002", "conclusion": "PAE MobiLLM\u4e3a\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684LLM\u5fae\u8c03\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01535", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01535", "abs": "https://arxiv.org/abs/2507.01535", "authors": ["Bingxi Liu", "Calvin Chen", "Junhao Li", "Guyang Yu", "Haoqian Song", "Xuchen Liu", "Jinqiang Cui", "Hong Zhang"], "title": "TrackingMiM: Efficient Mamba-in-Mamba Serialization for Real-time UAV Object Tracking", "comment": "12 pages", "summary": "The Vision Transformer (ViT) model has long struggled with the challenge of\nquadratic complexity, a limitation that becomes especially critical in unmanned\naerial vehicle (UAV) tracking systems, where data must be processed in real\ntime. In this study, we explore the recently proposed State-Space Model, Mamba,\nleveraging its computational efficiency and capability for long-sequence\nmodeling to effectively process dense image sequences in tracking tasks. First,\nwe highlight the issue of temporal inconsistency in existing Mamba-based\nmethods, specifically the failure to account for temporal continuity in the\nMamba scanning mechanism. Secondly, building upon this insight,we propose\nTrackingMiM, a Mamba-in-Mamba architecture, a minimal-computation burden model\nfor handling image sequence of tracking problem. In our framework, the mamba\nscan is performed in a nested way while independently process temporal and\nspatial coherent patch tokens. While the template frame is encoded as query\ntoken and utilized for tracking in every scan. Extensive experiments conducted\non five UAV tracking benchmarks confirm that the proposed TrackingMiM achieves\nstate-of-the-art precision while offering noticeable higher speed in UAV\ntracking.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMamba\u6a21\u578b\u7684TrackingMiM\u67b6\u6784\uff0c\u7528\u4e8e\u89e3\u51b3Vision Transformer\u5728\u65e0\u4eba\u673a\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u957f\u5e8f\u5217\u5efa\u6a21\u548c\u5b9e\u65f6\u5904\u7406\u3002", "motivation": "\u89e3\u51b3Vision Transformer\u5728\u65e0\u4eba\u673a\u5b9e\u65f6\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u56e0\u4e8c\u6b21\u590d\u6742\u5ea6\u5bfc\u81f4\u7684\u6548\u7387\u95ee\u9898\uff0c\u540c\u65f6\u6539\u8fdb\u73b0\u6709Mamba\u65b9\u6cd5\u5728\u65f6\u95f4\u8fde\u7eed\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51faTrackingMiM\u67b6\u6784\uff0c\u91c7\u7528\u5d4c\u5957\u7684Mamba\u626b\u63cf\u673a\u5236\uff0c\u72ec\u7acb\u5904\u7406\u65f6\u95f4\u548c\u7a7a\u95f4\u8fde\u8d2f\u7684\u8865\u4e01\u6807\u8bb0\uff0c\u5e76\u5c06\u6a21\u677f\u5e27\u7f16\u7801\u4e3a\u67e5\u8be2\u6807\u8bb0\u7528\u4e8e\u8ddf\u8e2a\u3002", "result": "\u5728\u4e94\u4e2a\u65e0\u4eba\u673a\u8ddf\u8e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTrackingMiM\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u5904\u7406\u901f\u5ea6\u3002", "conclusion": "TrackingMiM\u901a\u8fc7\u9ad8\u6548\u7684\u957f\u5e8f\u5217\u5efa\u6a21\u548c\u5b9e\u65f6\u5904\u7406\u80fd\u529b\uff0c\u4e3a\u65e0\u4eba\u673a\u8ddf\u8e2a\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6027\u80fd\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01235", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.01235", "abs": "https://arxiv.org/abs/2507.01235", "authors": ["Bara Rababa", "Bilal Farooq"], "title": "Quantum Machine Learning in Transportation: A Case Study of Pedestrian Stress Modelling", "comment": "Proceedings of IEEE Intelligent Transportation Systems Conference,\n  2025", "summary": "Quantum computing has opened new opportunities to tackle complex machine\nlearning tasks, for instance, high-dimensional data representations commonly\nrequired in intelligent transportation systems. We explore quantum machine\nlearning to model complex skin conductance response (SCR) events that reflect\npedestrian stress in a virtual reality road crossing experiment. For this\npurpose, Quantum Support Vector Machine (QSVM) with an eight-qubit ZZ feature\nmap and a Quantum Neural Network (QNN) using a Tree Tensor Network ansatz and\nan eight-qubit ZZ feature map, were developed on Pennylane. The dataset\nconsists of SCR measurements along with features such as the response amplitude\nand elapsed time, which have been categorized into amplitude-based classes. The\nQSVM achieved good training accuracy, but had an overfitting problem, showing a\nlow test accuracy of 45% and therefore impacting the reliability of the\nclassification model. The QNN model reached a higher test accuracy of 55%,\nmaking it a better classification model than the QSVM and the classic versions.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u5728\u6a21\u62df\u590d\u6742\u76ae\u80a4\u7535\u5bfc\u53cd\u5e94\uff08SCR\uff09\u4e8b\u4ef6\u4e2d\u7684\u5e94\u7528\uff0c\u6bd4\u8f83\u4e86\u91cf\u5b50\u652f\u6301\u5411\u91cf\u673a\uff08QSVM\uff09\u548c\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\uff08QNN\uff09\u7684\u6027\u80fd\u3002", "motivation": "\u5229\u7528\u91cf\u5b50\u8ba1\u7b97\u89e3\u51b3\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u9ad8\u7ef4\u6570\u636e\u8868\u793a\u7684\u590d\u6742\u673a\u5668\u5b66\u4e60\u4efb\u52a1\uff0c\u7279\u522b\u662f\u6a21\u62df\u884c\u4eba\u538b\u529b\u76f8\u5173\u7684SCR\u4e8b\u4ef6\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8ePennylane\u7684QSVM\uff08\u4f7f\u7528\u516b\u91cf\u5b50\u4f4dZZ\u7279\u5f81\u6620\u5c04\uff09\u548cQNN\uff08\u4f7f\u7528\u6811\u5f20\u91cf\u7f51\u7edc\u7ed3\u6784\u548c\u516b\u91cf\u5b50\u4f4dZZ\u7279\u5f81\u6620\u5c04\uff09\uff0c\u5e76\u5bf9SCR\u6570\u636e\u8fdb\u884c\u4e86\u5206\u7c7b\u3002", "result": "QSVM\u8bad\u7ec3\u7cbe\u5ea6\u9ad8\u4f46\u6d4b\u8bd5\u7cbe\u5ea6\u4ec545%\uff0c\u5b58\u5728\u8fc7\u62df\u5408\u95ee\u9898\uff1bQNN\u6d4b\u8bd5\u7cbe\u5ea6\u8fbe55%\uff0c\u4f18\u4e8eQSVM\u548c\u7ecf\u5178\u65b9\u6cd5\u3002", "conclusion": "QNN\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u5c55\u793a\u4e86\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u5728\u590d\u6742\u6570\u636e\u5206\u6790\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.01539", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01539", "abs": "https://arxiv.org/abs/2507.01539", "authors": ["Mohammadreza Amirian", "Michael Bach", "Oscar Jimenez-del-Toro", "Christoph Aberle", "Roger Schaer", "Vincent Andrearczyk", "Jean-F\u00e9lix Maestrati", "Maria Martin Asiain", "Kyriakos Flouris", "Markus Obmann", "Clarisse Dromain", "Beno\u00eet Dufour", "Pierre-Alexandre Alois Poletti", "Hendrik von Tengg-Kobligk", "Rolf H\u00fcgli", "Martin Kretzschmar", "Hatem Alkadhi", "Ender Konukoglu", "Henning M\u00fcller", "Bram Stieltjes", "Adrien Depeursinge"], "title": "A Multi-Centric Anthropomorphic 3D CT Phantom-Based Benchmark Dataset for Harmonization", "comment": null, "summary": "Artificial intelligence (AI) has introduced numerous opportunities for human\nassistance and task automation in medicine. However, it suffers from poor\ngeneralization in the presence of shifts in the data distribution. In the\ncontext of AI-based computed tomography (CT) analysis, significant data\ndistribution shifts can be caused by changes in scanner manufacturer,\nreconstruction technique or dose. AI harmonization techniques can address this\nproblem by reducing distribution shifts caused by various acquisition settings.\nThis paper presents an open-source benchmark dataset containing CT scans of an\nanthropomorphic phantom acquired with various scanners and settings, which\npurpose is to foster the development of AI harmonization techniques. Using a\nphantom allows fixing variations attributed to inter- and intra-patient\nvariations. The dataset includes 1378 image series acquired with 13 scanners\nfrom 4 manufacturers across 8 institutions using a harmonized protocol as well\nas several acquisition doses. Additionally, we present a methodology, baseline\nresults and open-source code to assess image- and feature-level stability and\nliver tissue classification, promoting the development of AI harmonization\nstrategies.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f00\u6e90\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u4fc3\u8fdbAI\u5728CT\u5206\u6790\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u63d0\u4f9b\u4e86\u8bc4\u4f30\u65b9\u6cd5\u548c\u57fa\u7ebf\u7ed3\u679c\u3002", "motivation": "\u89e3\u51b3AI\u5728\u533b\u5b66CT\u5206\u6790\u4e2d\u56e0\u6570\u636e\u5206\u5e03\u53d8\u5316\u5bfc\u81f4\u7684\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u591a\u5382\u5546\u3001\u591a\u673a\u6784\u7684CT\u626b\u63cf\u6570\u636e\uff0c\u5f00\u53d1AI\u534f\u8c03\u6280\u672f\uff0c\u5e76\u63d0\u4f9b\u8bc4\u4f30\u56fe\u50cf\u548c\u7279\u5f81\u7a33\u5b9a\u6027\u7684\u65b9\u6cd5\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b1378\u4e2a\u56fe\u50cf\u7cfb\u5217\uff0c\u6db5\u76d613\u53f0\u626b\u63cf\u4eea\u548c\u591a\u79cd\u5242\u91cf\u8bbe\u7f6e\uff0c\u5e76\u63d0\u4f9b\u4e86\u57fa\u7ebf\u7ed3\u679c\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u6709\u52a9\u4e8e\u63a8\u52a8AI\u534f\u8c03\u6280\u672f\u7684\u53d1\u5c55\uff0c\u63d0\u9ad8CT\u5206\u6790\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.01470", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01470", "abs": "https://arxiv.org/abs/2507.01470", "authors": ["Yannick Molinghen", "Tom Lenaerts"], "title": "Zero-Incentive Dynamics: a look at reward sparsity through the lens of unrewarded subgoals", "comment": "Accepted at \"Finding the Frame 2025\", workshop at RLC", "summary": "This work re-examines the commonly held assumption that the frequency of\nrewards is a reliable measure of task difficulty in reinforcement learning. We\nidentify and formalize a structural challenge that undermines the effectiveness\nof current policy learning methods: when essential subgoals do not directly\nyield rewards. We characterize such settings as exhibiting zero-incentive\ndynamics, where transitions critical to success remain unrewarded. We show that\nstate-of-the-art deep subgoal-based algorithms fail to leverage these dynamics\nand that learning performance is highly sensitive to the temporal proximity\nbetween subgoal completion and eventual reward. These findings reveal a\nfundamental limitation in current approaches and point to the need for\nmechanisms that can infer latent task structure without relying on immediate\nincentives.", "AI": {"tldr": "\u8bba\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u5956\u52b1\u9891\u7387\u4f5c\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e2d\u4efb\u52a1\u96be\u5ea6\u53ef\u9760\u6307\u6807\u7684\u5e38\u89c1\u5047\u8bbe\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u8ba8\u5728\u5173\u952e\u5b50\u76ee\u6807\u4e0d\u76f4\u63a5\u4ea7\u751f\u5956\u52b1\u65f6\uff0c\u73b0\u6709\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5f62\u5f0f\u5316\u96f6\u6fc0\u52b1\u52a8\u6001\u95ee\u9898\uff0c\u5e76\u5206\u6790\u73b0\u6709\u5b50\u76ee\u6807\u7b97\u6cd5\u7684\u8868\u73b0\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5b50\u76ee\u6807\u7b97\u6cd5\u65e0\u6cd5\u6709\u6548\u5229\u7528\u96f6\u6fc0\u52b1\u52a8\u6001\uff0c\u4e14\u5b66\u4e60\u6027\u80fd\u5bf9\u5b50\u76ee\u6807\u4e0e\u5956\u52b1\u7684\u65f6\u95f4\u63a5\u8fd1\u6027\u9ad8\u5ea6\u654f\u611f\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\u5f53\u524d\u65b9\u6cd5\u5b58\u5728\u6839\u672c\u6027\u5c40\u9650\uff0c\u9700\u5f00\u53d1\u80fd\u63a8\u65ad\u6f5c\u5728\u4efb\u52a1\u7ed3\u6784\u7684\u65b0\u673a\u5236\u3002"}}
{"id": "2507.01557", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01557", "abs": "https://arxiv.org/abs/2507.01557", "authors": ["Marcin Kowlaczyk", "Tomasz Kryjak"], "title": "Interpolation-Based Event Visual Data Filtering Algorithms", "comment": "This paper has been accepted for publication at the IEEE Conference\n  on Computer Vision and Pattern Recognition (CVPR) Workshops, Vancouver, 2023.\n  Copyright IEEE", "summary": "The field of neuromorphic vision is developing rapidly, and event cameras are\nfinding their way into more and more applications. However, the data stream\nfrom these sensors is characterised by significant noise. In this paper, we\npropose a method for event data that is capable of removing approximately 99\\%\nof noise while preserving the majority of the valid signal. We have proposed\nfour algorithms based on the matrix of infinite impulse response (IIR) filters\nmethod. We compared them on several event datasets that were further modified\nby adding artificially generated noise and noise recorded with dynamic vision\nsensor. The proposed methods use about 30KB of memory for a sensor with a\nresolution of 1280 x 720 and is therefore well suited for implementation in\nembedded devices.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u9650\u8109\u51b2\u54cd\u5e94\uff08IIR\uff09\u6ee4\u6ce2\u5668\u77e9\u9635\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u53bb\u9664\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u4e2d\u7ea699%\u7684\u566a\u58f0\uff0c\u540c\u65f6\u4fdd\u7559\u6709\u6548\u4fe1\u53f7\uff0c\u9002\u7528\u4e8e\u5d4c\u5165\u5f0f\u8bbe\u5907\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u6d41\u4e2d\u5b58\u5728\u663e\u8457\u566a\u58f0\uff0c\u5f71\u54cd\u5176\u5e94\u7528\u6548\u679c\uff0c\u9700\u5f00\u53d1\u9ad8\u6548\u53bb\u566a\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u56db\u79cd\u57fa\u4e8eIIR\u6ee4\u6ce2\u5668\u77e9\u9635\u7684\u7b97\u6cd5\uff0c\u5e76\u5728\u591a\u4e2a\u4e8b\u4ef6\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u5305\u62ec\u4eba\u5de5\u751f\u6210\u566a\u58f0\u548c\u52a8\u6001\u89c6\u89c9\u4f20\u611f\u5668\u8bb0\u5f55\u7684\u566a\u58f0\u3002", "result": "\u65b9\u6cd5\u80fd\u53bb\u9664\u7ea699%\u7684\u566a\u58f0\uff0c\u5185\u5b58\u5360\u7528\u4ec530KB\uff081280x720\u5206\u8fa8\u7387\uff09\uff0c\u9002\u5408\u5d4c\u5165\u5f0f\u8bbe\u5907\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u53bb\u566a\u4e14\u8d44\u6e90\u5360\u7528\u4f4e\uff0c\u4e3a\u4e8b\u4ef6\u76f8\u673a\u5728\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2507.01573", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01573", "abs": "https://arxiv.org/abs/2507.01573", "authors": ["Hao Wang", "Keyan Hu", "Xin Guo", "Haifeng Li", "Chao Tao"], "title": "A Gift from the Integration of Discriminative and Diffusion-based Generative Learning: Boundary Refinement Remote Sensing Semantic Segmentation", "comment": "20 pages, 14 figures", "summary": "Remote sensing semantic segmentation must address both what the ground\nobjects are within an image and where they are located. Consequently,\nsegmentation models must ensure not only the semantic correctness of\nlarge-scale patches (low-frequency information) but also the precise\nlocalization of boundaries between patches (high-frequency information).\nHowever, most existing approaches rely heavily on discriminative learning,\nwhich excels at capturing low-frequency features, while overlooking its\ninherent limitations in learning high-frequency features for semantic\nsegmentation. Recent studies have revealed that diffusion generative models\nexcel at generating high-frequency details. Our theoretical analysis confirms\nthat the diffusion denoising process significantly enhances the model's ability\nto learn high-frequency features; however, we also observe that these models\nexhibit insufficient semantic inference for low-frequency features when guided\nsolely by the original image. Therefore, we integrate the strengths of both\ndiscriminative and generative learning, proposing the Integration of\nDiscriminative and diffusion-based Generative learning for Boundary Refinement\n(IDGBR) framework. The framework first generates a coarse segmentation map\nusing a discriminative backbone model. This map and the original image are fed\ninto a conditioning guidance network to jointly learn a guidance representation\nsubsequently leveraged by an iterative denoising diffusion process refining the\ncoarse segmentation. Extensive experiments across five remote sensing semantic\nsegmentation datasets (binary and multi-class segmentation) confirm our\nframework's capability of consistent boundary refinement for coarse results\nfrom diverse discriminative architectures. The source code will be available at\nhttps://github.com/KeyanHu-git/IDGBR.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faIDGBR\u6846\u67b6\uff0c\u7ed3\u5408\u5224\u522b\u5f0f\u5b66\u4e60\u548c\u6269\u6563\u751f\u6210\u6a21\u578b\uff0c\u4f18\u5316\u9065\u611f\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u7684\u8fb9\u754c\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5224\u522b\u5f0f\u5b66\u4e60\uff0c\u64c5\u957f\u4f4e\u9891\u7279\u5f81\u4f46\u5ffd\u89c6\u9ad8\u9891\u8fb9\u754c\u7ec6\u8282\uff0c\u6269\u6563\u751f\u6210\u6a21\u578b\u64c5\u957f\u9ad8\u9891\u7ec6\u8282\u4f46\u4f4e\u9891\u8bed\u4e49\u63a8\u65ad\u4e0d\u8db3\u3002", "method": "IDGBR\u6846\u67b6\u5148\u7528\u5224\u522b\u5f0f\u6a21\u578b\u751f\u6210\u7c97\u5206\u5272\u56fe\uff0c\u518d\u901a\u8fc7\u6761\u4ef6\u5f15\u5bfc\u7f51\u7edc\u548c\u6269\u6563\u53bb\u566a\u8fc7\u7a0b\u7ec6\u5316\u8fb9\u754c\u3002", "result": "\u5728\u4e94\u4e2a\u9065\u611f\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86IDGBR\u80fd\u4e00\u81f4\u4f18\u5316\u4e0d\u540c\u5224\u522b\u5f0f\u67b6\u6784\u7684\u7c97\u5206\u5272\u7ed3\u679c\u3002", "conclusion": "IDGBR\u6709\u6548\u7ed3\u5408\u5224\u522b\u5f0f\u548c\u751f\u6210\u5f0f\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u8fb9\u754c\u5206\u5272\u7cbe\u5ea6\u3002"}}
{"id": "2507.01551", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01551", "abs": "https://arxiv.org/abs/2507.01551", "authors": ["Wu Fei", "Hao Kong", "Shuxian Liang", "Yang Lin", "Yibo Yang", "Jing Tang", "Lei Chen", "Xiansheng Hua"], "title": "Self-Guided Process Reward Optimization with Masked Step Advantage for Process Reinforcement Learning", "comment": null, "summary": "Process Reinforcement Learning~(PRL) has demonstrated considerable potential\nin enhancing the reasoning capabilities of Large Language Models~(LLMs).\nHowever, introducing additional process reward models incurs substantial\ncomputational overhead, and there is no unified theoretical framework for\nprocess-level advantage estimation. To bridge this gap, we propose\n\\textbf{S}elf-Guided \\textbf{P}rocess \\textbf{R}eward\n\\textbf{O}ptimization~(\\textbf{SPRO}), a novel framework that enables\nprocess-aware RL through two key innovations: (1) we first theoretically\ndemonstrate that process rewards can be derived intrinsically from the policy\nmodel itself, and (2) we introduce well-defined cumulative process rewards and\n\\textbf{M}asked \\textbf{S}tep \\textbf{A}dvantage (\\textbf{MSA}), which\nfacilitates rigorous step-wise action advantage estimation within shared-prompt\nsampling groups. Our experimental results demonstrate that SPRO outperforms\nvaniila GRPO with 3.4x higher training efficiency and a 17.5\\% test accuracy\nimprovement. Furthermore, SPRO maintains a stable and elevated policy entropy\nthroughout training while reducing the average response length by approximately\n$1/3$, evidencing sufficient exploration and prevention of reward hacking.\nNotably, SPRO incurs no additional computational overhead compared to\noutcome-supervised RL methods such as GRPO, which benefit industrial\nimplementation.", "AI": {"tldr": "SPRO\u662f\u4e00\u79cd\u81ea\u5f15\u5bfc\u8fc7\u7a0b\u5956\u52b1\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5185\u5728\u63a8\u5bfc\u8fc7\u7a0b\u5956\u52b1\u548c\u5f15\u5165\u7d2f\u79ef\u8fc7\u7a0b\u5956\u52b1\u4e0e\u63a9\u7801\u6b65\u9aa4\u4f18\u52bf\uff0c\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6d4b\u8bd5\u51c6\u786e\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u8fc7\u7a0b\u5f3a\u5316\u5b66\u4e60\uff08PRL\uff09\u65b9\u6cd5\u56e0\u5f15\u5165\u989d\u5916\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u4e14\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u3002", "method": "SPRO\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u8fc7\u7a0b\u5956\u52b1\u53ef\u4ece\u7b56\u7565\u6a21\u578b\u81ea\u8eab\u63a8\u5bfc\uff0c\u5e76\u5f15\u5165\u7d2f\u79ef\u8fc7\u7a0b\u5956\u52b1\u548c\u63a9\u7801\u6b65\u9aa4\u4f18\u52bf\uff08MSA\uff09\u8fdb\u884c\u6b65\u9aa4\u7ea7\u4f18\u52bf\u4f30\u8ba1\u3002", "result": "SPRO\u8bad\u7ec3\u6548\u7387\u6bd4GRPO\u9ad83.4\u500d\uff0c\u6d4b\u8bd5\u51c6\u786e\u7387\u63d0\u534717.5%\uff0c\u54cd\u5e94\u957f\u5ea6\u51cf\u5c111/3\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u4e0eGRPO\u76f8\u5f53\u3002", "conclusion": "SPRO\u5728\u63d0\u5347\u6027\u80fd\u7684\u540c\u65f6\u907f\u514d\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u9002\u7528\u4e8e\u5de5\u4e1a\u5b9e\u73b0\u3002"}}
{"id": "2507.01285", "categories": ["cs.LG", "cs.DC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.01285", "abs": "https://arxiv.org/abs/2507.01285", "authors": ["Aymen Rayane Khouas", "Mohamed Reda Bouadjenek", "Hakim Hacid", "Sunil Aryal"], "title": "Far From Sight, Far From Mind: Inverse Distance Weighting for Graph Federated Recommendation", "comment": "17 pages, 5 figures", "summary": "Graph federated recommendation systems offer a privacy-preserving alternative\nto traditional centralized recommendation architectures, which often raise\nconcerns about data security. While federated learning enables personalized\nrecommendations without exposing raw user data, existing aggregation methods\noverlook the unique properties of user embeddings in this setting. Indeed,\ntraditional aggregation methods fail to account for their complexity and the\ncritical role of user similarity in recommendation effectiveness. Moreover,\nevolving user interactions require adaptive aggregation while preserving the\ninfluence of high-relevance anchor users (the primary users before expansion in\ngraph-based frameworks). To address these limitations, we introduce\nDist-FedAvg, a novel distance-based aggregation method designed to enhance\npersonalization and aggregation efficiency in graph federated learning. Our\nmethod assigns higher aggregation weights to users with similar embeddings,\nwhile ensuring that anchor users retain significant influence in local updates.\nEmpirical evaluations on multiple datasets demonstrate that Dist-FedAvg\nconsistently outperforms baseline aggregation techniques, improving\nrecommendation accuracy while maintaining seamless integration into existing\nfederated learning frameworks.", "AI": {"tldr": "Dist-FedAvg\u662f\u4e00\u79cd\u57fa\u4e8e\u8ddd\u79bb\u7684\u805a\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u56fe\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\u7684\u4e2a\u6027\u5316\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u805a\u5408\u65b9\u6cd5\u672a\u8003\u8651\u7528\u6237\u5d4c\u5165\u7684\u590d\u6742\u6027\u548c\u7528\u6237\u76f8\u4f3c\u6027\u5bf9\u63a8\u8350\u6548\u679c\u7684\u5f71\u54cd\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u9ad8\u76f8\u5173\u6027\u951a\u7528\u6237\u7684\u9002\u5e94\u6027\u4fdd\u62a4\u3002", "method": "\u63d0\u51faDist-FedAvg\uff0c\u901a\u8fc7\u57fa\u4e8e\u8ddd\u79bb\u7684\u6743\u91cd\u5206\u914d\u589e\u5f3a\u76f8\u4f3c\u7528\u6237\u7684\u5f71\u54cd\u529b\uff0c\u540c\u65f6\u4fdd\u7559\u951a\u7528\u6237\u7684\u91cd\u8981\u6027\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cDist-FedAvg\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u63a8\u8350\u51c6\u786e\u6027\u3002", "conclusion": "Dist-FedAvg\u5728\u63d0\u5347\u63a8\u8350\u6548\u679c\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u4e0e\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u7684\u65e0\u7f1d\u96c6\u6210\u3002"}}
{"id": "2507.01586", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01586", "abs": "https://arxiv.org/abs/2507.01586", "authors": ["Bryan Constantine Sadihin", "Michael Hua Wang", "Shei Pern Chua", "Hang Su"], "title": "SketchColour: Channel Concat Guided DiT-based Sketch-to-Colour Pipeline for 2D Animation", "comment": "Project page and code: https://bconstantine.github.io/SketchColour", "summary": "The production of high-quality 2D animation is highly labor-intensive\nprocess, as animators are currently required to draw and color a large number\nof frames by hand. We present SketchColour, the first sketch-to-colour pipeline\nfor 2D animation built on a diffusion transformer (DiT) backbone. By replacing\nthe conventional U-Net denoiser with a DiT-style architecture and injecting\nsketch information via lightweight channel-concatenation adapters accompanied\nwith LoRA finetuning, our method natively integrates conditioning without the\nparameter and memory bloat of a duplicated ControlNet, greatly reducing\nparameter count and GPU memory usage. Evaluated on the SAKUGA dataset,\nSketchColour outperforms previous state-of-the-art video colourization methods\nacross all metrics, despite using only half the training data of competing\nmodels. Our approach produces temporally coherent animations with minimal\nartifacts such as colour bleeding or object deformation. Our code is available\nat: https://bconstantine.github.io/SketchColour .", "AI": {"tldr": "SketchColour\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u53d8\u538b\u5668\uff08DiT\uff09\u76842D\u52a8\u753b\u8349\u56fe\u7740\u8272\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u901a\u9053\u8fde\u63a5\u9002\u914d\u5668\u548cLoRA\u5fae\u8c03\uff0c\u663e\u8457\u51cf\u5c11\u53c2\u6570\u548cGPU\u5185\u5b58\u4f7f\u7528\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf2D\u52a8\u753b\u7740\u8272\u8fc7\u7a0b\u52b3\u52a8\u5bc6\u96c6\uff0c\u9700\u8981\u624b\u5de5\u7ed8\u5236\u548c\u7740\u8272\u5927\u91cf\u5e27\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6269\u6563\u53d8\u538b\u5668\uff08DiT\uff09\u67b6\u6784\uff0c\u66ff\u6362\u4f20\u7edfU-Net\u53bb\u566a\u5668\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u901a\u9053\u8fde\u63a5\u9002\u914d\u5668\u548cLoRA\u5fae\u8c03\u6ce8\u5165\u8349\u56fe\u4fe1\u606f\uff0c\u51cf\u5c11\u53c2\u6570\u548c\u5185\u5b58\u5360\u7528\u3002", "result": "\u5728SAKUGA\u6570\u636e\u96c6\u4e0a\uff0cSketchColour\u5728\u6240\u6709\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u89c6\u9891\u7740\u8272\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528\u4e00\u534a\u8bad\u7ec3\u6570\u636e\uff0c\u751f\u6210\u65f6\u95f4\u4e00\u81f4\u4e14\u65e0\u663e\u8457\u4f2a\u5f71\u7684\u52a8\u753b\u3002", "conclusion": "SketchColour\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u76842D\u52a8\u753b\u7740\u8272\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u52b3\u52a8\u548c\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002"}}
{"id": "2507.01587", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.01587", "abs": "https://arxiv.org/abs/2507.01587", "authors": ["Youngjin Oh", "Junhyeong Kwon", "Keuntek Lee", "Nam Ik Cho"], "title": "Towards Controllable Real Image Denoising with Camera Parameters", "comment": "Accepted for publication in ICIP 2025, IEEE International Conference\n  on Image Processing", "summary": "Recent deep learning-based image denoising methods have shown impressive\nperformance; however, many lack the flexibility to adjust the denoising\nstrength based on the noise levels, camera settings, and user preferences. In\nthis paper, we introduce a new controllable denoising framework that adaptively\nremoves noise from images by utilizing information from camera parameters.\nSpecifically, we focus on ISO, shutter speed, and F-number, which are closely\nrelated to noise levels. We convert these selected parameters into a vector to\ncontrol and enhance the performance of the denoising network. Experimental\nresults show that our method seamlessly adds controllability to standard\ndenoising neural networks and improves their performance. Code is available at\nhttps://github.com/OBAKSA/CPADNet.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f8\u673a\u53c2\u6570\u7684\u53ef\u63a7\u53bb\u566a\u6846\u67b6\uff0c\u901a\u8fc7ISO\u3001\u5feb\u95e8\u901f\u5ea6\u548c\u5149\u5708\u503c\u8c03\u6574\u53bb\u566a\u5f3a\u5ea6\uff0c\u63d0\u5347\u4e86\u53bb\u566a\u7f51\u7edc\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u6839\u636e\u566a\u58f0\u6c34\u5e73\u3001\u76f8\u673a\u8bbe\u7f6e\u548c\u7528\u6237\u504f\u597d\u8c03\u6574\u53bb\u566a\u5f3a\u5ea6\u7684\u7075\u6d3b\u6027\u3002", "method": "\u5c06ISO\u3001\u5feb\u95e8\u901f\u5ea6\u548c\u5149\u5708\u503c\u8f6c\u6362\u4e3a\u5411\u91cf\uff0c\u7528\u4e8e\u63a7\u5236\u548c\u589e\u5f3a\u53bb\u566a\u7f51\u7edc\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e3a\u6807\u51c6\u53bb\u566a\u795e\u7ecf\u7f51\u7edc\u589e\u52a0\u4e86\u53ef\u63a7\u6027\u5e76\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u57fa\u4e8e\u76f8\u673a\u53c2\u6570\u7684\u81ea\u9002\u5e94\u53bb\u566a\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.01679", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01679", "abs": "https://arxiv.org/abs/2507.01679", "authors": ["Zeyu Huang", "Tianhao Cheng", "Zihan Qiu", "Zili Wang", "Yinghui Xu", "Edoardo M. Ponti", "Ivan Titov"], "title": "Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling", "comment": "Work in progress", "summary": "Existing post-training techniques for large language models are broadly\ncategorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning\n(RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking\ndemonstration data but can lead to problematic generalization as a form of\nbehavior cloning. Conversely, RFT can significantly enhance a model's\nperformance but is prone to learn unexpected behaviors, and its performance is\nhighly sensitive to the initial policy. In this paper, we propose a unified\nview of these methods and introduce Prefix-RFT, a hybrid approach that\nsynergizes learning from both demonstration and exploration. Using mathematical\nreasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is\nboth simple and effective. It not only surpasses the performance of standalone\nSFT and RFT but also outperforms parallel mixed-policy RFT methods. A key\nadvantage is its seamless integration into existing open-source frameworks,\nrequiring only minimal modifications to the standard RFT pipeline. Our analysis\nhighlights the complementary nature of SFT and RFT, and validates that\nPrefix-RFT effectively harmonizes these two learning paradigms. Furthermore,\nablation studies confirm the method's robustness to variations in the quality\nand quantity of demonstration data. We hope this work offers a new perspective\non LLM post-training, suggesting that a unified paradigm that judiciously\nintegrates demonstration and exploration could be a promising direction for\nfuture research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPrefix-RFT\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u5f3a\u5316\u5fae\u8c03\uff08RFT\uff09\u7684\u4f18\u52bf\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u6846\u67b6\u4e2d\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u540e\u8bad\u7ec3\u65b9\u6cd5\uff08SFT\u548cRFT\uff09\u5404\u6709\u4f18\u7f3a\u70b9\uff0cSFT\u53ef\u80fd\u6cdb\u5316\u4e0d\u4f73\uff0cRFT\u5219\u5bf9\u521d\u59cb\u7b56\u7565\u654f\u611f\u4e14\u53ef\u80fd\u5b66\u4e60\u5230\u610f\u5916\u884c\u4e3a\u3002\u8bba\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u7edf\u4e00\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u3002", "method": "\u63d0\u51faPrefix-RFT\uff0c\u4e00\u79cd\u6df7\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u4ece\u6f14\u793a\u6570\u636e\u5b66\u4e60\u548c\u63a2\u7d22\u5b66\u4e60\u7684\u4f18\u52bf\u3002\u8be5\u65b9\u6cd5\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8fdb\u884c\u4e86\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "result": "Prefix-RFT\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u5355\u72ec\u7684SFT\u548cRFT\uff0c\u5e76\u4f18\u4e8e\u5e76\u884c\u6df7\u5408\u7b56\u7565\u7684RFT\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u5b83\u5bf9\u6f14\u793a\u6570\u636e\u7684\u8d28\u91cf\u548c\u6570\u91cf\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "Prefix-RFT\u6709\u6548\u7ed3\u5408\u4e86SFT\u548cRFT\u7684\u4e92\u8865\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u8303\u5f0f\u3002"}}
{"id": "2507.01590", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01590", "abs": "https://arxiv.org/abs/2507.01590", "authors": ["Ameer Hamza", "Zuhaib Hussain But", "Umar Arif", "Samiya", "M. Abdullah Asad", "Muhammad Naeem"], "title": "Autonomous AI Surveillance: Multimodal Deep Learning for Cognitive and Behavioral Monitoring", "comment": null, "summary": "This study presents a novel classroom surveillance system that integrates\nmultiple modalities, including drowsiness, tracking of mobile phone usage, and\nface recognition,to assess student attentiveness with enhanced precision.The\nsystem leverages the YOLOv8 model to detect both mobile phone and sleep\nusage,(Ghatge et al., 2024) while facial recognition is achieved through\nLResNet Occ FC body tracking using YOLO and MTCNN.(Durai et al., 2024) These\nmodels work in synergy to provide comprehensive, real-time monitoring, offering\ninsights into student engagement and behavior.(S et al., 2023) The framework is\ntrained on specialized datasets, such as the RMFD dataset for face recognition\nand a Roboflow dataset for mobile phone detection. The extensive evaluation of\nthe system shows promising results. Sleep detection achieves 97. 42% mAP@50,\nface recognition achieves 86. 45% validation accuracy and mobile phone\ndetection reach 85. 89% mAP@50. The system is implemented within a core PHP web\napplication and utilizes ESP32-CAM hardware for seamless data capture.(Neto et\nal., 2024) This integrated approach not only enhances classroom monitoring, but\nalso ensures automatic attendance recording via face recognition as students\nremain seated in the classroom, offering scalability for diverse educational\nenvironments.(Banada,2025)", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u8bfe\u5802\u76d1\u63a7\u7cfb\u7edf\uff0c\u7ed3\u5408\u7761\u610f\u68c0\u6d4b\u3001\u624b\u673a\u4f7f\u7528\u8ffd\u8e2a\u548c\u4eba\u8138\u8bc6\u522b\uff0c\u63d0\u5347\u5b66\u751f\u6ce8\u610f\u529b\u8bc4\u4f30\u7684\u7cbe\u786e\u5ea6\u3002", "motivation": "\u901a\u8fc7\u591a\u6a21\u6001\u6280\u672f\u5b9e\u65f6\u76d1\u63a7\u5b66\u751f\u884c\u4e3a\uff0c\u63d0\u9ad8\u8bfe\u5802\u53c2\u4e0e\u5ea6\u548c\u81ea\u52a8\u8003\u52e4\u6548\u7387\u3002", "method": "\u4f7f\u7528YOLOv8\u68c0\u6d4b\u624b\u673a\u548c\u7761\u7720\u884c\u4e3a\uff0cLResNet Occ FC\u548cMTCNN\u8fdb\u884c\u4eba\u8138\u8bc6\u522b\uff0c\u7ed3\u5408PHP\u548cESP32-CAM\u786c\u4ef6\u5b9e\u73b0\u3002", "result": "\u7761\u7720\u68c0\u6d4bmAP@50\u4e3a97.42%\uff0c\u4eba\u8138\u8bc6\u522b\u51c6\u786e\u738786.45%\uff0c\u624b\u673a\u68c0\u6d4bmAP@50\u4e3a85.89%\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u6559\u80b2\u73af\u5883\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u76d1\u63a7\u548c\u8003\u52e4\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01735", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01735", "abs": "https://arxiv.org/abs/2507.01735", "authors": ["Kai Chen", "Ruiyuan Gao", "Lanqing Hong", "Hang Xu", "Xu Jia", "Holger Caesar", "Dengxin Dai", "Bingbing Liu", "Dzmitry Tsishkou", "Songcen Xu", "Chunjing Xu", "Qiang Xu", "Huchuan Lu", "Dit-Yan Yeung"], "title": "ECCV 2024 W-CODA: 1st Workshop on Multimodal Perception and Comprehension of Corner Cases in Autonomous Driving", "comment": "ECCV 2024. Workshop page: https://coda-dataset.github.io/w-coda2024/", "summary": "In this paper, we present details of the 1st W-CODA workshop, held in\nconjunction with the ECCV 2024. W-CODA aims to explore next-generation\nsolutions for autonomous driving corner cases, empowered by state-of-the-art\nmultimodal perception and comprehension techniques. 5 Speakers from both\nacademia and industry are invited to share their latest progress and opinions.\nWe collect research papers and hold a dual-track challenge, including both\ncorner case scene understanding and generation. As the pioneering effort, we\nwill continuously bridge the gap between frontier autonomous driving techniques\nand fully intelligent, reliable self-driving agents robust towards corner\ncases.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u7b2c\u4e00\u5c4aW-CODA\u7814\u8ba8\u4f1a\u7684\u7ec6\u8282\uff0c\u805a\u7126\u4e8e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u6781\u7aef\u573a\u666f\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u63a2\u7d22\u4e0b\u4e00\u4ee3\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\uff0c\u89e3\u51b3\u6781\u7aef\u573a\u666f\u95ee\u9898\u3002", "method": "\u9080\u8bf75\u4f4d\u6f14\u8bb2\u8005\u5206\u4eab\u6700\u65b0\u8fdb\u5c55\uff0c\u5e76\u4e3e\u529e\u53cc\u8f68\u6311\u6218\u8d5b\uff08\u573a\u666f\u7406\u89e3\u4e0e\u751f\u6210\uff09\u3002", "result": "\u6c47\u96c6\u7814\u7a76\u6210\u679c\uff0c\u63a8\u52a8\u524d\u6cbf\u6280\u672f\u4e0e\u53ef\u9760\u81ea\u52a8\u9a7e\u9a76\u7684\u878d\u5408\u3002", "conclusion": "W-CODA\u5c06\u6301\u7eed\u5f25\u5408\u6280\u672f\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2507.01522", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.01522", "abs": "https://arxiv.org/abs/2507.01522", "authors": ["Koen Ponse", "Jan Felix Kleuker", "Aske Plaat", "Thomas Moerland"], "title": "Chargax: A JAX Accelerated EV Charging Simulator", "comment": "Accepted at RLC 2025", "summary": "Deep Reinforcement Learning can play a key role in addressing sustainable\nenergy challenges. For instance, many grid systems are heavily congested,\nhighlighting the urgent need to enhance operational efficiency. However,\nreinforcement learning approaches have traditionally been slow due to the high\nsample complexity and expensive simulation requirements. While recent works\nhave effectively used GPUs to accelerate data generation by converting\nenvironments to JAX, these works have largely focussed on classical toy\nproblems. This paper introduces Chargax, a JAX-based environment for realistic\nsimulation of electric vehicle charging stations designed for accelerated\ntraining of RL agents. We validate our environment in a variety of scenarios\nbased on real data, comparing reinforcement learning agents against baselines.\nChargax delivers substantial computational performance improvements of over\n100x-1000x over existing environments. Additionally, Chargax' modular\narchitecture enables the representation of diverse real-world charging station\nconfigurations.", "AI": {"tldr": "Chargax\u662f\u4e00\u4e2a\u57fa\u4e8eJAX\u7684\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u7ad9\u4eff\u771f\u73af\u5883\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u7535\u7f51\u7cfb\u7edf\u62e5\u5835\u95ee\u9898\u4e9f\u9700\u63d0\u5347\u8fd0\u884c\u6548\u7387\uff0c\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u56e0\u9ad8\u6837\u672c\u590d\u6742\u5ea6\u548c\u4eff\u771f\u6210\u672c\u800c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8eJAX\u7684Chargax\u73af\u5883\uff0c\u652f\u6301\u771f\u5b9e\u6570\u636e\u573a\u666f\u4e0b\u7684\u4eff\u771f\uff0c\u5e76\u5bf9\u6bd4\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u3002", "result": "Chargax\u5728\u8ba1\u7b97\u6027\u80fd\u4e0a\u6bd4\u73b0\u6709\u73af\u5883\u63d0\u5347100x-1000x\uff0c\u5e76\u80fd\u6a21\u5757\u5316\u8868\u793a\u591a\u6837\u5316\u7684\u5145\u7535\u7ad9\u914d\u7f6e\u3002", "conclusion": "Chargax\u4e3a\u53ef\u6301\u7eed\u80fd\u6e90\u6311\u6218\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5de5\u5177\u3002"}}
{"id": "2507.01603", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01603", "abs": "https://arxiv.org/abs/2507.01603", "authors": ["Yue-Jiang Dong", "Wang Zhao", "Jiale Xu", "Ying Shan", "Song-Hai Zhang"], "title": "DepthSync: Diffusion Guidance-Based Depth Synchronization for Scale- and Geometry-Consistent Video Depth Estimation", "comment": "Accepted by ICCV 2025", "summary": "Diffusion-based video depth estimation methods have achieved remarkable\nsuccess with strong generalization ability. However, predicting depth for long\nvideos remains challenging. Existing methods typically split videos into\noverlapping sliding windows, leading to accumulated scale discrepancies across\ndifferent windows, particularly as the number of windows increases.\nAdditionally, these methods rely solely on 2D diffusion priors, overlooking the\ninherent 3D geometric structure of video depths, which results in geometrically\ninconsistent predictions. In this paper, we propose DepthSync, a novel,\ntraining-free framework using diffusion guidance to achieve scale- and\ngeometry-consistent depth predictions for long videos. Specifically, we\nintroduce scale guidance to synchronize the depth scale across windows and\ngeometry guidance to enforce geometric alignment within windows based on the\ninherent 3D constraints in video depths. These two terms work synergistically,\nsteering the denoising process toward consistent depth predictions. Experiments\non various datasets validate the effectiveness of our method in producing depth\nestimates with improved scale and geometry consistency, particularly for long\nvideos.", "AI": {"tldr": "DepthSync\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u6563\u5f15\u5bfc\u5b9e\u73b0\u957f\u89c6\u9891\u6df1\u5ea6\u4f30\u8ba1\u7684\u5c3a\u5ea6\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u957f\u89c6\u9891\u65f6\u5b58\u5728\u5c3a\u5ea6\u4e0d\u4e00\u81f4\u548c\u51e0\u4f55\u9884\u6d4b\u4e0d\u51c6\u786e\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u4f9d\u8d562D\u6269\u6563\u5148\u9a8c\u800c\u5ffd\u7565\u4e863D\u51e0\u4f55\u7ed3\u6784\u3002", "method": "\u5f15\u5165\u5c3a\u5ea6\u5f15\u5bfc\u548c\u51e0\u4f55\u5f15\u5bfc\uff0c\u534f\u540c\u4f18\u5316\u53bb\u566a\u8fc7\u7a0b\uff0c\u786e\u4fdd\u8de8\u7a97\u53e3\u7684\u5c3a\u5ea6\u540c\u6b65\u548c\u7a97\u53e3\u5185\u7684\u51e0\u4f55\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDepthSync\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u957f\u89c6\u9891\u6df1\u5ea6\u4f30\u8ba1\u7684\u5c3a\u5ea6\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u3002", "conclusion": "DepthSync\u4e3a\u957f\u89c6\u9891\u6df1\u5ea6\u4f30\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.01752", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.01752", "abs": "https://arxiv.org/abs/2507.01752", "authors": ["Ismail Labiad", "Mathurin Videau", "Matthieu Kowalski", "Marc Schoenauer", "Alessandro Leite", "Julia Kempe", "Olivier Teytaud"], "title": "Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training", "comment": null, "summary": "Gradient-based optimization is the workhorse of deep learning, offering\nefficient and scalable training via backpropagation. However, its reliance on\nlarge volumes of labeled data raises privacy and security concerns such as\nsusceptibility to data poisoning attacks and the risk of overfitting. In\ncontrast, black box optimization methods, which treat the model as an opaque\nfunction, relying solely on function evaluations to guide optimization, offer a\npromising alternative in scenarios where data access is restricted, adversarial\nrisks are high, or overfitting is a concern. However, black box methods also\npose significant challenges, including poor scalability to high-dimensional\nparameter spaces, as prevalent in large language models (LLMs), and high\ncomputational costs due to reliance on numerous model evaluations. This paper\nintroduces BBoxER, an evolutionary black-box method for LLM post-training that\ninduces an information bottleneck via implicit compression of the training\ndata. Leveraging the tractability of information flow, we provide strong\ntheoretical bounds on generalization, differential privacy, susceptibility to\ndata poisoning attacks, and robustness to extraction attacks. BBoxER operates\non top of pre-trained LLMs, offering a lightweight and modular enhancement\nsuitable for deployment in restricted or privacy-sensitive environments, in\naddition to non-vacuous generalization guarantees. In experiments with LLMs, we\ndemonstrate empirically that Retrofitting methods are able to learn, showing\nhow a few iterations of BBoxER improve performance and generalize well on a\nbenchmark of reasoning datasets. This positions BBoxER as an attractive add-on\non top of gradient-based optimization.", "AI": {"tldr": "BBoxER\u662f\u4e00\u79cd\u57fa\u4e8e\u8fdb\u5316\u7684\u9ed1\u76d2\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u540e\u8bad\u7ec3\uff0c\u901a\u8fc7\u9690\u5f0f\u538b\u7f29\u8bad\u7ec3\u6570\u636e\u5f15\u5165\u4fe1\u606f\u74f6\u9888\uff0c\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\uff0c\u9002\u7528\u4e8e\u9690\u79c1\u654f\u611f\u73af\u5883\u3002", "motivation": "\u4f20\u7edf\u68af\u5ea6\u4f18\u5316\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u5b58\u5728\u9690\u79c1\u548c\u5b89\u5168\u98ce\u9669\uff1b\u9ed1\u76d2\u4f18\u5316\u65b9\u6cd5\u5728\u6570\u636e\u53d7\u9650\u6216\u5bf9\u6297\u98ce\u9669\u9ad8\u65f6\u66f4\u9002\u7528\uff0c\u4f46\u9762\u4e34\u53ef\u6269\u5c55\u6027\u548c\u8ba1\u7b97\u6210\u672c\u6311\u6218\u3002", "method": "BBoxER\u901a\u8fc7\u4fe1\u606f\u74f6\u9888\u9690\u5f0f\u538b\u7f29\u8bad\u7ec3\u6570\u636e\uff0c\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u5728\u9884\u8bad\u7ec3LLM\u4e0a\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u6a21\u5757\u5316\u589e\u5f3a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBBoxER\u80fd\u63d0\u5347LLM\u6027\u80fd\u5e76\u5728\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u6cdb\u5316\u826f\u597d\u3002", "conclusion": "BBoxER\u662f\u68af\u5ea6\u4f18\u5316\u7684\u6709\u529b\u8865\u5145\uff0c\u9002\u7528\u4e8e\u9690\u79c1\u654f\u611f\u548c\u9ad8\u5bf9\u6297\u73af\u5883\u3002"}}
{"id": "2507.01354", "categories": ["cs.LG", "physics.ao-ph", "86A10 (Primary) 86A22, 68U10 (Secondary)", "J.2; I.4.4"], "pdf": "https://arxiv.org/pdf/2507.01354", "abs": "https://arxiv.org/abs/2507.01354", "authors": ["Chugang Yi", "Minghan Yu", "Weikang Qian", "Yixin Wen", "Haizhao Yang"], "title": "Efficient Kilometer-Scale Precipitation Downscaling with Conditional Wavelet Diffusion", "comment": null, "summary": "Effective hydrological modeling and extreme weather analysis demand\nprecipitation data at a kilometer-scale resolution, which is significantly\nfiner than the 10 km scale offered by standard global products like IMERG. To\naddress this, we propose the Wavelet Diffusion Model (WDM), a generative\nframework that achieves 10x spatial super-resolution (downscaling to 1 km) and\ndelivers a 9x inference speedup over pixel-based diffusion models. WDM is a\nconditional diffusion model that learns the learns the complex structure of\nprecipitation from MRMS radar data directly in the wavelet domain. By focusing\non high-frequency wavelet coefficients, it generates exceptionally realistic\nand detailed 1-km precipitation fields. This wavelet-based approach produces\nvisually superior results with fewer artifacts than pixel-space models, and\ndelivers a significant gains in sampling efficiency. Our results demonstrate\nthat WDM provides a robust solution to the dual challenges of accuracy and\nspeed in geoscience super-resolution, paving the way for more reliable\nhydrological forecasts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u7684\u6269\u6563\u6a21\u578b\uff08WDM\uff09\uff0c\u7528\u4e8e\u5c06\u964d\u6c34\u6570\u636e\u4ece10\u516c\u91cc\u5206\u8fa8\u7387\u964d\u5c3a\u5ea6\u52301\u516c\u91cc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u5168\u7403\u964d\u6c34\u6570\u636e\uff08\u5982IMERG\uff09\u5206\u8fa8\u7387\u8f83\u4f4e\uff0810\u516c\u91cc\uff09\uff0c\u65e0\u6cd5\u6ee1\u8db3\u6c34\u6587\u5efa\u6a21\u548c\u6781\u7aef\u5929\u6c14\u5206\u6790\u7684\u9700\u6c42\u3002", "method": "WDM\u662f\u4e00\u79cd\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u76f4\u63a5\u5728\u5c0f\u6ce2\u57df\u5b66\u4e60\u964d\u6c34\u6570\u636e\u7684\u590d\u6742\u7ed3\u6784\uff0c\u4e13\u6ce8\u4e8e\u9ad8\u9891\u5c0f\u6ce2\u7cfb\u6570\uff0c\u751f\u62101\u516c\u91cc\u5206\u8fa8\u7387\u7684\u964d\u6c34\u573a\u3002", "result": "WDM\u5b9e\u73b0\u4e8610\u500d\u7a7a\u95f4\u8d85\u5206\u8fa8\u7387\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4\u57fa\u4e8e\u50cf\u7d20\u7684\u6269\u6563\u6a21\u578b\u5feb9\u500d\uff0c\u751f\u6210\u7ed3\u679c\u66f4\u771f\u5b9e\u4e14\u7ec6\u8282\u4e30\u5bcc\u3002", "conclusion": "WDM\u4e3a\u5730\u7403\u79d1\u5b66\u8d85\u5206\u8fa8\u7387\u95ee\u9898\u63d0\u4f9b\u4e86\u51c6\u786e\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u66f4\u53ef\u9760\u7684\u6c34\u6587\u9884\u62a5\u3002"}}
{"id": "2507.01607", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01607", "abs": "https://arxiv.org/abs/2507.01607", "authors": ["Quentin Le Roux", "Yannick Teglia", "Teddy Furon", "Philippe Loubet-Moundi", "Eric Bourbao"], "title": "Survivability of Backdoor Attacks on Unconstrained Face Recognition Systems", "comment": null, "summary": "The widespread use of deep learning face recognition raises several security\nconcerns. Although prior works point at existing vulnerabilities, DNN backdoor\nattacks against real-life, unconstrained systems dealing with images captured\nin the wild remain a blind spot of the literature. This paper conducts the\nfirst system-level study of backdoors in deep learning-based face recognition\nsystems. This paper yields four contributions by exploring the feasibility of\nDNN backdoors on these pipelines in a holistic fashion. We demonstrate for the\nfirst time two backdoor attacks on the face detection task: face generation and\nface landmark shift attacks. We then show that face feature extractors trained\nwith large margin losses also fall victim to backdoor attacks. Combining our\nmodels, we then show using 20 possible pipeline configurations and 15 attack\ncases that a single backdoor enables an attacker to bypass the entire function\nof a system. Finally, we provide stakeholders with several best practices and\ncountermeasures.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u6df1\u5ea6\u5b66\u4e60\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u4e2d\u7684\u540e\u95e8\u653b\u51fb\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u578b\u653b\u51fb\u65b9\u5f0f\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u591a\u79cd\u914d\u7f6e\u4e0b\u7684\u6709\u6548\u6027\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u9632\u62a4\u5efa\u8bae\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u7684\u5e7f\u6cdb\u5e94\u7528\u5f15\u53d1\u4e86\u5b89\u5168\u9690\u60a3\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5bf9\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u540e\u95e8\u653b\u51fb\u5173\u6ce8\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u63a2\u7d22\u6df1\u5ea6\u5b66\u4e60\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u7684\u540e\u95e8\u653b\u51fb\u53ef\u884c\u6027\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u578b\u653b\u51fb\uff08\u4eba\u8138\u751f\u6210\u548c\u9762\u90e8\u6807\u5fd7\u70b9\u504f\u79fb\u653b\u51fb\uff09\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u572820\u79cd\u7cfb\u7edf\u914d\u7f6e\u548c15\u79cd\u653b\u51fb\u6848\u4f8b\u4e2d\u7684\u6709\u6548\u6027\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5355\u4e00\u540e\u95e8\u653b\u51fb\u53ef\u7ed5\u8fc7\u6574\u4e2a\u7cfb\u7edf\u529f\u80fd\uff0c\u4e14\u57fa\u4e8e\u5927\u95f4\u9694\u635f\u5931\u8bad\u7ec3\u7684\u7279\u5f81\u63d0\u53d6\u5668\u540c\u6837\u6613\u53d7\u653b\u51fb\u3002", "conclusion": "\u672c\u6587\u63ed\u793a\u4e86\u6df1\u5ea6\u5b66\u4e60\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u7684\u540e\u95e8\u653b\u51fb\u98ce\u9669\uff0c\u5e76\u63d0\u51fa\u4e86\u9632\u62a4\u63aa\u65bd\uff0c\u4e3a\u76f8\u5173\u5229\u76ca\u65b9\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2507.01806", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.01806", "abs": "https://arxiv.org/abs/2507.01806", "authors": ["Reza Arabpour", "Haitz S\u00e1ez de Oc\u00e1riz Borde", "Anastasis Kratsios"], "title": "LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework for LLMs", "comment": "5-page main paper (excluding references) + 11-page appendix, 3\n  tables, 1 figure. Accepted to ICML 2025 Workshop on Efficient Systems for\n  Foundation Models", "summary": "Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language\nModels (LLMs) by enabling parameter-efficient updates. However, their\nwidespread adoption remains limited by the reliance on GPU-based training. In\nthis work, we propose a theoretically grounded approach to LoRA fine-tuning\ndesigned specifically for users with limited computational resources,\nparticularly those restricted to standard laptop CPUs. Our method learns a\nmeta-operator that maps any input dataset, represented as a probability\ndistribution, to a set of LoRA weights by leveraging a large bank of\npre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of\nperforming new gradient-based updates, our pipeline constructs adapters via\nlightweight combinations of existing LoRAs directly on CPU. While the resulting\nadapters do not match the performance of GPU-trained counterparts, they\nconsistently outperform the base Mistral model on downstream tasks, offering a\npractical and accessible alternative to traditional GPU-based fine-tuning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9CPU\u8bbe\u5907\u7684LoRA\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u9002\u914d\u5668\u7ec4\u5408\u751f\u6210\u65b0\u9002\u914d\u5668\uff0c\u65e0\u9700GPU\u8bad\u7ec3\u3002", "motivation": "\u89e3\u51b3LoRA\u5fae\u8c03\u5bf9GPU\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u4e3a\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u7528\u6237\u63d0\u4f9b\u53ef\u884c\u65b9\u6848\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u9002\u914d\u5668\u5e93\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7ec4\u5408\u751f\u6210\u65b0\u9002\u914d\u5668\uff0c\u76f4\u63a5\u5728CPU\u4e0a\u8fd0\u884c\u3002", "result": "\u751f\u6210\u7684\u9002\u914d\u5668\u6027\u80fd\u867d\u4e0d\u53caGPU\u8bad\u7ec3\u7248\u672c\uff0c\u4f46\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\uff0c\u9002\u5408\u8d44\u6e90\u6709\u9650\u573a\u666f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aCPU\u7528\u6237\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684LoRA\u5fae\u8c03\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2507.01608", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.01608", "abs": "https://arxiv.org/abs/2507.01608", "authors": ["Xu Zhang", "Ming Lu", "Yan Chen", "Zhan Ma"], "title": "Perception-Oriented Latent Coding for High-Performance Compressed Domain Semantic Inference", "comment": "International Conference on Multimedia and Expo (ICME), 2025", "summary": "In recent years, compressed domain semantic inference has primarily relied on\nlearned image coding models optimized for mean squared error (MSE). However,\nMSE-oriented optimization tends to yield latent spaces with limited semantic\nrichness, which hinders effective semantic inference in downstream tasks.\nMoreover, achieving high performance with these models often requires\nfine-tuning the entire vision model, which is computationally intensive,\nespecially for large models. To address these problems, we introduce\nPerception-Oriented Latent Coding (POLC), an approach that enriches the\nsemantic content of latent features for high-performance compressed domain\nsemantic inference. With the semantically rich latent space, POLC requires only\na plug-and-play adapter for fine-tuning, significantly reducing the parameter\ncount compared to previous MSE-oriented methods. Experimental results\ndemonstrate that POLC achieves rate-perception performance comparable to\nstate-of-the-art generative image coding methods while markedly enhancing\nperformance in vision tasks, with minimal fine-tuning overhead. Code is\navailable at https://github.com/NJUVISION/POLC.", "AI": {"tldr": "POLC\u63d0\u51fa\u4e86\u4e00\u79cd\u611f\u77e5\u5bfc\u5411\u7684\u6f5c\u5728\u7f16\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e30\u5bcc\u6f5c\u5728\u7279\u5f81\u7684\u8bed\u4e49\u5185\u5bb9\uff0c\u63d0\u5347\u4e86\u538b\u7f29\u57df\u8bed\u4e49\u63a8\u7406\u7684\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u5fae\u8c03\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8eMSE\u4f18\u5316\u7684\u56fe\u50cf\u7f16\u7801\u6a21\u578b\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8bed\u4e49\u4e30\u5bcc\u6027\u4e0d\u8db3\uff0c\u4e14\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u8fdb\u884c\u5fae\u8c03\uff0c\u9650\u5236\u4e86\u538b\u7f29\u57df\u8bed\u4e49\u63a8\u7406\u7684\u6548\u679c\u3002", "method": "\u63d0\u51faPOLC\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u6f5c\u5728\u7279\u5f81\u7684\u8bed\u4e49\u5185\u5bb9\uff0c\u4ec5\u9700\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u9002\u914d\u5668\u8fdb\u884c\u5fae\u8c03\uff0c\u663e\u8457\u51cf\u5c11\u53c2\u6570\u6570\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPOLC\u5728\u7387\u611f\u77e5\u6027\u80fd\u4e0a\u4e0e\u751f\u6210\u5f0f\u56fe\u50cf\u7f16\u7801\u65b9\u6cd5\u76f8\u5f53\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u4efb\u52a1\u6027\u80fd\uff0c\u4e14\u5fae\u8c03\u5f00\u9500\u6781\u5c0f\u3002", "conclusion": "POLC\u901a\u8fc7\u8bed\u4e49\u4e30\u5bcc\u7684\u6f5c\u5728\u7a7a\u95f4\u548c\u8f7b\u91cf\u7ea7\u5fae\u8c03\u673a\u5236\uff0c\u4e3a\u538b\u7f29\u57df\u8bed\u4e49\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01951", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01951", "abs": "https://arxiv.org/abs/2507.01951", "authors": ["Zixiao Wang", "Yuxin Wang", "Xiaorui Wang", "Mengting Xing", "Jie Gao", "Jianjun Xu", "Guangcan Liu", "Chenhui Jin", "Zhuo Wang", "Shengzhuo Zhang", "Hongtao Xie"], "title": "Test-Time Scaling with Reflective Generative Model", "comment": null, "summary": "We introduce our first reflective generative model MetaStone-S1, which\nobtains OpenAI o3's performance via the self-supervised process reward model\n(SPRM). Through sharing the backbone network and using task-specific heads for\nnext token prediction and process scoring respectively, SPRM successfully\nintegrates the policy model and process reward model(PRM) into a unified\ninterface without extra process annotation, reducing over 99% PRM parameters\nfor efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable\nfor test time scaling (TTS), and we provide three reasoning effort modes (low,\nmedium, and high), based on the controllable thinking length. Moreover, we\nempirically establish a scaling law that reveals the relationship between total\nthinking computation and TTS performance. Experiments demonstrate that our\nMetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with\nonly 32B parameter size. To support the research community, we have\nopen-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.", "AI": {"tldr": "MetaStone-S1\u662f\u4e00\u79cd\u53cd\u5c04\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08SPRM\uff09\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\uff0c\u53c2\u6570\u51cf\u5c1199%\uff0c\u6027\u80fd\u5ab2\u7f8eOpenAI o3\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u7edf\u4e00\u63a5\u53e3\u6574\u5408\u7b56\u7565\u6a21\u578b\u548c\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff0c\u51cf\u5c11\u53c2\u6570\u5e76\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002", "method": "\u91c7\u7528\u5171\u4eab\u4e3b\u5e72\u7f51\u7edc\u548c\u4efb\u52a1\u7279\u5b9a\u5934\uff0c\u7ed3\u5408SPRM\u5b9e\u73b0\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u652f\u6301\u53ef\u63a7\u601d\u8003\u957f\u5ea6\u3002", "result": "\u5b9e\u9a8c\u663e\u793aMetaStone-S1\u4ec5\u752832B\u53c2\u6570\u5373\u8fbe\u5230OpenAI-o3-mini\u7cfb\u5217\u6027\u80fd\u3002", "conclusion": "MetaStone-S1\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u751f\u6210\u6a21\u578b\uff0c\u5e76\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.01389", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.01389", "abs": "https://arxiv.org/abs/2507.01389", "authors": ["Anbang Wang", "Dunbo Cai", "Yu Zhang", "Yangqing Huang", "Xiangyang Feng", "Zhihong Zhang"], "title": "Surrogate Modeling via Factorization Machine and Ising Model with Enhanced Higher-Order Interaction Learning", "comment": null, "summary": "Recently, a surrogate model was proposed that employs a factorization machine\nto approximate the underlying input-output mapping of the original system, with\nquantum annealing used to optimize the resulting surrogate function. Inspired\nby this approach, we propose an enhanced surrogate model that incorporates\nadditional slack variables into both the factorization machine and its\nassociated Ising representation thereby unifying what was by design a two-step\nprocess into a single, integrated step. During the training phase, the slack\nvariables are iteratively updated, enabling the model to account for\nhigher-order feature interactions. We apply the proposed method to the task of\npredicting drug combination effects. Experimental results indicate that the\nintroduction of slack variables leads to a notable improvement of performance.\nOur algorithm offers a promising approach for building efficient surrogate\nmodels that exploit potential quantum advantages.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u7684\u66ff\u4ee3\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u677e\u5f1b\u53d8\u91cf\u5c06\u4e24\u6b65\u8fc7\u7a0b\u7edf\u4e00\u4e3a\u4e00\u6b65\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u53d7\u73b0\u6709\u91cf\u5b50\u9000\u706b\u4f18\u5316\u66ff\u4ee3\u6a21\u578b\u7684\u542f\u53d1\uff0c\u5e0c\u671b\u8fdb\u4e00\u6b65\u4f18\u5316\u6a21\u578b\u6027\u80fd\u3002", "method": "\u5728\u56e0\u5b50\u5206\u89e3\u673a\u53ca\u5176Ising\u8868\u793a\u4e2d\u5f15\u5165\u677e\u5f1b\u53d8\u91cf\uff0c\u7edf\u4e00\u4e24\u6b65\u4e3a\u4e00\u6b65\uff0c\u5e76\u5728\u8bad\u7ec3\u4e2d\u8fed\u4ee3\u66f4\u65b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u677e\u5f1b\u53d8\u91cf\u7684\u5f15\u5165\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5229\u7528\u91cf\u5b50\u4f18\u52bf\u6784\u5efa\u9ad8\u6548\u66ff\u4ee3\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\u3002"}}
{"id": "2507.01630", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01630", "abs": "https://arxiv.org/abs/2507.01630", "authors": ["Yuxiao Wang", "Yu Lei", "Zhenao Wei", "Weiying Xue", "Xinyu Jiang", "Nan Zhuang", "Qi Liu"], "title": "Prompt Guidance and Human Proximal Perception for HOT Prediction with Regional Joint Loss", "comment": "Accepted by ICCV 2025", "summary": "The task of Human-Object conTact (HOT) detection involves identifying the\nspecific areas of the human body that are touching objects. Nevertheless,\ncurrent models are restricted to just one type of image, often leading to too\nmuch segmentation in areas with little interaction, and struggling to maintain\ncategory consistency within specific regions. To tackle this issue, a HOT\nframework, termed \\textbf{P3HOT}, is proposed, which blends \\textbf{P}rompt\nguidance and human \\textbf{P}roximal \\textbf{P}erception. To begin with, we\nutilize a semantic-driven prompt mechanism to direct the network's attention\ntowards the relevant regions based on the correlation between image and text.\nThen a human proximal perception mechanism is employed to dynamically perceive\nkey depth range around the human, using learnable parameters to effectively\neliminate regions where interactions are not expected. Calculating depth\nresolves the uncertainty of the overlap between humans and objects in a 2D\nperspective, providing a quasi-3D viewpoint. Moreover, a Regional Joint Loss\n(RJLoss) has been created as a new loss to inhibit abnormal categories in the\nsame area. A new evaluation metric called ``AD-Acc.'' is introduced to address\nthe shortcomings of existing methods in addressing negative samples.\nComprehensive experimental results demonstrate that our approach achieves\nstate-of-the-art performance in four metrics across two benchmark datasets.\nSpecifically, our model achieves an improvement of \\textbf{0.7}$\\uparrow$,\n\\textbf{2.0}$\\uparrow$, \\textbf{1.6}$\\uparrow$, and \\textbf{11.0}$\\uparrow$ in\nSC-Acc., mIoU, wIoU, and AD-Acc. metrics, respectively, on the HOT-Annotated\ndataset. Code is available at https://github.com/YuxiaoWang-AI/P3HOT.", "AI": {"tldr": "P3HOT\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u63d0\u793a\u5f15\u5bfc\u548c\u4eba\u7c7b\u8fd1\u7aef\u611f\u77e5\uff0c\u6539\u8fdbHOT\u68c0\u6d4b\u4efb\u52a1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u533a\u57df\u5206\u5272\u548c\u7c7b\u522b\u4e00\u81f4\u6027\u4e0a\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u5f53\u524dHOT\u68c0\u6d4b\u6a21\u578b\u5c40\u9650\u4e8e\u5355\u4e00\u56fe\u50cf\u7c7b\u578b\uff0c\u5bfc\u81f4\u8fc7\u591a\u5206\u5272\u548c\u7c7b\u522b\u4e00\u81f4\u6027\u5dee\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "P3HOT\u6846\u67b6\u7ed3\u5408\u8bed\u4e49\u9a71\u52a8\u7684\u63d0\u793a\u673a\u5236\u548c\u4eba\u7c7b\u8fd1\u7aef\u611f\u77e5\u673a\u5236\uff0c\u5f15\u5165\u533a\u57df\u8054\u5408\u635f\u5931\uff08RJLoss\uff09\u548c\u65b0\u8bc4\u4f30\u6307\u6807AD-Acc\u3002", "result": "\u5728HOT-Annotated\u6570\u636e\u96c6\u4e0a\uff0cSC-Acc.\u3001mIoU\u3001wIoU\u548cAD-Acc.\u6307\u6807\u5206\u522b\u63d0\u53470.7\u30012.0\u30011.6\u548c11.0\u3002", "conclusion": "P3HOT\u5728HOT\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.01414", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01414", "abs": "https://arxiv.org/abs/2507.01414", "authors": ["Sultan Daniels", "Dylan Davis", "Dhruv Gautam", "Wentinn Liao", "Gireeja Ranade", "Anant Sahai"], "title": "Decomposing Prediction Mechanisms for In-Context Recall", "comment": "44 pages, 47 figures, 2 tables", "summary": "We introduce a new family of toy problems that combine features of\nlinear-regression-style continuous in-context learning (ICL) with discrete\nassociative recall. We pretrain transformer models on sample traces from this\ntoy, specifically symbolically-labeled interleaved state observations from\nrandomly drawn linear deterministic dynamical systems. We study if the\ntransformer models can recall the state of a sequence previously seen in its\ncontext when prompted to do so with the corresponding in-context label. Taking\na closer look at this task, it becomes clear that the model must perform two\nfunctions: (1) identify which system's state should be recalled and apply that\nsystem to its last seen state, and (2) continuing to apply the correct system\nto predict the subsequent states. Training dynamics reveal that the first\ncapability emerges well into a model's training. Surprisingly, the second\ncapability, of continuing the prediction of a resumed sequence, develops much\nearlier.\n  Via out-of-distribution experiments, and a mechanistic analysis on model\nweights via edge pruning, we find that next-token prediction for this toy\nproblem involves at least two separate mechanisms. One mechanism uses the\ndiscrete symbolic labels to do the associative recall required to predict the\nstart of a resumption of a previously seen sequence. The second mechanism,\nwhich is largely agnostic to the discrete symbolic labels, performs a\n\"Bayesian-style\" prediction based on the previous token and the context. These\ntwo mechanisms have different learning dynamics.\n  To confirm that this multi-mechanism (manifesting as separate phase\ntransitions) phenomenon is not just an artifact of our toy setting, we used\nOLMo training checkpoints on an ICL translation task to see a similar\nphenomenon: a decisive gap in the emergence of first-task-token performance vs\nsecond-task-token performance.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7ed3\u5408\u7ebf\u6027\u56de\u5f52\u5f0f\u8fde\u7eed\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u4e0e\u79bb\u6563\u5173\u8054\u8bb0\u5fc6\u7684\u73a9\u5177\u95ee\u9898\uff0c\u7814\u7a76\u4e86Transformer\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u9700\u5177\u5907\u4e24\u79cd\u80fd\u529b\uff0c\u4e14\u8fd9\u4e24\u79cd\u80fd\u529b\u7684\u5b66\u4e60\u52a8\u6001\u4e0d\u540c\u3002", "motivation": "\u63a2\u7d22Transformer\u6a21\u578b\u5728\u7ed3\u5408\u8fde\u7eed\u4e0e\u79bb\u6563\u4efb\u52a1\u4e2d\u7684\u5b66\u4e60\u80fd\u529b\uff0c\u7279\u522b\u662f\u5176\u5728\u4e0a\u4e0b\u6587\u4e2d\u7684\u72b6\u6001\u56de\u5fc6\u4e0e\u9884\u6d4b\u80fd\u529b\u3002", "method": "\u9884\u8bad\u7ec3Transformer\u6a21\u578b\u4e8e\u7b26\u53f7\u6807\u8bb0\u7684\u968f\u673a\u7ebf\u6027\u786e\u5b9a\u6027\u52a8\u6001\u7cfb\u7edf\u6837\u672c\uff0c\u5206\u6790\u5176\u72b6\u6001\u56de\u5fc6\u4e0e\u9884\u6d4b\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u673a\u5236\u3002", "result": "\u6a21\u578b\u9700\u4e24\u79cd\u673a\u5236\u5b8c\u6210\u4efb\u52a1\uff1a\u57fa\u4e8e\u7b26\u53f7\u6807\u7b7e\u7684\u5173\u8054\u56de\u5fc6\u4e0e\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u8d1d\u53f6\u65af\u5f0f\u9884\u6d4b\uff0c\u4e14\u4e24\u79cd\u673a\u5236\u5b66\u4e60\u52a8\u6001\u4e0d\u540c\u3002", "conclusion": "\u591a\u673a\u5236\u73b0\u8c61\u4e0d\u4ec5\u9650\u4e8e\u73a9\u5177\u95ee\u9898\uff0c\u7c7b\u4f3c\u73b0\u8c61\u5728ICL\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u4e5f\u88ab\u89c2\u5bdf\u5230\u3002"}}
{"id": "2507.01631", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01631", "abs": "https://arxiv.org/abs/2507.01631", "authors": ["Camille Billouard", "Dawa Derksen", "Alexandre Constantin", "Bruno Vallet"], "title": "Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation", "comment": "Accepted at ICCV 2025 Workshop 3D-VAST (From street to space: 3D\n  Vision Across Altitudes). Version before camera ready. Our code will be made\n  public after the conference", "summary": "Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D\nreconstruction from multiview satellite imagery. However, state-of-the-art NeRF\nmethods are typically constrained to small scenes due to the memory footprint\nduring training, which we study in this paper. Previous work on large-scale\nNeRFs palliate this by dividing the scene into NeRFs. This paper introduces\nSnake-NeRF, a framework that scales to large scenes. Our out-of-core method\neliminates the need to load all images and networks simultaneously, and\noperates on a single device. We achieve this by dividing the region of interest\ninto NeRFs that 3D tile without overlap. Importantly, we crop the images with\noverlap to ensure each NeRFs is trained with all the necessary pixels. We\nintroduce a novel $2\\times 2$ 3D tile progression strategy and segmented\nsampler, which together prevent 3D reconstruction errors along the tile edges.\nOur experiments conclude that large satellite images can effectively be\nprocessed with linear time complexity, on a single GPU, and without compromise\nin quality.", "AI": {"tldr": "Snake-NeRF\u662f\u4e00\u79cd\u6269\u5c55\u5230\u5927\u573a\u666f\u7684NeRF\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5757\u5904\u7406\u548c\u91cd\u53e0\u88c1\u526a\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u5185\u5b58\u9650\u5236\u95ee\u9898\uff0c\u5e76\u5728\u5355GPU\u4e0a\u5b9e\u73b0\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u7684\u5904\u7406\u3002", "motivation": "\u73b0\u6709NeRF\u65b9\u6cd5\u56e0\u5185\u5b58\u9650\u5236\u4ec5\u9002\u7528\u4e8e\u5c0f\u573a\u666f\uff0c\u65e0\u6cd5\u5904\u7406\u5927\u89c4\u6a21\u536b\u661f\u56fe\u50cf\u3002", "method": "\u5c06\u611f\u5174\u8da3\u533a\u57df\u5212\u5206\u4e3a\u65e0\u91cd\u53e0\u76843D\u5757\uff0c\u91c7\u75282\u00d72 3D\u5757\u6e10\u8fdb\u7b56\u7565\u548c\u5206\u6bb5\u91c7\u6837\u5668\uff0c\u907f\u514d\u8fb9\u7f18\u91cd\u5efa\u9519\u8bef\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSnake-NeRF\u80fd\u5728\u5355GPU\u4e0a\u9ad8\u6548\u5904\u7406\u5927\u89c4\u6a21\u536b\u661f\u56fe\u50cf\uff0c\u4e14\u4e0d\u727a\u7272\u8d28\u91cf\u3002", "conclusion": "Snake-NeRF\u4e3a\u5927\u89c4\u6a213D\u91cd\u5efa\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01634", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01634", "abs": "https://arxiv.org/abs/2507.01634", "authors": ["Boyuan Sun", "Modi Jin", "Bowen Yin", "Qibin Hou"], "title": "Depth Anything at Any Condition", "comment": null, "summary": "We present Depth Anything at Any Condition (DepthAnything-AC), a foundation\nmonocular depth estimation (MDE) model capable of handling diverse\nenvironmental conditions. Previous foundation MDE models achieve impressive\nperformance across general scenes but not perform well in complex open-world\nenvironments that involve challenging conditions, such as illumination\nvariations, adverse weather, and sensor-induced distortions. To overcome the\nchallenges of data scarcity and the inability of generating high-quality\npseudo-labels from corrupted images, we propose an unsupervised consistency\nregularization finetuning paradigm that requires only a relatively small amount\nof unlabeled data. Furthermore, we propose the Spatial Distance Constraint to\nexplicitly enforce the model to learn patch-level relative relationships,\nresulting in clearer semantic boundaries and more accurate details.\nExperimental results demonstrate the zero-shot capabilities of DepthAnything-AC\nacross diverse benchmarks, including real-world adverse weather benchmarks,\nsynthetic corruption benchmarks, and general benchmarks.\n  Project Page: https://ghost233lism.github.io/depthanything-AC-page\n  Code: https://github.com/HVision-NKU/DepthAnythingAC", "AI": {"tldr": "DepthAnything-AC\u662f\u4e00\u79cd\u57fa\u7840\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\uff0c\u80fd\u591f\u5728\u591a\u6837\u73af\u5883\u6761\u4ef6\u4e0b\u5de5\u4f5c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u590d\u6742\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u7840\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\uff08\u5982\u5149\u7167\u53d8\u5316\u3001\u6076\u52a3\u5929\u6c14\u548c\u4f20\u611f\u5668\u5931\u771f\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\u751f\u6210\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u65e0\u76d1\u7763\u4e00\u81f4\u6027\u6b63\u5219\u5316\u5fae\u8c03\u8303\u5f0f\uff0c\u4ec5\u9700\u5c11\u91cf\u672a\u6807\u8bb0\u6570\u636e\uff0c\u5e76\u5f15\u5165\u7a7a\u95f4\u8ddd\u79bb\u7ea6\u675f\u4ee5\u5b66\u4e60\u8865\u4e01\u7ea7\u76f8\u5bf9\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDepthAnything-AC\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ec\u6076\u52a3\u5929\u6c14\u3001\u5408\u6210\u5931\u771f\u548c\u901a\u7528\u573a\u666f\uff09\u4e2d\u5177\u5907\u96f6\u6837\u672c\u80fd\u529b\u3002", "conclusion": "DepthAnything-AC\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u89e3\u51b3\u4e86\u590d\u6742\u73af\u5883\u4e0b\u7684\u6df1\u5ea6\u4f30\u8ba1\u95ee\u9898\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2507.01469", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.01469", "abs": "https://arxiv.org/abs/2507.01469", "authors": ["Alessio Ferrato", "Fabio Gasparetti", "Carla Limongelli", "Stefano Mastandrea", "Giuseppe Sansonetti", "Joaqu\u00edn Torres-Sospedra"], "title": "Cross-platform Smartphone Positioning at Museums", "comment": "Accepted at the 2025 International Conference on Indoor Positioning\n  and Indoor Navigation (IPIN), Tampere, Finland, September 15-18, 2025", "summary": "Indoor Positioning Systems (IPSs) hold significant potential for enhancing\nvisitor experiences in cultural heritage institutions. By enabling personalized\nnavigation, efficient artifact organization, and better interaction with\nexhibits, IPSs can transform the modalities of how individuals engage with\nmuseums, galleries and libraries. However, these institutions face several\nchallenges in implementing IPSs, including environmental constraints, technical\nlimits, and limited experimentation. In other contexts, Received Signal\nStrength (RSS)-based approaches using Bluetooth Low Energy (BLE) and WiFi have\nemerged as preferred solutions due to their non-invasive nature and minimal\ninfrastructure requirements. Nevertheless, the lack of publicly available RSS\ndatasets that specifically reflect museum environments presents a substantial\nbarrier to developing and evaluating positioning algorithms designed for the\nintricate spatial characteristics typical of cultural heritage sites. To\naddress this limitation, we present BAR, a novel RSS dataset collected in front\nof 90 artworks across 13 museum rooms using two different platforms, i.e.,\nAndroid and iOS. Additionally, we provide an advanced position classification\nbaseline taking advantage of a proximity-based method and $k$-NN algorithms. In\nour analysis, we discuss the results and offer suggestions for potential\nresearch directions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faBAR\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u89e3\u51b3\u535a\u7269\u9986\u73af\u5883\u4e2dRSS\u6570\u636e\u7f3a\u4e4f\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u57fa\u4e8e\u90bb\u8fd1\u6027\u548ck-NN\u7b97\u6cd5\u7684\u5206\u7c7b\u57fa\u7ebf\u3002", "motivation": "\u6587\u5316\u673a\u6784\u5728\u5ba4\u5185\u5b9a\u4f4d\u7cfb\u7edf\uff08IPS\uff09\u5b9e\u65bd\u4e2d\u9762\u4e34\u73af\u5883\u548c\u6280\u672f\u9650\u5236\uff0c\u7f3a\u4e4f\u516c\u5f00\u7684\u535a\u7269\u9986\u73af\u5883RSS\u6570\u636e\u96c6\u963b\u788d\u4e86\u76f8\u5173\u7b97\u6cd5\u7684\u5f00\u53d1\u4e0e\u8bc4\u4f30\u3002", "method": "\u6536\u96c6\u4e8690\u4ef6\u827a\u672f\u54c1\u524d\u7684RSS\u6570\u636e\uff08Android\u548ciOS\u5e73\u53f0\uff09\uff0c\u5e76\u91c7\u7528\u90bb\u8fd1\u6027\u65b9\u6cd5\u548ck-NN\u7b97\u6cd5\u5efa\u7acb\u5206\u7c7b\u57fa\u7ebf\u3002", "result": "\u63d0\u4f9b\u4e86BAR\u6570\u636e\u96c6\uff0c\u5e76\u5c55\u793a\u4e86\u57fa\u4e8e\u8be5\u6570\u636e\u96c6\u7684\u5206\u7c7b\u7ed3\u679c\u3002", "conclusion": "BAR\u6570\u636e\u96c6\u586b\u8865\u4e86\u535a\u7269\u9986\u73af\u5883RSS\u6570\u636e\u7684\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u548c\u5efa\u8bae\u3002"}}
{"id": "2507.01643", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01643", "abs": "https://arxiv.org/abs/2507.01643", "authors": ["Weijie Yin", "Dingkang Yang", "Hongyuan Dong", "Zijian Kang", "Jiacong Wang", "Xiao Liang", "Chao Feng", "Jiao Ran"], "title": "SAILViT: Towards Robust and Generalizable Visual Backbones for MLLMs via Gradual Feature Refinement", "comment": "We release SAILViT, a series of versatile vision foundation models", "summary": "Vision Transformers (ViTs) are essential as foundation backbones in\nestablishing the visual comprehension capabilities of Multimodal Large Language\nModels (MLLMs). Although most ViTs achieve impressive performance through\nimage-text pair-based contrastive learning or self-supervised mechanisms, they\nstruggle to engage in connector-based co-training directly with LLMs due to\npotential parameter initialization conflicts and modality semantic gaps. To\naddress the above challenges, this paper proposes SAILViT, a gradual feature\nlearning-enhanced ViT for facilitating MLLMs to break through performance\nbottlenecks in complex multimodal interactions. SAILViT achieves\ncoarse-to-fine-grained feature alignment and world knowledge infusion with\ngradual feature refinement, which better serves target training demands. We\nperform thorough empirical analyses to confirm the powerful robustness and\ngeneralizability of SAILViT across different dimensions, including parameter\nsizes, model architectures, training strategies, and data scales. Equipped with\nSAILViT, existing MLLMs show significant and consistent performance\nimprovements on the OpenCompass benchmark across extensive downstream tasks.\nSAILViT series models are released at\nhttps://huggingface.co/BytedanceDouyinContent.", "AI": {"tldr": "SAILViT\u662f\u4e00\u79cd\u9010\u6b65\u7279\u5f81\u5b66\u4e60\u589e\u5f3a\u7684Vision Transformer\uff0c\u65e8\u5728\u89e3\u51b3ViT\u4e0eLLM\u76f4\u63a5\u8054\u5408\u8bad\u7ec3\u65f6\u7684\u53c2\u6570\u51b2\u7a81\u548c\u6a21\u6001\u8bed\u4e49\u5dee\u8ddd\u95ee\u9898\uff0c\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684ViT\u5728\u56fe\u50cf-\u6587\u672c\u5bf9\u6bd4\u5b66\u4e60\u6216\u81ea\u76d1\u7763\u673a\u5236\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u4e0eLLM\u76f4\u63a5\u8054\u5408\u8bad\u7ec3\u65f6\u5b58\u5728\u53c2\u6570\u521d\u59cb\u5316\u51b2\u7a81\u548c\u6a21\u6001\u8bed\u4e49\u5dee\u8ddd\u95ee\u9898\uff0c\u9650\u5236\u4e86\u591a\u6a21\u6001\u4ea4\u4e92\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51faSAILViT\uff0c\u901a\u8fc7\u9010\u6b65\u7279\u5f81\u7ec6\u5316\u5b9e\u73b0\u4ece\u7c97\u5230\u7ec6\u7684\u7279\u5f81\u5bf9\u9f50\u548c\u4e16\u754c\u77e5\u8bc6\u6ce8\u5165\uff0c\u9002\u5e94\u76ee\u6807\u8bad\u7ec3\u9700\u6c42\u3002", "result": "SAILViT\u5728\u4e0d\u540c\u53c2\u6570\u89c4\u6a21\u3001\u67b6\u6784\u3001\u8bad\u7ec3\u7b56\u7565\u548c\u6570\u636e\u89c4\u6a21\u4e0b\u5747\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86MLLM\u5728OpenCompass\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "SAILViT\u6709\u6548\u89e3\u51b3\u4e86ViT\u4e0eLLM\u8054\u5408\u8bad\u7ec3\u7684\u6311\u6218\uff0c\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u63d0\u5347\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.01649", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01649", "abs": "https://arxiv.org/abs/2507.01649", "authors": ["Yoav Gelberg", "Yam Eitan", "Aviv Navon", "Aviv Shamsian", "Theo", "Putterman", "Michael Bronstein", "Haggai Maron"], "title": "GradMetaNet: An Equivariant Architecture for Learning on Gradients", "comment": null, "summary": "Gradients of neural networks encode valuable information for optimization,\nediting, and analysis of models. Therefore, practitioners often treat gradients\nas inputs to task-specific algorithms, e.g. for pruning or optimization. Recent\nworks explore learning algorithms that operate directly on gradients but use\narchitectures that are not specifically designed for gradient processing,\nlimiting their applicability. In this paper, we present a principled approach\nfor designing architectures that process gradients. Our approach is guided by\nthree principles: (1) equivariant design that preserves neuron permutation\nsymmetries, (2) processing sets of gradients across multiple data points to\ncapture curvature information, and (3) efficient gradient representation\nthrough rank-1 decomposition. Based on these principles, we introduce\nGradMetaNet, a novel architecture for learning on gradients, constructed from\nsimple equivariant blocks. We prove universality results for GradMetaNet, and\nshow that previous approaches cannot approximate natural gradient-based\nfunctions that GradMetaNet can. We then demonstrate GradMetaNet's effectiveness\non a diverse set of gradient-based tasks on MLPs and transformers, such as\nlearned optimization, INR editing, and estimating loss landscape curvature.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGradMetaNet\u7684\u65b0\u67b6\u6784\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u5904\u7406\u795e\u7ecf\u7f51\u7edc\u68af\u5ea6\uff0c\u57fa\u4e8e\u4e09\u4e2a\u539f\u5219\uff1a\u7b49\u53d8\u6027\u8bbe\u8ba1\u3001\u591a\u6570\u636e\u70b9\u68af\u5ea6\u5904\u7406\u548c\u9ad8\u6548\u68af\u5ea6\u8868\u793a\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u68af\u5ea6\u65f6\u672a\u9488\u5bf9\u68af\u5ea6\u5904\u7406\u8bbe\u8ba1\u7279\u5b9a\u67b6\u6784\uff0c\u9650\u5236\u4e86\u5176\u9002\u7528\u6027\u3002", "method": "\u63d0\u51faGradMetaNet\u67b6\u6784\uff0c\u57fa\u4e8e\u7b49\u53d8\u6027\u8bbe\u8ba1\u3001\u591a\u6570\u636e\u70b9\u68af\u5ea6\u5904\u7406\u548c\u79e9-1\u5206\u89e3\u7684\u9ad8\u6548\u8868\u793a\u3002", "result": "GradMetaNet\u80fd\u591f\u903c\u8fd1\u81ea\u7136\u68af\u5ea6\u51fd\u6570\uff0c\u5e76\u5728\u591a\u79cd\u4efb\u52a1\uff08\u5982\u4f18\u5316\u3001\u7f16\u8f91\u548c\u66f2\u7387\u4f30\u8ba1\uff09\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "GradMetaNet\u4e3a\u68af\u5ea6\u5904\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.01652", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.01652", "abs": "https://arxiv.org/abs/2507.01652", "authors": ["Yuxin Mao", "Zhen Qin", "Jinxing Zhou", "Hui Deng", "Xuyang Shen", "Bin Fan", "Jing Zhang", "Yiran Zhong", "Yuchao Dai"], "title": "Autoregressive Image Generation with Linear Complexity: A Spatial-Aware Decay Perspective", "comment": null, "summary": "Autoregressive (AR) models have garnered significant attention in image\ngeneration for their ability to effectively capture both local and global\nstructures within visual data. However, prevalent AR models predominantly rely\non the transformer architectures, which are beset by quadratic computational\ncomplexity concerning input sequence length and substantial memory overhead due\nto the necessity of maintaining key-value caches. Although linear attention\nmechanisms have successfully reduced this burden in language models, our\ninitial experiments reveal that they significantly degrade image generation\nquality because of their inability to capture critical long-range dependencies\nin visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a\nnovel attention mechanism that explicitly preserves genuine 2D spatial\nrelationships within the flattened image sequences by computing\nposition-dependent decay factors based on true 2D spatial location rather than\n1D sequence positions. Based on this mechanism, we present LASADGen, an\nautoregressive image generator that enables selective attention to relevant\nspatial contexts with linear complexity. Experiments on ImageNet show LASADGen\nachieves state-of-the-art image generation performance and computational\nefficiency, bridging the gap between linear attention's efficiency and spatial\nunderstanding needed for high-quality generation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLASAD\u7684\u65b0\u578b\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u4fdd\u75592D\u7a7a\u95f4\u5173\u7cfb\u89e3\u51b3\u4e86\u7ebf\u6027\u6ce8\u610f\u529b\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u6027\u80fd\u95ee\u9898\uff0c\u5e76\u57fa\u4e8e\u6b64\u6784\u5efa\u4e86\u9ad8\u6548\u7684\u56fe\u50cf\u751f\u6210\u6a21\u578bLASADGen\u3002", "motivation": "\u73b0\u6709\u81ea\u56de\u5f52\u6a21\u578b\u4f9d\u8d56Transformer\u67b6\u6784\uff0c\u8ba1\u7b97\u590d\u6742\u4e14\u5185\u5b58\u5f00\u9500\u5927\uff0c\u800c\u7ebf\u6027\u6ce8\u610f\u529b\u673a\u5236\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u56e0\u65e0\u6cd5\u6355\u6349\u957f\u8ddd\u79bb\u4f9d\u8d56\u5bfc\u81f4\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u63d0\u51faLASAD\u673a\u5236\uff0c\u901a\u8fc7\u57fa\u4e8e\u771f\u5b9e2D\u7a7a\u95f4\u4f4d\u7f6e\u7684\u4f4d\u7f6e\u76f8\u5173\u8870\u51cf\u56e0\u5b50\u4fdd\u7559\u7a7a\u95f4\u5173\u7cfb\uff0c\u6784\u5efaLASADGen\u6a21\u578b\u3002", "result": "\u5728ImageNet\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cLASADGen\u5728\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u8fbe\u5230\u6700\u4f18\u3002", "conclusion": "LASADGen\u6210\u529f\u5e73\u8861\u4e86\u7ebf\u6027\u6ce8\u610f\u529b\u7684\u6548\u7387\u4e0e\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\u6240\u9700\u7684\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2507.01516", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01516", "abs": "https://arxiv.org/abs/2507.01516", "authors": ["Dibyanshu Kumar", "Philipp Vaeth", "Magda Gregorov\u00e1"], "title": "Loss Functions in Diffusion Models: A Comparative Study", "comment": "Accepted to ECML 2025", "summary": "Diffusion models have emerged as powerful generative models, inspiring\nextensive research into their underlying mechanisms. One of the key questions\nin this area is the loss functions these models shall train with. Multiple\nformulations have been introduced in the literature over the past several years\nwith some links and some critical differences stemming from various initial\nconsiderations. In this paper, we explore the different target objectives and\ncorresponding loss functions in detail. We present a systematic overview of\ntheir relationships, unifying them under the framework of the variational lower\nbound objective. We complement this theoretical analysis with an empirical\nstudy providing insights into the conditions under which these objectives\ndiverge in performance and the underlying factors contributing to such\ndeviations. Additionally, we evaluate how the choice of objective impacts the\nmodel ability to achieve specific goals, such as generating high-quality\nsamples or accurately estimating likelihoods. This study offers a unified\nunderstanding of loss functions in diffusion models, contributing to more\nefficient and goal-oriented model designs in future research.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u6269\u6563\u6a21\u578b\u4e2d\u4e0d\u540c\u76ee\u6807\u51fd\u6570\u548c\u635f\u5931\u51fd\u6570\u7684\u5173\u7cfb\uff0c\u7edf\u4e00\u4e86\u53d8\u5206\u4e0b\u754c\u76ee\u6807\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u63ed\u793a\u4e86\u6027\u80fd\u5dee\u5f02\u7684\u6761\u4ef6\u548c\u56e0\u7d20\u3002", "motivation": "\u63a2\u7d22\u6269\u6563\u6a21\u578b\u4e2d\u4e0d\u540c\u76ee\u6807\u51fd\u6570\u548c\u635f\u5931\u51fd\u6570\u7684\u5173\u7cfb\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u66f4\u9ad8\u6548\u548c\u9488\u5bf9\u6027\u7684\u6a21\u578b\u8bbe\u8ba1\u6307\u5bfc\u3002", "method": "\u7406\u8bba\u5206\u6790\u7ed3\u5408\u5b9e\u8bc1\u7814\u7a76\uff0c\u7cfb\u7edf\u6bd4\u8f83\u4e0d\u540c\u635f\u5931\u51fd\u6570\u5728\u6027\u80fd\u4e0a\u7684\u5dee\u5f02\u53ca\u5176\u5f71\u54cd\u56e0\u7d20\u3002", "result": "\u63ed\u793a\u4e86\u4e0d\u540c\u76ee\u6807\u51fd\u6570\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u6837\u672c\u548c\u51c6\u786e\u4f30\u8ba1\u4f3c\u7136\u65b9\u9762\u7684\u6027\u80fd\u5dee\u5f02\u53ca\u5176\u539f\u56e0\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6269\u6563\u6a21\u578b\u7684\u635f\u5931\u51fd\u6570\u63d0\u4f9b\u4e86\u7edf\u4e00\u7406\u89e3\uff0c\u6709\u52a9\u4e8e\u672a\u6765\u66f4\u9ad8\u6548\u548c\u9488\u5bf9\u6027\u7684\u6a21\u578b\u8bbe\u8ba1\u3002"}}
{"id": "2507.01653", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01653", "abs": "https://arxiv.org/abs/2507.01653", "authors": ["Yuran Wang", "Yingping Liang", "Yutao Hu", "Ying Fu"], "title": "RobuSTereo: Robust Zero-Shot Stereo Matching under Adverse Weather", "comment": "accepted by ICCV25", "summary": "Learning-based stereo matching models struggle in adverse weather conditions\ndue to the scarcity of corresponding training data and the challenges in\nextracting discriminative features from degraded images. These limitations\nsignificantly hinder zero-shot generalization to out-of-distribution weather\nconditions. In this paper, we propose \\textbf{RobuSTereo}, a novel framework\nthat enhances the zero-shot generalization of stereo matching models under\nadverse weather by addressing both data scarcity and feature extraction\nchallenges. First, we introduce a diffusion-based simulation pipeline with a\nstereo consistency module, which generates high-quality stereo data tailored\nfor adverse conditions. By training stereo matching models on our synthetic\ndatasets, we reduce the domain gap between clean and degraded images,\nsignificantly improving the models' robustness to unseen weather conditions.\nThe stereo consistency module ensures structural alignment across synthesized\nimage pairs, preserving geometric integrity and enhancing depth estimation\naccuracy. Second, we design a robust feature encoder that combines a\nspecialized ConvNet with a denoising transformer to extract stable and reliable\nfeatures from degraded images. The ConvNet captures fine-grained local\nstructures, while the denoising transformer refines global representations,\neffectively mitigating the impact of noise, low visibility, and weather-induced\ndistortions. This enables more accurate disparity estimation even under\nchallenging visual conditions. Extensive experiments demonstrate that\n\\textbf{RobuSTereo} significantly improves the robustness and generalization of\nstereo matching models across diverse adverse weather scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRobuSTereo\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u62df\u548c\u7a33\u5065\u7279\u5f81\u7f16\u7801\u63d0\u5347\u7acb\u4f53\u5339\u914d\u6a21\u578b\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5b66\u4e60\u7684\u7acb\u4f53\u5339\u914d\u6a21\u578b\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u56e0\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u548c\u7279\u5f81\u63d0\u53d6\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u6269\u6563\u6a21\u62df\u7ba1\u7ebf\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u8bbe\u8ba1\u7a33\u5065\u7279\u5f81\u7f16\u7801\u5668\u7ed3\u5408ConvNet\u548c\u53bb\u566aTransformer\u3002", "result": "\u5b9e\u9a8c\u8868\u660eRobuSTereo\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "RobuSTereo\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u548c\u7279\u5f81\u63d0\u53d6\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u7acb\u4f53\u5339\u914d\u6a21\u578b\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.01663", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01663", "abs": "https://arxiv.org/abs/2507.01663", "authors": ["Zhenyu Han", "Ansheng You", "Haibo Wang", "Kui Luo", "Guang Yang", "Wenqi Shi", "Menglong Chen", "Sicheng Zhang", "Zeshun Lan", "Chunshi Deng", "Huazhong Ji", "Wenjie Liu", "Yu Huang", "Yixiang Zhang", "Chenyi Pan", "Jing Wang", "Xin Huang", "Chunsheng Li", "Jianping Wu"], "title": "AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM Post-Training", "comment": null, "summary": "Reinforcement learning (RL) has become a pivotal technology in the\npost-training phase of large language models (LLMs). Traditional task-colocated\nRL frameworks suffer from significant scalability bottlenecks, while\ntask-separated RL frameworks face challenges in complex dataflows and the\ncorresponding resource idling and workload imbalance. Moreover, most existing\nframeworks are tightly coupled with LLM training or inference engines, making\nit difficult to support custom-designed engines. To address these challenges,\nwe propose AsyncFlow, an asynchronous streaming RL framework for efficient\npost-training. Specifically, we introduce a distributed data storage and\ntransfer module that provides a unified data management and fine-grained\nscheduling capability in a fully streamed manner. This architecture inherently\nfacilitates automated pipeline overlapping among RL tasks and dynamic load\nbalancing. Moreover, we propose a producer-consumer-based asynchronous workflow\nengineered to minimize computational idleness by strategically deferring\nparameter update process within staleness thresholds. Finally, the core\ncapability of AsynFlow is architecturally decoupled from underlying training\nand inference engines and encapsulated by service-oriented user interfaces,\noffering a modular and customizable user experience. Extensive experiments\ndemonstrate an average of 1.59 throughput improvement compared with\nstate-of-the-art baseline. The presented architecture in this work provides\nactionable insights for next-generation RL training system designs.", "AI": {"tldr": "AsyncFlow\u662f\u4e00\u4e2a\u5f02\u6b65\u6d41\u5f0f\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u7684\u540e\u8bad\u7ec3\u9636\u6bb5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6846\u67b6\u7684\u53ef\u6269\u5c55\u6027\u548c\u8d44\u6e90\u5229\u7528\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u540e\u8bad\u7ec3\u4e2d\u5b58\u5728\u53ef\u6269\u5c55\u6027\u74f6\u9888\u548c\u8d44\u6e90\u5229\u7528\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faAsyncFlow\u6846\u67b6\uff0c\u5305\u62ec\u5206\u5e03\u5f0f\u6570\u636e\u5b58\u50a8\u4e0e\u4f20\u8f93\u6a21\u5757\u3001\u5f02\u6b65\u5de5\u4f5c\u6d41\u5f15\u64ce\uff0c\u5e76\u4e0e\u5e95\u5c42\u8bad\u7ec3\u548c\u63a8\u7406\u5f15\u64ce\u89e3\u8026\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u541e\u5410\u91cf\u5e73\u5747\u63d0\u53471.59\u500d\u3002", "conclusion": "AsyncFlow\u4e3a\u4e0b\u4e00\u4ee3\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2507.01654", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01654", "abs": "https://arxiv.org/abs/2507.01654", "authors": ["Martine Hjelkrem-Tan", "Marius Aasan", "Gabriel Y. Arteaga", "Ad\u00edn Ram\u00edrez Rivera"], "title": "SPoT: Subpixel Placement of Tokens in Vision Transformers", "comment": "To appear in Workshop on Efficient Computing under Limited Resources:\n  Visual Computing (ICCV 2025). Code available at\n  https://github.com/dsb-ifi/SPoT", "summary": "Vision Transformers naturally accommodate sparsity, yet standard tokenization\nmethods confine features to discrete patch grids. This constraint prevents\nmodels from fully exploiting sparse regimes, forcing awkward compromises. We\npropose Subpixel Placement of Tokens (SPoT), a novel tokenization strategy that\npositions tokens continuously within images, effectively sidestepping\ngrid-based limitations. With our proposed oracle-guided search, we uncover\nsubstantial performance gains achievable with ideal subpixel token positioning,\ndrastically reducing the number of tokens necessary for accurate predictions\nduring inference. SPoT provides a new direction for flexible, efficient, and\ninterpretable ViT architectures, redefining sparsity as a strategic advantage\nrather than an imposed limitation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSPoT\u7684\u65b0\u578btokenization\u7b56\u7565\uff0c\u901a\u8fc7\u8fde\u7eed\u653e\u7f6etoken\u6765\u7a81\u7834\u4f20\u7edf\u7f51\u683c\u9650\u5236\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u51cf\u5c11\u63a8\u7406\u6240\u9700\u7684token\u6570\u91cf\u3002", "motivation": "\u6807\u51c6tokenization\u65b9\u6cd5\u5c06\u7279\u5f81\u9650\u5236\u5728\u79bb\u6563\u7684patch\u7f51\u683c\u4e2d\uff0c\u963b\u788d\u4e86\u6a21\u578b\u5728\u7a00\u758f\u573a\u666f\u4e0b\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51faSubpixel Placement of Tokens (SPoT)\uff0c\u7ed3\u5408oracle-guided\u641c\u7d22\uff0c\u5b9e\u73b0token\u5728\u56fe\u50cf\u4e2d\u7684\u8fde\u7eed\u5b9a\u4f4d\u3002", "result": "\u901a\u8fc7\u7406\u60f3\u5b50\u50cf\u7d20\u5b9a\u4f4d\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u51cf\u5c11\u63a8\u7406\u6240\u9700\u7684token\u6570\u91cf\u3002", "conclusion": "SPoT\u4e3aViT\u67b6\u6784\u63d0\u4f9b\u4e86\u7075\u6d3b\u3001\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u65b0\u65b9\u5411\uff0c\u5c06\u7a00\u758f\u6027\u8f6c\u5316\u4e3a\u6218\u7565\u4f18\u52bf\u3002"}}
{"id": "2507.01544", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01544", "abs": "https://arxiv.org/abs/2507.01544", "authors": ["Benjamin Feuer", "Lennart Purucker", "Oussama Elachqar", "Chinmay Hegde"], "title": "MARVIS: Modality Adaptive Reasoning over VISualizations", "comment": null, "summary": "Scientific applications of machine learning often rely on small, specialized\nmodels tuned to particular domains. Such models often achieve excellent\nperformance, but lack flexibility. Foundation models offer versatility, but\ntypically underperform specialized approaches, especially on non-traditional\nmodalities and long-tail domains. We propose MARVIS (Modality Adaptive\nReasoning over VISualizations), a training-free method that enables even small\nvision-language models to predict any data modality with high accuracy. MARVIS\ntransforms latent embedding spaces into visual representations and then\nleverages the spatial and fine-grained reasoning skills of VLMs to successfully\ninterpret and utilize them. MARVIS achieves competitive performance on vision,\naudio, biological, and tabular domains using a single 3B parameter model,\nachieving results that beat Gemini by 16\\% on average and approach specialized\nmethods, without exposing personally identifiable information (P.I.I.) or\nrequiring any domain-specific training. We open source our code and datasets at\nhttps://github.com/penfever/marvis", "AI": {"tldr": "MARVIS\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6f5c\u5728\u5d4c\u5165\u7a7a\u95f4\u8f6c\u6362\u4e3a\u89c6\u89c9\u8868\u793a\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u548c\u7ec6\u7c92\u5ea6\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u6570\u636e\u7684\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u3002", "motivation": "\u89e3\u51b3\u5c0f\u89c4\u6a21\u4e13\u4e1a\u6a21\u578b\u7f3a\u4e4f\u7075\u6d3b\u6027\u548c\u57fa\u7840\u6a21\u578b\u5728\u975e\u4f20\u7edf\u6a21\u6001\u53ca\u957f\u5c3e\u9886\u57df\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "method": "\u5c06\u6f5c\u5728\u5d4c\u5165\u7a7a\u95f4\u8f6c\u6362\u4e3a\u89c6\u89c9\u8868\u793a\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u63a8\u7406\u3002", "result": "\u5728\u89c6\u89c9\u3001\u97f3\u9891\u3001\u751f\u7269\u548c\u8868\u683c\u6570\u636e\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u5e73\u5747\u6027\u80fd\u8d85\u8fc7Gemini 16%\uff0c\u63a5\u8fd1\u4e13\u4e1a\u65b9\u6cd5\u3002", "conclusion": "MARVIS\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u4fdd\u62a4\u9690\u79c1\u7684\u591a\u6a21\u6001\u9884\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.01667", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01667", "abs": "https://arxiv.org/abs/2507.01667", "authors": ["Gianluca Monaci", "Philippe Weinzaepfel", "Christian Wolf"], "title": "What does really matter in image goal navigation?", "comment": null, "summary": "Image goal navigation requires two different skills: firstly, core navigation\nskills, including the detection of free space and obstacles, and taking\ndecisions based on an internal representation; and secondly, computing\ndirectional information by comparing visual observations to the goal image.\nCurrent state-of-the-art methods either rely on dedicated image-matching, or\npre-training of computer vision modules on relative pose estimation. In this\npaper, we study whether this task can be efficiently solved with end-to-end\ntraining of full agents with RL, as has been claimed by recent work. A positive\nanswer would have impact beyond Embodied AI and allow training of relative pose\nestimation from reward for navigation alone. In a large study we investigate\nthe effect of architectural choices like late fusion, channel stacking,\nspace-to-depth projections and cross-attention, and their role in the emergence\nof relative pose estimators from navigation training. We show that the success\nof recent methods is influenced up to a certain extent by simulator settings,\nleading to shortcuts in simulation. However, we also show that these\ncapabilities can be transferred to more realistic setting, up to some extend.\nWe also find evidence for correlations between navigation performance and\nprobed (emerging) relative pose estimation performance, an important sub skill.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u5728\u56fe\u50cf\u76ee\u6807\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u67b6\u6784\u9009\u62e9\u7684\u5f71\u54cd\uff0c\u5e76\u53d1\u73b0\u6a21\u62df\u5668\u8bbe\u7f6e\u5bf9\u7ed3\u679c\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5b8c\u6574\u4ee3\u7406\u89e3\u51b3\u56fe\u50cf\u76ee\u6807\u5bfc\u822a\u4efb\u52a1\uff0c\u907f\u514d\u4f9d\u8d56\u4e13\u7528\u56fe\u50cf\u5339\u914d\u6216\u9884\u8bad\u7ec3\u6a21\u5757\u3002", "method": "\u901a\u8fc7\u5927\u89c4\u6a21\u7814\u7a76\u5206\u6790\u4e0d\u540c\u67b6\u6784\u9009\u62e9\uff08\u5982\u5ef6\u8fdf\u878d\u5408\u3001\u901a\u9053\u5806\u53e0\u3001\u7a7a\u95f4\u5230\u6df1\u5ea6\u6295\u5f71\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\uff09\u5bf9\u5bfc\u822a\u8bad\u7ec3\u4e2d\u76f8\u5bf9\u59ff\u6001\u4f30\u8ba1\u5668\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u6a21\u62df\u5668\u8bbe\u7f6e\u5bf9\u65b9\u6cd5\u6210\u529f\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4f46\u90e8\u5206\u80fd\u529b\u53ef\u8fc1\u79fb\u5230\u66f4\u771f\u5b9e\u573a\u666f\uff1b\u5bfc\u822a\u6027\u80fd\u4e0e\u76f8\u5bf9\u59ff\u6001\u4f30\u8ba1\u6027\u80fd\u76f8\u5173\u3002", "conclusion": "\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u5728\u56fe\u50cf\u76ee\u6807\u5bfc\u822a\u4e2d\u53ef\u884c\uff0c\u4f46\u9700\u6ce8\u610f\u6a21\u62df\u5668\u8bbe\u7f6e\u7684\u5f71\u54cd\uff0c\u4e14\u90e8\u5206\u80fd\u529b\u53ef\u8fc1\u79fb\u3002"}}
{"id": "2507.01693", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01693", "abs": "https://arxiv.org/abs/2507.01693", "authors": ["Adrians Skapars", "Edoardo Manino", "Youcheng Sun", "Lucas C. Cordeiro"], "title": "GPT, But Backwards: Exactly Inverting Language Model Outputs", "comment": "9 pages, ICML 2025 Workshop on Reliable and Responsible Foundation\n  Models", "summary": "While existing auditing techniques attempt to identify potential unwanted\nbehaviours in large language models (LLMs), we address the complementary\nforensic problem of reconstructing the exact input that led to an existing LLM\noutput - enabling post-incident analysis and potentially the detection of fake\noutput reports. We formalize exact input reconstruction as a discrete\noptimisation problem with a unique global minimum and introduce SODA, an\nefficient gradient-based algorithm that operates on a continuous relaxation of\nthe input search space with periodic restarts and parameter decay. Through\ncomprehensive experiments on LLMs ranging in size from 33M to 3B parameters, we\ndemonstrate that SODA significantly outperforms existing approaches. We succeed\nin fully recovering 79.5% of shorter out-of-distribution inputs from next-token\nlogits, without a single false positive, but struggle to extract private\ninformation from the outputs of longer (15+ token) input sequences. This\nsuggests that standard deployment practices may currently provide adequate\nprotection against malicious use of our method. Our code is available at\nhttps://doi.org/10.5281/zenodo.15539879.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSODA\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u4ece\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8f93\u51fa\u4e2d\u7cbe\u786e\u91cd\u6784\u8f93\u5165\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5ba1\u8ba1\u6280\u672f\u7684\u8865\u5145\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5ba1\u8ba1\u6280\u672f\u4e3b\u8981\u5173\u6ce8\u8bc6\u522bLLM\u4e2d\u7684\u6f5c\u5728\u4e0d\u826f\u884c\u4e3a\uff0c\u800c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4e8b\u540e\u5206\u6790\u4e2d\u7cbe\u786e\u91cd\u6784\u8f93\u5165\u7684\u95ee\u9898\uff0c\u4ee5\u68c0\u6d4b\u865a\u5047\u8f93\u51fa\u62a5\u544a\u3002", "method": "\u5c06\u7cbe\u786e\u8f93\u5165\u91cd\u6784\u5f62\u5f0f\u5316\u4e3a\u79bb\u6563\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51faSODA\u7b97\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u641c\u7d22\u548c\u5468\u671f\u6027\u91cd\u542f\u7b49\u65b9\u6cd5\u5728\u8fde\u7eed\u677e\u5f1b\u7684\u8f93\u5165\u7a7a\u95f4\u4e2d\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u572833M\u52303B\u53c2\u6570\u7684LLM\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cSODA\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6210\u529f\u6062\u590d\u4e8679.5%\u7684\u77ed\u8f93\u5165\uff0c\u4f46\u5bf9\u957f\u8f93\u5165\uff0815+ token\uff09\u7684\u9690\u79c1\u4fe1\u606f\u63d0\u53d6\u6548\u679c\u4e0d\u4f73\u3002", "conclusion": "\u6807\u51c6\u90e8\u7f72\u5b9e\u8df5\u53ef\u80fd\u5df2\u8db3\u591f\u9632\u6b62\u6076\u610f\u4f7f\u7528\u8be5\u65b9\u6cd5\uff0c\u4f46SODA\u5728\u77ed\u8f93\u5165\u91cd\u6784\u4e0a\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2507.01673", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01673", "abs": "https://arxiv.org/abs/2507.01673", "authors": ["Muzammil Behzad"], "title": "Facial Emotion Learning with Text-Guided Multiview Fusion via Vision-Language Model for 3D/4D Facial Expression Recognition", "comment": null, "summary": "Facial expression recognition (FER) in 3D and 4D domains presents a\nsignificant challenge in affective computing due to the complexity of spatial\nand temporal facial dynamics. Its success is crucial for advancing applications\nin human behavior understanding, healthcare monitoring, and human-computer\ninteraction. In this work, we propose FACET-VLM, a vision-language framework\nfor 3D/4D FER that integrates multiview facial representation learning with\nsemantic guidance from natural language prompts. FACET-VLM introduces three key\ncomponents: Cross-View Semantic Aggregation (CVSA) for view-consistent fusion,\nMultiview Text-Guided Fusion (MTGF) for semantically aligned facial emotions,\nand a multiview consistency loss to enforce structural coherence across views.\nOur model achieves state-of-the-art accuracy across multiple benchmarks,\nincluding BU-3DFE, Bosphorus, BU-4DFE, and BP4D-Spontaneous. We further extend\nFACET-VLM to 4D micro-expression recognition (MER) on the 4DME dataset,\ndemonstrating strong performance in capturing subtle, short-lived emotional\ncues. The extensive experimental results confirm the effectiveness and\nsubstantial contributions of each individual component within the framework.\nOverall, FACET-VLM offers a robust, extensible, and high-performing solution\nfor multimodal FER in both posed and spontaneous settings.", "AI": {"tldr": "FACET-VLM\u662f\u4e00\u4e2a\u7528\u4e8e3D/4D\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u7684\u89c6\u89c9\u8bed\u8a00\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u8868\u793a\u5b66\u4e60\u548c\u8bed\u4e49\u5f15\u5bfc\u5b9e\u73b0\u9ad8\u6027\u80fd\u3002", "motivation": "\u89e3\u51b33D/4D\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u4e2d\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u52a8\u6001\u590d\u6742\u6027\uff0c\u63a8\u52a8\u60c5\u611f\u8ba1\u7b97\u5728\u884c\u4e3a\u7406\u89e3\u3001\u533b\u7597\u76d1\u63a7\u548c\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faCVSA\u3001MTGF\u548c\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u635f\u5931\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff0c\u5b9e\u73b0\u591a\u89c6\u89d2\u8bed\u4e49\u878d\u5408\u548c\u7ed3\u6784\u4e00\u81f4\u6027\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7cbe\u5ea6\uff0c\u5e76\u6210\u529f\u6269\u5c55\u52304D\u5fae\u8868\u60c5\u8bc6\u522b\u3002", "conclusion": "FACET-VLM\u4e3a\u591a\u6a21\u6001\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01700", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01700", "abs": "https://arxiv.org/abs/2507.01700", "authors": ["Andrea Piras", "Matteo Negro", "Ragib Ahsan", "David Arbour", "Elena Zheleva"], "title": "Relational Causal Discovery with Latent Confounders", "comment": "30 pages, 19 figures. Accepted for publication at the 41st Conference\n  on Uncertainty in Artificial Intelligence (UAI 2025). Andrea Piras and Matteo\n  Negro contributed equally to this work", "summary": "Estimating causal effects from real-world relational data can be challenging\nwhen the underlying causal model and potential confounders are unknown. While\nseveral causal discovery algorithms exist for learning causal models with\nlatent confounders from data, they assume that the data is independent and\nidentically distributed (i.i.d.) and are not well-suited for learning from\nrelational data. Similarly, existing relational causal discovery algorithms\nassume causal sufficiency, which is unrealistic for many real-world datasets.\nTo address this gap, we propose RelFCI, a sound and complete causal discovery\nalgorithm for relational data with latent confounders. Our work builds upon the\nFast Causal Inference (FCI) and Relational Causal Discovery (RCD) algorithms\nand it defines new graphical models, necessary to support causal discovery in\nrelational domains. We also establish soundness and completeness guarantees for\nrelational d-separation with latent confounders. We present experimental\nresults demonstrating the effectiveness of RelFCI in identifying the correct\ncausal structure in relational causal models with latent confounders.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRelFCI\u7684\u56e0\u679c\u53d1\u73b0\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u5177\u6709\u6f5c\u5728\u6df7\u6742\u56e0\u7d20\u7684\u5173\u7cfb\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u5173\u7cfb\u6570\u636e\u4e2d\u56e0\u679c\u6548\u5e94\u4f30\u8ba1\u56f0\u96be\uff0c\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u6570\u636e\u72ec\u7acb\u540c\u5206\u5e03\u6216\u56e0\u679c\u5145\u5206\u6027\uff0c\u4e0d\u9002\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u3002", "method": "\u57fa\u4e8eFCI\u548cRCD\u7b97\u6cd5\uff0c\u63d0\u51faRelFCI\u7b97\u6cd5\uff0c\u5b9a\u4e49\u4e86\u65b0\u7684\u56fe\u6a21\u578b\u4ee5\u652f\u6301\u5173\u7cfb\u6570\u636e\u4e2d\u7684\u56e0\u679c\u53d1\u73b0\uff0c\u5e76\u5efa\u7acb\u4e86\u5173\u7cfbd-\u5206\u79bb\u7684\u7406\u8bba\u4fdd\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eRelFCI\u80fd\u6709\u6548\u8bc6\u522b\u5177\u6709\u6f5c\u5728\u6df7\u6742\u56e0\u7d20\u7684\u5173\u7cfb\u56e0\u679c\u6a21\u578b\u4e2d\u7684\u6b63\u786e\u56e0\u679c\u7ed3\u6784\u3002", "conclusion": "RelFCI\u586b\u8865\u4e86\u5173\u7cfb\u6570\u636e\u56e0\u679c\u53d1\u73b0\u7684\u7a7a\u767d\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\u3002"}}
{"id": "2507.01559", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01559", "abs": "https://arxiv.org/abs/2507.01559", "authors": ["Lapo Frati", "Neil Traft", "Jeff Clune", "Nick Cheney"], "title": "How Weight Resampling and Optimizers Shape the Dynamics of Continual Learning and Forgetting in Neural Networks", "comment": null, "summary": "Recent work in continual learning has highlighted the beneficial effect of\nresampling weights in the last layer of a neural network (``zapping\"). Although\nempirical results demonstrate the effectiveness of this approach, the\nunderlying mechanisms that drive these improvements remain unclear. In this\nwork, we investigate in detail the pattern of learning and forgetting that take\nplace inside a convolutional neural network when trained in challenging\nsettings such as continual learning and few-shot transfer learning, with\nhandwritten characters and natural images. Our experiments show that models\nthat have undergone zapping during training more quickly recover from the shock\nof transferring to a new domain. Furthermore, to better observe the effect of\ncontinual learning in a multi-task setting we measure how each individual task\nis affected. This shows that, not only zapping, but the choice of optimizer can\nalso deeply affect the dynamics of learning and forgetting, causing complex\npatterns of synergy/interference between tasks to emerge when the model learns\nsequentially at transfer time.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u795e\u7ecf\u7f51\u7edc\u6700\u540e\u4e00\u5c42\u6743\u91cd\u91cd\u91c7\u6837\uff08\u201czapping\u201d\uff09\u5728\u6301\u7eed\u5b66\u4e60\u548c\u5c11\u6837\u672c\u8fc1\u79fb\u5b66\u4e60\u4e2d\u7684\u6548\u679c\uff0c\u53d1\u73b0\u5176\u80fd\u52a0\u901f\u6a21\u578b\u5728\u65b0\u9886\u57df\u7684\u6062\u590d\uff0c\u5e76\u63a2\u8ba8\u4e86\u4f18\u5316\u5668\u9009\u62e9\u5bf9\u5b66\u4e60\u52a8\u6001\u7684\u5f71\u54cd\u3002", "motivation": "\u63a2\u7d22zapping\u5728\u6301\u7eed\u5b66\u4e60\u548c\u8fc1\u79fb\u5b66\u4e60\u4e2d\u7684\u5177\u4f53\u4f5c\u7528\u673a\u5236\uff0c\u4ee5\u53ca\u4f18\u5316\u5668\u9009\u62e9\u5bf9\u4efb\u52a1\u95f4\u5b66\u4e60\u4e0e\u9057\u5fd8\u52a8\u6001\u7684\u5f71\u54cd\u3002", "method": "\u5728\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4e2d\uff0c\u901a\u8fc7\u624b\u5199\u5b57\u7b26\u548c\u81ea\u7136\u56fe\u50cf\u7684\u5b9e\u9a8c\uff0c\u5206\u6790zapping\u548c\u4e0d\u540c\u4f18\u5316\u5668\u5bf9\u5b66\u4e60\u52a8\u6001\u7684\u5f71\u54cd\u3002", "result": "zapping\u80fd\u5e2e\u52a9\u6a21\u578b\u66f4\u5feb\u9002\u5e94\u65b0\u9886\u57df\uff1b\u4f18\u5316\u5668\u9009\u62e9\u4f1a\u663e\u8457\u5f71\u54cd\u4efb\u52a1\u95f4\u7684\u5b66\u4e60\u4e0e\u9057\u5fd8\u52a8\u6001\uff0c\u4ea7\u751f\u590d\u6742\u7684\u534f\u540c/\u5e72\u6270\u6a21\u5f0f\u3002", "conclusion": "zapping\u548c\u4f18\u5316\u5668\u9009\u62e9\u662f\u5f71\u54cd\u6301\u7eed\u5b66\u4e60\u52a8\u6001\u7684\u5173\u952e\u56e0\u7d20\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u673a\u5236\u3002"}}
{"id": "2507.01711", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01711", "abs": "https://arxiv.org/abs/2507.01711", "authors": ["Mingfu Yan", "Jiancheng Huang", "Yifan Liu", "Shifeng Chen"], "title": "Component Adaptive Clustering for Generalized Category Discovery", "comment": "Accepted by IEEE ICME 2025", "summary": "Generalized Category Discovery (GCD) tackles the challenging problem of\ncategorizing unlabeled images into both known and novel classes within a\npartially labeled dataset, without prior knowledge of the number of unknown\ncategories. Traditional methods often rely on rigid assumptions, such as\npredefining the number of classes, which limits their ability to handle the\ninherent variability and complexity of real-world data. To address these\nshortcomings, we propose AdaGCD, a cluster-centric contrastive learning\nframework that incorporates Adaptive Slot Attention (AdaSlot) into the GCD\nframework. AdaSlot dynamically determines the optimal number of slots based on\ndata complexity, removing the need for predefined slot counts. This adaptive\nmechanism facilitates the flexible clustering of unlabeled data into known and\nnovel categories by dynamically allocating representational capacity. By\nintegrating adaptive representation with dynamic slot allocation, our method\ncaptures both instance-specific and spatially clustered features, improving\nclass discovery in open-world scenarios. Extensive experiments on public and\nfine-grained datasets validate the effectiveness of our framework, emphasizing\nthe advantages of leveraging spatial local information for category discovery\nin unlabeled image datasets.", "AI": {"tldr": "AdaGCD\u662f\u4e00\u4e2a\u57fa\u4e8e\u81ea\u9002\u5e94\u69fd\u6ce8\u610f\u529b\u7684\u805a\u7c7b\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5e7f\u4e49\u7c7b\u522b\u53d1\u73b0\uff08GCD\uff09\u95ee\u9898\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u7c7b\u522b\u6570\u91cf\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u7c7b\u522b\u6570\u91cf\u7684\u5047\u8bbe\uff0c\u65e0\u6cd5\u5904\u7406\u771f\u5b9e\u6570\u636e\u7684\u590d\u6742\u6027\u3002", "method": "\u63d0\u51faAdaGCD\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u69fd\u6ce8\u610f\u529b\uff08AdaSlot\uff09\u52a8\u6001\u786e\u5b9a\u69fd\u6570\u91cf\uff0c\u5b9e\u73b0\u7075\u6d3b\u805a\u7c7b\u3002", "result": "\u5728\u516c\u5f00\u548c\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5c24\u5176\u5728\u5229\u7528\u7a7a\u95f4\u5c40\u90e8\u4fe1\u606f\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "AdaGCD\u901a\u8fc7\u81ea\u9002\u5e94\u673a\u5236\u548c\u52a8\u6001\u69fd\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e0b\u7684\u7c7b\u522b\u53d1\u73b0\u80fd\u529b\u3002"}}
{"id": "2507.01581", "categories": ["cs.LG", "cs.CR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.01581", "abs": "https://arxiv.org/abs/2507.01581", "authors": ["Masood Jan", "Wafa Njima", "Xun Zhang"], "title": "A Privacy-Preserving Indoor Localization System based on Hierarchical Federated Learning", "comment": null, "summary": "Location information serves as the fundamental element for numerous Internet\nof Things (IoT) applications. Traditional indoor localization techniques often\nproduce significant errors and raise privacy concerns due to centralized data\ncollection. In response, Machine Learning (ML) techniques offer promising\nsolutions by capturing indoor environment variations. However, they typically\nrequire central data aggregation, leading to privacy, bandwidth, and server\nreliability issues. To overcome these challenges, in this paper, we propose a\nFederated Learning (FL)-based approach for dynamic indoor localization using a\nDeep Neural Network (DNN) model. Experimental results show that FL has the\nnearby performance to Centralized Model (CL) while keeping the data privacy,\nbandwidth efficiency and server reliability. This research demonstrates that\nour proposed FL approach provides a viable solution for privacy-enhanced indoor\nlocalization, paving the way for advancements in secure and efficient indoor\nlocalization systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u7684\u52a8\u6001\u5ba4\u5185\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u96c6\u4e2d\u5f0f\u65b9\u6cd5\u7684\u9690\u79c1\u3001\u5e26\u5bbd\u548c\u670d\u52a1\u5668\u53ef\u9760\u6027\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5ba4\u5185\u5b9a\u4f4d\u6280\u672f\u5b58\u5728\u8bef\u5dee\u5927\u548c\u9690\u79c1\u95ee\u9898\uff0c\u800c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u867d\u80fd\u6355\u6349\u73af\u5883\u53d8\u5316\uff0c\u4f46\u96c6\u4e2d\u5f0f\u6570\u636e\u6536\u96c6\u4ecd\u5e26\u6765\u9690\u79c1\u548c\u6548\u7387\u6311\u6218\u3002", "method": "\u91c7\u7528\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u6a21\u578b\uff0c\u5b9e\u73b0\u52a8\u6001\u5ba4\u5185\u5b9a\u4f4d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFL\u6027\u80fd\u63a5\u8fd1\u96c6\u4e2d\u5f0f\u6a21\u578b\uff08CL\uff09\uff0c\u540c\u65f6\u4fdd\u969c\u6570\u636e\u9690\u79c1\u3001\u5e26\u5bbd\u6548\u7387\u548c\u670d\u52a1\u5668\u53ef\u9760\u6027\u3002", "conclusion": "FL\u4e3a\u9690\u79c1\u589e\u5f3a\u7684\u5ba4\u5185\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u5b89\u5168\u9ad8\u6548\u5ba4\u5185\u5b9a\u4f4d\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.01712", "categories": ["cs.CV", "eess.IV", "stat.AP"], "pdf": "https://arxiv.org/pdf/2507.01712", "abs": "https://arxiv.org/abs/2507.01712", "authors": ["Xinle Tian", "Matthew Nunes", "Emiko Dupont", "Shaunagh Downing", "Freddie Lichtenstein", "Matt Burns"], "title": "Using Wavelet Domain Fingerprints to Improve Source Camera Identification", "comment": null, "summary": "Camera fingerprint detection plays a crucial role in source identification\nand image forensics, with wavelet denoising approaches proving to be\nparticularly effective in extracting sensor pattern noise (SPN). In this\narticle, we propose a modification to wavelet-based SPN extraction. Rather than\nconstructing the fingerprint as an image, we introduce the notion of a wavelet\ndomain fingerprint. This avoids the final inversion step of the denoising\nalgorithm and allows fingerprint comparisons to be made directly in the wavelet\ndomain. As such, our modification streamlines the extraction and comparison\nprocess. Experimental results on real-world datasets demonstrate that our\nmethod not only achieves higher detection accuracy but can also significantly\nimprove processing speed.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u57df\u7684\u76f8\u673a\u6307\u7eb9\u63d0\u53d6\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u53cd\u6f14\u6b65\u9aa4\uff0c\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u548c\u5904\u7406\u901f\u5ea6\u3002", "motivation": "\u4f20\u7edf\u5c0f\u6ce2\u53bb\u566a\u65b9\u6cd5\u5728\u63d0\u53d6\u4f20\u611f\u5668\u6a21\u5f0f\u566a\u58f0\uff08SPN\uff09\u65f6\u9700\u8981\u53cd\u6f14\u6b65\u9aa4\uff0c\u6548\u7387\u8f83\u4f4e\u3002\u672c\u6587\u65e8\u5728\u4f18\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u5c06\u6307\u7eb9\u6784\u5efa\u4e3a\u5c0f\u6ce2\u57df\u6307\u7eb9\uff0c\u76f4\u63a5\u5728\u9891\u57df\u8fdb\u884c\u6bd4\u8f83\uff0c\u907f\u514d\u4e86\u53cd\u6f14\u6b65\u9aa4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u68c0\u6d4b\u7cbe\u5ea6\u66f4\u9ad8\uff0c\u8fd8\u80fd\u663e\u8457\u63d0\u5347\u5904\u7406\u901f\u5ea6\u3002", "conclusion": "\u5c0f\u6ce2\u57df\u6307\u7eb9\u63d0\u53d6\u65b9\u6cd5\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2507.01598", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01598", "abs": "https://arxiv.org/abs/2507.01598", "authors": ["Naoki Sato", "Hiroki Naganuma", "Hideaki Iiduka"], "title": "Analysis of Muon's Convergence and Critical Batch Size", "comment": null, "summary": "This paper presents a theoretical analysis of Muon, a new optimizer that\nleverages the inherent matrix structure of neural network parameters. We\nprovide convergence proofs for four practical variants of Muon: with and\nwithout Nesterov momentum, and with and without weight decay. We then show that\nadding weight decay leads to strictly tighter bounds on both the parameter and\ngradient norms, and we clarify the relationship between the weight decay\ncoefficient and the learning rate. Finally, we derive Muon's critical batch\nsize minimizing the stochastic first-order oracle (SFO) complexity, which is\nthe stochastic computational cost, and validate our theoretical findings with\nexperiments.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86Muon\u4f18\u5316\u5668\u7684\u7406\u8bba\u6027\u80fd\uff0c\u5305\u62ec\u56db\u79cd\u53d8\u4f53\u7684\u6536\u655b\u6027\u8bc1\u660e\uff0c\u63a2\u8ba8\u4e86\u6743\u91cd\u8870\u51cf\u5bf9\u53c2\u6570\u548c\u68af\u5ea6\u8303\u6570\u7684\u5f71\u54cd\uff0c\u5e76\u63a8\u5bfc\u4e86\u6700\u5c0f\u5316\u8ba1\u7b97\u6210\u672c\u7684\u5173\u952e\u6279\u91cf\u5927\u5c0f\u3002", "motivation": "\u7814\u7a76Muon\u4f18\u5316\u5668\u7684\u7406\u8bba\u7279\u6027\uff0c\u660e\u786e\u5176\u6536\u655b\u6027\u53ca\u6743\u91cd\u8870\u51cf\u4e0e\u5b66\u4e60\u7387\u7684\u5173\u7cfb\u3002", "method": "\u7406\u8bba\u5206\u6790\u56db\u79cdMuon\u53d8\u4f53\u7684\u6536\u655b\u6027\uff0c\u63a8\u5bfc\u6743\u91cd\u8870\u51cf\u7684\u5f71\u54cd\uff0c\u8ba1\u7b97\u5173\u952e\u6279\u91cf\u5927\u5c0f\u3002", "result": "\u6743\u91cd\u8870\u51cf\u80fd\u663e\u8457\u6536\u7d27\u53c2\u6570\u548c\u68af\u5ea6\u8303\u6570\u7684\u754c\u9650\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u3002", "conclusion": "Muon\u4f18\u5316\u5668\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u6743\u91cd\u8870\u51cf\u548c\u6279\u91cf\u5927\u5c0f\u7684\u9009\u62e9\u5bf9\u5176\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2507.01721", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01721", "abs": "https://arxiv.org/abs/2507.01721", "authors": ["Zhongwen Zhang", "Yuri Boykov"], "title": "Soft Self-labeling and Potts Relaxations for Weakly-Supervised Segmentation", "comment": "published at CVPR 2025", "summary": "We consider weakly supervised segmentation where only a fraction of pixels\nhave ground truth labels (scribbles) and focus on a self-labeling approach\noptimizing relaxations of the standard unsupervised CRF/Potts loss on unlabeled\npixels. While WSSS methods can directly optimize such losses via gradient\ndescent, prior work suggests that higher-order optimization can improve network\ntraining by introducing hidden pseudo-labels and powerful CRF sub-problem\nsolvers, e.g. graph cut. However, previously used hard pseudo-labels can not\nrepresent class uncertainty or errors, which motivates soft self-labeling. We\nderive a principled auxiliary loss and systematically evaluate standard and new\nCRF relaxations (convex and non-convex), neighborhood systems, and terms\nconnecting network predictions with soft pseudo-labels. We also propose a\ngeneral continuous sub-problem solver. Using only standard architectures, soft\nself-labeling consistently improves scribble-based training and outperforms\nsignificantly more complex specialized WSSS systems. It can outperform full\npixel-precise supervision. Our general ideas apply to other weakly-supervised\nproblems/systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f6f\u81ea\u6807\u8bb0\u7684\u5f31\u76d1\u7763\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316CRF/Potts\u635f\u5931\u6539\u8fdb\u7f51\u7edc\u8bad\u7ec3\uff0c\u6027\u80fd\u4f18\u4e8e\u590d\u6742\u7cfb\u7edf\u3002", "motivation": "\u89e3\u51b3\u786c\u4f2a\u6807\u7b7e\u65e0\u6cd5\u8868\u793a\u7c7b\u522b\u4e0d\u786e\u5b9a\u6027\u548c\u9519\u8bef\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u8f6f\u81ea\u6807\u8bb0\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d\u4f18\u5316CRF/Potts\u635f\u5931\uff0c\u5f15\u5165\u8f6f\u4f2a\u6807\u7b7e\u548c\u8fde\u7eed\u5b50\u95ee\u9898\u6c42\u89e3\u5668\u3002", "result": "\u8f6f\u81ea\u6807\u8bb0\u663e\u8457\u63d0\u5347\u4e86\u57fa\u4e8e\u6d82\u9e26\u7684\u8bad\u7ec3\u6548\u679c\uff0c\u6027\u80fd\u4f18\u4e8e\u590d\u6742\u7cfb\u7edf\u548c\u5168\u76d1\u7763\u65b9\u6cd5\u3002", "conclusion": "\u8f6f\u81ea\u6807\u8bb0\u65b9\u6cd5\u5728\u5f31\u76d1\u7763\u5206\u5272\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u5176\u4ed6\u5f31\u76d1\u7763\u95ee\u9898\u3002"}}
{"id": "2507.01636", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.01636", "abs": "https://arxiv.org/abs/2507.01636", "authors": ["Ghasem Alipoor", "Karl Skretting"], "title": "Kernel Recursive Least Squares Dictionary Learning Algorithm", "comment": "Published in Digital Signal Processing, Volume 141, 2023. DOI:\n  https://doi.org/10.1016/j.dsp.2023.104159 12 pages, 8 figures. Code and data\n  available at: https://github.com/G-Alipoor/kernel-rls-dictionary-learning", "summary": "We propose an efficient online dictionary learning algorithm for kernel-based\nsparse representations. In this framework, input signals are nonlinearly mapped\nto a high-dimensional feature space and represented sparsely using a virtual\ndictionary. At each step, the dictionary is updated recursively using a novel\nalgorithm based on the recursive least squares (RLS) method. This update\nmechanism works with single samples or mini-batches and maintains low\ncomputational complexity. Experiments on four datasets across different domains\nshow that our method not only outperforms existing online kernel dictionary\nlearning approaches but also achieves classification accuracy close to that of\nbatch-trained models, while remaining significantly more efficient.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5728\u7ebf\u5b57\u5178\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u57fa\u4e8e\u6838\u7684\u7a00\u758f\u8868\u793a\uff0c\u901a\u8fc7\u9012\u5f52\u6700\u5c0f\u4e8c\u4e58\u6cd5\u66f4\u65b0\u5b57\u5178\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u6548\u7387\u9ad8\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5728\u7ebf\u6838\u5b57\u5178\u5b66\u4e60\u65b9\u6cd5\u6548\u7387\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u5206\u7c7b\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u9012\u5f52\u6700\u5c0f\u4e8c\u4e58\u6cd5\uff08RLS\uff09\u9012\u5f52\u66f4\u65b0\u5b57\u5178\uff0c\u652f\u6301\u5355\u6837\u672c\u6216\u5c0f\u6279\u91cf\u5904\u7406\uff0c\u4fdd\u6301\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u5728\u7ebf\u65b9\u6cd5\uff0c\u4e14\u5206\u7c7b\u51c6\u786e\u6027\u63a5\u8fd1\u6279\u91cf\u8bad\u7ec3\u6a21\u578b\uff0c\u540c\u65f6\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u5728\u7ebf\u5b66\u4e60\u573a\u666f\u3002"}}
{"id": "2507.01722", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01722", "abs": "https://arxiv.org/abs/2507.01722", "authors": ["Enrico Cassano", "Riccardo Renzulli", "Andrea Bragagnolo", "Marco Grangetto"], "title": "When Does Pruning Benefit Vision Representations?", "comment": null, "summary": "Pruning is widely used to reduce the complexity of deep learning models, but\nits effects on interpretability and representation learning remain poorly\nunderstood. This paper investigates how pruning influences vision models across\nthree key dimensions: (i) interpretability, (ii) unsupervised object discovery,\nand (iii) alignment with human perception. We first analyze different vision\nnetwork architectures to examine how varying sparsity levels affect feature\nattribution interpretability methods. Additionally, we explore whether pruning\npromotes more succinct and structured representations, potentially improving\nunsupervised object discovery by discarding redundant information while\npreserving essential features. Finally, we assess whether pruning enhances the\nalignment between model representations and human perception, investigating\nwhether sparser models focus on more discriminative features similarly to\nhumans. Our findings also reveal the presence of sweet spots, where sparse\nmodels exhibit higher interpretability, downstream generalization and human\nalignment. However, these spots highly depend on the network architectures and\ntheir size in terms of trainable parameters. Our results suggest a complex\ninterplay between these three dimensions, highlighting the importance of\ninvestigating when and how pruning benefits vision representations.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u526a\u679d\u5bf9\u89c6\u89c9\u6a21\u578b\u5728\u53ef\u89e3\u91ca\u6027\u3001\u65e0\u76d1\u7763\u76ee\u6807\u53d1\u73b0\u548c\u4eba\u7c7b\u611f\u77e5\u5bf9\u9f50\u4e09\u4e2a\u7ef4\u5ea6\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u4e86\u526a\u679d\u4e0e\u8fd9\u4e9b\u7ef4\u5ea6\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\u3002", "motivation": "\u526a\u679d\u5e7f\u6cdb\u7528\u4e8e\u964d\u4f4e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u590d\u6742\u5ea6\uff0c\u4f46\u5176\u5bf9\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u8868\u793a\u5b66\u4e60\u7684\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u5206\u6790\u4e0d\u540c\u89c6\u89c9\u7f51\u7edc\u67b6\u6784\u5728\u4e0d\u540c\u7a00\u758f\u5ea6\u4e0b\u5bf9\u7279\u5f81\u5f52\u56e0\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u7684\u5f71\u54cd\uff0c\u63a2\u7d22\u526a\u679d\u662f\u5426\u4fc3\u8fdb\u66f4\u7b80\u6d01\u7684\u7ed3\u6784\u5316\u8868\u793a\uff0c\u5e76\u8bc4\u4f30\u526a\u679d\u662f\u5426\u589e\u5f3a\u6a21\u578b\u8868\u793a\u4e0e\u4eba\u7c7b\u611f\u77e5\u7684\u5bf9\u9f50\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5b58\u5728\u2018\u751c\u70b9\u2019\uff0c\u5373\u7a00\u758f\u6a21\u578b\u5728\u53ef\u89e3\u91ca\u6027\u3001\u4e0b\u6e38\u6cdb\u5316\u80fd\u529b\u548c\u4eba\u7c7b\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u8fd9\u4e9b\u2018\u751c\u70b9\u2019\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u7f51\u7edc\u67b6\u6784\u548c\u53c2\u6570\u89c4\u6a21\u3002", "conclusion": "\u526a\u679d\u4e0e\u89c6\u89c9\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3001\u8868\u793a\u5b66\u4e60\u548c\u4eba\u7c7b\u5bf9\u9f50\u4e4b\u95f4\u5b58\u5728\u590d\u6742\u5173\u7cfb\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u526a\u679d\u4f55\u65f6\u53ca\u5982\u4f55\u6709\u76ca\u4e8e\u89c6\u89c9\u8868\u793a\u3002"}}
{"id": "2507.01761", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.01761", "abs": "https://arxiv.org/abs/2507.01761", "authors": ["Nicolas Salvy", "Hugues Talbot", "Bertrand Thirion"], "title": "Enhanced Generative Model Evaluation with Clipped Density and Coverage", "comment": null, "summary": "Although generative models have made remarkable progress in recent years,\ntheir use in critical applications has been hindered by their incapacity to\nreliably evaluate sample quality. Quality refers to at least two complementary\nconcepts: fidelity and coverage. Current quality metrics often lack reliable,\ninterpretable values due to an absence of calibration or insufficient\nrobustness to outliers. To address these shortcomings, we introduce two novel\nmetrics, Clipped Density and Clipped Coverage. By clipping individual sample\ncontributions and, for fidelity, the radii of nearest neighbor balls, our\nmetrics prevent out-of-distribution samples from biasing the aggregated values.\nThrough analytical and empirical calibration, these metrics exhibit linear\nscore degradation as the proportion of poor samples increases. Thus, they can\nbe straightforwardly interpreted as equivalent proportions of good samples.\nExtensive experiments on synthetic and real-world datasets demonstrate that\nClipped Density and Clipped Coverage outperform existing methods in terms of\nrobustness, sensitivity, and interpretability for evaluating generative models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u6307\u6807\uff08Clipped Density\u548cClipped Coverage\uff09\uff0c\u7528\u4e8e\u66f4\u53ef\u9760\u5730\u8bc4\u4f30\u751f\u6210\u6a21\u578b\u6837\u672c\u7684\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6307\u6807\u5728\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u5728\u5173\u952e\u5e94\u7528\u4e2d\u7684\u4f7f\u7528\u53d7\u5230\u6837\u672c\u8d28\u91cf\u8bc4\u4f30\u4e0d\u53ef\u9760\u7684\u9650\u5236\uff0c\u73b0\u6709\u6307\u6807\u7f3a\u4e4f\u6821\u51c6\u6216\u5bf9\u5f02\u5e38\u503c\u7684\u9c81\u68d2\u6027\u3002", "method": "\u901a\u8fc7\u526a\u88c1\u5355\u4e2a\u6837\u672c\u8d21\u732e\u548c\u6700\u8fd1\u90bb\u7403\u7684\u534a\u5f84\uff0c\u63d0\u51faClipped Density\u548cClipped Coverage\u6307\u6807\uff0c\u9632\u6b62\u5206\u5e03\u5916\u6837\u672c\u5f71\u54cd\u805a\u5408\u503c\u3002", "result": "\u65b0\u6307\u6807\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u9c81\u68d2\u6027\u3001\u654f\u611f\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "Clipped Density\u548cClipped Coverage\u4e3a\u751f\u6210\u6a21\u578b\u7684\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u548c\u76f4\u89c2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01644", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01644", "abs": "https://arxiv.org/abs/2507.01644", "authors": ["Miguel O'Malley"], "title": "Dance Dance ConvLSTM", "comment": "15 pages, 9 figures, 4 tables", "summary": "\\textit{Dance Dance Revolution} is a rhythm game consisting of songs and\naccompanying choreography, referred to as charts. Players press arrows on a\ndevice referred to as a dance pad in time with steps determined by the song's\nchart. In 2017, the authors of Dance Dance Convolution (DDC) developed an\nalgorithm for the automatic generation of \\textit{Dance Dance Revolution}\ncharts, utilizing a CNN-LSTM architecture. We introduce Dance Dance ConvLSTM\n(DDCL), a new method for the automatic generation of DDR charts using a\nConvLSTM based model, which improves upon the DDC methodology and substantially\nincreases the accuracy of chart generation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eConvLSTM\u7684\u65b0\u65b9\u6cd5DDCL\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u300aDance Dance Revolution\u300b\u6e38\u620f\u4e2d\u7684\u821e\u8e48\u56fe\u8868\uff0c\u76f8\u6bd4\u4e4b\u524d\u7684DDC\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684DDC\u65b9\u6cd5\u867d\u7136\u80fd\u81ea\u52a8\u751f\u6210\u821e\u8e48\u56fe\u8868\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u66f4\u5148\u8fdb\u7684ConvLSTM\u67b6\u6784\u63d0\u5347\u751f\u6210\u51c6\u786e\u6027\u548c\u8d28\u91cf\u3002", "method": "\u91c7\u7528ConvLSTM\u67b6\u6784\uff0c\u7ed3\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\uff0c\u4f18\u5316\u4e86\u56fe\u8868\u751f\u6210\u8fc7\u7a0b\u3002", "result": "DDCL\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u821e\u8e48\u56fe\u8868\u751f\u6210\u7684\u51c6\u786e\u6027\u3002", "conclusion": "ConvLSTM\u67b6\u6784\u5728\u81ea\u52a8\u751f\u6210\u821e\u8e48\u56fe\u8868\u65b9\u9762\u4f18\u4e8e\u4f20\u7edfCNN-LSTM\u65b9\u6cd5\uff0c\u4e3a\u6e38\u620f\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2507.01781", "categories": ["cs.LG", "cs.AI", "68T07 (Primary) 62H30, 68T05 (Secondary)"], "pdf": "https://arxiv.org/pdf/2507.01781", "abs": "https://arxiv.org/abs/2507.01781", "authors": ["Dalia Rodr\u00edguez-Salas", "Christian Riess"], "title": "BranchNet: A Neuro-Symbolic Learning Framework for Structured Multi-Class Classification", "comment": "18 pages, 3 figures (with two images each)", "summary": "We introduce BranchNet, a neuro-symbolic learning framework that transforms\ndecision tree ensembles into sparse, partially connected neural networks. Each\nbranch, defined as a decision path from root to a parent of leaves, is mapped\nto a hidden neuron, preserving symbolic structure while enabling gradient-based\noptimization. The resulting models are compact, interpretable, and require no\nmanual architecture tuning. Evaluated on a suite of structured multi-class\nclassification benchmarks, BranchNet consistently outperforms XGBoost in\naccuracy, with statistically significant gains. We detail the architecture,\ntraining procedure, and sparsity dynamics, and discuss the model's strengths in\nsymbolic interpretability as well as its current limitations, particularly on\nbinary tasks where further adaptive calibration may be beneficial.", "AI": {"tldr": "BranchNet\u5c06\u51b3\u7b56\u6811\u96c6\u6210\u8f6c\u6362\u4e3a\u7a00\u758f\u3001\u90e8\u5206\u8fde\u63a5\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u4fdd\u7559\u7b26\u53f7\u7ed3\u6784\u5e76\u652f\u6301\u68af\u5ea6\u4f18\u5316\uff0c\u6027\u80fd\u4f18\u4e8eXGBoost\u3002", "motivation": "\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u7684\u68af\u5ea6\u4f18\u5316\u80fd\u529b\u4e0e\u51b3\u7b56\u6811\u7684\u7b26\u53f7\u53ef\u89e3\u91ca\u6027\uff0c\u6784\u5efa\u7d27\u51d1\u4e14\u65e0\u9700\u624b\u52a8\u8c03\u4f18\u7684\u6a21\u578b\u3002", "method": "\u5c06\u51b3\u7b56\u6811\u7684\u6bcf\u4e2a\u5206\u652f\u6620\u5c04\u4e3a\u9690\u85cf\u795e\u7ecf\u5143\uff0c\u5f62\u6210\u7a00\u758f\u795e\u7ecf\u7f51\u7edc\uff0c\u652f\u6301\u68af\u5ea6\u4f18\u5316\u3002", "result": "\u5728\u591a\u7c7b\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cBranchNet\u663e\u8457\u4f18\u4e8eXGBoost\uff0c\u51c6\u786e\u7387\u66f4\u9ad8\u3002", "conclusion": "BranchNet\u5728\u7b26\u53f7\u53ef\u89e3\u91ca\u6027\u548c\u6027\u80fd\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u4e8c\u5143\u4efb\u52a1\u4e2d\u53ef\u80fd\u9700\u8981\u8fdb\u4e00\u6b65\u4f18\u5316\u3002"}}
{"id": "2507.01737", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01737", "abs": "https://arxiv.org/abs/2507.01737", "authors": ["Lin Wu", "Zhixiang Chen", "Jianglin Lan"], "title": "HOI-Dyn: Learning Interaction Dynamics for Human-Object Motion Diffusion", "comment": null, "summary": "Generating realistic 3D human-object interactions (HOIs) remains a\nchallenging task due to the difficulty of modeling detailed interaction\ndynamics. Existing methods treat human and object motions independently,\nresulting in physically implausible and causally inconsistent behaviors. In\nthis work, we present HOI-Dyn, a novel framework that formulates HOI generation\nas a driver-responder system, where human actions drive object responses. At\nthe core of our method is a lightweight transformer-based interaction dynamics\nmodel that explicitly predicts how objects should react to human motion. To\nfurther enforce consistency, we introduce a residual-based dynamics loss that\nmitigates the impact of dynamics prediction errors and prevents misleading\noptimization signals. The dynamics model is used only during training,\npreserving inference efficiency. Through extensive qualitative and quantitative\nexperiments, we demonstrate that our approach not only enhances the quality of\nHOI generation but also establishes a feasible metric for evaluating the\nquality of generated interactions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHOI-Dyn\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u9a71\u52a8-\u54cd\u5e94\u7cfb\u7edf\u751f\u6210\u903c\u771f\u76843D\u4eba-\u7269\u4ea4\u4e92\uff08HOI\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u72ec\u7acb\u5904\u7406\u4eba\u548c\u7269\u4f53\u8fd0\u52a8\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u72ec\u7acb\u5904\u7406\u4eba\u548c\u7269\u4f53\u8fd0\u52a8\uff0c\u5bfc\u81f4\u7269\u7406\u4e0a\u4e0d\u5408\u7406\u4e14\u56e0\u679c\u4e0d\u4e00\u81f4\u7684\u884c\u4e3a\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u5efa\u6a21\u8be6\u7ec6\u7684\u4ea4\u4e92\u52a8\u6001\u3002", "method": "HOI-Dyn\u6846\u67b6\u5c06HOI\u751f\u6210\u5efa\u6a21\u4e3a\u9a71\u52a8-\u54cd\u5e94\u7cfb\u7edf\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u57fa\u4e8eTransformer\u7684\u4ea4\u4e92\u52a8\u6001\u6a21\u578b\u9884\u6d4b\u7269\u4f53\u5bf9\u4eba\u7269\u8fd0\u52a8\u7684\u54cd\u5e94\uff0c\u5e76\u901a\u8fc7\u6b8b\u5dee\u52a8\u6001\u635f\u5931\u589e\u5f3a\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86HOI\u751f\u6210\u7684\u8d28\u91cf\uff0c\u8fd8\u63d0\u4f9b\u4e86\u8bc4\u4f30\u751f\u6210\u4ea4\u4e92\u8d28\u91cf\u7684\u53ef\u884c\u6307\u6807\u3002", "conclusion": "HOI-Dyn\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e863D\u4eba-\u7269\u4ea4\u4e92\u751f\u6210\u7684\u6311\u6218\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.01738", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01738", "abs": "https://arxiv.org/abs/2507.01738", "authors": ["Ming Dai", "Wenxuan Cheng", "Jiang-jiang Liu", "Sen Yang", "Wenxiao Cai", "Yanpeng Sun", "Wankou Yang"], "title": "DeRIS: Decoupling Perception and Cognition for Enhanced Referring Image Segmentation through Loopback Synergy", "comment": "ICCV 2025", "summary": "Referring Image Segmentation (RIS) is a challenging task that aims to segment\nobjects in an image based on natural language expressions. While prior studies\nhave predominantly concentrated on improving vision-language interactions and\nachieving fine-grained localization, a systematic analysis of the fundamental\nbottlenecks in existing RIS frameworks remains underexplored. To bridge this\ngap, we propose DeRIS, a novel framework that decomposes RIS into two key\ncomponents: perception and cognition. This modular decomposition facilitates a\nsystematic analysis of the primary bottlenecks impeding RIS performance. Our\nfindings reveal that the predominant limitation lies not in perceptual\ndeficiencies, but in the insufficient multi-modal cognitive capacity of current\nmodels. To mitigate this, we propose a Loopback Synergy mechanism, which\nenhances the synergy between the perception and cognition modules, thereby\nenabling precise segmentation while simultaneously improving robust image-text\ncomprehension. Additionally, we analyze and introduce a simple non-referent\nsample conversion data augmentation to address the long-tail distribution issue\nrelated to target existence judgement in general scenarios. Notably, DeRIS\ndemonstrates inherent adaptability to both non- and multi-referents scenarios\nwithout requiring specialized architectural modifications, enhancing its\ngeneral applicability. The codes and models are available at\nhttps://github.com/Dmmm1997/DeRIS.", "AI": {"tldr": "DeRIS\u6846\u67b6\u5c06Referring Image Segmentation\uff08RIS\uff09\u5206\u89e3\u4e3a\u611f\u77e5\u548c\u8ba4\u77e5\u4e24\u4e2a\u6a21\u5757\uff0c\u901a\u8fc7Loopback Synergy\u673a\u5236\u63d0\u5347\u591a\u6a21\u6001\u8ba4\u77e5\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u6570\u636e\u589e\u5f3a\u89e3\u51b3\u957f\u5c3e\u5206\u5e03\u95ee\u9898\u3002", "motivation": "\u73b0\u6709RIS\u6846\u67b6\u7f3a\u4e4f\u5bf9\u6027\u80fd\u74f6\u9888\u7684\u7cfb\u7edf\u5206\u6790\uff0c\u5c24\u5176\u662f\u591a\u6a21\u6001\u8ba4\u77e5\u80fd\u529b\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51faDeRIS\u6846\u67b6\uff0c\u5206\u89e3RIS\u4e3a\u611f\u77e5\u548c\u8ba4\u77e5\u6a21\u5757\uff0c\u91c7\u7528Loopback Synergy\u673a\u5236\u589e\u5f3a\u6a21\u5757\u534f\u540c\uff0c\u5e76\u5f15\u5165\u975e\u53c2\u8003\u6837\u672c\u8f6c\u6362\u6570\u636e\u589e\u5f3a\u3002", "result": "DeRIS\u5728\u7cbe\u786e\u5206\u5272\u548c\u56fe\u50cf-\u6587\u672c\u7406\u89e3\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u65e0\u9700\u4e13\u95e8\u67b6\u6784\u4fee\u6539\u5373\u53ef\u9002\u5e94\u591a\u573a\u666f\u3002", "conclusion": "DeRIS\u901a\u8fc7\u6a21\u5757\u5316\u5206\u6790\u548c\u534f\u540c\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86RIS\u7684\u4e3b\u8981\u74f6\u9888\uff0c\u63d0\u5347\u4e86\u901a\u7528\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2507.01744", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01744", "abs": "https://arxiv.org/abs/2507.01744", "authors": ["Benjamin Jin", "Grant Mair", "Joanna M. Wardlaw", "Maria del C. Vald\u00e9s Hern\u00e1ndez"], "title": "Calibrated Self-supervised Vision Transformers Improve Intracranial Arterial Calcification Segmentation from Clinical CT Head Scans", "comment": null, "summary": "Vision Transformers (ViTs) have gained significant popularity in the natural\nimage domain but have been less successful in 3D medical image segmentation.\nNevertheless, 3D ViTs are particularly interesting for large medical imaging\nvolumes due to their efficient self-supervised training within the masked\nautoencoder (MAE) framework, which enables the use of imaging data without the\nneed for expensive manual annotations. intracranial arterial calcification\n(IAC) is an imaging biomarker visible on routinely acquired CT scans linked to\nneurovascular diseases such as stroke and dementia, and automated IAC\nquantification could enable their large-scale risk assessment. We pre-train\nViTs with MAE and fine-tune them for IAC segmentation for the first time. To\ndevelop our models, we use highly heterogeneous data from a large clinical\ntrial, the third International Stroke Trial (IST-3). We evaluate key aspects of\nMAE pre-trained ViTs in IAC segmentation, and analyse the clinical\nimplications. We show: 1) our calibrated self-supervised ViT beats a strong\nsupervised nnU-Net baseline by 3.2 Dice points, 2) low patch sizes are crucial\nfor ViTs for IAC segmentation and interpolation upsampling with regular\nconvolutions is preferable to transposed convolutions for ViT-based models, and\n3) our ViTs increase robustness to higher slice thicknesses and improve risk\ngroup classification in a clinical scenario by 46%. Our code is available\nonline.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8eVision Transformers (ViTs) \u548c\u63a9\u7801\u81ea\u7f16\u7801\u5668 (MAE) \u76843D\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\uff0c\u9996\u6b21\u5c06\u5176\u5e94\u7528\u4e8e\u9885\u5185\u52a8\u8109\u9499\u5316 (IAC) \u5206\u5272\uff0c\u5e76\u5728\u4e34\u5e8a\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "3D ViTs\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u5176\u81ea\u76d1\u7763\u8bad\u7ec3\u7279\u6027\u4f7f\u5176\u9002\u5408\u5904\u7406\u65e0\u9700\u6602\u8d35\u6807\u6ce8\u7684\u5927\u89c4\u6a21\u533b\u5b66\u5f71\u50cf\u6570\u636e\u3002IAC\u4f5c\u4e3a\u795e\u7ecf\u8840\u7ba1\u75be\u75c5\u7684\u751f\u7269\u6807\u5fd7\u7269\uff0c\u81ea\u52a8\u5316\u91cf\u5316\u6709\u52a9\u4e8e\u5927\u89c4\u6a21\u98ce\u9669\u8bc4\u4f30\u3002", "method": "\u91c7\u7528MAE\u6846\u67b6\u9884\u8bad\u7ec3ViTs\uff0c\u5e76\u5728IST-3\u4e34\u5e8a\u8bd5\u9a8c\u7684\u5f02\u6784\u6570\u636e\u4e0a\u5fae\u8c03\u7528\u4e8eIAC\u5206\u5272\u3002\u7814\u7a76\u4e86\u4f4epatch\u5927\u5c0f\u548c\u63d2\u503c\u4e0a\u91c7\u6837\u5bf9ViTs\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "1) \u81ea\u76d1\u7763ViT\u6bd4\u76d1\u7763nnU-Net\u57fa\u7ebf\u9ad83.2 Dice\u5206\u6570\uff1b2) \u4f4epatch\u5927\u5c0f\u548c\u63d2\u503c\u4e0a\u91c7\u6837\u5bf9ViTs\u66f4\u4f18\uff1b3) ViTs\u63d0\u9ad8\u4e86\u5bf9\u9ad8\u5207\u7247\u539a\u5ea6\u7684\u9c81\u68d2\u6027\uff0c\u4e34\u5e8a\u98ce\u9669\u5206\u7c7b\u63d0\u534746%\u3002", "conclusion": "MAE\u9884\u8bad\u7ec3\u7684ViTs\u5728IAC\u5206\u5272\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01788", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01788", "abs": "https://arxiv.org/abs/2507.01788", "authors": ["Montasir Shams", "Chashi Mahiul Islam", "Shaeke Salman", "Phat Tran", "Xiuwen Liu"], "title": "Are Vision Transformer Representations Semantically Meaningful? A Case Study in Medical Imaging", "comment": "9 pages", "summary": "Vision transformers (ViTs) have rapidly gained prominence in medical imaging\ntasks such as disease classification, segmentation, and detection due to their\nsuperior accuracy compared to conventional deep learning models. However, due\nto their size and complex interactions via the self-attention mechanism, they\nare not well understood. In particular, it is unclear whether the\nrepresentations produced by such models are semantically meaningful. In this\npaper, using a projected gradient-based algorithm, we show that their\nrepresentations are not semantically meaningful and they are inherently\nvulnerable to small changes. Images with imperceptible differences can have\nvery different representations; on the other hand, images that should belong to\ndifferent semantic classes can have nearly identical representations. Such\nvulnerability can lead to unreliable classification results; for example,\nunnoticeable changes cause the classification accuracy to be reduced by over\n60\\%. %. To the best of our knowledge, this is the first work to systematically\ndemonstrate this fundamental lack of semantic meaningfulness in ViT\nrepresentations for medical image classification, revealing a critical\nchallenge for their deployment in safety-critical systems.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u89c6\u89c9\u53d8\u6362\u5668\uff08ViT\uff09\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u8868\u793a\u7f3a\u4e4f\u8bed\u4e49\u610f\u4e49\uff0c\u4e14\u5bf9\u5fae\u5c0f\u53d8\u5316\u654f\u611f\uff0c\u5bfc\u81f4\u5206\u7c7b\u7ed3\u679c\u4e0d\u53ef\u9760\u3002", "motivation": "\u63a2\u7d22\u89c6\u89c9\u53d8\u6362\u5668\u5728\u533b\u5b66\u56fe\u50cf\u4efb\u52a1\u4e2d\u7684\u8868\u793a\u662f\u5426\u5177\u6709\u8bed\u4e49\u610f\u4e49\u53ca\u5176\u7a33\u5b9a\u6027\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u6295\u5f71\u68af\u5ea6\u7684\u7b97\u6cd5\u5206\u6790ViT\u7684\u8868\u793a\u3002", "result": "ViT\u7684\u8868\u793a\u5bf9\u5fae\u5c0f\u53d8\u5316\u654f\u611f\uff0c\u8bed\u4e49\u610f\u4e49\u4e0d\u8db3\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u53ef\u4e0b\u964d60%\u4ee5\u4e0a\u3002", "conclusion": "ViT\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u8868\u793a\u5b58\u5728\u6839\u672c\u6027\u95ee\u9898\uff0c\u5f71\u54cd\u5176\u5728\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u4e2d\u7684\u90e8\u7f72\u3002"}}
{"id": "2507.01747", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01747", "abs": "https://arxiv.org/abs/2507.01747", "authors": ["Nora Gourmelon", "Marcel Dreier", "Martin Mayr", "Thorsten Seehaus", "Dakota Pyles", "Matthias Braun", "Andreas Maier", "Vincent Christlein"], "title": "SSL4SAR: Self-Supervised Learning for Glacier Calving Front Extraction from SAR Imagery", "comment": "in IEEE Transactions on Geoscience and Remote Sensing. arXiv admin\n  note: text overlap with arXiv:2501.05281", "summary": "Glaciers are losing ice mass at unprecedented rates, increasing the need for\naccurate, year-round monitoring to understand frontal ablation, particularly\nthe factors driving the calving process. Deep learning models can extract\ncalving front positions from Synthetic Aperture Radar imagery to track seasonal\nice losses at the calving fronts of marine- and lake-terminating glaciers. The\ncurrent state-of-the-art model relies on ImageNet-pretrained weights. However,\nthey are suboptimal due to the domain shift between the natural images in\nImageNet and the specialized characteristics of remote sensing imagery, in\nparticular for Synthetic Aperture Radar imagery. To address this challenge, we\npropose two novel self-supervised multimodal pretraining techniques that\nleverage SSL4SAR, a new unlabeled dataset comprising 9,563 Sentinel-1 and 14\nSentinel-2 images of Arctic glaciers, with one optical image per glacier in the\ndataset. Additionally, we introduce a novel hybrid model architecture that\ncombines a Swin Transformer encoder with a residual Convolutional Neural\nNetwork (CNN) decoder. When pretrained on SSL4SAR, this model achieves a mean\ndistance error of 293 m on the \"CAlving Fronts and where to Find thEm\" (CaFFe)\nbenchmark dataset, outperforming the prior best model by 67 m. Evaluating an\nensemble of the proposed model on a multi-annotator study of the benchmark\ndataset reveals a mean distance error of 75 m, approaching the human\nperformance of 38 m. This advancement enables precise monitoring of seasonal\nchanges in glacier calving fronts.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u81ea\u76d1\u7763\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u6280\u672f\u548c\u4e00\u79cd\u6df7\u5408\u6a21\u578b\u67b6\u6784\uff0c\u7528\u4e8e\u4eceSAR\u56fe\u50cf\u4e2d\u63d0\u53d6\u51b0\u5ddd\u5d29\u89e3\u524d\u6cbf\u4f4d\u7f6e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cbe\u5ea6\u3002", "motivation": "\u51b0\u5ddd\u51b0\u91cf\u6d41\u5931\u52a0\u5267\uff0c\u9700\u7cbe\u786e\u76d1\u6d4b\u5d29\u89e3\u524d\u6cbf\u4ee5\u7406\u89e3\u5d29\u89e3\u8fc7\u7a0b\uff0c\u73b0\u6709\u57fa\u4e8eImageNet\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u56e0\u9886\u57df\u5dee\u5f02\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u81ea\u76d1\u7763\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u6280\u672f\uff08\u5229\u7528\u65b0\u6570\u636e\u96c6SSL4SAR\uff09\u548c\u6df7\u5408\u6a21\u578b\u67b6\u6784\uff08Swin Transformer\u7f16\u7801\u5668+\u6b8b\u5deeCNN\u89e3\u7801\u5668\uff09\u3002", "result": "\u6a21\u578b\u5728CaFFe\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u8ddd\u79bb\u8bef\u5dee\u4e3a293\u7c73\uff0c\u4f18\u4e8e\u4e4b\u524d\u6700\u4f73\u6a21\u578b67\u7c73\uff1b\u96c6\u6210\u6a21\u578b\u8bef\u5dee75\u7c73\uff0c\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\uff0838\u7c73\uff09\u3002", "conclusion": "\u8be5\u6280\u672f\u80fd\u7cbe\u786e\u76d1\u6d4b\u51b0\u5ddd\u5d29\u89e3\u524d\u6cbf\u7684\u5b63\u8282\u6027\u53d8\u5316\uff0c\u4e3a\u51b0\u5ddd\u7814\u7a76\u63d0\u4f9b\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2507.01695", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01695", "abs": "https://arxiv.org/abs/2507.01695", "authors": ["Omkar Shende", "Gayathri Ananthanarayanan", "Marcello Traiola"], "title": "PERTINENCE: Input-based Opportunistic Neural Network Dynamic Execution", "comment": null, "summary": "Deep neural networks (DNNs) have become ubiquitous thanks to their remarkable\nability to model complex patterns across various domains such as computer\nvision, speech recognition, robotics, etc. While large DNN models are often\nmore accurate than simpler, lightweight models, they are also resource- and\nenergy-hungry. Hence, it is imperative to design methods to reduce reliance on\nsuch large models without significant degradation in output accuracy. The high\ncomputational cost of these models is often necessary only for a reduced set of\nchallenging inputs, while lighter models can handle most simple ones. Thus,\ncarefully combining properties of existing DNN models in a dynamic, input-based\nway opens opportunities to improve efficiency without impacting accuracy.\n  In this work, we introduce PERTINENCE, a novel online method designed to\nanalyze the complexity of input features and dynamically select the most\nsuitable model from a pre-trained set to process a given input effectively. To\nachieve this, we employ a genetic algorithm to explore the training space of an\nML-based input dispatcher, enabling convergence towards the Pareto front in the\nsolution space that balances overall accuracy and computational efficiency.\n  We showcase our approach on state-of-the-art Convolutional Neural Networks\n(CNNs) trained on the CIFAR-10 and CIFAR-100, as well as Vision Transformers\n(ViTs) trained on TinyImageNet dataset. We report results showing PERTINENCE's\nability to provide alternative solutions to existing state-of-the-art models in\nterms of trade-offs between accuracy and number of operations. By\nopportunistically selecting among models trained for the same task, PERTINENCE\nachieves better or comparable accuracy with up to 36% fewer operations.", "AI": {"tldr": "PERTINENCE\u662f\u4e00\u79cd\u52a8\u6001\u9009\u62e9\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u8f93\u5165\u8c03\u5ea6\u5668\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u91cf\u3002", "motivation": "\u5927\u578bDNN\u6a21\u578b\u867d\u7136\u7cbe\u5ea6\u9ad8\uff0c\u4f46\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5927\uff0c\u9700\u8981\u5728\u4e0d\u663e\u8457\u964d\u4f4e\u7cbe\u5ea6\u7684\u60c5\u51b5\u4e0b\u51cf\u5c11\u5bf9\u5927\u578b\u6a21\u578b\u7684\u4f9d\u8d56\u3002", "method": "\u4f7f\u7528\u9057\u4f20\u7b97\u6cd5\u8bad\u7ec3\u8f93\u5165\u8c03\u5ea6\u5668\uff0c\u52a8\u6001\u9009\u62e9\u6700\u9002\u5408\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u5904\u7406\u8f93\u5165\u3002", "result": "\u5728CIFAR-10\u3001CIFAR-100\u548cTinyImageNet\u6570\u636e\u96c6\u4e0a\uff0cPERTINENCE\u5728\u4fdd\u6301\u6216\u63d0\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u51cf\u5c11\u4e8636%\u7684\u8ba1\u7b97\u91cf\u3002", "conclusion": "PERTINENCE\u901a\u8fc7\u52a8\u6001\u6a21\u578b\u9009\u62e9\uff0c\u5b9e\u73b0\u4e86\u7cbe\u5ea6\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u5e73\u8861\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2507.01756", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01756", "abs": "https://arxiv.org/abs/2507.01756", "authors": ["Peng Zheng", "Junke Wang", "Yi Chang", "Yizhou Yu", "Rui Ma", "Zuxuan Wu"], "title": "Rethinking Discrete Tokens: Treating Them as Conditions for Continuous Autoregressive Image Synthesis", "comment": "accepted by iccv 2025", "summary": "Recent advances in large language models (LLMs) have spurred interests in\nencoding images as discrete tokens and leveraging autoregressive (AR)\nframeworks for visual generation. However, the quantization process in AR-based\nvisual generation models inherently introduces information loss that degrades\nimage fidelity. To mitigate this limitation, recent studies have explored to\nautoregressively predict continuous tokens. Unlike discrete tokens that reside\nin a structured and bounded space, continuous representations exist in an\nunbounded, high-dimensional space, making density estimation more challenging\nand increasing the risk of generating out-of-distribution artifacts. Based on\nthe above findings, this work introduces DisCon (Discrete-Conditioned\nContinuous Autoregressive Model), a novel framework that reinterprets discrete\ntokens as conditional signals rather than generation targets. By modeling the\nconditional probability of continuous representations conditioned on discrete\ntokens, DisCon circumvents the optimization challenges of continuous token\nmodeling while avoiding the information loss caused by quantization. DisCon\nachieves a gFID score of 1.38 on ImageNet 256$\\times$256 generation,\noutperforming state-of-the-art autoregressive approaches by a clear margin.", "AI": {"tldr": "DisCon\u6846\u67b6\u901a\u8fc7\u5c06\u79bb\u6563\u6807\u8bb0\u4f5c\u4e3a\u6761\u4ef6\u4fe1\u53f7\u800c\u975e\u751f\u6210\u76ee\u6807\uff0c\u89e3\u51b3\u4e86\u8fde\u7eed\u6807\u8bb0\u5efa\u6a21\u7684\u4f18\u5316\u6311\u6218\uff0c\u540c\u65f6\u907f\u514d\u4e86\u91cf\u5316\u5e26\u6765\u7684\u4fe1\u606f\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u81ea\u56de\u5f52\u89c6\u89c9\u751f\u6210\u6a21\u578b\u4e2d\u91cf\u5316\u8fc7\u7a0b\u5bfc\u81f4\u7684\u4fe1\u606f\u635f\u5931\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u8fde\u7eed\u6807\u8bb0\u5efa\u6a21\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faDisCon\u6846\u67b6\uff0c\u5c06\u79bb\u6563\u6807\u8bb0\u4f5c\u4e3a\u6761\u4ef6\u4fe1\u53f7\uff0c\u5efa\u6a21\u8fde\u7eed\u8868\u793a\u7684\u6761\u4ef6\u6982\u7387\u3002", "result": "\u5728ImageNet 256\u00d7256\u751f\u6210\u4efb\u52a1\u4e0a\uff0cDisCon\u7684gFID\u5f97\u5206\u4e3a1.38\uff0c\u4f18\u4e8e\u73b0\u6709\u81ea\u56de\u5f52\u65b9\u6cd5\u3002", "conclusion": "DisCon\u901a\u8fc7\u7ed3\u5408\u79bb\u6563\u548c\u8fde\u7eed\u8868\u793a\u7684\u4f18\u70b9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u751f\u6210\u6027\u80fd\u3002"}}
{"id": "2507.01699", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01699", "abs": "https://arxiv.org/abs/2507.01699", "authors": ["Illia Oleksiienko", "Juho Kanniainen", "Alexandros Iosifidis"], "title": "Variational Graph Convolutional Neural Networks", "comment": "This work has been submitted to the IEEE for possible publication. 9\n  pages, 6 figures", "summary": "Estimation of model uncertainty can help improve the explainability of Graph\nConvolutional Networks and the accuracy of the models at the same time.\nUncertainty can also be used in critical applications to verify the results of\nthe model by an expert or additional models. In this paper, we propose\nVariational Neural Network versions of spatial and spatio-temporal Graph\nConvolutional Networks. We estimate uncertainty in both outputs and layer-wise\nattentions of the models, which has the potential for improving model\nexplainability. We showcase the benefits of these models in the social trading\nanalysis and the skeleton-based human action recognition tasks on the Finnish\nboard membership, NTU-60, NTU-120 and Kinetics datasets, where we show\nimprovement in model accuracy in addition to estimated model uncertainties.", "AI": {"tldr": "\u63d0\u51fa\u53d8\u5206\u795e\u7ecf\u7f51\u7edc\u7248\u672c\u7684\u56fe\u5377\u79ef\u7f51\u7edc\uff0c\u4f30\u8ba1\u6a21\u578b\u8f93\u51fa\u548c\u6ce8\u610f\u529b\u5c42\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u5347\u6a21\u578b\u89e3\u91ca\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u901a\u8fc7\u4f30\u8ba1\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u5347\u56fe\u5377\u79ef\u7f51\u7edc\u7684\u89e3\u91ca\u6027\uff0c\u5e76\u5728\u5173\u952e\u5e94\u7528\u4e2d\u9a8c\u8bc1\u6a21\u578b\u7ed3\u679c\u3002", "method": "\u63d0\u51fa\u7a7a\u95f4\u548c\u65f6\u7a7a\u56fe\u5377\u79ef\u7f51\u7edc\u7684\u53d8\u5206\u795e\u7ecf\u7f51\u7edc\u7248\u672c\uff0c\u4f30\u8ba1\u8f93\u51fa\u548c\u5c42\u6ce8\u610f\u529b\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728\u793e\u4ea4\u4ea4\u6613\u5206\u6790\u548c\u9aa8\u67b6\u52a8\u4f5c\u8bc6\u522b\u4efb\u52a1\u4e2d\uff0c\u6a21\u578b\u51c6\u786e\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5747\u6709\u6240\u63d0\u5347\u3002", "conclusion": "\u53d8\u5206\u56fe\u5377\u79ef\u7f51\u7edc\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u89e3\u91ca\u6027\u548c\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u3002"}}
{"id": "2507.01825", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01825", "abs": "https://arxiv.org/abs/2507.01825", "authors": ["Franco Alberto Cardillo", "Hamza Khyari", "Umberto Straccia"], "title": "MILP-SAT-GNN: Yet Another Neural SAT Solver", "comment": null, "summary": "We proposes a novel method that enables Graph Neural Networks (GNNs) to solve\nSAT problems by leveraging a technique developed for applying GNNs to Mixed\nInteger Linear Programming (MILP). Specifically, k-CNF formulae are mapped into\nMILP problems, which are then encoded as weighted bipartite graphs and\nsubsequently fed into a GNN for training and testing. From a theoretical\nperspective: (i) we establish permutation and equivalence invariance results,\ndemonstrating that the method produces outputs that are stable under reordering\nof clauses and variables; (ii) we identify a theoretical limitation, showing\nthat for a class of formulae called foldable formulae, standard GNNs cannot\nalways distinguish satisfiable from unsatisfiable instances; (iii) we prove a\nuniversal approximation theorem, establishing that with Random Node\nInitialization (RNI), the method can approximate SAT solving to arbitrary\nprecision on finite datasets, that is, the GNN becomes approximately sound and\ncomplete on such datasets. Furthermore, we show that for unfoldable formulae,\nthe same approximation guarantee can be achieved without the need for RNI.\nFinally, we conduct an experimental evaluation of our approach, which show\nthat, despite the simplicity of the neural architecture, the method achieves\npromising results.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06GNN\u5e94\u7528\u4e8eSAT\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06k-CNF\u516c\u5f0f\u6620\u5c04\u4e3aMILP\u95ee\u9898\u5e76\u7f16\u7801\u4e3a\u52a0\u6743\u4e8c\u5206\u56fe\uff0c\u5229\u7528GNN\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002", "motivation": "\u63a2\u7d22GNN\u5728\u89e3\u51b3SAT\u95ee\u9898\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u9a8c\u8bc1\u5176\u7406\u8bba\u6027\u80fd\u548c\u5b9e\u9645\u6548\u679c\u3002", "method": "\u5c06k-CNF\u516c\u5f0f\u8f6c\u6362\u4e3aMILP\u95ee\u9898\uff0c\u7f16\u7801\u4e3a\u52a0\u6743\u4e8c\u5206\u56fe\u540e\u8f93\u5165GNN\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u7a33\u5b9a\u6027\u548c\u5c40\u9650\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u5177\u6709\u6f5c\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4e3aGNN\u5728SAT\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.01791", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01791", "abs": "https://arxiv.org/abs/2507.01791", "authors": ["Zihong Guo", "Chen Wan", "Yayin Zheng", "Hailing Kuang", "Xiaohai Lu"], "title": "Boosting Adversarial Transferability Against Defenses via Multi-Scale Transformation", "comment": null, "summary": "The transferability of adversarial examples poses a significant security\nchallenge for deep neural networks, which can be attacked without knowing\nanything about them. In this paper, we propose a new Segmented Gaussian Pyramid\n(SGP) attack method to enhance the transferability, particularly against\ndefense models. Unlike existing methods that generally focus on single-scale\nimages, our approach employs Gaussian filtering and three types of downsampling\nto construct a series of multi-scale examples. Then, the gradients of the loss\nfunction with respect to each scale are computed, and their average is used to\ndetermine the adversarial perturbations. The proposed SGP can be considered an\ninput transformation with high extensibility that is easily integrated into\nmost existing adversarial attacks. Extensive experiments demonstrate that in\ncontrast to the state-of-the-art methods, SGP significantly enhances attack\nsuccess rates against black-box defense models, with average attack success\nrates increasing by 2.3% to 32.6%, based only on transferability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u6bb5\u9ad8\u65af\u91d1\u5b57\u5854\uff08SGP\uff09\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u56fe\u50cf\u5904\u7406\u589e\u5f3a\u5bf9\u6297\u6837\u672c\u7684\u53ef\u8fc1\u79fb\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u9ed1\u76d2\u9632\u5fa1\u6a21\u578b\u7684\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u5bf9\u6297\u6837\u672c\u7684\u53ef\u8fc1\u79fb\u6027\u5bf9\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6784\u6210\u91cd\u5927\u5b89\u5168\u5a01\u80c1\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4ec5\u5173\u6ce8\u5355\u5c3a\u5ea6\u56fe\u50cf\uff0c\u9650\u5236\u4e86\u653b\u51fb\u6548\u679c\u3002", "method": "\u91c7\u7528\u9ad8\u65af\u6ee4\u6ce2\u548c\u4e09\u79cd\u4e0b\u91c7\u6837\u65b9\u6cd5\u6784\u5efa\u591a\u5c3a\u5ea6\u6837\u672c\uff0c\u8ba1\u7b97\u5404\u5c3a\u5ea6\u635f\u5931\u51fd\u6570\u7684\u68af\u5ea6\u5e76\u53d6\u5e73\u5747\u503c\u4ee5\u786e\u5b9a\u5bf9\u6297\u6270\u52a8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSGP\u663e\u8457\u63d0\u5347\u4e86\u653b\u51fb\u6210\u529f\u7387\uff0c\u5e73\u5747\u63d0\u9ad82.3%\u81f332.6%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SGP\u4f5c\u4e3a\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u8f93\u5165\u53d8\u6362\u65b9\u6cd5\uff0c\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u653b\u51fb\u4e2d\uff0c\u6709\u6548\u589e\u5f3a\u5bf9\u6297\u6837\u672c\u7684\u53ef\u8fc1\u79fb\u6027\u3002"}}
{"id": "2507.01829", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01829", "abs": "https://arxiv.org/abs/2507.01829", "authors": ["Tristan Torchet", "Christian Metzner", "Laura Kriener", "Melika Payvand"], "title": "mGRADE: Minimal Recurrent Gating Meets Delay Convolutions for Lightweight Sequence Modeling", "comment": null, "summary": "Edge devices for temporal processing demand models that capture both short-\nand long- range dynamics under tight memory constraints. While Transformers\nexcel at sequence modeling, their quadratic memory scaling with sequence length\nmakes them impractical for such settings. Recurrent Neural Networks (RNNs)\noffer constant memory but train sequentially, and Temporal Convolutional\nNetworks (TCNs), though efficient, scale memory with kernel size. To address\nthis, we propose mGRADE (mininally Gated Recurrent Architecture with Delay\nEmbedding), a hybrid-memory system that integrates a temporal 1D-convolution\nwith learnable spacings followed by a minimal gated recurrent unit (minGRU).\nThis design allows the convolutional layer to realize a flexible delay\nembedding that captures rapid temporal variations, while the recurrent module\nefficiently maintains global context with minimal memory overhead. We validate\nour approach on two synthetic tasks, demonstrating that mGRADE effectively\nseparates and preserves multi-scale temporal features. Furthermore, on\nchallenging pixel-by-pixel image classification benchmarks, mGRADE consistently\noutperforms both pure convolutional and pure recurrent counterparts using\napproximately 20% less memory footprint, highlighting its suitability for\nmemory-constrained temporal processing at the edge. This highlights mGRADE's\npromise as an efficient solution for memory-constrained multi-scale temporal\nprocessing at the edge.", "AI": {"tldr": "mGRADE\u662f\u4e00\u79cd\u6df7\u5408\u5185\u5b58\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e861D\u5377\u79ef\u548c\u6700\u5c0f\u95e8\u63a7\u5faa\u73af\u5355\u5143\uff0c\u9002\u7528\u4e8e\u5185\u5b58\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u8fdb\u884c\u591a\u5c3a\u5ea6\u65f6\u95f4\u5904\u7406\u3002", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u9700\u8981\u540c\u65f6\u6355\u83b7\u77ed\u65f6\u548c\u957f\u65f6\u52a8\u6001\u7684\u6a21\u578b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\uff08\u5982Transformer\u3001RNN\u548cTCN\uff09\u5728\u5185\u5b58\u6216\u6548\u7387\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51famGRADE\uff0c\u6574\u54081D\u5377\u79ef\u548cminGRU\uff0c\u5377\u79ef\u5c42\u5b9e\u73b0\u7075\u6d3b\u5ef6\u8fdf\u5d4c\u5165\uff0c\u5faa\u73af\u6a21\u5757\u9ad8\u6548\u7ef4\u62a4\u5168\u5c40\u4e0a\u4e0b\u6587\u3002", "result": "\u5728\u5408\u6210\u4efb\u52a1\u548c\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cmGRADE\u4f18\u4e8e\u7eaf\u5377\u79ef\u548c\u5faa\u73af\u6a21\u578b\uff0c\u5185\u5b58\u5360\u7528\u51cf\u5c1120%\u3002", "conclusion": "mGRADE\u662f\u5185\u5b58\u53d7\u9650\u8fb9\u7f18\u8bbe\u5907\u8fdb\u884c\u591a\u5c3a\u5ea6\u65f6\u95f4\u5904\u7406\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01714", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01714", "abs": "https://arxiv.org/abs/2507.01714", "authors": ["Kevin Innerebner", "Franz M. Rohrhofer", "Bernhard C. Geiger"], "title": "B-PL-PINN: Stabilizing PINN Training with Bayesian Pseudo Labeling", "comment": null, "summary": "Training physics-informed neural networks (PINNs) for forward problems often\nsuffers from severe convergence issues, hindering the propagation of\ninformation from regions where the desired solution is well-defined.\nHaitsiukevich and Ilin (2023) proposed an ensemble approach that extends the\nactive training domain of each PINN based on i) ensemble consensus and ii)\nvicinity to (pseudo-)labeled points, thus ensuring that the information from\nthe initial condition successfully propagates to the interior of the\ncomputational domain.\n  In this work, we suggest replacing the ensemble by a Bayesian PINN, and\nconsensus by an evaluation of the PINN's posterior variance. Our experiments\nshow that this mathematically principled approach outperforms the ensemble on a\nset of benchmark problems and is competitive with PINN ensembles trained with\ncombinations of Adam and LBFGS.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u7528\u8d1d\u53f6\u65af\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u66ff\u4ee3\u96c6\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc4\u4f30\u540e\u9a8c\u65b9\u5dee\u63d0\u5347\u4fe1\u606f\u4f20\u64ad\u6548\u679c\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u96c6\u6210\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfPINN\u5728\u524d\u5411\u95ee\u9898\u4e2d\u4fe1\u606f\u4f20\u64ad\u4e0d\u8db3\u7684\u6536\u655b\u95ee\u9898\u3002", "method": "\u7528\u8d1d\u53f6\u65afPINN\u66ff\u4ee3\u96c6\u6210\u65b9\u6cd5\uff0c\u5229\u7528\u540e\u9a8c\u65b9\u5dee\u8bc4\u4f30\u4fe1\u606f\u4f20\u64ad\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u57fa\u51c6\u95ee\u9898\u4e0a\u4f18\u4e8e\u96c6\u6210\u65b9\u6cd5\uff0c\u5e76\u4e0eAdam\u548cLBFGS\u7ed3\u5408\u7684PINN\u96c6\u6210\u7ade\u4e89\u3002", "conclusion": "\u8d1d\u53f6\u65afPINN\u662f\u89e3\u51b3PINN\u4fe1\u606f\u4f20\u64ad\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2507.01792", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01792", "abs": "https://arxiv.org/abs/2507.01792", "authors": ["Peng Zheng", "Ye Wang", "Rui Ma", "Zuxuan Wu"], "title": "FreeLoRA: Enabling Training-Free LoRA Fusion for Autoregressive Multi-Subject Personalization", "comment": null, "summary": "Subject-driven image generation plays a crucial role in applications such as\nvirtual try-on and poster design. Existing approaches typically fine-tune\npretrained generative models or apply LoRA-based adaptations for individual\nsubjects. However, these methods struggle with multi-subject personalization,\nas combining independently adapted modules often requires complex re-tuning or\njoint optimization. We present FreeLoRA, a simple and generalizable framework\nthat enables training-free fusion of subject-specific LoRA modules for\nmulti-subject personalization. Each LoRA module is adapted on a few images of a\nspecific subject using a Full Token Tuning strategy, where it is applied across\nall tokens in the prompt to encourage weakly supervised token-content\nalignment. At inference, we adopt Subject-Aware Inference, activating each\nmodule only on its corresponding subject tokens. This enables training-free\nfusion of multiple personalized subjects within a single image, while\nmitigating overfitting and mutual interference between subjects. Extensive\nexperiments show that FreeLoRA achieves strong performance in both subject\nfidelity and prompt consistency.", "AI": {"tldr": "FreeLoRA\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u7279\u5b9a\u4e3b\u9898\u7684LoRA\u6a21\u5757\u5b9e\u73b0\u591a\u4e3b\u9898\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u4e3b\u9898\u4e2a\u6027\u5316\u65f6\u9700\u590d\u6742\u8c03\u6574\u6216\u8054\u5408\u4f18\u5316\uff0cFreeLoRA\u65e8\u5728\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528Full Token Tuning\u7b56\u7565\u548cSubject-Aware Inference\uff0c\u5b9e\u73b0\u6a21\u5757\u7684\u65e0\u8bad\u7ec3\u878d\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660eFreeLoRA\u5728\u4e3b\u9898\u4fdd\u771f\u5ea6\u548c\u63d0\u793a\u4e00\u81f4\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "FreeLoRA\u4e3a\u591a\u4e3b\u9898\u4e2a\u6027\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01875", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01875", "abs": "https://arxiv.org/abs/2507.01875", "authors": ["Gast\u00f3n Garc\u00eda Gonz\u00e1lez", "Pedro Casas", "Emilio Mart\u00ednez", "Alicia Fern\u00e1ndez"], "title": "Towards Foundation Auto-Encoders for Time-Series Anomaly Detection", "comment": "Presented at ACM KDD 2024, MiLeTS 2024 Workshop, August 25, 2024,\n  Barcelona, Spain", "summary": "We investigate a novel approach to time-series modeling, inspired by the\nsuccesses of large pretrained foundation models. We introduce FAE (Foundation\nAuto-Encoders), a foundation generative-AI model for anomaly detection in\ntime-series data, based on Variational Auto-Encoders (VAEs). By foundation, we\nmean a model pretrained on massive amounts of time-series data which can learn\ncomplex temporal patterns useful for accurate modeling, forecasting, and\ndetection of anomalies on previously unseen datasets. FAE leverages VAEs and\nDilated Convolutional Neural Networks (DCNNs) to build a generic model for\nunivariate time-series modeling, which could eventually perform properly in\nout-of-the-box, zero-shot anomaly detection applications. We introduce the main\nconcepts of FAE, and present preliminary results in different multi-dimensional\ntime-series datasets from various domains, including a real dataset from an\noperational mobile ISP, and the well known KDD 2021 Anomaly Detection dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u548c\u6269\u5f20\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08DCNN\uff09\u7684\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u57fa\u7840\u6a21\u578bFAE\uff0c\u9002\u7528\u4e8e\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u3002", "motivation": "\u53d7\u5927\u578b\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u6210\u529f\u7684\u542f\u53d1\uff0c\u65e8\u5728\u6784\u5efa\u4e00\u4e2a\u80fd\u591f\u5b66\u4e60\u590d\u6742\u65f6\u95f4\u6a21\u5f0f\u5e76\u9002\u7528\u4e8e\u591a\u79cd\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u901a\u7528\u6a21\u578b\u3002", "method": "\u7ed3\u5408VAE\u548cDCNN\uff0c\u9884\u8bad\u7ec3\u5927\u89c4\u6a21\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u6784\u5efa\u901a\u7528\u6a21\u578bFAE\u3002", "result": "\u5728\u591a\u7ef4\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\uff08\u5305\u62ec\u79fb\u52a8ISP\u6570\u636e\u548cKDD 2021\u6570\u636e\u96c6\uff09\u4e0a\u5c55\u793a\u4e86\u521d\u6b65\u7ed3\u679c\u3002", "conclusion": "FAE\u4e3a\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u7684\u57fa\u7840\u6a21\u578b\uff0c\u5177\u6709\u96f6\u6837\u672c\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.01724", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01724", "abs": "https://arxiv.org/abs/2507.01724", "authors": ["Micha Henheik", "Theresa Eimer", "Marius Lindauer"], "title": "Revisiting Learning Rate Control", "comment": null, "summary": "The learning rate is one of the most important hyperparameters in deep\nlearning, and how to control it is an active area within both AutoML and deep\nlearning research. Approaches for learning rate control span from classic\noptimization to online scheduling based on gradient statistics. This paper\ncompares paradigms to assess the current state of learning rate control. We\nfind that methods from multi-fidelity hyperparameter optimization,\nfixed-hyperparameter schedules, and hyperparameter-free learning often perform\nvery well on selected deep learning tasks but are not reliable across settings.\nThis highlights the need for algorithm selection methods in learning rate\ncontrol, which have been neglected so far by both the AutoML and deep learning\ncommunities. We also observe a trend of hyperparameter optimization approaches\nbecoming less effective as models and tasks grow in complexity, even when\ncombined with multi-fidelity approaches for more expensive model trainings. A\nfocus on more relevant test tasks and new promising directions like finetunable\nmethods and meta-learning will enable the AutoML community to significantly\nstrengthen its impact on this crucial factor in deep learning.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u5b66\u4e60\u7387\u63a7\u5236\u7684\u4e0d\u540c\u65b9\u6cd5\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\u4f46\u7f3a\u4e4f\u666e\u9002\u6027\uff0c\u9700\u5173\u6ce8\u7b97\u6cd5\u9009\u62e9\u3002", "motivation": "\u5b66\u4e60\u7387\u662f\u6df1\u5ea6\u5b66\u4e60\u7684\u5173\u952e\u8d85\u53c2\u6570\uff0c\u7814\u7a76\u5176\u63a7\u5236\u65b9\u6cd5\u5bf9AutoML\u548c\u6df1\u5ea6\u5b66\u4e60\u9886\u57df\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6bd4\u8f83\u4e86\u591a\u4fdd\u771f\u8d85\u53c2\u6570\u4f18\u5316\u3001\u56fa\u5b9a\u5b66\u4e60\u7387\u8c03\u5ea6\u548c\u65e0\u8d85\u53c2\u6570\u5b66\u4e60\u7b49\u65b9\u6cd5\u3002", "result": "\u73b0\u6709\u65b9\u6cd5\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u590d\u6742\u6a21\u578b\u548c\u4efb\u52a1\u4e2d\u6548\u679c\u4e0b\u964d\u3002", "conclusion": "\u9700\u5173\u6ce8\u7b97\u6cd5\u9009\u62e9\u548c\u65b0\u5174\u65b9\u5411\uff08\u5982\u53ef\u5fae\u8c03\u65b9\u6cd5\u548c\u5143\u5b66\u4e60\uff09\u4ee5\u63d0\u5347\u5b66\u4e60\u7387\u63a7\u5236\u6548\u679c\u3002"}}
{"id": "2507.01800", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.01800", "abs": "https://arxiv.org/abs/2507.01800", "authors": ["Shengli Zhou", "Jianuo Zhu", "Qilin Huang", "Fangjing Wang", "Yanfu Zhang", "Feng Zheng"], "title": "HCNQA: Enhancing 3D VQA with Hierarchical Concentration Narrowing Supervision", "comment": "ICANN 2025", "summary": "3D Visual Question-Answering (3D VQA) is pivotal for models to perceive the\nphysical world and perform spatial reasoning. Answer-centric supervision is a\ncommonly used training method for 3D VQA models. Many models that utilize this\nstrategy have achieved promising results in 3D VQA tasks. However, the\nanswer-centric approach only supervises the final output of models and allows\nmodels to develop reasoning pathways freely. The absence of supervision on the\nreasoning pathway enables the potential for developing superficial shortcuts\nthrough common patterns in question-answer pairs. Moreover, although\nslow-thinking methods advance large language models, they suffer from\nunderthinking. To address these issues, we propose \\textbf{HCNQA}, a 3D VQA\nmodel leveraging a hierarchical concentration narrowing supervision method. By\nmimicking the human process of gradually focusing from a broad area to specific\nobjects while searching for answers, our method guides the model to perform\nthree phases of concentration narrowing through hierarchical supervision. By\nsupervising key checkpoints on a general reasoning pathway, our method can\nensure the development of a rational and effective reasoning pathway. Extensive\nexperimental results demonstrate that our method can effectively ensure that\nthe model develops a rational reasoning pathway and performs better. The code\nis available at https://github.com/JianuoZhu/HCNQA.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHCNQA\u76843D VQA\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u5c42\u76d1\u7763\u65b9\u6cd5\u89e3\u51b3\u73b0\u6709\u7b54\u6848\u4e2d\u5fc3\u76d1\u7763\u65b9\u6cd5\u5728\u63a8\u7406\u8def\u5f84\u4e0a\u7684\u4e0d\u8db3\uff0c\u786e\u4fdd\u6a21\u578b\u53d1\u5c55\u51fa\u5408\u7406\u7684\u63a8\u7406\u8def\u5f84\u3002", "motivation": "\u73b0\u6709\u7684\u7b54\u6848\u4e2d\u5fc3\u76d1\u7763\u65b9\u6cd5\u4ec5\u76d1\u7763\u6a21\u578b\u6700\u7ec8\u8f93\u51fa\uff0c\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u901a\u8fc7\u6d45\u5c42\u6377\u5f84\u5b66\u4e60\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u63a8\u7406\u8def\u5f84\u7684\u76d1\u7763\u3002\u6b64\u5916\uff0c\u6162\u601d\u8003\u65b9\u6cd5\u5b58\u5728\u6b20\u601d\u8003\u95ee\u9898\u3002", "method": "\u63d0\u51faHCNQA\u6a21\u578b\uff0c\u91c7\u7528\u5206\u5c42\u96c6\u4e2d\u7f29\u7a84\u76d1\u7763\u65b9\u6cd5\uff0c\u6a21\u62df\u4eba\u7c7b\u4ece\u5e7f\u6cdb\u533a\u57df\u9010\u6b65\u805a\u7126\u5230\u7279\u5b9a\u5bf9\u8c61\u7684\u641c\u7d22\u8fc7\u7a0b\uff0c\u901a\u8fc7\u5206\u5c42\u76d1\u7763\u5f15\u5bfc\u6a21\u578b\u5b8c\u6210\u4e09\u4e2a\u9636\u6bb5\u7684\u96c6\u4e2d\u7f29\u7a84\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u786e\u4fdd\u6a21\u578b\u53d1\u5c55\u51fa\u5408\u7406\u7684\u63a8\u7406\u8def\u5f84\uff0c\u5e76\u53d6\u5f97\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "HCNQA\u901a\u8fc7\u5206\u5c42\u76d1\u7763\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u63d0\u5347\u4e863D VQA\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2507.01740", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.01740", "abs": "https://arxiv.org/abs/2507.01740", "authors": ["Trung-Dung Hoang", "Alceu Bissoto", "Vihangkumar V. Naik", "Tim Fl\u00fchmann", "Artemii Shlychkov", "Jos\u00e9 Garcia-Tirado", "Lisa M. Koch"], "title": "A Real-Time Digital Twin for Type 1 Diabetes using Simulation-Based Inference", "comment": null, "summary": "Accurately estimating parameters of physiological models is essential to\nachieving reliable digital twins. For Type 1 Diabetes, this is particularly\nchallenging due to the complexity of glucose-insulin interactions. Traditional\nmethods based on Markov Chain Monte Carlo struggle with high-dimensional\nparameter spaces and fit parameters from scratch at inference time, making them\nslow and computationally expensive. In this study, we propose a\nSimulation-Based Inference approach based on Neural Posterior Estimation to\nefficiently capture the complex relationships between meal intake, insulin, and\nglucose level, providing faster, amortized inference. Our experiments\ndemonstrate that SBI not only outperforms traditional methods in parameter\nestimation but also generalizes better to unseen conditions, offering real-time\nposterior inference with reliable uncertainty quantification.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u540e\u9a8c\u4f30\u8ba1\u7684\u4eff\u771f\u63a8\u7406\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u4f30\u8ba11\u578b\u7cd6\u5c3f\u75c5\u7684\u751f\u7406\u6a21\u578b\u53c2\u6570\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u9ad8\u7ef4\u53c2\u6570\u7a7a\u95f4\u4e2d\u6548\u7387\u4f4e\u4e0b\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u8981\u66f4\u5feb\u901f\u3001\u66f4\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u4f30\u8ba1\u8461\u8404\u7cd6-\u80f0\u5c9b\u7d20\u76f8\u4e92\u4f5c\u7528\u7684\u590d\u6742\u5173\u7cfb\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u795e\u7ecf\u540e\u9a8c\u4f30\u8ba1\u7684\u4eff\u771f\u63a8\u7406\u65b9\u6cd5\uff08SBI\uff09\uff0c\u6355\u6349\u9910\u98df\u6444\u5165\u3001\u80f0\u5c9b\u7d20\u548c\u8840\u7cd6\u6c34\u5e73\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\u3002", "result": "SBI\u5728\u53c2\u6570\u4f30\u8ba1\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e14\u80fd\u66f4\u597d\u5730\u6cdb\u5316\u5230\u672a\u89c1\u6761\u4ef6\uff0c\u63d0\u4f9b\u5b9e\u65f6\u540e\u9a8c\u63a8\u65ad\u548c\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "conclusion": "SBI\u4e3a1\u578b\u7cd6\u5c3f\u75c5\u7684\u751f\u7406\u6a21\u578b\u53c2\u6570\u4f30\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u9760\u4e14\u8ba1\u7b97\u6210\u672c\u4f4e\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.01801", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01801", "abs": "https://arxiv.org/abs/2507.01801", "authors": ["Bin Rao", "Haicheng Liao", "Yanchen Guan", "Chengyue Wang", "Bonan Wang", "Jiaxun Zhang", "Zhenning Li"], "title": "AMD: Adaptive Momentum and Decoupled Contrastive Learning Framework for Robust Long-Tail Trajectory Prediction", "comment": null, "summary": "Accurately predicting the future trajectories of traffic agents is essential\nin autonomous driving. However, due to the inherent imbalance in trajectory\ndistributions, tail data in natural datasets often represents more complex and\nhazardous scenarios. Existing studies typically rely solely on a base model's\nprediction error, without considering the diversity and uncertainty of\nlong-tail trajectory patterns. We propose an adaptive momentum and decoupled\ncontrastive learning framework (AMD), which integrates unsupervised and\nsupervised contrastive learning strategies. By leveraging an improved momentum\ncontrast learning (MoCo-DT) and decoupled contrastive learning (DCL) module,\nour framework enhances the model's ability to recognize rare and complex\ntrajectories. Additionally, we design four types of trajectory random\naugmentation methods and introduce an online iterative clustering strategy,\nallowing the model to dynamically update pseudo-labels and better adapt to the\ndistributional shifts in long-tail data. We propose three different criteria to\ndefine long-tail trajectories and conduct extensive comparative experiments on\nthe nuScenes and ETH$/$UCY datasets. The results show that AMD not only\nachieves optimal performance in long-tail trajectory prediction but also\ndemonstrates outstanding overall prediction accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u52a8\u91cf\u548c\u89e3\u8026\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff08AMD\uff09\uff0c\u7528\u4e8e\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u4e2d\u957f\u5c3e\u8f68\u8ff9\u9884\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u901a\u5e38\u4ec5\u4f9d\u8d56\u57fa\u7840\u6a21\u578b\u7684\u9884\u6d4b\u8bef\u5dee\uff0c\u5ffd\u89c6\u4e86\u957f\u5c3e\u8f68\u8ff9\u6a21\u5f0f\u7684\u591a\u6837\u6027\u548c\u4e0d\u786e\u5b9a\u6027\uff0c\u5bfc\u81f4\u5bf9\u590d\u6742\u548c\u5371\u9669\u573a\u666f\u7684\u9884\u6d4b\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u6539\u8fdb\u7684\u52a8\u91cf\u5bf9\u6bd4\u5b66\u4e60\uff08MoCo-DT\uff09\u548c\u89e3\u8026\u5bf9\u6bd4\u5b66\u4e60\uff08DCL\uff09\u6a21\u5757\uff0c\u8bbe\u8ba1\u56db\u79cd\u8f68\u8ff9\u968f\u673a\u589e\u5f3a\u65b9\u6cd5\u548c\u5728\u7ebf\u8fed\u4ee3\u805a\u7c7b\u7b56\u7565\u3002", "result": "\u5728nuScenes\u548cETH/UCY\u6570\u636e\u96c6\u4e0a\uff0cAMD\u5728\u957f\u5c3e\u8f68\u8ff9\u9884\u6d4b\u548c\u6574\u4f53\u9884\u6d4b\u7cbe\u5ea6\u4e0a\u5747\u8868\u73b0\u6700\u4f18\u3002", "conclusion": "AMD\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u957f\u5c3e\u8f68\u8ff9\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u5e76\u9002\u5e94\u6570\u636e\u5206\u5e03\u53d8\u5316\u3002"}}
{"id": "2507.01835", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01835", "abs": "https://arxiv.org/abs/2507.01835", "authors": ["Daniil Reutsky", "Daniil Vladimirov", "Yasin Mamedov", "Georgy Perevozchikov", "Nancy Mehta", "Egor Ershov", "Radu Timofte"], "title": "Modulate and Reconstruct: Learning Hyperspectral Imaging from Misaligned Smartphone Views", "comment": null, "summary": "Hyperspectral reconstruction (HSR) from RGB images is a fundamentally\nill-posed problem due to severe spectral information loss. Existing approaches\ntypically rely on a single RGB image, limiting reconstruction accuracy. In this\nwork, we propose a novel multi-image-to-hyperspectral reconstruction (MI-HSR)\nframework that leverages a triple-camera smartphone system, where two lenses\nare equipped with carefully selected spectral filters. Our configuration,\ngrounded in theoretical and empirical analysis, enables richer and more diverse\nspectral observations than conventional single-camera setups. To support this\nnew paradigm, we introduce Doomer, the first dataset for MI-HSR, comprising\naligned images from three smartphone cameras and a hyperspectral reference\ncamera across diverse scenes. We show that the proposed HSR model achieves\nconsistent improvements over existing methods on the newly proposed benchmark.\nIn a nutshell, our setup allows 30% towards more accurately estimated spectra\ncompared to an ordinary RGB camera. Our findings suggest that multi-view\nspectral filtering with commodity hardware can unlock more accurate and\npractical hyperspectral imaging solutions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u56fe\u50cf\u7684\u8d85\u5149\u8c31\u91cd\u5efa\uff08MI-HSR\uff09\u6846\u67b6\uff0c\u5229\u7528\u914d\u5907\u5149\u8c31\u6ee4\u955c\u7684\u4e09\u6444\u50cf\u5934\u667a\u80fd\u624b\u673a\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u7cbe\u5ea6\u3002", "motivation": "\u7531\u4e8eRGB\u56fe\u50cf\u4e2d\u5149\u8c31\u4fe1\u606f\u4e25\u91cd\u4e22\u5931\uff0c\u4f20\u7edf\u5355\u56fe\u50cf\u65b9\u6cd5\u91cd\u5efa\u7cbe\u5ea6\u6709\u9650\u3002", "method": "\u63d0\u51faMI-HSR\u6846\u67b6\uff0c\u4f7f\u7528\u4e09\u6444\u50cf\u5934\u7cfb\u7edf\uff08\u4e24\u955c\u5934\u914d\u5907\u5149\u8c31\u6ee4\u955c\uff09\uff0c\u5e76\u5f15\u5165\u9996\u4e2aMI-HSR\u6570\u636e\u96c6Doomer\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5149\u8c31\u4f30\u8ba1\u7cbe\u5ea6\u63d0\u534730%\u3002", "conclusion": "\u591a\u89c6\u89d2\u5149\u8c31\u6ee4\u955c\u7ed3\u5408\u6d88\u8d39\u7ea7\u786c\u4ef6\u53ef\u5b9e\u73b0\u66f4\u51c6\u786e\u3001\u5b9e\u7528\u7684\u8d85\u5149\u8c31\u6210\u50cf\u3002"}}
{"id": "2507.01924", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01924", "abs": "https://arxiv.org/abs/2507.01924", "authors": ["Samirah Bakker", "Yao Ma", "Seyed Sahand Mohammadi Ziabari"], "title": "Exploring a Hybrid Deep Learning Approach for Anomaly Detection in Mental Healthcare Provider Billing: Addressing Label Scarcity through Semi-Supervised Anomaly Detection", "comment": null, "summary": "The complexity of mental healthcare billing enables anomalies, including\nfraud. While machine learning methods have been applied to anomaly detection,\nthey often struggle with class imbalance, label scarcity, and complex\nsequential patterns. This study explores a hybrid deep learning approach\ncombining Long Short-Term Memory (LSTM) networks and Transformers, with\npseudo-labeling via Isolation Forests (iForest) and Autoencoders (AE). Prior\nwork has not evaluated such hybrid models trained on pseudo-labeled data in the\ncontext of healthcare billing. The approach is evaluated on two real-world\nbilling datasets related to mental healthcare. The iForest LSTM baseline\nachieves the highest recall (0.963) on declaration-level data. On the\noperation-level data, the hybrid iForest-based model achieves the highest\nrecall (0.744), though at the cost of lower precision. These findings highlight\nthe potential of combining pseudo-labeling with hybrid deep learning in\ncomplex, imbalanced anomaly detection settings.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LSTM\u548cTransformer\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5229\u7528iForest\u548cAE\u8fdb\u884c\u4f2a\u6807\u8bb0\uff0c\u7528\u4e8e\u5fc3\u7406\u5065\u5eb7\u8d26\u5355\u5f02\u5e38\u68c0\u6d4b\uff0c\u89e3\u51b3\u4e86\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u6807\u7b7e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u5fc3\u7406\u5065\u5eb7\u8d26\u5355\u7684\u590d\u6742\u6027\u5bfc\u81f4\u5f02\u5e38\uff08\u5982\u6b3a\u8bc8\uff09\u9891\u53d1\uff0c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u7c7b\u522b\u4e0d\u5e73\u8861\u3001\u6807\u7b7e\u7a00\u7f3a\u548c\u590d\u6742\u5e8f\u5217\u6a21\u5f0f\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408LSTM\u548cTransformer\uff0c\u5e76\u901a\u8fc7iForest\u548cAE\u751f\u6210\u4f2a\u6807\u7b7e\u3002\u5728\u4e24\u4e2a\u771f\u5b9e\u5fc3\u7406\u5065\u5eb7\u8d26\u5355\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u3002", "result": "\u5728\u58f0\u660e\u7ea7\u6570\u636e\u4e0a\uff0ciForest LSTM\u57fa\u7ebf\u6a21\u578b\u53ec\u56de\u7387\u6700\u9ad8\uff080.963\uff09\uff1b\u5728\u64cd\u4f5c\u7ea7\u6570\u636e\u4e0a\uff0c\u57fa\u4e8eiForest\u7684\u6df7\u5408\u6a21\u578b\u53ec\u56de\u7387\u6700\u9ad8\uff080.744\uff09\uff0c\u4f46\u7cbe\u5ea6\u8f83\u4f4e\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u4f2a\u6807\u8bb0\u4e0e\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u7ed3\u5408\u5728\u590d\u6742\u3001\u4e0d\u5e73\u8861\u7684\u5f02\u5e38\u68c0\u6d4b\u573a\u666f\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2507.01838", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01838", "abs": "https://arxiv.org/abs/2507.01838", "authors": ["Hailong Yan", "Ao Li", "Xiangtao Zhang", "Zhe Liu", "Zenglin Shi", "Ce Zhu", "Le Zhang"], "title": "MobileIE: An Extremely Lightweight and Effective ConvNet for Real-Time Image Enhancement on Mobile Devices", "comment": "Accepted by ICCV 2025", "summary": "Recent advancements in deep neural networks have driven significant progress\nin image enhancement (IE). However, deploying deep learning models on\nresource-constrained platforms, such as mobile devices, remains challenging due\nto high computation and memory demands. To address these challenges and\nfacilitate real-time IE on mobile, we introduce an extremely lightweight\nConvolutional Neural Network (CNN) framework with around 4K parameters. Our\napproach integrates reparameterization with an Incremental Weight Optimization\nstrategy to ensure efficiency. Additionally, we enhance performance with a\nFeature Self-Transform module and a Hierarchical Dual-Path Attention mechanism,\noptimized with a Local Variance-Weighted loss. With this efficient framework,\nwe are the first to achieve real-time IE inference at up to 1,100 frames per\nsecond (FPS) while delivering competitive image quality, achieving the best\ntrade-off between speed and performance across multiple IE tasks. The code will\nbe available at https://github.com/AVC2-UESTC/MobileIE.git.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8d85\u8f7b\u91cf\u7ea7CNN\u6846\u67b6\uff0c\u4ec5\u7ea64K\u53c2\u6570\uff0c\u901a\u8fc7\u91cd\u53c2\u6570\u5316\u548c\u589e\u91cf\u6743\u91cd\u4f18\u5316\u7b56\u7565\u5b9e\u73b0\u9ad8\u6548\u6027\uff0c\u652f\u6301\u79fb\u52a8\u8bbe\u5907\u4e0a\u5b9e\u65f6\u56fe\u50cf\u589e\u5f3a\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u90e8\u7f72\u7684\u9ad8\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u95ee\u9898\uff0c\u5b9e\u73b0\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u56fe\u50cf\u589e\u5f3a\u3002", "method": "\u7ed3\u5408\u91cd\u53c2\u6570\u5316\u548c\u589e\u91cf\u6743\u91cd\u4f18\u5316\u7b56\u7565\uff0c\u5f15\u5165\u7279\u5f81\u81ea\u53d8\u6362\u6a21\u5757\u548c\u5206\u5c42\u53cc\u8def\u5f84\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u4f7f\u7528\u5c40\u90e8\u65b9\u5dee\u52a0\u6743\u635f\u5931\u4f18\u5316\u3002", "result": "\u9996\u6b21\u5b9e\u73b0\u9ad8\u8fbe1,100 FPS\u7684\u5b9e\u65f6\u56fe\u50cf\u589e\u5f3a\u63a8\u7406\uff0c\u540c\u65f6\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8fbe\u5230\u901f\u5ea6\u548c\u6027\u80fd\u7684\u6700\u4f73\u5e73\u8861\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5b9e\u65f6\u56fe\u50cf\u589e\u5f3a\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01882", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01882", "abs": "https://arxiv.org/abs/2507.01882", "authors": ["Guiqiu Liao", "Matjaz Jogan", "Marcel Hussing", "Edward Zhang", "Eric Eaton", "Daniel A. Hashimoto"], "title": "Future Slot Prediction for Unsupervised Object Discovery in Surgical Video", "comment": "Accepted by MICCAI2025", "summary": "Object-centric slot attention is an emerging paradigm for unsupervised\nlearning of structured, interpretable object-centric representations (slots).\nThis enables effective reasoning about objects and events at a low\ncomputational cost and is thus applicable to critical healthcare applications,\nsuch as real-time interpretation of surgical video. The heterogeneous scenes in\nreal-world applications like surgery are, however, difficult to parse into a\nmeaningful set of slots. Current approaches with an adaptive slot count perform\nwell on images, but their performance on surgical videos is low. To address\nthis challenge, we propose a dynamic temporal slot transformer (DTST) module\nthat is trained both for temporal reasoning and for predicting the optimal\nfuture slot initialization. The model achieves state-of-the-art performance on\nmultiple surgical databases, demonstrating that unsupervised object-centric\nmethods can be applied to real-world data and become part of the common arsenal\nin healthcare applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u65f6\u5e8f\u69fd\u53d8\u6362\u5668\uff08DTST\uff09\u6a21\u5757\uff0c\u7528\u4e8e\u89e3\u51b3\u624b\u672f\u89c6\u9891\u4e2d\u5bf9\u8c61\u4e2d\u5fc3\u8868\u793a\u5b66\u4e60\u7684\u6311\u6218\uff0c\u5e76\u5728\u591a\u4e2a\u624b\u672f\u6570\u636e\u5e93\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5f02\u6784\u573a\u666f\uff08\u5982\u624b\u672f\u89c6\u9891\uff09\u96be\u4ee5\u89e3\u6790\u4e3a\u6709\u610f\u4e49\u7684\u4e00\u7ec4\u69fd\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u624b\u672f\u89c6\u9891\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u52a8\u6001\u65f6\u5e8f\u69fd\u53d8\u6362\u5668\uff08DTST\uff09\u6a21\u5757\uff0c\u7ed3\u5408\u65f6\u5e8f\u63a8\u7406\u548c\u9884\u6d4b\u672a\u6765\u6700\u4f18\u69fd\u521d\u59cb\u5316\u3002", "result": "\u6a21\u578b\u5728\u591a\u4e2a\u624b\u672f\u6570\u636e\u5e93\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u65e0\u76d1\u7763\u5bf9\u8c61\u4e2d\u5fc3\u65b9\u6cd5\u53ef\u5e94\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u6570\u636e\uff0c\u5e76\u6210\u4e3a\u533b\u7597\u5e94\u7528\u4e2d\u7684\u5e38\u89c1\u5de5\u5177\u3002"}}
{"id": "2507.01955", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01955", "abs": "https://arxiv.org/abs/2507.01955", "authors": ["Rahul Ramachandran", "Ali Garjani", "Roman Bachmann", "Andrei Atanov", "O\u011fuzhan Fatih Kar", "Amir Zamir"], "title": "How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation Models on Standard Computer Vision Tasks", "comment": "Project page at https://fm-vision-evals.epfl.ch/", "summary": "Multimodal foundation models, such as GPT-4o, have recently made remarkable\nprogress, but it is not clear where exactly these models stand in terms of\nunderstanding vision. In this paper, we benchmark the performance of popular\nmultimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0\nFlash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision\ntasks (semantic segmentation, object detection, image classification, depth and\nsurface normal prediction) using established datasets (e.g., COCO, ImageNet and\nits variants, etc).\n  The main challenges to performing this are: 1) most models are trained to\noutput text and cannot natively express versatile domains, such as segments or\n3D geometry, and 2) many leading models are proprietary and accessible only at\nan API level, i.e., there is no weight access to adapt them. We address these\nchallenges by translating standard vision tasks into equivalent text-promptable\nand API-compatible tasks via prompt chaining to create a standardized\nbenchmarking framework.\n  We observe that 1) the models are not close to the state-of-the-art\nspecialist models at any task. However, 2) they are respectable generalists;\nthis is remarkable as they are presumably trained on primarily image-text-based\ntasks. 3) They perform semantic tasks notably better than geometric ones. 4)\nWhile the prompt-chaining techniques affect performance, better models exhibit\nless sensitivity to prompt variations. 5) GPT-4o performs the best among\nnon-reasoning models, securing the top position in 4 out of 6 tasks, 6)\nreasoning models, e.g. o3, show improvements in geometric tasks, and 7) a\npreliminary analysis of models with native image generation, like the latest\nGPT-4o, shows they exhibit quirks like hallucinations and spatial\nmisalignments.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5728\u6807\u51c6\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5b83\u4eec\u867d\u4e0d\u53ca\u4e13\u4e1a\u6a21\u578b\uff0c\u4f46\u4f5c\u4e3a\u901a\u7528\u6a21\u578b\u8868\u73b0\u5c1a\u53ef\uff0c\u4e14\u5728\u8bed\u4e49\u4efb\u52a1\u4e0a\u4f18\u4e8e\u51e0\u4f55\u4efb\u52a1\u3002", "motivation": "\u7814\u7a76\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5728\u89c6\u89c9\u7406\u89e3\u65b9\u9762\u7684\u5b9e\u9645\u80fd\u529b\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u63d0\u793a\u94fe\u5c06\u6807\u51c6\u89c6\u89c9\u4efb\u52a1\u8f6c\u5316\u4e3a\u6587\u672c\u53ef\u63d0\u793a\u548cAPI\u517c\u5bb9\u7684\u4efb\u52a1\uff0c\u5efa\u7acb\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u6a21\u578b\u5728\u8bed\u4e49\u4efb\u52a1\u4e0a\u8868\u73b0\u8f83\u597d\uff0c\u4f46\u4e0d\u53ca\u4e13\u4e1a\u6a21\u578b\uff1bGPT-4o\u5728\u975e\u63a8\u7406\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u5c24\u5176\u662f\u5728\u51e0\u4f55\u4efb\u52a1\u548c\u63d0\u793a\u654f\u611f\u6027\u65b9\u9762\u3002"}}
{"id": "2507.01803", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01803", "abs": "https://arxiv.org/abs/2507.01803", "authors": ["Leyang Xue", "Meghana Madhyastha", "Randal Burns", "Myungjin Lee", "Mahesh K. Marina"], "title": "Towards Decentralized and Sustainable Foundation Model Training with the Edge", "comment": null, "summary": "Foundation models are at the forefront of AI research, appealing for their\nability to learn from vast datasets and cater to diverse tasks. Yet, their\nsignificant computational demands raise issues of environmental impact and the\nrisk of centralized control in their development. We put forward a vision\ntowards decentralized and sustainable foundation model training that leverages\nthe collective compute of sparingly used connected edge AI devices. We present\nthe rationale behind our vision, particularly in support of its sustainability\nbenefit. We further outline a set of challenges that need to be addressed to\nturn this vision into reality.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u3001\u53ef\u6301\u7eed\u7684\u57fa\u7840\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5229\u7528\u8fb9\u7f18AI\u8bbe\u5907\u7684\u95f2\u7f6e\u8ba1\u7b97\u8d44\u6e90\uff0c\u4ee5\u51cf\u5c11\u73af\u5883\u5f71\u54cd\u548c\u96c6\u4e2d\u63a7\u5236\u98ce\u9669\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u7684\u8ba1\u7b97\u9700\u6c42\u9ad8\uff0c\u5bfc\u81f4\u73af\u5883\u95ee\u9898\u548c\u96c6\u4e2d\u63a7\u5236\u98ce\u9669\uff0c\u9700\u8981\u66f4\u53ef\u6301\u7eed\u548c\u5206\u5e03\u5f0f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528\u8fde\u63a5\u8fb9\u7f18AI\u8bbe\u5907\u7684\u95f2\u7f6e\u8ba1\u7b97\u8d44\u6e90\uff0c\u8fdb\u884c\u53bb\u4e2d\u5fc3\u5316\u7684\u57fa\u7840\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u63d0\u51fa\u4e86\u5b9e\u73b0\u8fd9\u4e00\u613f\u666f\u7684\u6311\u6218\u548c\u53ef\u6301\u7eed\u6027\u4f18\u52bf\u3002", "conclusion": "\u53bb\u4e2d\u5fc3\u5316\u548c\u53ef\u6301\u7eed\u7684\u57fa\u7840\u6a21\u578b\u8bad\u7ec3\u662f\u53ef\u884c\u7684\uff0c\u4f46\u9700\u89e3\u51b3\u4e00\u7cfb\u5217\u6280\u672f\u6311\u6218\u3002"}}
{"id": "2507.01884", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01884", "abs": "https://arxiv.org/abs/2507.01884", "authors": ["Kunlun Xu", "Fan Zhuo", "Jiangmeng Li", "Xu Zou", "Jiahuan Zhou"], "title": "Self-Reinforcing Prototype Evolution with Dual-Knowledge Cooperation for Semi-Supervised Lifelong Person Re-Identification", "comment": "Accepted by ICCV 2025", "summary": "Current lifelong person re-identification (LReID) methods predominantly rely\non fully labeled data streams. However, in real-world scenarios where\nannotation resources are limited, a vast amount of unlabeled data coexists with\nscarce labeled samples, leading to the Semi-Supervised LReID (Semi-LReID)\nproblem where LReID methods suffer severe performance degradation. Existing\nLReID methods, even when combined with semi-supervised strategies, suffer from\nlimited long-term adaptation performance due to struggling with the noisy\nknowledge occurring during unlabeled data utilization. In this paper, we\npioneer the investigation of Semi-LReID, introducing a novel Self-Reinforcing\nPrototype Evolution with Dual-Knowledge Cooperation framework (SPRED). Our key\ninnovation lies in establishing a self-reinforcing cycle between dynamic\nprototype-guided pseudo-label generation and new-old knowledge collaborative\npurification to enhance the utilization of unlabeled data. Specifically,\nlearnable identity prototypes are introduced to dynamically capture the\nidentity distributions and generate high-quality pseudo-labels. Then, the\ndual-knowledge cooperation scheme integrates current model specialization and\nhistorical model generalization, refining noisy pseudo-labels. Through this\ncyclic design, reliable pseudo-labels are progressively mined to improve\ncurrent-stage learning and ensure positive knowledge propagation over long-term\nlearning. Experiments on the established Semi-LReID benchmarks show that our\nSPRED achieves state-of-the-art performance. Our source code is available at\nhttps://github.com/zhoujiahuan1991/ICCV2025-SPRED", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSPRED\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u534a\u76d1\u7763\u7ec8\u8eab\u884c\u4eba\u91cd\u8bc6\u522b\uff08Semi-LReID\uff09\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u539f\u578b\u548c\u53cc\u77e5\u8bc6\u534f\u4f5c\u63d0\u5347\u672a\u6807\u8bb0\u6570\u636e\u7684\u5229\u7528\u6548\u679c\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u6807\u6ce8\u8d44\u6e90\u6709\u9650\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u672a\u6807\u8bb0\u6570\u636e\u5229\u7528\u65f6\u6027\u80fd\u4e0b\u964d\u4e25\u91cd\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u957f\u671f\u9002\u5e94\u566a\u58f0\u6570\u636e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u8eab\u4efd\u539f\u578b\u52a8\u6001\u751f\u6210\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\uff0c\u5e76\u901a\u8fc7\u53cc\u77e5\u8bc6\u534f\u4f5c\uff08\u5f53\u524d\u6a21\u578b\u548c\u5386\u53f2\u6a21\u578b\uff09\u51c0\u5316\u566a\u58f0\u4f2a\u6807\u7b7e\u3002", "result": "\u5728Semi-LReID\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSPRED\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "SPRED\u901a\u8fc7\u81ea\u589e\u5f3a\u5faa\u73af\u8bbe\u8ba1\uff0c\u6709\u6548\u63d0\u5347\u4e86\u672a\u6807\u8bb0\u6570\u636e\u7684\u5229\u7528\u548c\u957f\u671f\u5b66\u4e60\u6027\u80fd\u3002"}}
{"id": "2507.01957", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01957", "abs": "https://arxiv.org/abs/2507.01957", "authors": ["Zhuoyang Zhang", "Luke J. Huang", "Chengyue Wu", "Shang Yang", "Kelly Peng", "Yao Lu", "Song Han"], "title": "Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation", "comment": "The first two authors contributed equally to this work", "summary": "We present Locality-aware Parallel Decoding (LPD) to accelerate\nautoregressive image generation. Traditional autoregressive image generation\nrelies on next-patch prediction, a memory-bound process that leads to high\nlatency. Existing works have tried to parallelize next-patch prediction by\nshifting to multi-patch prediction to accelerate the process, but only achieved\nlimited parallelization. To achieve high parallelization while maintaining\ngeneration quality, we introduce two key techniques: (1) Flexible Parallelized\nAutoregressive Modeling, a novel architecture that enables arbitrary generation\nordering and degrees of parallelization. It uses learnable position query\ntokens to guide generation at target positions while ensuring mutual visibility\namong concurrently generated tokens for consistent parallel decoding. (2)\nLocality-aware Generation Ordering, a novel schedule that forms groups to\nminimize intra-group dependencies and maximize contextual support, enhancing\ngeneration quality. With these designs, we reduce the generation steps from 256\nto 20 (256$\\times$256 res.) and 1024 to 48 (512$\\times$512 res.) without\ncompromising quality on the ImageNet class-conditional generation, and\nachieving at least 3.4$\\times$ lower latency than previous parallelized\nautoregressive models.", "AI": {"tldr": "Locality-aware Parallel Decoding (LPD) \u901a\u8fc7\u7075\u6d3b\u7684\u81ea\u56de\u5f52\u5efa\u6a21\u548c\u5c40\u90e8\u611f\u77e5\u751f\u6210\u987a\u5e8f\uff0c\u663e\u8457\u52a0\u901f\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\uff0c\u51cf\u5c11\u751f\u6210\u6b65\u9aa4\u5e76\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u4f20\u7edf\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u4f9d\u8d56\u987a\u5e8f\u9884\u6d4b\uff0c\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\u3002\u73b0\u6709\u65b9\u6cd5\u5c1d\u8bd5\u591a\u5757\u9884\u6d4b\u5e76\u884c\u5316\uff0c\u4f46\u6548\u679c\u6709\u9650\u3002LPD\u65e8\u5728\u5b9e\u73b0\u9ad8\u5e76\u884c\u5316\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "method": "1. \u7075\u6d3b\u5e76\u884c\u81ea\u56de\u5f52\u5efa\u6a21\uff1a\u652f\u6301\u4efb\u610f\u751f\u6210\u987a\u5e8f\u548c\u5e76\u884c\u5ea6\uff0c\u5229\u7528\u53ef\u5b66\u4e60\u4f4d\u7f6e\u67e5\u8be2\u4ee4\u724c\u786e\u4fdd\u5e76\u884c\u89e3\u7801\u4e00\u81f4\u6027\u30022. \u5c40\u90e8\u611f\u77e5\u751f\u6210\u987a\u5e8f\uff1a\u901a\u8fc7\u5206\u7ec4\u6700\u5c0f\u5316\u7ec4\u5185\u4f9d\u8d56\uff0c\u6700\u5927\u5316\u4e0a\u4e0b\u6587\u652f\u6301\u3002", "result": "\u5728ImageNet\u7c7b\u6761\u4ef6\u751f\u6210\u4e2d\uff0c\u5c06\u751f\u6210\u6b65\u9aa4\u4ece256\u964d\u81f320\uff08256\u00d7256\u5206\u8fa8\u7387\uff09\u548c1024\u964d\u81f348\uff08512\u00d7512\u5206\u8fa8\u7387\uff09\uff0c\u5ef6\u8fdf\u964d\u4f4e\u81f3\u5c113.4\u500d\u3002", "conclusion": "LPD\u901a\u8fc7\u65b0\u9896\u7684\u67b6\u6784\u548c\u8c03\u5ea6\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u7684\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2507.01908", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01908", "abs": "https://arxiv.org/abs/2507.01908", "authors": ["Qingdong He", "Xueqin Chen", "Chaoyi Wang", "Yanjie Pan", "Xiaobin Hu", "Zhenye Gan", "Yabiao Wang", "Chengjie Wang", "Xiangtai Li", "Jiangning Zhang"], "title": "Reasoning to Edit: Hypothetical Instruction-Based Image Editing with Visual Reasoning", "comment": null, "summary": "Instruction-based image editing (IIE) has advanced rapidly with the success\nof diffusion models. However, existing efforts primarily focus on simple and\nexplicit instructions to execute editing operations such as adding, deleting,\nmoving, or swapping objects. They struggle to handle more complex implicit\nhypothetical instructions that require deeper reasoning to infer plausible\nvisual changes and user intent. Additionally, current datasets provide limited\nsupport for training and evaluating reasoning-aware editing capabilities.\nArchitecturally, these methods also lack mechanisms for fine-grained detail\nextraction that support such reasoning. To address these limitations, we\npropose Reason50K, a large-scale dataset specifically curated for training and\nevaluating hypothetical instruction reasoning image editing, along with\nReasonBrain, a novel framework designed to reason over and execute implicit\nhypothetical instructions across diverse scenarios. Reason50K includes over 50K\nsamples spanning four key reasoning scenarios: Physical, Temporal, Causal, and\nStory reasoning. ReasonBrain leverages Multimodal Large Language Models (MLLMs)\nfor editing guidance generation and a diffusion model for image synthesis,\nincorporating a Fine-grained Reasoning Cue Extraction (FRCE) module to capture\ndetailed visual and textual semantics essential for supporting instruction\nreasoning. To mitigate the semantic loss, we further introduce a Cross-Modal\nEnhancer (CME) that enables rich interactions between the fine-grained cues and\nMLLM-derived features. Extensive experiments demonstrate that ReasonBrain\nconsistently outperforms state-of-the-art baselines on reasoning scenarios\nwhile exhibiting strong zero-shot generalization to conventional IIE tasks. Our\ndataset and code will be released publicly.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faReason50K\u6570\u636e\u96c6\u548cReasonBrain\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u590d\u6742\u5047\u8bbe\u6027\u6307\u4ee4\u7684\u56fe\u50cf\u7f16\u8f91\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u7ec6\u7c92\u5ea6\u63a8\u7406\u7ebf\u7d22\u63d0\u53d6\u6a21\u5757\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u9700\u8981\u6df1\u5ea6\u63a8\u7406\u7684\u590d\u6742\u5047\u8bbe\u6027\u6307\u4ee4\uff0c\u4e14\u7f3a\u4e4f\u652f\u6301\u8bad\u7ec3\u548c\u8bc4\u4f30\u7684\u6570\u636e\u96c6\u548c\u67b6\u6784\u3002", "method": "\u63d0\u51faReason50K\u6570\u636e\u96c6\u548cReasonBrain\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u7ec6\u7c92\u5ea6\u63a8\u7406\u7ebf\u7d22\u63d0\u53d6\u6a21\u5757\u3002", "result": "ReasonBrain\u5728\u63a8\u7406\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5177\u5907\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Reason50K\u548cReasonBrain\u4e3a\u590d\u6742\u6307\u4ee4\u7684\u56fe\u50cf\u7f16\u8f91\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01823", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01823", "abs": "https://arxiv.org/abs/2507.01823", "authors": ["Dmytro Kuzmenko", "Nadiya Shvai"], "title": "TD-MPC-Opt: Distilling Model-Based Multi-Task Reinforcement Learning Agents", "comment": "Preprint of a manuscript submitted for peer review", "summary": "We present a novel approach to knowledge transfer in model-based\nreinforcement learning, addressing the critical challenge of deploying large\nworld models in resource-constrained environments. Our method efficiently\ndistills a high-capacity multi-task agent (317M parameters) into a compact\nmodel (1M parameters) on the MT30 benchmark, significantly improving\nperformance across diverse tasks. Our distilled model achieves a\nstate-of-the-art normalized score of 28.45, surpassing the original 1M\nparameter model score of 18.93. This improvement demonstrates the ability of\nour distillation technique to capture and consolidate complex multi-task\nknowledge. We further optimize the distilled model through FP16 post-training\nquantization, reducing its size by $\\sim$50\\%. Our approach addresses practical\ndeployment limitations and offers insights into knowledge representation in\nlarge world models, paving the way for more efficient and accessible multi-task\nreinforcement learning systems in robotics and other resource-constrained\napplications. Code available at https://github.com/dmytro-kuzmenko/td-mpc-opt.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u77e5\u8bc6\u8fc1\u79fb\u65b9\u6cd5\uff0c\u7528\u4e8e\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u5927\u578b\u4e16\u754c\u6a21\u578b\u7684\u5173\u952e\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u5927\u578b\u4e16\u754c\u6a21\u578b\u7684\u5b9e\u9645\u95ee\u9898\uff0c\u63d0\u5347\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u7684\u6548\u7387\u548c\u53ef\u8bbf\u95ee\u6027\u3002", "method": "\u901a\u8fc7\u84b8\u998f\u6280\u672f\u5c06\u9ad8\u5bb9\u91cf\u591a\u4efb\u52a1\u4ee3\u7406\uff08317M\u53c2\u6570\uff09\u538b\u7f29\u4e3a\u7d27\u51d1\u6a21\u578b\uff081M\u53c2\u6570\uff09\uff0c\u5e76\u5728MT30\u57fa\u51c6\u4e0a\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u84b8\u998f\u6a21\u578b\u5728MT30\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e8628.45\u7684\u5f52\u4e00\u5316\u5206\u6570\uff0c\u663e\u8457\u4f18\u4e8e\u539f\u59cb1M\u53c2\u6570\u6a21\u578b\u768418.93\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u89e3\u51b3\u4e86\u5b9e\u9645\u90e8\u7f72\u9650\u5236\uff0c\u8fd8\u4e3a\u5927\u578b\u4e16\u754c\u6a21\u578b\u4e2d\u7684\u77e5\u8bc6\u8868\u793a\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u63a8\u52a8\u4e86\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.01909", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01909", "abs": "https://arxiv.org/abs/2507.01909", "authors": ["Jorge Tapias Gomez", "Nishant Nadkarni", "Lando S. Bosma", "Jue Jiang", "Ergys D. Subashi", "William P. Segars", "James M. Balter", "Mert R Sabuncu", "Neelam Tyagi", "Harini Veeraraghavan"], "title": "Modality Agnostic, patient-specific digital twins modeling temporally varying digestive motion", "comment": "7 Pages, 6 figures, 4 tables", "summary": "Objective: Clinical implementation of deformable image registration (DIR)\nrequires voxel-based spatial accuracy metrics such as manually identified\nlandmarks, which are challenging to implement for highly mobile\ngastrointestinal (GI) organs. To address this, patient-specific digital twins\n(DT) modeling temporally varying motion were created to assess the accuracy of\nDIR methods. Approach: 21 motion phases simulating digestive GI motion as 4D\nsequences were generated from static 3D patient scans using published\nanalytical GI motion models through a semi-automated pipeline. Eleven datasets,\nincluding six T2w FSE MRI (T2w MRI), two T1w 4D golden-angle stack-of-stars,\nand three contrast-enhanced CT scans. The motion amplitudes of the DTs were\nassessed against real patient stomach motion amplitudes extracted from\nindependent 4D MRI datasets. The generated DTs were then used to assess six\ndifferent DIR methods using target registration error, Dice similarity\ncoefficient, and the 95th percentile Hausdorff distance using summary metrics\nand voxel-level granular visualizations. Finally, for a subset of T2w MRI scans\nfrom patients treated with MR-guided radiation therapy, dose distributions were\nwarped and accumulated to assess dose warping errors, including evaluations of\nDIR performance in both low- and high-dose regions for patient-specific error\nestimation. Main results: Our proposed pipeline synthesized DTs modeling\nrealistic GI motion, achieving mean and maximum motion amplitudes and a mean\nlog Jacobian determinant within 0.8 mm and 0.01, respectively, similar to\npublished real-patient gastric motion data. It also enables the extraction of\ndetailed quantitative DIR performance metrics and rigorous validation of dose\nmapping accuracy. Significance: The pipeline enables rigorously testing DIR\ntools for dynamic, anatomically complex regions enabling granular spatial and\ndosimetric accuracies.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u5b57\u5b6a\u751f\uff08DT\uff09\u7684\u7ba1\u9053\uff0c\u7528\u4e8e\u8bc4\u4f30\u80c3\u80a0\u9053\uff08GI\uff09\u5668\u5b98\u53d8\u5f62\u56fe\u50cf\u914d\u51c6\uff08DIR\uff09\u7684\u51c6\u786e\u6027\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u6307\u6807\u9a8c\u8bc1\u4e86DIR\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u4e34\u5e8a\u5b9e\u65bdDIR\u9700\u8981\u57fa\u4e8e\u4f53\u7d20\u7684\u7a7a\u95f4\u51c6\u786e\u6027\u6307\u6807\uff0c\u4f46\u624b\u52a8\u6807\u8bb0\u5bf9\u4e8e\u9ad8\u5ea6\u79fb\u52a8\u7684GI\u5668\u5b98\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u901a\u8fc7\u534a\u81ea\u52a8\u5316\u7ba1\u9053\u751f\u621021\u4e2a\u8fd0\u52a8\u9636\u6bb5\u76844D\u5e8f\u5217\uff0c\u6a21\u62dfGI\u8fd0\u52a8\uff0c\u5e76\u4f7f\u7528\u516d\u79cdDIR\u65b9\u6cd5\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "DT\u6a21\u578b\u6210\u529f\u6a21\u62df\u4e86\u771f\u5b9e\u7684GI\u8fd0\u52a8\uff0c\u8fd0\u52a8\u5e45\u5ea6\u4e0e\u771f\u5b9e\u60a3\u8005\u6570\u636e\u76f8\u4f3c\uff0c\u5e76\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684DIR\u6027\u80fd\u6307\u6807\u3002", "conclusion": "\u8be5\u7ba1\u9053\u80fd\u591f\u4e25\u683c\u6d4b\u8bd5DIR\u5de5\u5177\u5728\u52a8\u6001\u590d\u6742\u533a\u57df\u7684\u6027\u80fd\uff0c\u4e3a\u7a7a\u95f4\u548c\u5242\u91cf\u51c6\u786e\u6027\u63d0\u4f9b\u4e86\u8be6\u7ec6\u9a8c\u8bc1\u3002"}}
{"id": "2507.01912", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01912", "abs": "https://arxiv.org/abs/2507.01912", "authors": ["Ranjan Sapkota", "Zhichao Meng", "Martin Churuvija", "Xiaoqiang Du", "Zenghong Ma", "Manoj Karkee"], "title": "3D Reconstruction and Information Fusion between Dormant and Canopy Seasons in Commercial Orchards Using Deep Learning and Fast GICP", "comment": "17 pages, 4 tables, 11 figures", "summary": "In orchard automation, dense foliage during the canopy season severely\noccludes tree structures, minimizing visibility to various canopy parts such as\ntrunks and branches, which limits the ability of a machine vision system.\nHowever, canopy structure is more open and visible during the dormant season\nwhen trees are defoliated. In this work, we present an information fusion\nframework that integrates multi-seasonal structural data to support robotic and\nautomated crop load management during the entire growing season. The framework\ncombines high-resolution RGB-D imagery from both dormant and canopy periods\nusing YOLOv9-Seg for instance segmentation, Kinect Fusion for 3D\nreconstruction, and Fast Generalized Iterative Closest Point (Fast GICP) for\nmodel alignment. Segmentation outputs from YOLOv9-Seg were used to extract\ndepth-informed masks, which enabled accurate 3D point cloud reconstruction via\nKinect Fusion; these reconstructed models from each season were subsequently\naligned using Fast GICP to achieve spatially coherent multi-season fusion. The\nYOLOv9-Seg model, trained on manually annotated images, achieved a mean squared\nerror (MSE) of 0.0047 and segmentation mAP@50 scores up to 0.78 for trunks in\ndormant season dataset. Kinect Fusion enabled accurate reconstruction of tree\ngeometry, validated with field measurements resulting in root mean square\nerrors (RMSE) of 5.23 mm for trunk diameter, 4.50 mm for branch diameter, and\n13.72 mm for branch spacing. Fast GICP achieved precise cross-seasonal\nregistration with a minimum fitness score of 0.00197, allowing integrated,\ncomprehensive tree structure modeling despite heavy occlusions during the\ngrowing season. This fused structural representation enables robotic systems to\naccess otherwise obscured architectural information, improving the precision of\npruning, thinning, and other automated orchard operations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4fe1\u606f\u878d\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u5b63\u8282\u7ed3\u6784\u6570\u636e\uff0c\u652f\u6301\u679c\u56ed\u81ea\u52a8\u5316\u7ba1\u7406\u3002", "motivation": "\u679c\u56ed\u81ea\u52a8\u5316\u4e2d\uff0c\u8302\u5bc6\u6811\u51a0\u906e\u6321\u7ed3\u6784\uff0c\u5f71\u54cd\u673a\u5668\u89c6\u89c9\u7cfb\u7edf\u6027\u80fd\uff0c\u800c\u4f11\u7720\u5b63\u8282\u7ed3\u6784\u66f4\u6e05\u6670\u3002", "method": "\u4f7f\u7528YOLOv9-Seg\u8fdb\u884c\u5b9e\u4f8b\u5206\u5272\uff0cKinect Fusion\u8fdb\u884c3D\u91cd\u5efa\uff0cFast GICP\u8fdb\u884c\u6a21\u578b\u5bf9\u9f50\u3002", "result": "YOLOv9-Seg\u5728\u4f11\u7720\u5b63\u8282\u6570\u636e\u96c6\u7684MSE\u4e3a0.0047\uff0cmAP@50\u4e3a0.78\uff1bKinect Fusion\u91cd\u5efa\u8bef\u5dee\u5c0f\uff1bFast GICP\u914d\u51c6\u7cbe\u5ea6\u9ad8\u3002", "conclusion": "\u591a\u5b63\u8282\u878d\u5408\u6846\u67b6\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u7cfb\u7edf\u5bf9\u6811\u51a0\u7ed3\u6784\u7684\u7406\u89e3\uff0c\u4f18\u5316\u4e86\u679c\u56ed\u81ea\u52a8\u5316\u64cd\u4f5c\u3002"}}
{"id": "2507.01926", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01926", "abs": "https://arxiv.org/abs/2507.01926", "authors": ["Yaowei Li", "Xiaoyu Li", "Zhaoyang Zhang", "Yuxuan Bian", "Gan Liu", "Xinyuan Li", "Jiale Xu", "Wenbo Hu", "Yating Liu", "Lingen Li", "Jing Cai", "Yuexian Zou", "Yancheng He", "Ying Shan"], "title": "IC-Custom: Diverse Image Customization via In-Context Learning", "comment": "Project page: https://liyaowei-stu.github.io/project/IC_Custom", "summary": "Image customization, a crucial technique for industrial media production,\naims to generate content that is consistent with reference images. However,\ncurrent approaches conventionally separate image customization into\nposition-aware and position-free customization paradigms and lack a universal\nframework for diverse customization, limiting their applications across various\nscenarios. To overcome these limitations, we propose IC-Custom, a unified\nframework that seamlessly integrates position-aware and position-free image\ncustomization through in-context learning. IC-Custom concatenates reference\nimages with target images to a polyptych, leveraging DiT's multi-modal\nattention mechanism for fine-grained token-level interactions. We introduce the\nIn-context Multi-Modal Attention (ICMA) mechanism with learnable task-oriented\nregister tokens and boundary-aware positional embeddings to enable the model to\ncorrectly handle different task types and distinguish various inputs in\npolyptych configurations. To bridge the data gap, we carefully curated a\nhigh-quality dataset of 12k identity-consistent samples with 8k from real-world\nsources and 4k from high-quality synthetic data, avoiding the overly glossy and\nover-saturated synthetic appearance. IC-Custom supports various industrial\napplications, including try-on, accessory placement, furniture arrangement, and\ncreative IP customization. Extensive evaluations on our proposed ProductBench\nand the publicly available DreamBench demonstrate that IC-Custom significantly\noutperforms community workflows, closed-source models, and state-of-the-art\nopen-source approaches. IC-Custom achieves approximately 73% higher human\npreference across identity consistency, harmonicity, and text alignment\nmetrics, while training only 0.4% of the original model parameters. Project\npage: https://liyaowei-stu.github.io/project/IC_Custom", "AI": {"tldr": "IC-Custom\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u56fe\u50cf\u5b9a\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u6574\u5408\u4f4d\u7f6e\u611f\u77e5\u548c\u65e0\u4f4d\u7f6e\u5b9a\u5236\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u56fe\u50cf\u5b9a\u5236\u65b9\u6cd5\u7f3a\u4e4f\u901a\u7528\u6846\u67b6\uff0c\u9650\u5236\u4e86\u591a\u6837\u5316\u5e94\u7528\u3002", "method": "\u63d0\u51faIC-Custom\u6846\u67b6\uff0c\u5229\u7528DiT\u7684\u591a\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u548cICMA\u673a\u5236\u5904\u7406\u4e0d\u540c\u4efb\u52a1\u7c7b\u578b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4eba\u7c7b\u504f\u597d\u63d0\u534773%\uff0c\u4ec5\u8bad\u7ec30.4%\u7684\u6a21\u578b\u53c2\u6570\u3002", "conclusion": "IC-Custom\u4e3a\u5de5\u4e1a\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u901a\u7528\u7684\u56fe\u50cf\u5b9a\u5236\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01831", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.01831", "abs": "https://arxiv.org/abs/2507.01831", "authors": ["Yucen Lily Li", "Daohan Lu", "Polina Kirichenko", "Shikai Qiu", "Tim G. J. Rudner", "C. Bayan Bruss", "Andrew Gordon Wilson"], "title": "Out-of-Distribution Detection Methods Answer the Wrong Questions", "comment": "Extended version of ICML 2025 paper", "summary": "To detect distribution shifts and improve model safety, many\nout-of-distribution (OOD) detection methods rely on the predictive uncertainty\nor features of supervised models trained on in-distribution data. In this\npaper, we critically re-examine this popular family of OOD detection\nprocedures, and we argue that these methods are fundamentally answering the\nwrong questions for OOD detection. There is no simple fix to this misalignment,\nsince a classifier trained only on in-distribution classes cannot be expected\nto identify OOD points; for instance, a cat-dog classifier may confidently\nmisclassify an airplane if it contains features that distinguish cats from\ndogs, despite generally appearing nothing alike. We find that uncertainty-based\nmethods incorrectly conflate high uncertainty with being OOD, while\nfeature-based methods incorrectly conflate far feature-space distance with\nbeing OOD. We show how these pathologies manifest as irreducible errors in OOD\ndetection and identify common settings where these methods are ineffective.\nAdditionally, interventions to improve OOD detection such as feature-logit\nhybrid methods, scaling of model and data size, epistemic uncertainty\nrepresentation, and outlier exposure also fail to address this fundamental\nmisalignment in objectives. We additionally consider unsupervised density\nestimation and generative models for OOD detection, which we show have their\nown fundamental limitations.", "AI": {"tldr": "\u8bba\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u57fa\u4e8e\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u548c\u7279\u5f81\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u6307\u51fa\u8fd9\u4e9b\u65b9\u6cd5\u5b58\u5728\u6839\u672c\u6027\u9519\u8bef\uff0c\u65e0\u6cd5\u6709\u6548\u8bc6\u522bOOD\u6570\u636e\u3002", "motivation": "\u5f53\u524dOOD\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u76d1\u7763\u6a21\u578b\u7684\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u6216\u7279\u5f81\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u65e0\u6cd5\u6b63\u786e\u8bc6\u522bOOD\u6570\u636e\uff0c\u56e0\u4e3a\u5206\u7c7b\u5668\u4ec5\u9488\u5bf9\u5206\u5e03\u5185\u6570\u636e\u8bad\u7ec3\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e0d\u786e\u5b9a\u6027\u65b9\u6cd5\u548c\u7279\u5f81\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63a2\u8ba8\u4e86\u5176\u9519\u8bef\u6839\u6e90\uff0c\u5e76\u8bc4\u4f30\u4e86\u73b0\u6709\u6539\u8fdb\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u73b0\u6709\u65b9\u6cd5\u5728OOD\u68c0\u6d4b\u4e2d\u5b58\u5728\u4e0d\u53ef\u51cf\u5c11\u7684\u9519\u8bef\uff0c\u4e14\u5e38\u89c1\u6539\u8fdb\u63aa\u65bd\uff08\u5982\u6df7\u5408\u65b9\u6cd5\u3001\u6a21\u578b\u6269\u5c55\u7b49\uff09\u65e0\u6cd5\u89e3\u51b3\u6839\u672c\u95ee\u9898\u3002", "conclusion": "\u8bba\u6587\u63ed\u793a\u4e86OOD\u68c0\u6d4b\u65b9\u6cd5\u7684\u6839\u672c\u5c40\u9650\u6027\uff0c\u6307\u51fa\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002"}}
{"id": "2507.01927", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01927", "abs": "https://arxiv.org/abs/2507.01927", "authors": ["Zhentan Zheng"], "title": "evMLP: An Efficient Event-Driven MLP Architecture for Vision", "comment": null, "summary": "Deep neural networks have achieved remarkable results in computer vision\ntasks. In the early days, Convolutional Neural Networks (CNNs) were the\nmainstream architecture. In recent years, Vision Transformers (ViTs) have\nbecome increasingly popular. In addition, exploring applications of multi-layer\nperceptrons (MLPs) has provided new perspectives for research into vision model\narchitectures. In this paper, we present evMLP accompanied by a simple\nevent-driven local update mechanism. The proposed evMLP can independently\nprocess patches on images or feature maps via MLPs. We define changes between\nconsecutive frames as \"events\". Under the event-driven local update mechanism,\nevMLP selectively processes patches where events occur. For sequential image\ndata (e.g., video processing), this approach improves computational performance\nby avoiding redundant computations. Through ImageNet image classification\nexperiments, evMLP attains accuracy competitive with state-of-the-art models.\nMore significantly, experimental results on multiple video datasets demonstrate\nthat evMLP reduces computational cost via its event-driven local update\nmechanism while maintaining output consistency with its non-event-driven\nbaseline. The code and trained models are available at\nhttps://github.com/i-evi/evMLP.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86evMLP\uff0c\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u9a71\u52a8\u7684\u5c40\u90e8\u66f4\u65b0\u673a\u5236\u7684\u591a\u5c42\u611f\u77e5\u673a\u67b6\u6784\uff0c\u7528\u4e8e\u9ad8\u6548\u5904\u7406\u56fe\u50cf\u548c\u89c6\u9891\u6570\u636e\u3002", "motivation": "\u63a2\u7d22\u591a\u5c42\u611f\u77e5\u673a\uff08MLPs\uff09\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u901a\u8fc7\u4e8b\u4ef6\u9a71\u52a8\u673a\u5236\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\uff0c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u63d0\u51faevMLP\uff0c\u5229\u7528\u4e8b\u4ef6\u9a71\u52a8\u673a\u5236\u9009\u62e9\u6027\u5904\u7406\u56fe\u50cf\u6216\u7279\u5f81\u56fe\u4e2d\u7684\u53d8\u5316\u533a\u57df\uff08\u4e8b\u4ef6\uff09\uff0c\u907f\u514d\u5bf9\u672a\u53d8\u5316\u533a\u57df\u8fdb\u884c\u5197\u4f59\u8ba1\u7b97\u3002", "result": "\u5728ImageNet\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u5728\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u4fdd\u6301\u8f93\u51fa\u4e00\u81f4\u6027\u3002", "conclusion": "evMLP\u901a\u8fc7\u4e8b\u4ef6\u9a71\u52a8\u673a\u5236\u5b9e\u73b0\u4e86\u9ad8\u6548\u8ba1\u7b97\uff0c\u4e3a\u89c6\u89c9\u6a21\u578b\u67b6\u6784\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2507.01841", "categories": ["cs.LG", "cs.IT", "eess.SP", "math.IT", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.01841", "abs": "https://arxiv.org/abs/2507.01841", "authors": ["Yihang Gao", "Vincent Y. F. Tan"], "title": "Automatic Rank Determination for Low-Rank Adaptation via Submodular Function Maximization", "comment": null, "summary": "In this paper, we propose SubLoRA, a rank determination method for Low-Rank\nAdaptation (LoRA) based on submodular function maximization. In contrast to\nprior approaches, such as AdaLoRA, that rely on first-order (linearized)\napproximations of the loss function, SubLoRA utilizes second-order information\nto capture the potentially complex loss landscape by incorporating the Hessian\nmatrix. We show that the linearization becomes inaccurate and ill-conditioned\nwhen the LoRA parameters have been well optimized, motivating the need for a\nmore reliable and nuanced second-order formulation. To this end, we reformulate\nthe rank determination problem as a combinatorial optimization problem with a\nquadratic objective. However, solving this problem exactly is NP-hard in\ngeneral. To overcome the computational challenge, we introduce a submodular\nfunction maximization framework and devise a greedy algorithm with\napproximation guarantees. We derive a sufficient and necessary condition under\nwhich the rank-determination objective becomes submodular, and construct a\nclosed-form projection of the Hessian matrix that satisfies this condition\nwhile maintaining computational efficiency. Our method combines solid\ntheoretical foundations, second-order accuracy, and practical computational\nefficiency. We further extend SubLoRA to a joint optimization setting,\nalternating between LoRA parameter updates and rank determination under a rank\nbudget constraint. Extensive experiments on fine-tuning physics-informed neural\nnetworks (PINNs) for solving partial differential equations (PDEs) demonstrate\nthe effectiveness of our approach. Results show that SubLoRA outperforms\nexisting methods in both rank determination and joint training performance.", "AI": {"tldr": "SubLoRA\u662f\u4e00\u79cd\u57fa\u4e8e\u5b50\u6a21\u51fd\u6570\u6700\u5927\u5316\u7684\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u79e9\u786e\u5b9a\u65b9\u6cd5\uff0c\u5229\u7528\u4e8c\u9636\u4fe1\u606f\uff08Hessian\u77e9\u9635\uff09\u6539\u8fdb\u73b0\u6709\u7ebf\u6027\u5316\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5e76\u901a\u8fc7\u8d2a\u5fc3\u7b97\u6cd5\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982AdaLoRA\uff09\u4f9d\u8d56\u635f\u5931\u51fd\u6570\u7684\u4e00\u9636\u8fd1\u4f3c\uff0c\u5728LoRA\u53c2\u6570\u4f18\u5316\u826f\u597d\u65f6\u53ef\u80fd\u4e0d\u51c6\u786e\u4e14\u75c5\u6001\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u53ef\u9760\u7684\u4e8c\u9636\u65b9\u6cd5\u3002", "method": "\u5c06\u79e9\u786e\u5b9a\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a\u7ec4\u5408\u4f18\u5316\u95ee\u9898\uff0c\u5f15\u5165\u5b50\u6a21\u51fd\u6570\u6700\u5927\u5316\u6846\u67b6\u548c\u8d2a\u5fc3\u7b97\u6cd5\uff0c\u7ed3\u5408Hessian\u77e9\u9635\u7684\u95ed\u5f0f\u6295\u5f71\uff0c\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u3002", "result": "\u5728\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u7684\u5fae\u8c03\u5b9e\u9a8c\u4e2d\uff0cSubLoRA\u5728\u79e9\u786e\u5b9a\u548c\u8054\u5408\u8bad\u7ec3\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SubLoRA\u7ed3\u5408\u4e86\u7406\u8bba\u57fa\u7840\u3001\u4e8c\u9636\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u4e3aLoRA\u79e9\u786e\u5b9a\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01938", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01938", "abs": "https://arxiv.org/abs/2507.01938", "authors": ["Yiming Ju", "Jijin Hu", "Zhengxiong Luo", "Haoge Deng", "hanyu Zhao", "Li Du", "Chengwei Wu", "Donglin Hao", "Xinlong Wang", "Tengfei Pan"], "title": "CI-VID: A Coherent Interleaved Text-Video Dataset", "comment": null, "summary": "Text-to-video (T2V) generation has recently attracted considerable attention,\nresulting in the development of numerous high-quality datasets that have\npropelled progress in this area. However, existing public datasets are\nprimarily composed of isolated text-video (T-V) pairs and thus fail to support\nthe modeling of coherent multi-clip video sequences. To address this\nlimitation, we introduce CI-VID, a dataset that moves beyond isolated\ntext-to-video (T2V) generation toward text-and-video-to-video (TV2V)\ngeneration, enabling models to produce coherent, multi-scene video sequences.\nCI-VID contains over 340,000 samples, each featuring a coherent sequence of\nvideo clips with text captions that capture both the individual content of each\nclip and the transitions between them, enabling visually and textually grounded\ngeneration. To further validate the effectiveness of CI-VID, we design a\ncomprehensive, multi-dimensional benchmark incorporating human evaluation,\nVLM-based assessment, and similarity-based metrics. Experimental results\ndemonstrate that models trained on CI-VID exhibit significant improvements in\nboth accuracy and content consistency when generating video sequences. This\nfacilitates the creation of story-driven content with smooth visual transitions\nand strong temporal coherence, underscoring the quality and practical utility\nof the CI-VID dataset We release the CI-VID dataset and the accompanying code\nfor data construction and evaluation at: https://github.com/ymju-BAAI/CI-VID", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86CI-VID\u6570\u636e\u96c6\uff0c\u652f\u6301\u4ece\u6587\u672c\u5230\u591a\u573a\u666f\u89c6\u9891\u5e8f\u5217\u7684\u751f\u6210\uff0c\u63d0\u5347\u4e86\u89c6\u9891\u751f\u6210\u7684\u8fde\u8d2f\u6027\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u516c\u5f00\u6570\u636e\u96c6\u591a\u4e3a\u5b64\u7acb\u7684\u6587\u672c-\u89c6\u9891\u5bf9\uff0c\u65e0\u6cd5\u652f\u6301\u8fde\u8d2f\u591a\u573a\u666f\u89c6\u9891\u5e8f\u5217\u7684\u5efa\u6a21\u3002", "method": "\u63d0\u51faCI-VID\u6570\u636e\u96c6\uff0c\u5305\u542b34\u4e07\u6837\u672c\uff0c\u6bcf\u4e2a\u6837\u672c\u5305\u542b\u8fde\u8d2f\u7684\u89c6\u9891\u7247\u6bb5\u5e8f\u5217\u548c\u63cf\u8ff0\u6587\u672c\uff0c\u652f\u6301\u6587\u672c\u548c\u89c6\u9891\u5230\u89c6\u9891\u7684\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8eCI-VID\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u751f\u6210\u89c6\u9891\u5e8f\u5217\u65f6\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u5185\u5bb9\u4e00\u81f4\u6027\u3002", "conclusion": "CI-VID\u6570\u636e\u96c6\u4e3a\u6545\u4e8b\u9a71\u52a8\u5185\u5bb9\u7684\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u652f\u6301\uff0c\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.01945", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01945", "abs": "https://arxiv.org/abs/2507.01945", "authors": ["Nan Chen", "Mengqi Huang", "Yihao Meng", "Zhendong Mao"], "title": "LongAnimation: Long Animation Generation with Dynamic Global-Local Memory", "comment": null, "summary": "Animation colorization is a crucial part of real animation industry\nproduction. Long animation colorization has high labor costs. Therefore,\nautomated long animation colorization based on the video generation model has\nsignificant research value. Existing studies are limited to short-term\ncolorization. These studies adopt a local paradigm, fusing overlapping features\nto achieve smooth transitions between local segments. However, the local\nparadigm neglects global information, failing to maintain long-term color\nconsistency. In this study, we argue that ideal long-term color consistency can\nbe achieved through a dynamic global-local paradigm, i.e., dynamically\nextracting global color-consistent features relevant to the current generation.\nSpecifically, we propose LongAnimation, a novel framework, which mainly\nincludes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color\nConsistency Reward. The SketchDiT captures hybrid reference features to support\nthe DGLM module. The DGLM module employs a long video understanding model to\ndynamically compress global historical features and adaptively fuse them with\nthe current generation features. To refine the color consistency, we introduce\na Color Consistency Reward. During inference, we propose a color consistency\nfusion to smooth the video segment transition. Extensive experiments on both\nshort-term (14 frames) and long-term (average 500 frames) animations show the\neffectiveness of LongAnimation in maintaining short-term and long-term color\nconsistency for open-domain animation colorization task. The code can be found\nat https://cn-makers.github.io/long_animation_web/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLongAnimation\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5168\u5c40-\u5c40\u90e8\u8303\u5f0f\u89e3\u51b3\u957f\u52a8\u753b\u7740\u8272\u4e2d\u7684\u957f\u671f\u989c\u8272\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5305\u62ecSketchDiT\u3001DGLM\u6a21\u5757\u548c\u989c\u8272\u4e00\u81f4\u6027\u5956\u52b1\u3002", "motivation": "\u957f\u52a8\u753b\u7740\u8272\u5728\u52a8\u753b\u4ea7\u4e1a\u4e2d\u6210\u672c\u9ad8\uff0c\u73b0\u6709\u7814\u7a76\u5c40\u9650\u4e8e\u77ed\u671f\u7740\u8272\u4e14\u7f3a\u4e4f\u5168\u5c40\u4fe1\u606f\uff0c\u5bfc\u81f4\u957f\u671f\u989c\u8272\u4e00\u81f4\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u5168\u5c40-\u5c40\u90e8\u8303\u5f0f\uff0c\u7ed3\u5408SketchDiT\u6355\u83b7\u6df7\u5408\u53c2\u8003\u7279\u5f81\uff0cDGLM\u6a21\u5757\u52a8\u6001\u538b\u7f29\u5168\u5c40\u5386\u53f2\u7279\u5f81\u5e76\u4e0e\u5f53\u524d\u751f\u6210\u7279\u5f81\u878d\u5408\uff0c\u5f15\u5165\u989c\u8272\u4e00\u81f4\u6027\u5956\u52b1\u4f18\u5316\u3002", "result": "\u5728\u77ed\u671f\uff0814\u5e27\uff09\u548c\u957f\u671f\uff08\u5e73\u5747500\u5e27\uff09\u52a8\u753b\u4e0a\u5b9e\u9a8c\u9a8c\u8bc1\u4e86LongAnimation\u5728\u5f00\u653e\u57df\u52a8\u753b\u7740\u8272\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "LongAnimation\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u5168\u5c40-\u5c40\u90e8\u8303\u5f0f\u6210\u529f\u89e3\u51b3\u4e86\u957f\u671f\u989c\u8272\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u4e3a\u52a8\u753b\u4ea7\u4e1a\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01949", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01949", "abs": "https://arxiv.org/abs/2507.01949", "authors": ["Kwai Keye Team", "Biao Yang", "Bin Wen", "Changyi Liu", "Chenglong Chu", "Chengru Song", "Chongling Rao", "Chuan Yi", "Da Li", "Dunju Zang", "Fan Yang", "Guorui Zhou", "Hao Peng", "Haojie Ding", "Jiaming Huang", "Jiangxia Cao", "Jiankang Chen", "Jingyun Hua", "Jin Ouyang", "Kaibing Chen", "Kaiyu Jiang", "Kaiyu Tang", "Kun Gai", "Shengnan Zhang", "Siyang Mao", "Sui Huang", "Tianke Zhang", "Tingting Gao", "Wei Chen", "Wei Yuan", "Xiangyu Wu", "Xiao Hu", "Xingyu Lu", "Yang Zhou", "Yi-Fan Zhang", "Yiping Yang", "Yulong Chen", "Zhenhua Wu", "Zhenyu Li", "Zhixin Ling", "Ziming Li", "Dehua Ma", "Di Xu", "Haixuan Gao", "Hang Li", "Jiawei Guo", "Jing Wang", "Lejian Ren", "Muhao Wei", "Qianqian Wang", "Qigen Hu", "Shiyao Wang", "Tao Yu", "Xinchen Luo", "Yan Li", "Yiming Liang", "Yuhang Hu", "Zeyi Lu", "Zhuoran Yang", "Zixing Zhang"], "title": "Kwai Keye-VL Technical Report", "comment": "Technical Report: https://github.com/Kwai-Keye/Keye", "summary": "While Multimodal Large Language Models (MLLMs) demonstrate remarkable\ncapabilities on static images, they often fall short in comprehending dynamic,\ninformation-dense short-form videos, a dominant medium in today's digital\nlandscape. To bridge this gap, we introduce \\textbf{Kwai Keye-VL}, an\n8-billion-parameter multimodal foundation model engineered for leading-edge\nperformance in short-video understanding while maintaining robust\ngeneral-purpose vision-language abilities. The development of Keye-VL rests on\ntwo core pillars: a massive, high-quality dataset exceeding 600 billion tokens\nwith a strong emphasis on video, and an innovative training recipe. This recipe\nfeatures a four-stage pre-training process for solid vision-language alignment,\nfollowed by a meticulous two-phase post-training process. The first\npost-training stage enhances foundational capabilities like instruction\nfollowing, while the second phase focuses on stimulating advanced reasoning. In\nthis second phase, a key innovation is our five-mode ``cold-start'' data\nmixture, which includes ``thinking'', ``non-thinking'', ``auto-think'', ``think\nwith image'', and high-quality video data. This mixture teaches the model to\ndecide when and how to reason. Subsequent reinforcement learning (RL) and\nalignment steps further enhance these reasoning capabilities and correct\nabnormal model behaviors, such as repetitive outputs. To validate our approach,\nwe conduct extensive evaluations, showing that Keye-VL achieves\nstate-of-the-art results on public video benchmarks and remains highly\ncompetitive on general image-based tasks (Figure 1). Furthermore, we develop\nand release the \\textbf{KC-MMBench}, a new benchmark tailored for real-world\nshort-video scenarios, where Keye-VL shows a significant advantage.", "AI": {"tldr": "Kwai Keye-VL\u662f\u4e00\u4e2a8B\u53c2\u6570\u7684\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u4e13\u6ce8\u4e8e\u77ed\u89c6\u9891\u7406\u89e3\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u65b9\u6cd5\u548c\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u5b9e\u73b0\u9886\u5148\u6027\u80fd\u3002", "motivation": "\u73b0\u6709MLLMs\u5728\u52a8\u6001\u3001\u4fe1\u606f\u5bc6\u96c6\u7684\u77ed\u89c6\u9891\u7406\u89e3\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0cKwai Keye-VL\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u56db\u9636\u6bb5\u9884\u8bad\u7ec3\u548c\u4e24\u9636\u6bb5\u540e\u8bad\u7ec3\uff0c\u5305\u62ec\u4e94\u79cd\u6a21\u5f0f\u7684\u51b7\u542f\u52a8\u6570\u636e\u6df7\u5408\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u3002", "result": "Keye-VL\u5728\u516c\u5f00\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\uff0c\u5e76\u5728\u901a\u7528\u56fe\u50cf\u4efb\u52a1\u4e2d\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "Kwai Keye-VL\u6210\u529f\u63d0\u5347\u4e86\u77ed\u89c6\u9891\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u65b0\u57fa\u51c6KC-MMBench\u9a8c\u8bc1\u4e86\u5176\u4f18\u52bf\u3002"}}
{"id": "2507.01953", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01953", "abs": "https://arxiv.org/abs/2507.01953", "authors": ["Yukang Cao", "Chenyang Si", "Jinghao Wang", "Ziwei Liu"], "title": "FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model", "comment": "ICCV 2025. Project page: https://yukangcao.github.io/FreeMorph/", "summary": "We present FreeMorph, the first tuning-free method for image morphing that\naccommodates inputs with different semantics or layouts. Unlike existing\nmethods that rely on finetuning pre-trained diffusion models and are limited by\ntime constraints and semantic/layout discrepancies, FreeMorph delivers\nhigh-fidelity image morphing without requiring per-instance training. Despite\ntheir efficiency and potential, tuning-free methods face challenges in\nmaintaining high-quality results due to the non-linear nature of the multi-step\ndenoising process and biases inherited from the pre-trained diffusion model. In\nthis paper, we introduce FreeMorph to address these challenges by integrating\ntwo key innovations. 1) We first propose a guidance-aware spherical\ninterpolation design that incorporates explicit guidance from the input images\nby modifying the self-attention modules, thereby addressing identity loss and\nensuring directional transitions throughout the generated sequence. 2) We\nfurther introduce a step-oriented variation trend that blends self-attention\nmodules derived from each input image to achieve controlled and consistent\ntransitions that respect both inputs. Our extensive evaluations demonstrate\nthat FreeMorph outperforms existing methods, being 10x ~ 50x faster and\nestablishing a new state-of-the-art for image morphing.", "AI": {"tldr": "FreeMorph\u662f\u4e00\u79cd\u65e0\u9700\u8c03\u4f18\u7684\u56fe\u50cf\u53d8\u5f62\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u8bed\u4e49\u6216\u5e03\u5c40\u7684\u8f93\u5165\uff0c\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u5b9e\u73b0\u9ad8\u8d28\u91cf\u53d8\u5f62\uff0c\u901f\u5ea6\u63d0\u534710~50\u500d\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u8c03\u4f18\u3001\u53d7\u9650\u4e8e\u65f6\u95f4\u548c\u8bed\u4e49/\u5e03\u5c40\u5dee\u5f02\u7684\u95ee\u9898\u3002", "method": "1) \u63d0\u51fa\u6307\u5bfc\u611f\u77e5\u7684\u7403\u5f62\u63d2\u503c\u8bbe\u8ba1\uff0c\u4fee\u6539\u81ea\u6ce8\u610f\u529b\u6a21\u5757\uff1b2) \u5f15\u5165\u6b65\u8fdb\u5f0f\u53d8\u5316\u8d8b\u52bf\uff0c\u6df7\u5408\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u4ee5\u5b9e\u73b0\u53ef\u63a7\u8fc7\u6e21\u3002", "result": "FreeMorph\u5728\u56fe\u50cf\u53d8\u5f62\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u901f\u5ea6\u63d0\u534710~50\u500d\uff0c\u8fbe\u5230\u65b0SOTA\u3002", "conclusion": "FreeMorph\u4e3a\u65e0\u9700\u8c03\u4f18\u7684\u56fe\u50cf\u53d8\u5f62\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01019", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.01019", "abs": "https://arxiv.org/abs/2507.01019", "authors": ["Imran Mirza", "Cole Huang", "Ishwara Vasista", "Rohan Patil", "Asli Akalin", "Sean O'Brien", "Kevin Zhu"], "title": "MALIBU Benchmark: Multi-Agent LLM Implicit Bias Uncovered", "comment": "Accepted to Building Trust in LLMs @ ICLR 2025 and NAACL SRW 2025", "summary": "Multi-agent systems, which consist of multiple AI models interacting within a\nshared environment, are increasingly used for persona-based interactions.\nHowever, if not carefully designed, these systems can reinforce implicit biases\nin large language models (LLMs), raising concerns about fairness and equitable\nrepresentation. We present MALIBU, a novel benchmark developed to assess the\ndegree to which LLM-based multi-agent systems implicitly reinforce social\nbiases and stereotypes. MALIBU evaluates bias in LLM-based multi-agent systems\nthrough scenario-based assessments. AI models complete tasks within predefined\ncontexts, and their responses undergo evaluation by an LLM-based multi-agent\njudging system in two phases. In the first phase, judges score responses\nlabeled with specific demographic personas (e.g., gender, race, religion)\nacross four metrics. In the second phase, judges compare paired responses\nassigned to different personas, scoring them and selecting the superior\nresponse. Our study quantifies biases in LLM-generated outputs, revealing that\nbias mitigation may favor marginalized personas over true neutrality,\nemphasizing the need for nuanced detection, balanced fairness strategies, and\ntransparent evaluation benchmarks in multi-agent systems.", "AI": {"tldr": "MALIBU\u662f\u4e00\u4e2a\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u9690\u542b\u7684\u793e\u4f1a\u504f\u89c1\u548c\u523b\u677f\u5370\u8c61\uff0c\u901a\u8fc7\u573a\u666f\u8bc4\u4f30\u63ed\u793a\u504f\u89c1\u5e76\u5f3a\u8c03\u516c\u5e73\u7b56\u7565\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u53ef\u80fd\u5f3a\u5316LLM\u4e2d\u7684\u9690\u6027\u504f\u89c1\uff0c\u5f15\u53d1\u516c\u5e73\u548c\u4ee3\u8868\u6027\u62c5\u5fe7\uff0c\u9700\u5f00\u53d1\u8bc4\u4f30\u5de5\u5177\u3002", "method": "MALIBU\u901a\u8fc7\u4e24\u9636\u6bb5\u8bc4\u4f30\uff1a1\uff09\u5bf9\u6807\u6ce8\u4eba\u53e3\u7279\u5f81\u7684\u54cd\u5e94\u8bc4\u5206\uff1b2\uff09\u6bd4\u8f83\u4e0d\u540c\u4eba\u53e3\u7279\u5f81\u7684\u54cd\u5e94\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u504f\u89c1\u7f13\u89e3\u53ef\u80fd\u504f\u5411\u8fb9\u7f18\u5316\u7fa4\u4f53\u800c\u975e\u771f\u6b63\u4e2d\u7acb\uff0c\u9700\u66f4\u7ec6\u81f4\u7684\u68c0\u6d4b\u548c\u516c\u5e73\u7b56\u7565\u3002", "conclusion": "\u9700\u900f\u660e\u8bc4\u4f30\u57fa\u51c6\u548c\u5e73\u8861\u516c\u5e73\u7b56\u7565\u4ee5\u4f18\u5316\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u516c\u5e73\u6027\u3002"}}
{"id": "2507.01160", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01160", "abs": "https://arxiv.org/abs/2507.01160", "authors": ["Huiling You", "Samia Touileb", "Erik Velldal", "Lilja \u00d8vrelid"], "title": "Event-based evaluation of abstractive news summarization", "comment": "to appear at GEM2 workshop@ACL 2025", "summary": "An abstractive summary of a news article contains its most important\ninformation in a condensed version. The evaluation of automatically generated\nsummaries by generative language models relies heavily on human-authored\nsummaries as gold references, by calculating overlapping units or similarity\nscores. News articles report events, and ideally so should the summaries. In\nthis work, we propose to evaluate the quality of abstractive summaries by\ncalculating overlapping events between generated summaries, reference\nsummaries, and the original news articles. We experiment on a richly annotated\nNorwegian dataset comprising both events annotations and summaries authored by\nexpert human annotators. Our approach provides more insight into the event\ninformation contained in the summaries.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u91cd\u53e0\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u751f\u6210\u5f0f\u6458\u8981\u7684\u8d28\u91cf\uff0c\u901a\u8fc7\u6bd4\u8f83\u751f\u6210\u6458\u8981\u3001\u53c2\u8003\u6458\u8981\u548c\u539f\u6587\u4e2d\u7684\u4e8b\u4ef6\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e0e\u4eba\u5de5\u53c2\u8003\u6458\u8981\u7684\u91cd\u53e0\u5355\u5143\u6216\u76f8\u4f3c\u6027\u5f97\u5206\uff0c\u4f46\u65b0\u95fb\u6458\u8981\u5e94\u53cd\u6620\u4e8b\u4ef6\u4fe1\u606f\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u76f4\u63a5\u7684\u4e8b\u4ef6\u4fe1\u606f\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u8ba1\u7b97\u751f\u6210\u6458\u8981\u3001\u53c2\u8003\u6458\u8981\u548c\u539f\u6587\u4e4b\u95f4\u7684\u4e8b\u4ef6\u91cd\u53e0\uff0c\u5e76\u5728\u632a\u5a01\u8bed\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u66f4\u6df1\u5165\u5730\u5206\u6790\u6458\u8981\u4e2d\u7684\u4e8b\u4ef6\u4fe1\u606f\u3002", "conclusion": "\u57fa\u4e8e\u4e8b\u4ef6\u91cd\u53e0\u7684\u8bc4\u4f30\u65b9\u6cd5\u80fd\u66f4\u51c6\u786e\u5730\u53cd\u6620\u751f\u6210\u6458\u8981\u7684\u8d28\u91cf\u3002"}}
{"id": "2507.01170", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01170", "abs": "https://arxiv.org/abs/2507.01170", "authors": ["Simon B\u00f6rjesson", "Erik Ersmark", "Pierre Nugues"], "title": "Matching and Linking Entries in Historical Swedish Encyclopedias", "comment": "10 pages, 3 figures", "summary": "The \\textit{Nordisk familjebok} is a Swedish encyclopedia from the 19th and\n20th centuries. It was written by a team of experts and aimed to be an\nintellectual reference, stressing precision and accuracy. This encyclopedia had\nfour main editions remarkable by their size, ranging from 20 to 38 volumes. As\na consequence, the \\textit{Nordisk familjebok} had a considerable influence in\nuniversities, schools, the media, and society overall. As new editions were\nreleased, the selection of entries and their content evolved, reflecting\nintellectual changes in Sweden.\n  In this paper, we used digitized versions from \\textit{Project Runeberg}. We\nfirst resegmented the raw text into entries and matched pairs of entries\nbetween the first and second editions using semantic sentence embeddings. We\nthen extracted the geographical entries from both editions using a\ntransformer-based classifier and linked them to Wikidata. This enabled us to\nidentify geographic trends and possible shifts between the first and second\neditions, written between 1876-1899 and 1904-1926, respectively.\n  Interpreting the results, we observe a small but significant shift in\ngeographic focus away from Europe and towards North America, Africa, Asia,\nAustralia, and northern Scandinavia from the first to the second edition,\nconfirming the influence of the First World War and the rise of new powers. The\ncode and data are available on GitHub at\nhttps://github.com/sibbo/nordisk-familjebok.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u745e\u5178\u767e\u79d1\u5168\u4e66\u300aNordisk familjebok\u300b\u7b2c\u4e00\u7248\u548c\u7b2c\u4e8c\u7248\u4e2d\u5730\u7406\u6761\u76ee\u7684\u53d8\u5316\uff0c\u53d1\u73b0\u4ece\u6b27\u6d32\u8f6c\u5411\u5317\u7f8e\u3001\u975e\u6d32\u3001\u4e9a\u6d32\u3001\u6fb3\u5927\u5229\u4e9a\u548c\u5317\u6b27\u7684\u663e\u8457\u8d8b\u52bf\u3002", "motivation": "\u7814\u7a76\u76ee\u7684\u662f\u63ed\u793a19\u4e16\u7eaa\u672b\u81f320\u4e16\u7eaa\u521d\u745e\u5178\u77e5\u8bc6\u754c\u5bf9\u5730\u7406\u5173\u6ce8\u7684\u53d8\u5316\uff0c\u53cd\u6620\u5386\u53f2\u4e8b\u4ef6\uff08\u5982\u7b2c\u4e00\u6b21\u4e16\u754c\u5927\u6218\uff09\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u6570\u5b57\u5316\u6587\u672c\uff0c\u901a\u8fc7\u8bed\u4e49\u53e5\u5b50\u5d4c\u5165\u5339\u914d\u6761\u76ee\uff0c\u5e76\u5229\u7528\u57fa\u4e8eTransformer\u7684\u5206\u7c7b\u5668\u63d0\u53d6\u5730\u7406\u6761\u76ee\uff0c\u94fe\u63a5\u5230Wikidata\u8fdb\u884c\u5206\u6790\u3002", "result": "\u53d1\u73b0\u5730\u7406\u7126\u70b9\u4ece\u6b27\u6d32\u8f6c\u5411\u5176\u4ed6\u5730\u533a\uff0c\u8bc1\u5b9e\u4e86\u7b2c\u4e00\u6b21\u4e16\u754c\u5927\u6218\u548c\u65b0\u52bf\u529b\u5d1b\u8d77\u7684\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u9a8c\u8bc1\u4e86\u5386\u53f2\u4e8b\u4ef6\u5bf9\u767e\u79d1\u5168\u4e66\u5185\u5bb9\u7684\u5f71\u54cd\uff0c\u63d0\u4f9b\u4e86\u6570\u636e\u548c\u65b9\u6cd5\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2507.01213", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01213", "abs": "https://arxiv.org/abs/2507.01213", "authors": ["Adamu Lawan", "Juhua Pu", "Haruna Yunusa", "Jawad Muhammad", "Muhammad Lawan"], "title": "MEGA: xLSTM with Multihead Exponential Gated Fusion for Precise Aspect-based Sentiment Analysis", "comment": "6, 1 figure", "summary": "Aspect-based Sentiment Analysis (ABSA) is a critical Natural Language\nProcessing (NLP) task that extracts aspects from text and determines their\nassociated sentiments, enabling fine-grained analysis of user opinions.\nExisting ABSA methods struggle to balance computational efficiency with high\nperformance: deep learning models often lack global context, transformers\ndemand significant computational resources, and Mamba-based approaches face\nCUDA dependency and diminished local correlations. Recent advancements in\nExtended Long Short-Term Memory (xLSTM) models, particularly their efficient\nmodeling of long-range dependencies, have significantly advanced the NLP\ncommunity. However, their potential in ABSA remains untapped. To this end, we\npropose xLSTM with Multihead Exponential Gated Fusion (MEGA), a novel framework\nintegrating a bi-directional mLSTM architecture with forward and partially\nflipped backward (PF-mLSTM) streams. The PF-mLSTM enhances localized context\nmodeling by processing the initial sequence segment in reverse with dedicated\nparameters, preserving critical short-range patterns. We further introduce an\nmLSTM-based multihead cross exponential gated fusion mechanism (MECGAF) that\ndynamically combines forward mLSTM outputs as query and key with PF-mLSTM\noutputs as value, optimizing short-range dependency capture while maintaining\nglobal context and efficiency. Experimental results on three benchmark datasets\ndemonstrate that MEGA outperforms state-of-the-art baselines, achieving\nsuperior accuracy and efficiency in ABSA tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMEGA\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408xLSTM\u548cMultihead Exponential Gated Fusion\uff0c\u7528\u4e8e\u63d0\u5347Aspect-based Sentiment Analysis\uff08ABSA\uff09\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709ABSA\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u4e0e\u9ad8\u6027\u80fd\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0cxLSTM\u7684\u6f5c\u529b\u5728ABSA\u4e2d\u5c1a\u672a\u88ab\u5145\u5206\u6316\u6398\u3002", "method": "\u63d0\u51faMEGA\u6846\u67b6\uff0c\u6574\u5408\u53cc\u5411mLSTM\u67b6\u6784\u548c\u90e8\u5206\u7ffb\u8f6c\u53cd\u5411\u6d41\uff08PF-mLSTM\uff09\uff0c\u5e76\u5f15\u5165\u591a\u5934\u4ea4\u53c9\u6307\u6570\u95e8\u63a7\u878d\u5408\u673a\u5236\uff08MECGAF\uff09\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cMEGA\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "conclusion": "MEGA\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86ABSA\u4efb\u52a1\u4e2d\u7684\u5c40\u90e8\u4e0e\u5168\u5c40\u4f9d\u8d56\u5efa\u6a21\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2507.01231", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01231", "abs": "https://arxiv.org/abs/2507.01231", "authors": ["I\u00f1aki Dellibarda Varela", "Pablo Romero-Sorozabal", "Eduardo Rocon", "Manuel Cebrian"], "title": "Rethinking the Illusion of Thinking", "comment": "8 pages, 4 figures", "summary": "Earlier this year, Apple ignited controversy by publishing \"The Illusion of\nThinking,\" prompting heated debate within the AI community. Critics seized upon\nthe findings as conclusive evidence that Large Reasoning Models (LRMs) lack\ngenuine reasoning capabilities, branding them as mere stochastic parrots.\nMeanwhile, defenders-spearheaded by Lawsen et al. (2025)-fired back, condemning\nthe experimental setup as flawed and the conclusions overstated. We clarify\nthis debate by replicating and refining two of the original study's most\ncontentious benchmarks: Towers of Hanoi and River Crossing. By introducing\nincremental stepwise prompting and agentic collaborative dialogue, we show that\npreviously reported failures solving the Towers of Hanoi were not purely result\nof output constraints, but also partly a result of cognition limitations: LRMs\nstill stumble when complexity rises moderately (around 8 disks). Moreover, the\nRiver Crossing results initially heralded as catastrophic failures turn out to\nhinge upon testing unsolvable configurations. Once we limit tests strictly to\nsolvable problems-LRMs effortlessly solve large instances involving over 100\nagent pairs. Our findings ultimately defy simplistic narratives: today's LRMs\nare stochastic, RL-tuned searchers in a discrete state space we barely\nunderstand. Real progress in symbolic, long-horizon reasoning demands mapping\nthat terrain through fine-grained ablations like those introduced here.", "AI": {"tldr": "\u8bba\u6587\u6f84\u6e05\u4e86\u5173\u4e8e\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u662f\u5426\u5177\u5907\u771f\u6b63\u63a8\u7406\u80fd\u529b\u7684\u4e89\u8bae\uff0c\u901a\u8fc7\u590d\u73b0\u548c\u6539\u8fdb\u5b9e\u9a8c\uff0c\u53d1\u73b0LRMs\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u4f46\u5728\u53ef\u89e3\u95ee\u9898\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3AI\u793e\u533a\u5bf9LRMs\u662f\u5426\u5177\u5907\u771f\u6b63\u63a8\u7406\u80fd\u529b\u7684\u4e89\u8bae\uff0c\u6f84\u6e05\u5148\u524d\u7814\u7a76\u7684\u7ed3\u8bba\u3002", "method": "\u590d\u73b0\u5e76\u6539\u8fdb\u539f\u7814\u7a76\u4e2d\u7684\u4e24\u4e2a\u4e89\u8bae\u6027\u57fa\u51c6\u6d4b\u8bd5\uff08Towers of Hanoi\u548cRiver Crossing\uff09\uff0c\u5f15\u5165\u9010\u6b65\u63d0\u793a\u548c\u534f\u4f5c\u5bf9\u8bdd\u65b9\u6cd5\u3002", "result": "LRMs\u5728\u590d\u6742\u4efb\u52a1\uff08\u59828\u4e2a\u5706\u76d8\u7684Towers of Hanoi\uff09\u4e2d\u8868\u73b0\u53d7\u9650\uff0c\u4f46\u5728\u53ef\u89e3\u95ee\u9898\uff08\u5982River Crossing\uff09\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "LRMs\u662f\u79bb\u6563\u72b6\u6001\u7a7a\u95f4\u4e2d\u7684\u968f\u673a\u641c\u7d22\u5668\uff0c\u672a\u6765\u9700\u8981\u5728\u7b26\u53f7\u5316\u3001\u957f\u7a0b\u63a8\u7406\u65b9\u9762\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2507.01026", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01026", "abs": "https://arxiv.org/abs/2507.01026", "authors": ["Md Shakil Ahamed Shohag", "Q. M. Jonathan Wu", "Farhad Pourpanah"], "title": "Few-Shot Inspired Generative Zero-Shot Learning", "comment": null, "summary": "Generative zero-shot learning (ZSL) methods typically synthesize visual\nfeatures for unseen classes using predefined semantic attributes, followed by\ntraining a fully supervised classification model. While effective, these\nmethods require substantial computational resources and extensive synthetic\ndata, thereby relaxing the original ZSL assumptions. In this paper, we propose\nFSIGenZ, a few-shot-inspired generative ZSL framework that reduces reliance on\nlarge-scale feature synthesis. Our key insight is that class-level attributes\nexhibit instance-level variability, i.e., some attributes may be absent or\npartially visible, yet conventional ZSL methods treat them as uniformly\npresent. To address this, we introduce Model-Specific Attribute Scoring (MSAS),\nwhich dynamically re-scores class attributes based on model-specific\noptimization to approximate instance-level variability without access to unseen\ndata. We further estimate group-level prototypes as clusters of instances based\non MSAS-adjusted attribute scores, which serve as representative synthetic\nfeatures for each unseen class. To mitigate the resulting data imbalance, we\nintroduce a Dual-Purpose Semantic Regularization (DPSR) strategy while training\na semantic-aware contrastive classifier (SCC) using these prototypes.\nExperiments on SUN, AwA2, and CUB benchmarks demonstrate that FSIGenZ achieves\ncompetitive performance using far fewer synthetic features.", "AI": {"tldr": "FSIGenZ\u662f\u4e00\u79cd\u5c11\u6837\u672c\u542f\u53d1\u7684\u751f\u6210\u5f0f\u96f6\u6837\u672c\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5c5e\u6027\u8bc4\u5206\u548c\u539f\u578b\u4f30\u8ba1\uff0c\u51cf\u5c11\u5bf9\u5927\u89c4\u6a21\u7279\u5f81\u5408\u6210\u7684\u4f9d\u8d56\u3002", "motivation": "\u4f20\u7edf\u751f\u6210\u5f0f\u96f6\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u548c\u5408\u6210\u6570\u636e\uff0c\u8fdd\u80cc\u4e86\u96f6\u6837\u672c\u5b66\u4e60\u7684\u521d\u8877\u3002FSIGenZ\u65e8\u5728\u51cf\u5c11\u8fd9\u79cd\u4f9d\u8d56\uff0c\u540c\u65f6\u5904\u7406\u5c5e\u6027\u5728\u5b9e\u4f8b\u7ea7\u522b\u7684\u53d8\u5f02\u6027\u3002", "method": "\u63d0\u51fa\u6a21\u578b\u7279\u5b9a\u5c5e\u6027\u8bc4\u5206\uff08MSAS\uff09\u52a8\u6001\u8c03\u6574\u5c5e\u6027\u8bc4\u5206\uff0c\u4f30\u8ba1\u7ec4\u7ea7\u539f\u578b\u4f5c\u4e3a\u672a\u89c1\u7c7b\u7684\u4ee3\u8868\u6027\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u53cc\u91cd\u76ee\u7684\u8bed\u4e49\u6b63\u5219\u5316\uff08DPSR\uff09\u8bad\u7ec3\u8bed\u4e49\u611f\u77e5\u5bf9\u6bd4\u5206\u7c7b\u5668\uff08SCC\uff09\u3002", "result": "\u5728SUN\u3001AwA2\u548cCUB\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFSIGenZ\u4f7f\u7528\u66f4\u5c11\u7684\u5408\u6210\u7279\u5f81\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u6027\u80fd\u3002", "conclusion": "FSIGenZ\u901a\u8fc7\u52a8\u6001\u5c5e\u6027\u8bc4\u5206\u548c\u539f\u578b\u4f30\u8ba1\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u96f6\u6837\u672c\u5b66\u4e60\u7684\u6027\u80fd\u3002"}}
{"id": "2507.01099", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01099", "abs": "https://arxiv.org/abs/2507.01099", "authors": ["Zeyi Liu", "Shuang Li", "Eric Cousineau", "Siyuan Feng", "Benjamin Burchfiel", "Shuran Song"], "title": "Geometry-aware 4D Video Generation for Robot Manipulation", "comment": "Project website: https://robot4dgen.github.io", "summary": "Understanding and predicting the dynamics of the physical world can enhance a\nrobot's ability to plan and interact effectively in complex environments. While\nrecent video generation models have shown strong potential in modeling dynamic\nscenes, generating videos that are both temporally coherent and geometrically\nconsistent across camera views remains a significant challenge. To address\nthis, we propose a 4D video generation model that enforces multi-view 3D\nconsistency of videos by supervising the model with cross-view pointmap\nalignment during training. This geometric supervision enables the model to\nlearn a shared 3D representation of the scene, allowing it to predict future\nvideo sequences from novel viewpoints based solely on the given RGB-D\nobservations, without requiring camera poses as inputs. Compared to existing\nbaselines, our method produces more visually stable and spatially aligned\npredictions across multiple simulated and real-world robotic datasets. We\nfurther show that the predicted 4D videos can be used to recover robot\nend-effector trajectories using an off-the-shelf 6DoF pose tracker, supporting\nrobust robot manipulation and generalization to novel camera viewpoints.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd4D\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u8de8\u89c6\u89d2\u70b9\u56fe\u5bf9\u9f50\u76d1\u7763\uff0c\u5b9e\u73b0\u591a\u89c6\u89d23D\u4e00\u81f4\u6027\uff0c\u63d0\u5347\u673a\u5668\u4eba\u52a8\u6001\u573a\u666f\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u589e\u5f3a\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u89c4\u5212\u548c\u4ea4\u4e92\u7684\u80fd\u529b\uff0c\u89e3\u51b3\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u65f6\u95f4\u8fde\u8d2f\u6027\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u5229\u7528RGB-D\u89c2\u6d4b\u6570\u636e\uff0c\u901a\u8fc7\u8de8\u89c6\u89d2\u70b9\u56fe\u5bf9\u9f50\u76d1\u7763\u8bad\u7ec3\u6a21\u578b\uff0c\u5b66\u4e60\u5171\u4eab3D\u573a\u666f\u8868\u793a\uff0c\u65e0\u9700\u76f8\u673a\u59ff\u6001\u8f93\u5165\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u673a\u5668\u4eba\u6570\u636e\u96c6\u4e0a\u751f\u6210\u66f4\u7a33\u5b9a\u3001\u7a7a\u95f4\u5bf9\u9f50\u7684\u89c6\u9891\u9884\u6d4b\uff0c\u652f\u6301\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\u8f68\u8ff9\u6062\u590d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u65b0\u89c6\u89d2\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u52a8\u6001\u573a\u666f\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01234", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01234", "abs": "https://arxiv.org/abs/2507.01234", "authors": ["Yu Fan", "Yang Tian", "Shauli Ravfogel", "Mrinmaya Sachan", "Elliott Ash", "Alexander Hoyle"], "title": "The Medium Is Not the Message: Deconfounding Text Embeddings via Linear Concept Erasure", "comment": null, "summary": "Embedding-based similarity metrics between text sequences can be influenced\nnot just by the content dimensions we most care about, but can also be biased\nby spurious attributes like the text's source or language. These document\nconfounders cause problems for many applications, but especially those that\nneed to pool texts from different corpora. This paper shows that a debiasing\nalgorithm that removes information about observed confounders from the encoder\nrepresentations substantially reduces these biases at a minimal computational\ncost. Document similarity and clustering metrics improve across every embedding\nvariant and task we evaluate -- often dramatically. Interestingly, performance\non out-of-distribution benchmarks is not impacted, indicating that the\nembeddings are not otherwise degraded.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53bb\u504f\u7b97\u6cd5\uff0c\u901a\u8fc7\u4ece\u7f16\u7801\u5668\u8868\u793a\u4e2d\u79fb\u9664\u89c2\u5bdf\u5230\u7684\u6df7\u6742\u56e0\u7d20\u4fe1\u606f\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6587\u672c\u76f8\u4f3c\u6027\u5ea6\u91cf\u4e2d\u7684\u504f\u5dee\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u4f4e\u3002", "motivation": "\u6587\u672c\u5e8f\u5217\u7684\u5d4c\u5165\u76f8\u4f3c\u6027\u5ea6\u91cf\u53ef\u80fd\u53d7\u5230\u65e0\u5173\u5c5e\u6027\uff08\u5982\u6765\u6e90\u6216\u8bed\u8a00\uff09\u7684\u5e72\u6270\uff0c\u5f71\u54cd\u591a\u8bed\u6599\u5e93\u6587\u672c\u7684\u5e94\u7528\u6548\u679c\u3002", "method": "\u91c7\u7528\u53bb\u504f\u7b97\u6cd5\uff0c\u4ece\u7f16\u7801\u5668\u8868\u793a\u4e2d\u53bb\u9664\u6df7\u6742\u56e0\u7d20\u4fe1\u606f\u3002", "result": "\u5728\u6240\u6709\u8bc4\u4f30\u7684\u5d4c\u5165\u53d8\u4f53\u548c\u4efb\u52a1\u4e2d\uff0c\u6587\u6863\u76f8\u4f3c\u6027\u548c\u805a\u7c7b\u6307\u6807\u5747\u663e\u8457\u63d0\u5347\uff0c\u4e14\u4e0d\u5f71\u54cd\u5206\u5e03\u5916\u57fa\u51c6\u6027\u80fd\u3002", "conclusion": "\u53bb\u504f\u7b97\u6cd5\u80fd\u6709\u6548\u51cf\u5c11\u6587\u672c\u76f8\u4f3c\u6027\u5ea6\u91cf\u4e2d\u7684\u504f\u5dee\uff0c\u4e14\u4e0d\u635f\u5bb3\u5d4c\u5165\u7684\u5176\u4ed6\u6027\u80fd\u3002"}}
{"id": "2507.01282", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.01282", "abs": "https://arxiv.org/abs/2507.01282", "authors": ["Matthew JY Kang", "Wenli Yang", "Monica R Roberts", "Byeong Ho Kang", "Charles B Malpas"], "title": "Beyond Black-Box AI: Interpretable Hybrid Systems for Dementia Care", "comment": null, "summary": "The recent boom of large language models (LLMs) has re-ignited the hope that\nartificial intelligence (AI) systems could aid medical diagnosis. Yet despite\ndazzling benchmark scores, LLM assistants have yet to deliver measurable\nimprovements at the bedside. This scoping review aims to highlight the areas\nwhere AI is limited to make practical contributions in the clinical setting,\nspecifically in dementia diagnosis and care.\n  Standalone machine-learning models excel at pattern recognition but seldom\nprovide actionable, interpretable guidance, eroding clinician trust. Adjacent\nuse of LLMs by physicians did not result in better diagnostic accuracy or\nspeed. Key limitations trace to the data-driven paradigm: black-box outputs\nwhich lack transparency, vulnerability to hallucinations, and weak causal\nreasoning. Hybrid approaches that combine statistical learning with expert\nrule-based knowledge, and involve clinicians throughout the process help bring\nback interpretability. They also fit better with existing clinical workflows,\nas seen in examples like PEIRS and ATHENA-CDS.\n  Future decision-support should prioritise explanatory coherence by linking\npredictions to clinically meaningful causes. This can be done through\nneuro-symbolic or hybrid AI that combines the language ability of LLMs with\nhuman causal expertise. AI researchers have addressed this direction, with\nexplainable AI and neuro-symbolic AI being the next logical steps in further\nadvancement in AI. However, they are still based on data-driven knowledge\nintegration instead of human-in-the-loop approaches. Future research should\nmeasure success not only by accuracy but by improvements in clinician\nunderstanding, workflow fit, and patient outcomes. A better understanding of\nwhat helps improve human-computer interactions is greatly needed for AI systems\nto become part of clinical practice.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u533b\u5b66\u8bca\u65ad\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5b9e\u9645\u4e34\u5e8a\u8d21\u732e\u6709\u9650\uff0c\u5c24\u5176\u5728\u75f4\u5446\u75c7\u8bca\u65ad\u4e2d\u3002\u6df7\u5408\u65b9\u6cd5\u7ed3\u5408\u7edf\u8ba1\u5b66\u4e60\u548c\u4e13\u5bb6\u77e5\u8bc6\u53ef\u63d0\u5347\u900f\u660e\u5ea6\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u63a2\u8ba8AI\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u75f4\u5446\u75c7\u8bca\u65ad\u548c\u62a4\u7406\u4e2d\uff0c\u4ee5\u63a8\u52a8\u66f4\u5b9e\u7528\u7684AI\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u3002", "method": "\u901a\u8fc7\u8303\u56f4\u7efc\u8ff0\uff0c\u5206\u6790AI\u5728\u4e34\u5e8a\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u6df7\u5408\u65b9\u6cd5\uff08\u5982\u795e\u7ecf\u7b26\u53f7AI\uff09\u4ee5\u63d0\u5347\u89e3\u91ca\u6027\u548c\u5de5\u4f5c\u6d41\u9002\u914d\u6027\u3002", "result": "AI\u5728\u4e34\u5e8a\u4e2d\u7684\u4e3b\u8981\u95ee\u9898\u5305\u62ec\u7f3a\u4e4f\u900f\u660e\u5ea6\u3001\u5e7b\u89c9\u503e\u5411\u548c\u5f31\u56e0\u679c\u63a8\u7406\u3002\u6df7\u5408\u65b9\u6cd5\u5982PEIRS\u548cATHENA-CDS\u663e\u793a\u51fa\u66f4\u597d\u7684\u5b9e\u7528\u6027\u548c\u89e3\u91ca\u6027\u3002", "conclusion": "\u672a\u6765AI\u51b3\u7b56\u652f\u6301\u5e94\u6ce8\u91cd\u89e3\u91ca\u6027\u548c\u4e34\u5e8a\u5de5\u4f5c\u6d41\u9002\u914d\u6027\uff0c\u7ed3\u5408LLM\u7684\u8bed\u8a00\u80fd\u529b\u548c\u4eba\u7c7b\u56e0\u679c\u63a8\u7406\uff0c\u5e76\u8861\u91cf\u5176\u5bf9\u4e34\u5e8a\u7406\u89e3\u548c\u60a3\u8005\u7ed3\u5c40\u7684\u5f71\u54cd\u3002"}}
{"id": "2507.01027", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01027", "abs": "https://arxiv.org/abs/2507.01027", "authors": ["Zijian Ye", "Wei Huang", "Yifei Yu", "Tianhe Ren", "Zhongrui Wang", "Xiaojuan Qi"], "title": "DBellQuant: Breaking the Bell with Double-Bell Transformation for LLMs Post Training Binarization", "comment": "19 pages; Appendix added", "summary": "Large language models (LLMs) demonstrate remarkable performance but face\nsubstantial computational and memory challenges that limit their practical\ndeployment. Quantization has emerged as a promising solution; however, its\neffectiveness is often limited by quantization errors arising from weight\ndistributions that are not quantization-friendly and the presence of activation\noutliers. To address these challenges, we introduce DBellQuant, an innovative\npost-training quantization (PTQ) framework that achieves nearly 1-bit weight\ncompression and 6-bit activation quantization with minimal performance\ndegradation. DBellQuant uses Learnable Transformation for Dual-Bell (LTDB)\nalgorithm, which transforms single-bell weight distributions into dual-bell\nforms to reduce binarization errors and applies inverse transformations to\nsmooth activations. DBellQuant sets a new state-of-the-art by preserving\nsuperior model performance under aggressive weight and activation quantization.\nFor example, on the Wikitext2 dataset, DBellQuant achieves a perplexity of\n14.39 on LLaMA2-13B with 6-bit activation quantization, significantly\noutperforming BiLLM's 21.35 without activation quantization, underscoring its\npotential in compressing LLMs for real-world applications.", "AI": {"tldr": "DBellQuant\u662f\u4e00\u79cd\u521b\u65b0\u7684\u540e\u8bad\u7ec3\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7LTDB\u7b97\u6cd5\u5c06\u6743\u91cd\u5206\u5e03\u8f6c\u6362\u4e3a\u53cc\u949f\u5f62\u4ee5\u51cf\u5c11\u91cf\u5316\u8bef\u5dee\uff0c\u5b9e\u73b01\u4f4d\u6743\u91cd\u538b\u7f29\u548c6\u4f4d\u6fc0\u6d3b\u91cf\u5316\uff0c\u6027\u80fd\u635f\u5931\u6781\u5c0f\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9762\u4e34\u8ba1\u7b97\u548c\u5185\u5b58\u6311\u6218\uff0c\u91cf\u5316\u662f\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u91cf\u5316\u8bef\u5dee\u548c\u6fc0\u6d3b\u5f02\u5e38\u503c\u9650\u5236\u4e86\u5176\u6548\u679c\u3002", "method": "\u4f7f\u7528LTDB\u7b97\u6cd5\u5c06\u5355\u949f\u5f62\u6743\u91cd\u5206\u5e03\u8f6c\u6362\u4e3a\u53cc\u949f\u5f62\u4ee5\u51cf\u5c11\u4e8c\u503c\u5316\u8bef\u5dee\uff0c\u5e76\u5e94\u7528\u9006\u53d8\u6362\u5e73\u6ed1\u6fc0\u6d3b\u3002", "result": "\u5728Wikitext2\u6570\u636e\u96c6\u4e0a\uff0cDBellQuant\u5728LLaMA2-13B\u4e0a\u5b9e\u73b0\u4e8614.39\u7684\u56f0\u60d1\u5ea6\uff0c\u663e\u8457\u4f18\u4e8eBiLLM\u768421.35\u3002", "conclusion": "DBellQuant\u5728\u6fc0\u8fdb\u91cf\u5316\u7684\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\uff0c\u4e3aLLMs\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u538b\u7f29\u65b9\u6848\u3002"}}
{"id": "2507.01123", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.01123", "abs": "https://arxiv.org/abs/2507.01123", "authors": ["Rahul A. Burange", "Harsh K. Shinde", "Omkar Mutyalwar"], "title": "Landslide Detection and Mapping Using Deep Learning Across Multi-Source Satellite Data and Geographic Regions", "comment": "20 pages, 24 figures", "summary": "Landslides pose severe threats to infrastructure, economies, and human lives,\nnecessitating accurate detection and predictive mapping across diverse\ngeographic regions. With advancements in deep learning and remote sensing,\nautomated landslide detection has become increasingly effective. This study\npresents a comprehensive approach integrating multi-source satellite imagery\nand deep learning models to enhance landslide identification and prediction. We\nleverage Sentinel-2 multispectral data and ALOS PALSAR-derived slope and\nDigital Elevation Model (DEM) layers to capture critical environmental features\ninfluencing landslide occurrences. Various geospatial analysis techniques are\nemployed to assess the impact of terra in characteristics, vegetation cover,\nand rainfall on detection accuracy. Additionally, we evaluate the performance\nof multiple stateof-the-art deep learning segmentation models, including U-Net,\nDeepLabV3+, and Res-Net, to determine their effectiveness in landslide\ndetection. The proposed framework contributes to the development of reliable\nearly warning systems, improved disaster risk management, and sustainable\nland-use planning. Our findings provide valuable insights into the potential of\ndeep learning and multi-source remote sensing in creating robust, scalable, and\ntransferable landslide prediction models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u6e90\u536b\u661f\u5f71\u50cf\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6ed1\u5761\u68c0\u6d4b\u4e0e\u9884\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528Sentinel-2\u548cALOS PALSAR\u6570\u636e\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6548\u679c\u3002", "motivation": "\u6ed1\u5761\u5bf9\u57fa\u7840\u8bbe\u65bd\u3001\u7ecf\u6d4e\u548c\u4eba\u7c7b\u751f\u547d\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u9700\u8981\u51c6\u786e\u68c0\u6d4b\u548c\u9884\u6d4b\u3002", "method": "\u6574\u5408\u591a\u6e90\u536b\u661f\u5f71\u50cf\uff08Sentinel-2\u548cALOS PALSAR\uff09\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5982U-Net\u3001DeepLabV3+\u548cRes-Net\uff09\uff0c\u7ed3\u5408\u5730\u7406\u7a7a\u95f4\u5206\u6790\u6280\u672f\u3002", "result": "\u7814\u7a76\u4e3a\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u3001\u707e\u5bb3\u98ce\u9669\u7ba1\u7406\u548c\u53ef\u6301\u7eed\u571f\u5730\u5229\u7528\u89c4\u5212\u63d0\u4f9b\u4e86\u53ef\u9760\u6846\u67b6\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u4e0e\u591a\u6e90\u9065\u611f\u6570\u636e\u7ed3\u5408\u53ef\u6784\u5efa\u7a33\u5065\u3001\u53ef\u6269\u5c55\u4e14\u53ef\u8fc1\u79fb\u7684\u6ed1\u5761\u9884\u6d4b\u6a21\u578b\u3002"}}
{"id": "2507.01259", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01259", "abs": "https://arxiv.org/abs/2507.01259", "authors": ["Micha\u0142 Matak", "Jaros\u0142aw A. Chudziak"], "title": "GAIus: Combining Genai with Legal Clauses Retrieval for Knowledge-based Assistant", "comment": "8 pages, 2 figures, presented at ICAART 2025, in proceedings of the\n  17th International Conference on Agents and Artificial Intelligence - Volume\n  3: ICAART", "summary": "In this paper we discuss the capability of large language models to base\ntheir answer and provide proper references when dealing with legal matters of\nnon-english and non-chinese speaking country. We discuss the history of legal\ninformation retrieval, the difference between case law and statute law, its\nimpact on the legal tasks and analyze the latest research in this field. Basing\non that background we introduce gAIus, the architecture of the cognitive\nLLM-based agent, whose responses are based on the knowledge retrieved from\ncertain legal act, which is Polish Civil Code. We propose a retrieval mechanism\nwhich is more explainable, human-friendly and achieves better results than\nembedding-based approaches. To evaluate our method we create special dataset\nbased on single-choice questions from entrance exams for law apprenticeships\nconducted in Poland. The proposed architecture critically leveraged the\nabilities of used large language models, improving the gpt-3.5-turbo-0125 by\n419%, allowing it to beat gpt-4o and lifting gpt-4o-mini score from 31% to 86%.\nAt the end of our paper we show the possible future path of research and\npotential applications of our findings.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u975e\u82f1\u8bed\u548c\u975e\u4e2d\u6587\u56fd\u5bb6\u7684\u6cd5\u5f8b\u95ee\u9898\u65f6\u63d0\u4f9b\u51c6\u786e\u7b54\u6848\u548c\u5f15\u7528\u7684\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce2\u5170\u6c11\u6cd5\u5178\u7684\u68c0\u7d22\u673a\u5236gAIus\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6cd5\u5f8b\u4fe1\u606f\u68c0\u7d22\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u975e\u82f1\u8bed\u548c\u975e\u4e2d\u6587\u8bed\u5883\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5206\u6790\u6cd5\u5f8b\u4fe1\u606f\u68c0\u7d22\u5386\u53f2\u3001\u6848\u4f8b\u6cd5\u4e0e\u6210\u6587\u6cd5\u7684\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u6ce2\u5170\u6c11\u6cd5\u5178\u7684\u68c0\u7d22\u673a\u5236gAIus\uff0c\u5176\u6bd4\u5d4c\u5165\u65b9\u6cd5\u66f4\u53ef\u89e3\u91ca\u4e14\u6548\u679c\u66f4\u597d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cgAIus\u663e\u8457\u63d0\u5347\u4e86gpt-3.5-turbo-0125\u7684\u6027\u80fd\uff08\u63d0\u5347419%\uff09\uff0c\u5e76\u8ba9gpt-4o-mini\u7684\u5f97\u5206\u4ece31%\u63d0\u9ad8\u523086%\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fagAIus\u67b6\u6784\u7684\u6210\u529f\u5e94\u7528\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u6f5c\u5728\u7684\u6cd5\u5f8b\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.01376", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01376", "abs": "https://arxiv.org/abs/2507.01376", "authors": ["Yinwang Ren", "Yangyang Liu", "Tang Ji", "Xun Xu"], "title": "AI Agents and Agentic AI-Navigating a Plethora of Concepts for Future Manufacturing", "comment": "Submitted to JMS(March 2025)", "summary": "AI agents are autonomous systems designed to perceive, reason, and act within\ndynamic environments. With the rapid advancements in generative AI (GenAI),\nlarge language models (LLMs) and multimodal large language models (MLLMs) have\nsignificantly improved AI agents' capabilities in semantic comprehension,\ncomplex reasoning, and autonomous decision-making. At the same time, the rise\nof Agentic AI highlights adaptability and goal-directed autonomy in dynamic and\ncomplex environments. LLMs-based AI Agents (LLM-Agents), MLLMs-based AI Agents\n(MLLM-Agents), and Agentic AI contribute to expanding AI's capabilities in\ninformation processing, environmental perception, and autonomous\ndecision-making, opening new avenues for smart manufacturing. However, the\ndefinitions, capability boundaries, and practical applications of these\nemerging AI paradigms in smart manufacturing remain unclear. To address this\ngap, this study systematically reviews the evolution of AI and AI agent\ntechnologies, examines the core concepts and technological advancements of\nLLM-Agents, MLLM-Agents, and Agentic AI, and explores their potential\napplications in and integration into manufacturing, along with the potential\nchallenges they may face.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8e\u751f\u6210\u5f0fAI\uff08GenAI\uff09\u7684AI\u4ee3\u7406\uff08\u5982LLM-Agents\u3001MLLM-Agents\u548cAgentic AI\uff09\u5728\u667a\u80fd\u5236\u9020\u4e2d\u7684\u6f5c\u529b\u4e0e\u6311\u6218\uff0c\u7cfb\u7edf\u56de\u987e\u4e86\u5176\u6280\u672f\u6f14\u8fdb\u548c\u5e94\u7528\u524d\u666f\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u7684\u5feb\u901f\u53d1\u5c55\uff0cAI\u4ee3\u7406\u7684\u8bed\u4e49\u7406\u89e3\u3001\u590d\u6742\u63a8\u7406\u548c\u81ea\u4e3b\u51b3\u7b56\u80fd\u529b\u663e\u8457\u63d0\u5347\uff0c\u4f46\u5176\u5728\u667a\u80fd\u5236\u9020\u4e2d\u7684\u5b9a\u4e49\u3001\u80fd\u529b\u8fb9\u754c\u548c\u5b9e\u9645\u5e94\u7528\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u7cfb\u7edf\u56de\u987e\u4e86AI\u548cAI\u4ee3\u7406\u6280\u672f\u7684\u6f14\u8fdb\uff0c\u5206\u6790\u4e86LLM-Agents\u3001MLLM-Agents\u548cAgentic AI\u7684\u6838\u5fc3\u6982\u5ff5\u4e0e\u6280\u672f\u8fdb\u5c55\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5728\u667a\u80fd\u5236\u9020\u4e2d\u7684\u5e94\u7528\u4e0e\u6f5c\u5728\u6311\u6218\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u8fd9\u4e9b\u65b0\u5174AI\u8303\u5f0f\u5728\u667a\u80fd\u5236\u9020\u4e2d\u5177\u6709\u5e7f\u9614\u7684\u5e94\u7528\u524d\u666f\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u5b9a\u4e49\u6a21\u7cca\u3001\u80fd\u529b\u8fb9\u754c\u4e0d\u6e05\u7b49\u6311\u6218\u3002", "conclusion": "\u8bba\u6587\u4e3aAI\u4ee3\u7406\u6280\u672f\u5728\u667a\u80fd\u5236\u9020\u4e2d\u7684\u96c6\u6210\u4e0e\u5e94\u7528\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u89c6\u89d2\uff0c\u5f3a\u8c03\u4e86\u8fdb\u4e00\u6b65\u7814\u7a76\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2507.01028", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01028", "abs": "https://arxiv.org/abs/2507.01028", "authors": ["Jean Ponce", "Martial Hebert", "Basile Terver"], "title": "Dual Perspectives on Non-Contrastive Self-Supervised Learning", "comment": null, "summary": "The objective of non-contrastive approaches to self-supervised learning is to\ntrain on pairs of different views of the data an encoder and a predictor that\nminimize the mean discrepancy between the code predicted from the embedding of\nthe first view and the embedding of the second one. In this setting, the stop\ngradient and exponential moving average iterative procedures are commonly used\nto avoid representation collapse, with excellent performance in downstream\nsupervised applications. This presentation investigates these procedures from\nthe dual theoretical viewpoints of optimization and dynamical systems. We first\nshow that, in general, although they do not optimize the original objective, or\nfor that matter, any other smooth function, they do avoid collapse. Following\nTian et al. [2021], but without any of the extra assumptions used in their\nproofs, we then show using a dynamical system perspective that, in the linear\ncase, minimizing the original objective function without the use of a stop\ngradient or exponential moving average always leads to collapse. Conversely, we\nfinally show that the limit points of the dynamical systems associated with\nthese two procedures are, in general, asymptotically stable equilibria, with no\nrisk of degenerating to trivial solutions.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\u975e\u5bf9\u6bd4\u65b9\u6cd5\u7684\u4f18\u5316\u548c\u52a8\u6001\u7cfb\u7edf\u89c6\u89d2\uff0c\u8bc1\u660e\u4e86\u505c\u6b62\u68af\u5ea6\u548c\u6307\u6570\u79fb\u52a8\u5e73\u5747\u65b9\u6cd5\u80fd\u907f\u514d\u8868\u793a\u5d29\u6e83\uff0c\u5e76\u5728\u7ebf\u6027\u60c5\u51b5\u4e0b\u5206\u6790\u4e86\u5176\u7a33\u5b9a\u6027\u3002", "motivation": "\u63a2\u8ba8\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\u975e\u5bf9\u6bd4\u65b9\u6cd5\u7684\u4f18\u5316\u548c\u52a8\u6001\u7cfb\u7edf\u884c\u4e3a\uff0c\u7406\u89e3\u505c\u6b62\u68af\u5ea6\u548c\u6307\u6570\u79fb\u52a8\u5e73\u5747\u5982\u4f55\u907f\u514d\u8868\u793a\u5d29\u6e83\u3002", "method": "\u4ece\u4f18\u5316\u548c\u52a8\u6001\u7cfb\u7edf\u7406\u8bba\u89d2\u5ea6\u5206\u6790\u505c\u6b62\u68af\u5ea6\u548c\u6307\u6570\u79fb\u52a8\u5e73\u5747\u65b9\u6cd5\uff0c\u8bc1\u660e\u5176\u907f\u514d\u5d29\u6e83\u7684\u673a\u5236\u3002", "result": "\u505c\u6b62\u68af\u5ea6\u548c\u6307\u6570\u79fb\u52a8\u5e73\u5747\u65b9\u6cd5\u867d\u4e0d\u4f18\u5316\u539f\u59cb\u76ee\u6807\u51fd\u6570\uff0c\u4f46\u80fd\u907f\u514d\u5d29\u6e83\uff1b\u7ebf\u6027\u60c5\u51b5\u4e0b\uff0c\u539f\u59cb\u76ee\u6807\u51fd\u6570\u4f1a\u5bfc\u81f4\u5d29\u6e83\u3002", "conclusion": "\u505c\u6b62\u68af\u5ea6\u548c\u6307\u6570\u79fb\u52a8\u5e73\u5747\u65b9\u6cd5\u7684\u6781\u9650\u70b9\u662f\u6e10\u8fd1\u7a33\u5b9a\u7684\u5e73\u8861\u70b9\uff0c\u4e0d\u4f1a\u9000\u5316\u5230\u5e73\u51e1\u89e3\u3002"}}
{"id": "2507.01163", "categories": ["cs.CV", "q-bio.CB", "q-bio.QM", "I.4.7"], "pdf": "https://arxiv.org/pdf/2507.01163", "abs": "https://arxiv.org/abs/2507.01163", "authors": ["Al\u00e1n F. Mu\u00f1oz", "Tim Treis", "Alexandr A. Kalinin", "Shatavisha Dasgupta", "Fabian Theis", "Anne E. Carpenter", "Shantanu Singh"], "title": "cp_measure: API-first feature extraction for image-based profiling workflows", "comment": "10 pages, 4 figures, 4 supplementary figures. CODEML Workshop paper\n  accepted (non-archival), as a part of ICML2025 events", "summary": "Biological image analysis has traditionally focused on measuring specific\nvisual properties of interest for cells or other entities. A complementary\nparadigm gaining increasing traction is image-based profiling - quantifying\nmany distinct visual features to form comprehensive profiles which may reveal\nhidden patterns in cellular states, drug responses, and disease mechanisms.\nWhile current tools like CellProfiler can generate these feature sets, they\npose significant barriers to automated and reproducible analyses, hindering\nmachine learning workflows. Here we introduce cp_measure, a Python library that\nextracts CellProfiler's core measurement capabilities into a modular, API-first\ntool designed for programmatic feature extraction. We demonstrate that\ncp_measure features retain high fidelity with CellProfiler features while\nenabling seamless integration with the scientific Python ecosystem. Through\napplications to 3D astrocyte imaging and spatial transcriptomics, we showcase\nhow cp_measure enables reproducible, automated image-based profiling pipelines\nthat scale effectively for machine learning applications in computational\nbiology.", "AI": {"tldr": "cp_measure\u662f\u4e00\u4e2aPython\u5e93\uff0c\u5c06CellProfiler\u7684\u6838\u5fc3\u6d4b\u91cf\u529f\u80fd\u6a21\u5757\u5316\uff0c\u4fbf\u4e8e\u7a0b\u5e8f\u5316\u7279\u5f81\u63d0\u53d6\uff0c\u652f\u6301\u673a\u5668\u5b66\u4e60\u548c\u53ef\u91cd\u590d\u7684\u751f\u7269\u56fe\u50cf\u5206\u6790\u3002", "motivation": "\u4f20\u7edf\u751f\u7269\u56fe\u50cf\u5206\u6790\u5de5\u5177\uff08\u5982CellProfiler\uff09\u5728\u81ea\u52a8\u5316\u548c\u53ef\u91cd\u590d\u6027\u65b9\u9762\u5b58\u5728\u969c\u788d\uff0c\u963b\u788d\u4e86\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u6d41\u3002cp_measure\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f00\u53d1cp_measure\u5e93\uff0c\u63d0\u53d6CellProfiler\u7684\u6838\u5fc3\u6d4b\u91cf\u529f\u80fd\uff0c\u8bbe\u8ba1\u4e3a\u6a21\u5757\u5316\u3001API\u4f18\u5148\u7684\u5de5\u5177\u3002", "result": "cp_measure\u7684\u7279\u5f81\u4e0eCellProfiler\u9ad8\u5ea6\u4e00\u81f4\uff0c\u5e76\u80fd\u65e0\u7f1d\u96c6\u6210\u5230Python\u751f\u6001\u7cfb\u7edf\u4e2d\uff0c\u9002\u7528\u4e8e3D\u661f\u5f62\u80f6\u8d28\u7ec6\u80de\u6210\u50cf\u548c\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u3002", "conclusion": "cp_measure\u4e3a\u8ba1\u7b97\u751f\u7269\u5b66\u4e2d\u7684\u673a\u5668\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u81ea\u52a8\u5316\u548c\u53ef\u91cd\u590d\u7684\u56fe\u50cf\u5206\u6790\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01278", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01278", "abs": "https://arxiv.org/abs/2507.01278", "authors": ["Cindy Lie Tabuse", "David Restepo", "Carolina Gracitelli", "Fernando Korn Malerbi", "Caio Regatieri", "Luis Filipe Nakayama"], "title": "Evaluating Large Language Models for Multimodal Simulated Ophthalmic Decision-Making in Diabetic Retinopathy and Glaucoma Screening", "comment": null, "summary": "Large language models (LLMs) can simulate clinical reasoning based on natural\nlanguage prompts, but their utility in ophthalmology is largely unexplored.\nThis study evaluated GPT-4's ability to interpret structured textual\ndescriptions of retinal fundus photographs and simulate clinical decisions for\ndiabetic retinopathy (DR) and glaucoma screening, including the impact of\nadding real or synthetic clinical metadata. We conducted a retrospective\ndiagnostic validation study using 300 annotated fundus images. GPT-4 received\nstructured prompts describing each image, with or without patient metadata. The\nmodel was tasked with assigning an ICDR severity score, recommending DR\nreferral, and estimating the cup-to-disc ratio for glaucoma referral.\nPerformance was evaluated using accuracy, macro and weighted F1 scores, and\nCohen's kappa. McNemar's test and change rate analysis were used to assess the\ninfluence of metadata. GPT-4 showed moderate performance for ICDR\nclassification (accuracy 67.5%, macro F1 0.33, weighted F1 0.67, kappa 0.25),\ndriven mainly by correct identification of normal cases. Performance improved\nin the binary DR referral task (accuracy 82.3%, F1 0.54, kappa 0.44). For\nglaucoma referral, performance was poor across all settings (accuracy ~78%, F1\n<0.04, kappa <0.03). Metadata inclusion did not significantly affect outcomes\n(McNemar p > 0.05), and predictions remained consistent across conditions.\nGPT-4 can simulate basic ophthalmic decision-making from structured prompts but\nlacks precision for complex tasks. While not suitable for clinical use, LLMs\nmay assist in education, documentation, or image annotation workflows in\nophthalmology.", "AI": {"tldr": "GPT-4\u5728\u773c\u79d1\u4e2d\u6a21\u62df\u4e34\u5e8a\u51b3\u7b56\u7684\u80fd\u529b\u6709\u9650\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u53ef\u80fd\u9002\u7528\u4e8e\u6559\u80b2\u6216\u6587\u6863\u8f85\u52a9\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u773c\u79d1\u4e2d\u6a21\u62df\u4e34\u5e8a\u63a8\u7406\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\uff08DR\uff09\u548c\u9752\u5149\u773c\u7b5b\u67e5\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u4f7f\u7528300\u5f20\u6807\u6ce8\u7684\u773c\u5e95\u56fe\u50cf\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u63cf\u8ff0\u56fe\u50cf\uff0c\u8bc4\u4f30GPT-4\u5728ICDR\u5206\u7c7b\u3001DR\u8f6c\u8bca\u548c\u9752\u5149\u773c\u8f6c\u8bca\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5206\u6790\u4e34\u5e8a\u5143\u6570\u636e\u7684\u5f71\u54cd\u3002", "result": "GPT-4\u5728ICDR\u5206\u7c7b\u4e2d\u8868\u73b0\u4e2d\u7b49\uff08\u51c6\u786e\u738767.5%\uff09\uff0c\u5728DR\u8f6c\u8bca\u4efb\u52a1\u4e2d\u8868\u73b0\u8f83\u597d\uff08\u51c6\u786e\u738782.3%\uff09\uff0c\u4f46\u5728\u9752\u5149\u773c\u8f6c\u8bca\u4e2d\u8868\u73b0\u5dee\uff08\u51c6\u786e\u7387\u7ea678%\uff09\u3002\u5143\u6570\u636e\u5bf9\u7ed3\u679c\u65e0\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "GPT-4\u53ef\u4ece\u7ed3\u6784\u5316\u63d0\u793a\u4e2d\u6a21\u62df\u57fa\u672c\u773c\u79d1\u51b3\u7b56\uff0c\u4f46\u7f3a\u4e4f\u590d\u6742\u4efb\u52a1\u7684\u7cbe\u786e\u6027\uff0c\u4e0d\u9002\u5408\u4e34\u5e8a\u4f7f\u7528\uff0c\u53ef\u80fd\u9002\u7528\u4e8e\u6559\u80b2\u6216\u6587\u6863\u8f85\u52a9\u3002"}}
{"id": "2507.01410", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01410", "abs": "https://arxiv.org/abs/2507.01410", "authors": ["Abeer Dyoub", "Francesca A. Lisi"], "title": "A Fuzzy Approach to the Specification, Verification and Validation of Risk-Based Ethical Decision Making Models", "comment": null, "summary": "The ontological and epistemic complexities inherent in the moral domain make\nit challenging to establish clear standards for evaluating the performance of a\nmoral machine. In this paper, we present a formal method to describe Ethical\nDecision Making models based on ethical risk assessment. Then, we show how\nthese models that are specified as fuzzy rules can be verified and validated\nusing fuzzy Petri nets. A case study from the medical field is considered to\nillustrate the proposed approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f26\u7406\u98ce\u9669\u8bc4\u4f30\u7684\u6a21\u7cca\u89c4\u5219\u6a21\u578b\uff0c\u7528\u4e8e\u63cf\u8ff0\u548c\u9a8c\u8bc1\u9053\u5fb7\u51b3\u7b56\uff0c\u5e76\u901a\u8fc7\u533b\u7597\u9886\u57df\u7684\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u9053\u5fb7\u9886\u57df\u7684\u590d\u6742\u6027\u548c\u6a21\u7cca\u6027\u4f7f\u5f97\u8bc4\u4f30\u9053\u5fb7\u673a\u5668\u7684\u6027\u80fd\u6807\u51c6\u96be\u4ee5\u660e\u786e\u3002", "method": "\u4f7f\u7528\u6a21\u7cca\u89c4\u5219\u548c\u6a21\u7ccaPetri\u7f51\u6765\u5f62\u5f0f\u5316\u63cf\u8ff0\u3001\u9a8c\u8bc1\u548c\u9a8c\u8bc1\u4f26\u7406\u51b3\u7b56\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u533b\u7597\u9886\u57df\u7684\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9053\u5fb7\u51b3\u7b56\u6a21\u578b\u7684\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9014\u5f84\uff0c\u9002\u7528\u4e8e\u590d\u6742\u4f26\u7406\u95ee\u9898\u7684\u5206\u6790\u3002"}}
{"id": "2507.01029", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01029", "abs": "https://arxiv.org/abs/2507.01029", "authors": ["Junjie Zhou", "Yingli Zuo", "Shichang Feng", "Peng Wan", "Qi Zhu", "Daoqiang Zhang", "Wei Shao"], "title": "PathCoT: Chain-of-Thought Prompting for Zero-shot Pathology Visual Reasoning", "comment": null, "summary": "With the development of generative artificial intelligence and instruction\ntuning techniques, multimodal large language models (MLLMs) have made\nimpressive progress on general reasoning tasks. Benefiting from the\nchain-of-thought (CoT) methodology, MLLMs can solve the visual reasoning\nproblem step-by-step. However, existing MLLMs still face significant challenges\nwhen applied to pathology visual reasoning tasks: (1) LLMs often underperforms\nbecause they lack domain-specific information, which can lead to model\nhallucinations. (2) The additional reasoning steps in CoT may introduce errors,\nleading to the divergence of answers. To address these limitations, we propose\nPathCoT, a novel zero-shot CoT prompting method which integrates the pathology\nexpert-knowledge into the reasoning process of MLLMs and incorporates\nself-evaluation to mitigate divergence of answers. Specifically, PathCoT guides\nthe MLLM with prior knowledge to perform as pathology experts, and provides\ncomprehensive analysis of the image with their domain-specific knowledge. By\nincorporating the experts' knowledge, PathCoT can obtain the answers with CoT\nreasoning. Furthermore, PathCoT incorporates a self-evaluation step that\nassesses both the results generated directly by MLLMs and those derived through\nCoT, finally determining the reliable answer. The experimental results on the\nPathMMU dataset demonstrate the effectiveness of our method on pathology visual\nunderstanding and reasoning.", "AI": {"tldr": "\u63d0\u51faPathCoT\u65b9\u6cd5\uff0c\u7ed3\u5408\u75c5\u7406\u5b66\u4e13\u5bb6\u77e5\u8bc6\u548c\u81ea\u8bc4\u4f30\uff0c\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u75c5\u7406\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709MLLMs\u5728\u75c5\u7406\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u9886\u57df\u77e5\u8bc6\u4e14\u63a8\u7406\u6b65\u9aa4\u53ef\u80fd\u5f15\u5165\u9519\u8bef\u3002", "method": "PathCoT\u6574\u5408\u75c5\u7406\u5b66\u4e13\u5bb6\u77e5\u8bc6\u5230\u63a8\u7406\u8fc7\u7a0b\uff0c\u5e76\u52a0\u5165\u81ea\u8bc4\u4f30\u6b65\u9aa4\u4ee5\u51cf\u5c11\u7b54\u6848\u504f\u5dee\u3002", "result": "\u5728PathMMU\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "PathCoT\u901a\u8fc7\u4e13\u5bb6\u77e5\u8bc6\u548c\u81ea\u8bc4\u4f30\u663e\u8457\u63d0\u5347\u4e86\u75c5\u7406\u89c6\u89c9\u63a8\u7406\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2507.01182", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01182", "abs": "https://arxiv.org/abs/2507.01182", "authors": ["Zhuo Su", "Li Liu", "Matthias M\u00fcller", "Jiehua Zhang", "Diana Wofk", "Ming-Ming Cheng", "Matti Pietik\u00e4inen"], "title": "Rapid Salient Object Detection with Difference Convolutional Neural Networks", "comment": "16 pages, accepted in TPAMI", "summary": "This paper addresses the challenge of deploying salient object detection\n(SOD) on resource-constrained devices with real-time performance. While recent\nadvances in deep neural networks have improved SOD, existing top-leading models\nare computationally expensive. We propose an efficient network design that\ncombines traditional wisdom on SOD and the representation power of modern CNNs.\nLike biologically-inspired classical SOD methods relying on computing contrast\ncues to determine saliency of image regions, our model leverages Pixel\nDifference Convolutions (PDCs) to encode the feature contrasts. Differently,\nPDCs are incorporated in a CNN architecture so that the valuable contrast cues\nare extracted from rich feature maps. For efficiency, we introduce a difference\nconvolution reparameterization (DCR) strategy that embeds PDCs into standard\nconvolutions, eliminating computation and parameters at inference.\nAdditionally, we introduce SpatioTemporal Difference Convolution (STDC) for\nvideo SOD, enhancing the standard 3D convolution with spatiotemporal contrast\ncapture. Our models, SDNet for image SOD and STDNet for video SOD, achieve\nsignificant improvements in efficiency-accuracy trade-offs. On a Jetson Orin\ndevice, our models with $<$ 1M parameters operate at 46 FPS and 150 FPS on\nstreamed images and videos, surpassing the second-best lightweight models in\nour experiments by more than $2\\times$ and $3\\times$ in speed with superior\naccuracy. Code will be available at https://github.com/hellozhuo/stdnet.git.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7f51\u7edc\u8bbe\u8ba1\uff0c\u7ed3\u5408\u4f20\u7edf\u663e\u8457\u76ee\u6807\u68c0\u6d4b\uff08SOD\uff09\u4e0e\u73b0\u4ee3CNN\uff0c\u901a\u8fc7\u50cf\u7d20\u5dee\u5f02\u5377\u79ef\uff08PDCs\uff09\u548c\u5dee\u5f02\u5377\u79ef\u91cd\u53c2\u6570\u5316\uff08DCR\uff09\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548\u90e8\u7f72\u3002", "method": "\u7ed3\u5408\u4f20\u7edfSOD\u7684\u5bf9\u6bd4\u7ebf\u7d22\u4e0e\u73b0\u4ee3CNN\uff0c\u4f7f\u7528PDCs\u7f16\u7801\u7279\u5f81\u5bf9\u6bd4\uff0c\u5e76\u901a\u8fc7DCR\u7b56\u7565\u5d4c\u5165\u6807\u51c6\u5377\u79ef\u4ee5\u51cf\u5c11\u8ba1\u7b97\u91cf\u3002\u9488\u5bf9\u89c6\u9891SOD\uff0c\u5f15\u5165\u65f6\u7a7a\u5dee\u5f02\u5377\u79ef\uff08STDC\uff09\u3002", "result": "\u6a21\u578bSDNet\u548cSTDNet\u5728Jetson Orin\u8bbe\u5907\u4e0a\u5206\u522b\u4ee546 FPS\u548c150 FPS\u8fd0\u884c\uff0c\u53c2\u6570\u5c11\u4e8e1M\uff0c\u901f\u5ea6\u548c\u51c6\u786e\u6027\u5747\u4f18\u4e8e\u5176\u4ed6\u8f7b\u91cf\u7ea7\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u663e\u8457\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u7684\u5b9e\u65f6SOD\u4efb\u52a1\u3002"}}
{"id": "2507.01281", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01281", "abs": "https://arxiv.org/abs/2507.01281", "authors": ["Juan Chen", "Baolong Bi", "Wei Zhang", "Jingyan Sui", "Xiaofei Zhu", "Yuanzhuo Wang", "Lingrui Mei", "Shenghua Liu"], "title": "Rethinking All Evidence: Enhancing Trustworthy Retrieval-Augmented Generation via Conflict-Driven Summarization", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nintegrating their parametric knowledge with external retrieved content.\nHowever, knowledge conflicts caused by internal inconsistencies or noisy\nretrieved content can severely undermine the generation reliability of RAG\nsystems.In this work, we argue that LLMs should rethink all evidence, including\nboth retrieved content and internal knowledge, before generating responses.We\npropose CARE-RAG (Conflict-Aware and Reliable Evidence for RAG), a novel\nframework that improves trustworthiness through Conflict-Driven Summarization\nof all available evidence.CARE-RAG first derives parameter-aware evidence by\ncomparing parameter records to identify diverse internal perspectives. It then\nrefines retrieved evidences to produce context-aware evidence, removing\nirrelevant or misleading content. To detect and summarize conflicts, we distill\na 3B LLaMA3.2 model to perform conflict-driven summarization, enabling reliable\nsynthesis across multiple sources.To further ensure evaluation integrity, we\nintroduce a QA Repair step to correct outdated or ambiguous benchmark\nanswers.Experiments on revised QA datasets with retrieval data show that\nCARE-RAG consistently outperforms strong RAG baselines, especially in scenarios\nwith noisy or conflicting evidence.", "AI": {"tldr": "CARE-RAG\u6846\u67b6\u901a\u8fc7\u51b2\u7a81\u9a71\u52a8\u7684\u8bc1\u636e\u603b\u7ed3\u63d0\u5347RAG\u7cfb\u7edf\u7684\u53ef\u9760\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u89e3\u51b3RAG\u7cfb\u7edf\u4e2d\u56e0\u77e5\u8bc6\u51b2\u7a81\u5bfc\u81f4\u7684\u751f\u6210\u4e0d\u53ef\u9760\u95ee\u9898\u3002", "method": "\u63d0\u51faCARE-RAG\u6846\u67b6\uff0c\u5305\u62ec\u53c2\u6570\u611f\u77e5\u8bc1\u636e\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u8bc1\u636e\u3001\u51b2\u7a81\u9a71\u52a8\u603b\u7ed3\u548cQA\u4fee\u590d\u6b65\u9aa4\u3002", "result": "\u5728\u542b\u566a\u58f0\u6216\u51b2\u7a81\u8bc1\u636e\u7684\u573a\u666f\u4e2d\uff0cCARE-RAG\u8868\u73b0\u4f18\u4e8e\u73b0\u6709RAG\u57fa\u7ebf\u3002", "conclusion": "CARE-RAG\u901a\u8fc7\u91cd\u65b0\u601d\u8003\u6240\u6709\u8bc1\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86RAG\u7cfb\u7edf\u7684\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2507.01431", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01431", "abs": "https://arxiv.org/abs/2507.01431", "authors": ["Yoonseok Yang", "Minjune Kim", "Marlon Rondinelli", "Keren Shao"], "title": "Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading", "comment": "7 pages, 5 figues, 1 table", "summary": "Grading handwritten, open-ended responses remains a major bottleneck in large\nuniversity STEM courses. We introduce Pensieve (https://www.pensieve.co), an\nAI-assisted grading platform that leverages large language models (LLMs) to\ntranscribe and evaluate student work, providing instructors with rubric-aligned\nscores, transcriptions, and confidence ratings. Unlike prior tools that focus\nnarrowly on specific tasks like transcription or rubric generation, Pensieve\nsupports the entire grading pipeline-from scanned student submissions to final\nfeedback-within a human-in-the-loop interface.\n  Pensieve has been deployed in real-world courses at over 20 institutions and\nhas graded more than 300,000 student responses. We present system details and\nempirical results across four core STEM disciplines: Computer Science,\nMathematics, Physics, and Chemistry. Our findings show that Pensieve reduces\ngrading time by an average of 65%, while maintaining a 95.4% agreement rate\nwith instructor-assigned grades for high-confidence predictions.", "AI": {"tldr": "Pensieve\u662f\u4e00\u4e2aAI\u8f85\u52a9\u8bc4\u5206\u5e73\u53f0\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8f6c\u5f55\u548c\u8bc4\u4f30\u5b66\u751f\u624b\u5199\u4f5c\u4e1a\uff0c\u663e\u8457\u51cf\u5c11\u8bc4\u5206\u65f6\u95f4\u5e76\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21STEM\u8bfe\u7a0b\u4e2d\u624b\u5199\u5f00\u653e\u6027\u7b54\u6848\u8bc4\u5206\u7684\u74f6\u9888\u95ee\u9898\u3002", "method": "\u901a\u8fc7LLMs\u8f6c\u5f55\u548c\u8bc4\u4f30\u5b66\u751f\u4f5c\u4e1a\uff0c\u63d0\u4f9b\u4e0e\u8bc4\u5206\u6807\u51c6\u5bf9\u9f50\u7684\u5206\u6570\u3001\u8f6c\u5f55\u548c\u7f6e\u4fe1\u5ea6\u8bc4\u5206\uff0c\u652f\u6301\u4ece\u626b\u63cf\u63d0\u4ea4\u5230\u6700\u7ec8\u53cd\u9988\u7684\u5b8c\u6574\u8bc4\u5206\u6d41\u7a0b\u3002", "result": "\u572820\u591a\u6240\u673a\u6784\u7684\u5b9e\u9645\u8bfe\u7a0b\u4e2d\u90e8\u7f72\uff0c\u8bc4\u5206\u8d85\u8fc730\u4e07\u4efd\u5b66\u751f\u4f5c\u4e1a\uff0c\u5e73\u5747\u51cf\u5c1165%\u7684\u8bc4\u5206\u65f6\u95f4\uff0c\u9ad8\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u4e0e\u6559\u5e08\u8bc4\u5206\u7684\u4e00\u81f4\u7387\u8fbe95.4%\u3002", "conclusion": "Pensieve\u80fd\u6709\u6548\u63d0\u5347\u8bc4\u5206\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u591a\u5b66\u79d1STEM\u8bfe\u7a0b\u3002"}}
{"id": "2507.01030", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01030", "abs": "https://arxiv.org/abs/2507.01030", "authors": ["Reza Lotfi Navaei", "Mohammad Safarzadeh", "Seyed Mohammad Jafar Sobhani"], "title": "Optimizing Flamelet Generated Manifold Models: A Machine Learning Performance Study", "comment": "It has been submitted to ASME Journal of Heat and Mass Transfer", "summary": "In chemistry tabulations and Flamelet combustion models, the Flamelet\nGenerated Manifold (FGM) is recognized for its precision and physical\nrepresentation. The practical implementation of FGM requires a significant\nallocation of memory resources. FGM libraries are developed specifically for a\nspecific fuel and subsequently utilized for all numerical problems using\nmachine learning techniques. This research aims to develop libraries of Laminar\nFGM utilizing machine learning algorithms for application in combustion\nsimulations of methane fuel. This study employs four Machine Learning\nalgorithms to regenerate Flamelet libraries, based on an understanding of data\nsources, techniques, and data-driven concepts. 1. Multi-Layer Perceptron; 2.\nRandom Forest; 3. Linear Regression; 4. Support Vector Machine. Seven libraries\nwere identified as appropriate for constructing a database for training machine\nlearning models, giving an error rate of 2.30%. The default architectures of\neach method were evaluated to determine the optimal approach, leading to the\nselection of the MLP method as the primary choice. The method was enhanced\nthrough hyperparameter tuning to improve accuracy. The quantity of hidden\nlayers and neurons significantly influences method performance. The optimal\nmodel, comprising four hidden layers with 10, 15, 20, and 25 neurons\nrespectively, achieved an accuracy of 99.81%.", "AI": {"tldr": "\u7814\u7a76\u5229\u7528\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff08MLP\u3001\u968f\u673a\u68ee\u6797\u3001\u7ebf\u6027\u56de\u5f52\u3001SVM\uff09\u91cd\u5efa\u7532\u70f7\u71c3\u6599\u71c3\u70e7\u6a21\u62df\u7684FGM\u5e93\uff0cMLP\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\uff0c\u4f18\u5316\u540e\u51c6\u786e\u7387\u8fbe99.81%\u3002", "motivation": "FGM\u5728\u71c3\u70e7\u6a21\u578b\u4e2d\u7cbe\u5ea6\u9ad8\u4f46\u5185\u5b58\u9700\u6c42\u5927\uff0c\u9700\u9488\u5bf9\u7279\u5b9a\u71c3\u6599\u5f00\u53d1\u5e93\uff0c\u673a\u5668\u5b66\u4e60\u53ef\u63d0\u5347\u6548\u7387\u3002", "method": "\u4f7f\u7528\u56db\u79cd\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u91cd\u5efaFGM\u5e93\uff0c\u901a\u8fc7\u8d85\u53c2\u6570\u8c03\u4f18\u4f18\u5316MLP\u67b6\u6784\u3002", "result": "MLP\u65b9\u6cd5\uff08\u56db\u9690\u85cf\u5c42\uff0c10-25\u795e\u7ecf\u5143\uff09\u51c6\u786e\u738799.81%\uff0c\u8bef\u5dee\u73872.30%\u3002", "conclusion": "MLP\u662f\u91cd\u5efaFGM\u5e93\u7684\u6700\u4f73\u9009\u62e9\uff0c\u4f18\u5316\u540e\u663e\u8457\u63d0\u5347\u71c3\u70e7\u6a21\u62df\u7cbe\u5ea6\u3002"}}
{"id": "2507.01254", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01254", "abs": "https://arxiv.org/abs/2507.01254", "authors": ["Runze Cheng", "Xihang Qiu", "Ming Li", "Ye Zhang", "Chun Li", "Fei Yu"], "title": "Robust Brain Tumor Segmentation with Incomplete MRI Modalities Using H\u00f6lder Divergence and Mutual Information-Enhanced Knowledge Transfer", "comment": null, "summary": "Multimodal MRI provides critical complementary information for accurate brain\ntumor segmentation. However, conventional methods struggle when certain\nmodalities are missing due to issues such as image quality, protocol\ninconsistencies, patient allergies, or financial constraints. To address this,\nwe propose a robust single-modality parallel processing framework that achieves\nhigh segmentation accuracy even with incomplete modalities. Leveraging Holder\ndivergence and mutual information, our model maintains modality-specific\nfeatures while dynamically adjusting network parameters based on the available\ninputs. By using these divergence- and information-based loss functions, the\nframework effectively quantifies discrepancies between predictions and\nground-truth labels, resulting in consistently accurate segmentation. Extensive\nevaluations on the BraTS 2018 and BraTS 2020 datasets demonstrate superior\nperformance over existing methods in handling missing modalities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u6a21\u6001\u5e76\u884c\u5904\u7406\u7684\u9c81\u68d2\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u591a\u6a21\u6001MRI\u6570\u636e\u7f3a\u5931\u60c5\u51b5\u4e0b\u7684\u8111\u80bf\u7624\u5206\u5272\uff0c\u5229\u7528Holder\u6563\u5ea6\u548c\u4e92\u4fe1\u606f\u4fdd\u6301\u6a21\u6001\u7279\u5f81\u5e76\u52a8\u6001\u8c03\u6574\u7f51\u7edc\u53c2\u6570\uff0c\u5728BraTS\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u591a\u6a21\u6001MRI\u6570\u636e\u5e38\u56e0\u56fe\u50cf\u8d28\u91cf\u3001\u534f\u8bae\u4e0d\u4e00\u81f4\u3001\u60a3\u8005\u8fc7\u654f\u6216\u7ecf\u6d4e\u9650\u5236\u800c\u7f3a\u5931\u67d0\u4e9b\u6a21\u6001\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\uff0c\u9700\u5f00\u53d1\u9c81\u68d2\u7684\u5206\u5272\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5355\u6a21\u6001\u5e76\u884c\u5904\u7406\u6846\u67b6\uff0c\u7ed3\u5408Holder\u6563\u5ea6\u548c\u4e92\u4fe1\u606f\u8bbe\u8ba1\u635f\u5931\u51fd\u6570\uff0c\u52a8\u6001\u8c03\u6574\u7f51\u7edc\u53c2\u6570\u4ee5\u9002\u5e94\u8f93\u5165\u6a21\u6001\u7684\u53d8\u5316\u3002", "result": "\u5728BraTS 2018\u548c2020\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5904\u7406\u7f3a\u5931\u6a21\u6001\u65f6\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u6709\u6548\u5e94\u5bf9\u591a\u6a21\u6001MRI\u6570\u636e\u7f3a\u5931\u95ee\u9898\uff0c\u63d0\u5347\u5206\u5272\u51c6\u786e\u6027\u3002"}}
{"id": "2507.01297", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.01297", "abs": "https://arxiv.org/abs/2507.01297", "authors": ["Xinxi Lyu", "Michael Duan", "Rulin Shao", "Pang Wei Koh", "Sewon Min"], "title": "Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive Benchmarks", "comment": "33 pages, 2 figures, 27 tables", "summary": "Retrieval-augmented Generation (RAG) has primarily been studied in limited\nsettings, such as factoid question answering; more challenging,\nreasoning-intensive benchmarks have seen limited success from minimal RAG. In\nthis work, we challenge this prevailing view on established,\nreasoning-intensive benchmarks: MMLU, MMLU Pro, AGI Eval, GPQA, and MATH. We\nidentify a key missing component in prior work: a usable, web-scale datastore\naligned with the breadth of pretraining data. To this end, we introduce\nCompactDS: a diverse, high-quality, web-scale datastore that achieves high\nretrieval accuracy and subsecond latency on a single-node. The key insights are\n(1) most web content can be filtered out without sacrificing coverage, and a\ncompact, high-quality subset is sufficient; and (2) combining in-memory\napproximate nearest neighbor (ANN) retrieval and on-disk exact search balances\nspeed and recall. Using CompactDS, we show that a minimal RAG pipeline achieves\nconsistent accuracy improvements across all benchmarks and model sizes\n(8B--70B), with relative gains of 10% on MMLU, 33% on MMLU Pro, 14% on GPQA,\nand 19% on MATH. No single data source suffices alone, highlighting the\nimportance of diversity of sources (web crawls, curated math, academic papers,\ntextbooks). Finally, we show that our carefully designed in-house datastore\nmatches or outperforms web search engines such as Google Search, as well as\nrecently proposed, complex agent-based RAG systems--all while maintaining\nsimplicity, reproducibility, and self-containment. We release CompactDS and our\nretrieval pipeline, supporting future research exploring retrieval-based AI\nsystems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCompactDS\uff0c\u4e00\u4e2a\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u7684\u7f51\u7edc\u89c4\u6a21\u6570\u636e\u5b58\u50a8\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5728\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709RAG\u5728\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u7f3a\u4e4f\u4e0e\u9884\u8bad\u7ec3\u6570\u636e\u5e7f\u5ea6\u5bf9\u9f50\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u5b58\u50a8\u3002", "method": "\u8bbe\u8ba1CompactDS\uff0c\u7ed3\u5408\u5185\u5b58\u8fd1\u4f3c\u6700\u8fd1\u90bb\u68c0\u7d22\u548c\u78c1\u76d8\u7cbe\u786e\u641c\u7d22\uff0c\u5b9e\u73b0\u9ad8\u6548\u68c0\u7d22\u3002", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u5bc6\u96c6\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCompactDS\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u76f8\u5bf9\u589e\u76ca\u8fbe10%-33%\u3002", "conclusion": "CompactDS\u8bc1\u660e\u4e86\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u6570\u636e\u5b58\u50a8\u7684\u91cd\u8981\u6027\uff0c\u4e14\u5176\u6027\u80fd\u4f18\u4e8e\u590d\u6742\u4ee3\u7406\u7cfb\u7edf\uff0c\u540c\u65f6\u4fdd\u6301\u7b80\u5355\u548c\u53ef\u590d\u73b0\u6027\u3002"}}
{"id": "2507.01446", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01446", "abs": "https://arxiv.org/abs/2507.01446", "authors": ["Abd Elrahman Amer", "Magdi Amer"], "title": "Using multi-agent architecture to mitigate the risk of LLM hallucinations", "comment": null, "summary": "Improving customer service quality and response time are critical factors for\nmaintaining customer loyalty and increasing a company's market share. While\nadopting emerging technologies such as Large Language Models (LLMs) is becoming\na necessity to achieve these goals, the risk of hallucination remains a major\nchallenge. In this paper, we present a multi-agent system to handle customer\nrequests sent via SMS. This system integrates LLM based agents with fuzzy logic\nto mitigate hallucination risks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u4ee3\u7406\u7cfb\u7edf\u548c\u6a21\u7cca\u903b\u8f91\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u901a\u8fc7SMS\u5904\u7406\u5ba2\u6237\u8bf7\u6c42\uff0c\u4ee5\u51cf\u5c11LLM\u5e7b\u89c9\u98ce\u9669\u3002", "motivation": "\u63d0\u9ad8\u5ba2\u6237\u670d\u52a1\u8d28\u91cf\u548c\u54cd\u5e94\u65f6\u95f4\u662f\u7ef4\u6301\u5ba2\u6237\u5fe0\u8bda\u5ea6\u548c\u589e\u52a0\u5e02\u573a\u4efd\u989d\u7684\u5173\u952e\u56e0\u7d20\uff0c\u4f46LLM\u7684\u5e7b\u89c9\u98ce\u9669\u662f\u4e3b\u8981\u6311\u6218\u3002", "method": "\u7ed3\u5408LLM\u4ee3\u7406\u548c\u6a21\u7cca\u903b\u8f91\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u7528\u4e8e\u5904\u7406SMS\u5ba2\u6237\u8bf7\u6c42\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u51cf\u5c11LLM\u5e7b\u89c9\u98ce\u9669\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u63d0\u5347\u5ba2\u6237\u670d\u52a1\u8d28\u91cf\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01031", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.01031", "abs": "https://arxiv.org/abs/2507.01031", "authors": ["Fanchen Bu", "Kijung Shin"], "title": "PyTorch-based Geometric Learning with Non-CUDA Processing Units: Experiences from Intel Gaudi-v2 HPUs", "comment": "Conference paper: Accepted in Korea Computer Congress (KCC) 2025. The\n  library is available at https://github.com/bokveizen/gaudi-geometric-learning", "summary": "Geometric learning has emerged as a powerful paradigm for modeling\nnon-Euclidean data, especially graph-structured ones, with applications\nspanning social networks, molecular structures, knowledge graphs, and\nrecommender systems. While Nvidia's CUDA-enabled graphics processing units\n(GPUs) largely dominate the hardware landscape, emerging accelerators such as\nIntel's Gaudi Habana Processing Units (HPUs) offer competitive performance and\nenergy efficiency. However, the usage of such non-CUDA processing units\nrequires significant engineering effort and novel software adaptations. In this\nwork, we present our experiences porting PyTorch-based geometric learning\nframeworks to Gaudi-v2 HPUs. We introduce a collection of core utilities that\nrestore essential operations (e.g., scatter, sparse indexing, k-nearest\nneighbors) on Gaudi-v2 HPUs, and we consolidate sixteen guided tutorials and\neleven real-world examples with diagnostic analyses of encountered failures and\ndetailed workarounds. We collect all our experiences into a publicly accessible\nGitHub repository. Our contributions lower the barrier for researchers to\nexperiment with geometric-learning algorithms and models on non-CUDA hardware,\nproviding a foundation for further optimization and cross-platform portability.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5c06\u57fa\u4e8ePyTorch\u7684\u51e0\u4f55\u5b66\u4e60\u6846\u67b6\u79fb\u690d\u5230Gaudi-v2 HPU\u4e0a\u7684\u7ecf\u9a8c\uff0c\u63d0\u4f9b\u4e86\u6838\u5fc3\u5de5\u5177\u3001\u6559\u7a0b\u548c\u793a\u4f8b\uff0c\u964d\u4f4e\u4e86\u975eCUDA\u786c\u4ef6\u4e0a\u51e0\u4f55\u5b66\u4e60\u7684\u7814\u7a76\u95e8\u69db\u3002", "motivation": "\u51e0\u4f55\u5b66\u4e60\u5728\u975e\u6b27\u6570\u636e\u5efa\u6a21\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u975eCUDA\u786c\u4ef6\uff08\u5982Gaudi-v2 HPU\uff09\u7684\u4f7f\u7528\u9700\u8981\u5927\u91cf\u5de5\u7a0b\u52aa\u529b\u548c\u8f6f\u4ef6\u9002\u914d\u3002", "method": "\u5f00\u53d1\u6838\u5fc3\u5de5\u5177\u4ee5\u652f\u6301Gaudi-v2 HPU\u4e0a\u7684\u57fa\u672c\u64cd\u4f5c\uff0c\u5e76\u6574\u7406\u6559\u7a0b\u548c\u793a\u4f8b\uff0c\u5206\u6790\u5931\u8d25\u5e76\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u6210\u529f\u79fb\u690d\u51e0\u4f55\u5b66\u4e60\u6846\u67b6\uff0c\u63d0\u4f9b\u516c\u5f00\u7684GitHub\u8d44\u6e90\u5e93\uff0c\u652f\u6301\u7814\u7a76\u4eba\u5458\u5728\u975eCUDA\u786c\u4ef6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "conclusion": "\u672c\u6587\u7684\u5de5\u4f5c\u4e3a\u51e0\u4f55\u5b66\u4e60\u5728\u975eCUDA\u786c\u4ef6\u4e0a\u7684\u4f18\u5316\u548c\u8de8\u5e73\u53f0\u79fb\u690d\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.01255", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01255", "abs": "https://arxiv.org/abs/2507.01255", "authors": ["Xiao Liu", "Jiawei Zhang"], "title": "AIGVE-MACS: Unified Multi-Aspect Commenting and Scoring Model for AI-Generated Video Evaluation", "comment": "Working in Progress", "summary": "The rapid advancement of AI-generated video models has created a pressing\nneed for robust and interpretable evaluation frameworks. Existing metrics are\nlimited to producing numerical scores without explanatory comments, resulting\nin low interpretability and human evaluation alignment. To address those\nchallenges, we introduce AIGVE-MACS, a unified model for AI-Generated Video\nEvaluation(AIGVE), which can provide not only numerical scores but also\nmulti-aspect language comment feedback in evaluating these generated videos.\nCentral to our approach is AIGVE-BENCH 2, a large-scale benchmark comprising\n2,500 AI-generated videos and 22,500 human-annotated detailed comments and\nnumerical scores across nine critical evaluation aspects. Leveraging\nAIGVE-BENCH 2, AIGVE-MACS incorporates recent Vision-Language Models with a\nnovel token-wise weighted loss and a dynamic frame sampling strategy to better\nalign with human evaluators. Comprehensive experiments across supervised and\nzero-shot benchmarks demonstrate that AIGVE-MACS achieves state-of-the-art\nperformance in both scoring correlation and comment quality, significantly\noutperforming prior baselines including GPT-4o and VideoScore. In addition, we\nfurther showcase a multi-agent refinement framework where feedback from\nAIGVE-MACS drives iterative improvements in video generation, leading to 53.5%\nquality enhancement. This work establishes a new paradigm for comprehensive,\nhuman-aligned evaluation of AI-generated videos. We release the AIGVE-BENCH 2\nand AIGVE-MACS at https://huggingface.co/xiaoliux/AIGVE-MACS.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAIGVE-MACS\u6a21\u578b\uff0c\u7528\u4e8eAI\u751f\u6210\u89c6\u9891\u7684\u591a\u65b9\u9762\u8bc4\u4f30\uff0c\u63d0\u4f9b\u5206\u6570\u548c\u8bed\u8a00\u53cd\u9988\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709AI\u751f\u6210\u89c6\u9891\u8bc4\u4f30\u6307\u6807\u7f3a\u4e4f\u89e3\u91ca\u6027\uff0c\u4e0e\u4eba\u7c7b\u8bc4\u4ef7\u5bf9\u9f50\u4e0d\u8db3\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u7ed3\u5408AIGVE-BENCH 2\u57fa\u51c6\u548cVision-Language\u6a21\u578b\uff0c\u91c7\u7528\u52a0\u6743\u635f\u5931\u548c\u52a8\u6001\u5e27\u91c7\u6837\u7b56\u7565\u3002", "result": "AIGVE-MACS\u5728\u8bc4\u5206\u548c\u8bc4\u8bba\u8d28\u91cf\u4e0a\u8fbe\u5230\u6700\u4f18\uff0c\u89c6\u9891\u751f\u6210\u8d28\u91cf\u63d0\u534753.5%\u3002", "conclusion": "AIGVE-MACS\u4e3aAI\u751f\u6210\u89c6\u9891\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5168\u9762\u3001\u4eba\u7c7b\u5bf9\u9f50\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2507.01299", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01299", "abs": "https://arxiv.org/abs/2507.01299", "authors": ["Kai Liu", "Bowen Xu", "Shaoyu Wu", "Xin Chen", "Hao Zhou", "Yongliang Tao", "Lulu Hu"], "title": "La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation", "comment": "ICML 2025 Acceptance", "summary": "Activation sparsity can reduce the computational overhead and memory\ntransfers during the forward pass of Large Language Model (LLM) inference.\nExisting methods face limitations, either demanding time-consuming recovery\ntraining that hinders real-world adoption, or relying on empirical\nmagnitude-based pruning, which causes fluctuating sparsity and unstable\ninference speed-up. This paper introduces LaRoSA (Layerwise Rotated Sparse\nActivation), a novel method for activation sparsification designed to improve\nLLM efficiency without requiring additional training or magnitude-based\npruning. We leverage layerwise orthogonal rotations to transform input\nactivations into rotated forms that are more suitable for sparsification. By\nemploying a Top-K selection approach within the rotated activations, we achieve\nconsistent model-level sparsity and reliable wall-clock time speed-up. LaRoSA\nis effective across various sizes and types of LLMs, demonstrating minimal\nperformance degradation and robust inference acceleration. Specifically, for\nLLaMA2-7B at 40% sparsity, LaRoSA achieves a mere 0.17 perplexity gap with a\nconsistent 1.30x wall-clock time speed-up, and reduces the accuracy gap in\nzero-shot tasks compared to the dense model to just 0.54%, while surpassing\nTEAL by 1.77% and CATS by 17.14%.", "AI": {"tldr": "LaRoSA\u662f\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u57fa\u4e8e\u5e45\u503c\u526a\u679d\u7684\u6fc0\u6d3b\u7a00\u758f\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c42\u95f4\u6b63\u4ea4\u65cb\u8f6c\u548cTop-K\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u6fc0\u6d3b\u7a00\u758f\u5316\u65b9\u6cd5\u5b58\u5728\u6062\u590d\u8bad\u7ec3\u8017\u65f6\u6216\u57fa\u4e8e\u5e45\u503c\u526a\u679d\u5bfc\u81f4\u7a00\u758f\u6027\u6ce2\u52a8\u7684\u95ee\u9898\uff0cLaRoSA\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u5229\u7528\u5c42\u95f4\u6b63\u4ea4\u65cb\u8f6c\u5c06\u8f93\u5165\u6fc0\u6d3b\u8f6c\u6362\u4e3a\u66f4\u9002\u5408\u7a00\u758f\u5316\u7684\u5f62\u5f0f\uff0c\u5e76\u901a\u8fc7Top-K\u9009\u62e9\u5b9e\u73b0\u4e00\u81f4\u7684\u6a21\u578b\u7ea7\u7a00\u758f\u6027\u3002", "result": "\u5728LLaMA2-7B\u4e0a\uff0c40%\u7a00\u758f\u5ea6\u4e0b\uff0cLaRoSA\u4ec5\u589e\u52a00.17\u56f0\u60d1\u5ea6\u5dee\u8ddd\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53471.30\u500d\uff0c\u96f6\u6837\u672c\u4efb\u52a1\u51c6\u786e\u7387\u5dee\u8ddd\u7f29\u5c0f\u81f30.54%\u3002", "conclusion": "LaRoSA\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u6fc0\u6d3b\u7a00\u758f\u5316\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cdLLM\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u4e14\u6027\u80fd\u635f\u5931\u6781\u5c0f\u3002"}}
{"id": "2507.01489", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.01489", "abs": "https://arxiv.org/abs/2507.01489", "authors": ["Yanfei Zhang"], "title": "Agent-as-Tool: A Study on the Hierarchical Decision Making with Reinforcement Learning", "comment": "12 pages", "summary": "Large Language Models (LLMs) have emerged as one of the most significant\ntechnological advancements in artificial intelligence in recent years. Their\nability to understand, generate, and reason with natural language has\ntransformed how we interact with AI systems. With the development of LLM-based\nagents and reinforcement-learning-based reasoning models, the study of applying\nreinforcement learning in agent frameworks has become a new research focus.\nHowever, all previous studies face the challenge of deciding the tool calling\nprocess and the reasoning process simultaneously, and the chain of reasoning\nwas solely relied on the unprocessed raw result with redundant information and\nsymbols unrelated to the task from the tool, which impose a heavy burden on the\nmodel's capability to reason. Therefore, in our research, we proposed a\nhierarchical framework Agent-as-tool that detach the tool calling process and\nthe reasoning process, which enables the model to focus on the verbally\nreasoning process while the tool calling process is handled by another agent.\nOur work had achieved comparable results with only a slight reinforcement\nfine-tuning on 180 samples, and had achieved exceptionally well performance in\nBamboogle with 63.2% of exact match and 75.2% in cover exact match, exceeding\nSearch-R1 by 4.8% in exact match and 3.2% in cover exact match.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u6846\u67b6Agent-as-tool\uff0c\u5c06\u5de5\u5177\u8c03\u7528\u8fc7\u7a0b\u4e0e\u63a8\u7406\u8fc7\u7a0b\u5206\u79bb\uff0c\u51cf\u8f7b\u6a21\u578b\u8d1f\u62c5\uff0c\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u540c\u65f6\u5904\u7406\u5de5\u5177\u8c03\u7528\u548c\u63a8\u7406\u8fc7\u7a0b\uff0c\u4f9d\u8d56\u672a\u5904\u7406\u7684\u5de5\u5177\u7ed3\u679c\uff0c\u589e\u52a0\u4e86\u6a21\u578b\u63a8\u7406\u8d1f\u62c5\u3002", "method": "\u91c7\u7528\u5206\u5c42\u6846\u67b6\uff0c\u5206\u79bb\u5de5\u5177\u8c03\u7528\u548c\u63a8\u7406\u8fc7\u7a0b\uff0c\u7531\u4e0d\u540c\u4ee3\u7406\u5904\u7406\u3002", "result": "\u4ec5\u9700\u5c11\u91cf\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\uff08180\u6837\u672c\uff09\uff0c\u5728Bamboogle\u4e0a\u8868\u73b0\u4f18\u5f02\uff0863.2%\u7cbe\u786e\u5339\u914d\uff0c75.2%\u8986\u76d6\u7cbe\u786e\u5339\u914d\uff09\u3002", "conclusion": "Agent-as-tool\u6846\u67b6\u6709\u6548\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.01032", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.01032", "abs": "https://arxiv.org/abs/2507.01032", "authors": ["Nan Mu", "Hongbo Yang", "Chen Zhao"], "title": "An Uncertainty-Aware Dynamic Decision Framework for Progressive Multi-Omics Integration in Classification Tasks", "comment": null, "summary": "Background and Objective: High-throughput multi-omics technologies have\nproven invaluable for elucidating disease mechanisms and enabling early\ndiagnosis. However, the high cost of multi-omics profiling imposes a\nsignificant economic burden, with over reliance on full omics data potentially\nleading to unnecessary resource consumption. To address these issues, we\npropose an uncertainty-aware, multi-view dynamic decision framework for omics\ndata classification that aims to achieve high diagnostic accuracy while\nminimizing testing costs. Methodology: At the single-omics level, we refine the\nactivation functions of neural networks to generate Dirichlet distribution\nparameters, utilizing subjective logic to quantify both the belief masses and\nuncertainty mass of classification results. Belief mass reflects the support of\na specific omics modality for a disease class, while the uncertainty parameter\ncaptures limitations in data quality and model discriminability, providing a\nmore trustworthy basis for decision-making. At the multi omics level, we employ\na fusion strategy based on Dempster-Shafer theory to integrate heterogeneous\nmodalities, leveraging their complementarity to boost diagnostic accuracy and\nrobustness. A dynamic decision mechanism is then applied that omics data are\nincrementally introduced for each patient until either all data sources are\nutilized or the model confidence exceeds a predefined threshold, potentially\nbefore all data sources are utilized. Results and Conclusion: We evaluate our\napproach on four benchmark multi-omics datasets, ROSMAP, LGG, BRCA, and KIPAN.\nIn three datasets, over 50% of cases achieved accurate classification using a\nsingle omics modality, effectively reducing redundant testing. Meanwhile, our\nmethod maintains diagnostic performance comparable to full-omics models and\npreserves essential biological insights.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u591a\u89c6\u56fe\u52a8\u6001\u51b3\u7b56\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u7ec4\u5b66\u6570\u636e\u5206\u7c7b\uff0c\u65e8\u5728\u964d\u4f4e\u6d4b\u8bd5\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u3002", "motivation": "\u591a\u7ec4\u5b66\u6280\u672f\u6210\u672c\u9ad8\u6602\uff0c\u53ef\u80fd\u5bfc\u81f4\u8d44\u6e90\u6d6a\u8d39\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5728\u51cf\u5c11\u5197\u4f59\u6d4b\u8bd5\u7684\u540c\u65f6\u4fdd\u6301\u8bca\u65ad\u51c6\u786e\u6027\u3002", "method": "\u5728\u5355\u7ec4\u5b66\u5c42\u9762\uff0c\u901a\u8fc7\u6539\u8fdb\u795e\u7ecf\u7f51\u7edc\u6fc0\u6d3b\u51fd\u6570\u751f\u6210Dirichlet\u5206\u5e03\u53c2\u6570\uff0c\u91cf\u5316\u5206\u7c7b\u7ed3\u679c\u7684\u7f6e\u4fe1\u5ea6\u548c\u4e0d\u786e\u5b9a\u6027\uff1b\u5728\u591a\u7ec4\u5b66\u5c42\u9762\uff0c\u57fa\u4e8eDempster-Shafer\u7406\u8bba\u878d\u5408\u5f02\u6784\u6a21\u6001\uff0c\u52a8\u6001\u51b3\u7b56\u9010\u6b65\u5f15\u5165\u6570\u636e\u76f4\u81f3\u6ee1\u8db3\u7f6e\u4fe1\u5ea6\u9608\u503c\u3002", "result": "\u5728\u56db\u4e2a\u591a\u7ec4\u5b66\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c50%\u4ee5\u4e0a\u7684\u75c5\u4f8b\u4ec5\u9700\u5355\u7ec4\u5b66\u6570\u636e\u5373\u53ef\u51c6\u786e\u5206\u7c7b\uff0c\u663e\u8457\u51cf\u5c11\u5197\u4f59\u6d4b\u8bd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u5168\u7ec4\u5b66\u6a21\u578b\u76f8\u5f53\u7684\u8bca\u65ad\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u964d\u4f4e\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u8bca\u65ad\u51c6\u786e\u6027\u548c\u751f\u7269\u5b66\u610f\u4e49\uff0c\u4e3a\u591a\u7ec4\u5b66\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2507.01269", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.01269", "abs": "https://arxiv.org/abs/2507.01269", "authors": ["Mohammad Jahanbakht", "Alex Olsen", "Ross Marchant", "Emilie Fillols", "Mostafa Rahimi Azghadi"], "title": "Advancements in Weed Mapping: A Systematic Review", "comment": null, "summary": "Weed mapping plays a critical role in precision management by providing\naccurate and timely data on weed distribution, enabling targeted control and\nreduced herbicide use. This minimizes environmental impacts, supports\nsustainable land management, and improves outcomes across agricultural and\nnatural environments. Recent advances in weed mapping leverage ground-vehicle\nRed Green Blue (RGB) cameras, satellite and drone-based remote sensing combined\nwith sensors such as spectral, Near Infra-Red (NIR), and thermal cameras. The\nresulting data are processed using advanced techniques including big data\nanalytics and machine learning, significantly improving the spatial and\ntemporal resolution of weed maps and enabling site-specific management\ndecisions. Despite a growing body of research in this domain, there is a lack\nof comprehensive literature reviews specifically focused on weed mapping. In\nparticular, the absence of a structured analysis spanning the entire mapping\npipeline, from data acquisition to processing techniques and mapping tools,\nlimits progress in the field. This review addresses these gaps by\nsystematically examining state-of-the-art methods in data acquisition (sensor\nand platform technologies), data processing (including annotation and\nmodelling), and mapping techniques (such as spatiotemporal analysis and\ndecision support tools). Following PRISMA guidelines, we critically evaluate\nand synthesize key findings from the literature to provide a holistic\nunderstanding of the weed mapping landscape. This review serves as a\nfoundational reference to guide future research and support the development of\nefficient, scalable, and sustainable weed management systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u6742\u8349\u6d4b\u7ed8\u9886\u57df\u7684\u6700\u65b0\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u4ece\u6570\u636e\u83b7\u53d6\u5230\u5904\u7406\u6280\u672f\u53ca\u6d4b\u7ed8\u5de5\u5177\u7684\u5168\u6d41\u7a0b\u5206\u6790\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u53c2\u8003\u3002", "motivation": "\u6742\u8349\u6d4b\u7ed8\u5bf9\u7cbe\u51c6\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u5168\u9762\u7684\u6587\u732e\u7efc\u8ff0\uff0c\u9650\u5236\u4e86\u9886\u57df\u8fdb\u5c55\u3002", "method": "\u91c7\u7528PRISMA\u6307\u5357\uff0c\u7cfb\u7edf\u8bc4\u4f30\u6570\u636e\u83b7\u53d6\u3001\u5904\u7406\u53ca\u6d4b\u7ed8\u6280\u672f\u7684\u6700\u65b0\u65b9\u6cd5\u3002", "result": "\u7efc\u5408\u6587\u732e\u5173\u952e\u53d1\u73b0\uff0c\u63d0\u4f9b\u4e86\u6742\u8349\u6d4b\u7ed8\u9886\u57df\u7684\u6574\u4f53\u7406\u89e3\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e3a\u672a\u6765\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u53ef\u6301\u7eed\u7684\u6742\u8349\u7ba1\u7406\u7cfb\u7edf\u5f00\u53d1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.01334", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01334", "abs": "https://arxiv.org/abs/2507.01334", "authors": ["Nifu Dan", "Yujun Cai", "Yiwei Wang"], "title": "Symbolic or Numerical? Understanding Physics Problem Solving in Reasoning LLMs", "comment": null, "summary": "Navigating the complexities of physics reasoning has long been a difficult\ntask for Large Language Models (LLMs), requiring a synthesis of profound\nconceptual understanding and adept problem-solving techniques. In this study,\nwe investigate the application of advanced instruction-tuned reasoning models,\nsuch as Deepseek-R1, to address a diverse spectrum of physics problems curated\nfrom the challenging SciBench benchmark. Our comprehensive experimental\nevaluation reveals the remarkable capabilities of reasoning models. Not only do\nthey achieve state-of-the-art accuracy in answering intricate physics\nquestions, but they also generate distinctive reasoning patterns that emphasize\non symbolic derivation. Furthermore, our findings indicate that even for these\nhighly sophisticated reasoning models, the strategic incorporation of few-shot\nprompting can still yield measurable improvements in overall accuracy,\nhighlighting the potential for continued performance gains.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u6307\u4ee4\u8c03\u4f18\u63a8\u7406\u6a21\u578b\uff08\u5982Deepseek-R1\uff09\u5728\u89e3\u51b3\u590d\u6742\u7269\u7406\u95ee\u9898\u4e0a\u7684\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u5176\u5353\u8d8a\u7684\u51c6\u786e\u6027\u548c\u72ec\u7279\u7684\u7b26\u53f7\u63a8\u5bfc\u63a8\u7406\u6a21\u5f0f\uff0c\u5e76\u6307\u51fa\u5c11\u91cf\u63d0\u793a\u7b56\u7565\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7269\u7406\u63a8\u7406\u4e2d\u7684\u590d\u6742\u6027\u95ee\u9898\uff0c\u9700\u8981\u7ed3\u5408\u6df1\u523b\u7684\u6982\u5ff5\u7406\u89e3\u548c\u95ee\u9898\u89e3\u51b3\u6280\u5de7\u3002", "method": "\u5e94\u7528\u6307\u4ee4\u8c03\u4f18\u63a8\u7406\u6a21\u578b\uff08\u5982Deepseek-R1\uff09\u5904\u7406SciBench\u57fa\u51c6\u4e2d\u7684\u591a\u6837\u5316\u7269\u7406\u95ee\u9898\uff0c\u5e76\u8fdb\u884c\u5168\u9762\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "\u6a21\u578b\u5728\u590d\u6742\u7269\u7406\u95ee\u9898\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u51c6\u786e\u7387\uff0c\u5e76\u5c55\u73b0\u51fa\u72ec\u7279\u7684\u7b26\u53f7\u63a8\u5bfc\u63a8\u7406\u6a21\u5f0f\uff1b\u5c11\u91cf\u63d0\u793a\u7b56\u7565\u8fd8\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u6307\u4ee4\u8c03\u4f18\u63a8\u7406\u6a21\u578b\u5728\u7269\u7406\u63a8\u7406\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u672a\u6765\u901a\u8fc7\u4f18\u5316\u63d0\u793a\u7b56\u7565\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002"}}
{"id": "2507.01597", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01597", "abs": "https://arxiv.org/abs/2507.01597", "authors": ["Yuehang Si", "Zefan Zeng", "Jincai Huang", "Qing Cheng"], "title": "T3DM: Test-Time Training-Guided Distribution Shift Modelling for Temporal Knowledge Graph Reasoning", "comment": null, "summary": "Temporal Knowledge Graph (TKG) is an efficient method for describing the\ndynamic development of facts along a timeline. Most research on TKG reasoning\n(TKGR) focuses on modelling the repetition of global facts and designing\npatterns of local historical facts. However, they face two significant\nchallenges: inadequate modeling of the event distribution shift between\ntraining and test samples, and reliance on random entity substitution for\ngenerating negative samples, which often results in low-quality sampling. To\nthis end, we propose a novel distributional feature modeling approach for\ntraining TKGR models, Test-Time Training-guided Distribution shift Modelling\n(T3DM), to adjust the model based on distribution shift and ensure the global\nconsistency of model reasoning. In addition, we design a negative-sampling\nstrategy to generate higher-quality negative quadruples based on adversarial\ntraining. Extensive experiments show that T3DM provides better and more robust\nresults than the state-of-the-art baselines in most cases.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684TKG\u63a8\u7406\u65b9\u6cd5T3DM\uff0c\u901a\u8fc7\u5efa\u6a21\u5206\u5e03\u504f\u79fb\u548c\u8bbe\u8ba1\u9ad8\u8d28\u91cf\u8d1f\u91c7\u6837\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u63a8\u7406\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709TKG\u63a8\u7406\u65b9\u6cd5\u5728\u5efa\u6a21\u4e8b\u4ef6\u5206\u5e03\u504f\u79fb\u548c\u751f\u6210\u9ad8\u8d28\u91cf\u8d1f\u6837\u672c\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5f71\u54cd\u4e86\u63a8\u7406\u7684\u5168\u5c40\u4e00\u81f4\u6027\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51faT3DM\u65b9\u6cd5\uff0c\u7ed3\u5408\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u6307\u5bfc\u7684\u5206\u5e03\u504f\u79fb\u5efa\u6a21\u548c\u57fa\u4e8e\u5bf9\u6297\u8bad\u7ec3\u7684\u8d1f\u91c7\u6837\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cT3DM\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u66f4\u4f18\u4e14\u66f4\u9c81\u68d2\u7684\u7ed3\u679c\u3002", "conclusion": "T3DM\u901a\u8fc7\u6539\u8fdb\u5206\u5e03\u504f\u79fb\u5efa\u6a21\u548c\u8d1f\u91c7\u6837\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86TKG\u63a8\u7406\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.01034", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01034", "abs": "https://arxiv.org/abs/2507.01034", "authors": ["Asma Agaal", "Mansour Essgaer", "Hend M. Farkash", "Zulaiha Ali Othman"], "title": "Data-driven Insights for Informed Decision-Making: Applying LSTM Networks for Robust Electricity Forecasting in Libya", "comment": "This article was published in International Journal of Intelligent\n  Systems and Applications (IJISA) (MECS Press), Vol. 17, No. 3, 8 Jun. 2025,\n  DOI: https://doi.org/10.5815/ijisa.2025.03.05", "summary": "Accurate electricity forecasting is crucial for grid stability and energy\nplanning, especially in Benghazi, Libya, where frequent load shedding,\ngeneration deficits, and infrastructure limitations persist. This study\nproposes a data-driven approach to forecast electricity load, generation, and\ndeficits for 2025 using historical data from 2019 (a year marked by\ninstability) and 2023 (a more stable year). Multiple time series models were\napplied, including ARIMA, seasonal ARIMA, dynamic regression ARIMA, exponential\nsmoothing, extreme gradient boosting, and Long Short-Term Memory (LSTM) neural\nnetworks. The dataset was enhanced through missing value imputation, outlier\nsmoothing, and log transformation. Performance was assessed using mean squared\nerror, root mean squared error, mean absolute error, and mean absolute\npercentage error. LSTM outperformed all other models, showing strong\ncapabilities in modeling non-stationary and seasonal patterns. A key\ncontribution of this work is an optimized LSTM framework that integrates\nexogenous factors such as temperature and humidity, offering robust performance\nin forecasting multiple electricity indicators. These results provide practical\ninsights for policymakers and grid operators to enable proactive load\nmanagement and resource planning in data-scarce, volatile regions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u5386\u53f2\u6570\u636e\u9884\u6d4b2025\u5e74\u5229\u6bd4\u4e9a\u73ed\u52a0\u897f\u7684\u7535\u529b\u8d1f\u8377\u3001\u53d1\u7535\u91cf\u548c\u7f3a\u53e3\uff0cLSTM\u6a21\u578b\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u73ed\u52a0\u897f\u7535\u529b\u4f9b\u5e94\u4e0d\u7a33\u5b9a\uff0c\u4e9f\u9700\u51c6\u786e\u7684\u7535\u529b\u9884\u6d4b\u4ee5\u652f\u6301\u7535\u7f51\u7a33\u5b9a\u548c\u80fd\u6e90\u89c4\u5212\u3002", "method": "\u91c7\u7528\u591a\u79cd\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\uff08\u5982ARIMA\u3001LSTM\u7b49\uff09\uff0c\u5e76\u5bf9\u6570\u636e\u8fdb\u884c\u7f3a\u5931\u503c\u586b\u8865\u3001\u5f02\u5e38\u503c\u5e73\u6ed1\u548clog\u8f6c\u6362\u3002", "result": "LSTM\u6a21\u578b\u5728\u9884\u6d4b\u975e\u5e73\u7a33\u548c\u5b63\u8282\u6027\u6570\u636e\u65f6\u8868\u73b0\u6700\u4f18\uff0c\u4e14\u6574\u5408\u4e86\u6e29\u5ea6\u548c\u6e7f\u5ea6\u7b49\u5916\u751f\u53d8\u91cf\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u653f\u7b56\u5236\u5b9a\u8005\u548c\u7535\u7f51\u8fd0\u8425\u5546\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u9884\u6d4b\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u6570\u636e\u7a00\u7f3a\u4e14\u4e0d\u7a33\u5b9a\u7684\u5730\u533a\u3002"}}
{"id": "2507.01275", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01275", "abs": "https://arxiv.org/abs/2507.01275", "authors": ["Chengxu Liu", "Lu Qi", "Jinshan Pan", "Xueming Qian", "Ming-Hsuan Yang"], "title": "Frequency Domain-Based Diffusion Model for Unpaired Image Dehazing", "comment": "Accepted by ICCV 2025", "summary": "Unpaired image dehazing has attracted increasing attention due to its\nflexible data requirements during model training. Dominant methods based on\ncontrastive learning not only introduce haze-unrelated content information, but\nalso ignore haze-specific properties in the frequency domain (\\ie,~haze-related\ndegradation is mainly manifested in the amplitude spectrum). To address these\nissues, we propose a novel frequency domain-based diffusion model, named \\ours,\nfor fully exploiting the beneficial knowledge in unpaired clear data. In\nparticular, inspired by the strong generative ability shown by Diffusion Models\n(DMs), we tackle the dehazing task from the perspective of frequency domain\nreconstruction and perform the DMs to yield the amplitude spectrum consistent\nwith the distribution of clear images. To implement it, we propose an Amplitude\nResidual Encoder (ARE) to extract the amplitude residuals, which effectively\ncompensates for the amplitude gap from the hazy to clear domains, as well as\nprovide supervision for the DMs training. In addition, we propose a Phase\nCorrection Module (PCM) to eliminate artifacts by further refining the phase\nspectrum during dehazing with a simple attention mechanism. Experimental\nresults demonstrate that our \\ours outperforms other state-of-the-art methods\non both synthetic and real-world datasets.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.01335", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01335", "abs": "https://arxiv.org/abs/2507.01335", "authors": ["Xunjian Yin", "Sitao Cheng", "Yuxi Xie", "Xinyu Hu", "Li Lin", "Xinyi Wang", "Liangming Pan", "William Yang Wang", "Xiaojun Wan"], "title": "LEDOM: An Open and Fundamental Reverse Language Model", "comment": "Work in progress", "summary": "We introduce LEDOM, the first purely reverse language model, trained\nautoregressively on 435B tokens with 2B and 7B parameter variants, which\nprocesses sequences in reverse temporal order through previous token\nprediction. For the first time, we present the reverse language model as a\npotential foundational model across general tasks, accompanied by a set of\nintriguing examples and insights. Based on LEDOM, we further introduce a novel\napplication: Reverse Reward, where LEDOM-guided reranking of forward language\nmodel outputs leads to substantial performance improvements on mathematical\nreasoning tasks. This approach leverages LEDOM's unique backward reasoning\ncapability to refine generation quality through posterior evaluation. Our\nfindings suggest that LEDOM exhibits unique characteristics with broad\napplication potential. We will release all models, training code, and\npre-training data to facilitate future research.", "AI": {"tldr": "LEDOM\u662f\u9996\u4e2a\u7eaf\u9006\u5411\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u9006\u5411\u9884\u6d4b\u8bad\u7ec3\uff0c\u5c55\u793a\u51fa\u5e7f\u6cdb\u4efb\u52a1\u6f5c\u529b\uff0c\u5e76\u5f15\u5165\u9006\u5411\u5956\u52b1\u5e94\u7528\u63d0\u5347\u6570\u5b66\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u9006\u5411\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u57fa\u7840\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u5e76\u5f00\u53d1\u5176\u72ec\u7279\u5e94\u7528\u3002", "method": "\u8bad\u7ec32B\u548c7B\u53c2\u6570\u7684\u9006\u5411\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u9006\u5411\u9884\u6d4b\u5904\u7406\u5e8f\u5217\uff0c\u5e76\u5f15\u5165\u9006\u5411\u5956\u52b1\u4f18\u5316\u751f\u6210\u8d28\u91cf\u3002", "result": "LEDOM\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5c55\u793a\u51fa\u72ec\u7279\u7279\u6027\u548c\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "LEDOM\u4f5c\u4e3a\u9006\u5411\u8bed\u8a00\u6a21\u578b\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u672a\u6765\u5c06\u516c\u5f00\u6a21\u578b\u548c\u8bad\u7ec3\u6570\u636e\u4ee5\u4fc3\u8fdb\u7814\u7a76\u3002"}}
{"id": "2507.01717", "categories": ["cs.AI", "cs.IR", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.01717", "abs": "https://arxiv.org/abs/2507.01717", "authors": ["Gopichand Kanumolu", "Ashok Urlana", "Charaka Vinayak Kumar", "Bala Mallikarjunarao Garlapati"], "title": "Agent Ideate: A Framework for Product Idea Generation from Patents Using Agentic AI", "comment": "AgentScen Workshop, IJCAI 2025", "summary": "Patents contain rich technical knowledge that can inspire innovative product\nideas, yet accessing and interpreting this information remains a challenge.\nThis work explores the use of Large Language Models (LLMs) and autonomous\nagents to mine and generate product concepts from a given patent. In this work,\nwe design Agent Ideate, a framework for automatically generating product-based\nbusiness ideas from patents. We experimented with open-source LLMs and\nagent-based architectures across three domains: Computer Science, Natural\nLanguage Processing, and Material Chemistry. Evaluation results show that the\nagentic approach consistently outperformed standalone LLMs in terms of idea\nquality, relevance, and novelty. These findings suggest that combining LLMs\nwith agentic workflows can significantly enhance the innovation pipeline by\nunlocking the untapped potential of business idea generation from patent data.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u81ea\u4e3b\u4ee3\u7406\u4ece\u4e13\u5229\u4e2d\u6316\u6398\u548c\u751f\u6210\u4ea7\u54c1\u6982\u5ff5\u7684\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86Agent Ideate\u6846\u67b6\uff0c\u5e76\u5728\u591a\u4e2a\u9886\u57df\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u5355\u72ec\u4f7f\u7528LLMs\u7684\u6548\u679c\u3002", "motivation": "\u4e13\u5229\u8574\u542b\u4e30\u5bcc\u7684\u6280\u672f\u77e5\u8bc6\uff0c\u4f46\u8bbf\u95ee\u548c\u89e3\u8bfb\u8fd9\u4e9b\u4fe1\u606f\u4ecd\u5177\u6311\u6218\u6027\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7LLMs\u548c\u4ee3\u7406\u6280\u672f\u6316\u6398\u4e13\u5229\u4e2d\u7684\u521b\u65b0\u6f5c\u529b\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4ea7\u54c1\u6982\u5ff5\u3002", "method": "\u8bbe\u8ba1\u4e86Agent Ideate\u6846\u67b6\uff0c\u7ed3\u5408\u5f00\u6e90LLMs\u548c\u57fa\u4e8e\u4ee3\u7406\u7684\u67b6\u6784\uff0c\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u6750\u6599\u5316\u5b66\u4e09\u4e2a\u9886\u57df\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c\u4ee3\u7406\u65b9\u6cd5\u5728\u521b\u610f\u8d28\u91cf\u3001\u76f8\u5173\u6027\u548c\u65b0\u9896\u6027\u4e0a\u5747\u4f18\u4e8e\u5355\u72ec\u4f7f\u7528LLMs\u3002", "conclusion": "\u7ed3\u5408LLMs\u4e0e\u4ee3\u7406\u5de5\u4f5c\u6d41\u80fd\u663e\u8457\u63d0\u5347\u521b\u65b0\u6d41\u7a0b\uff0c\u91ca\u653e\u4e13\u5229\u6570\u636e\u4e2d\u672a\u88ab\u5f00\u53d1\u7684\u5546\u4e1a\u521b\u610f\u6f5c\u529b\u3002"}}
{"id": "2507.01035", "categories": ["cs.LG", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.01035", "abs": "https://arxiv.org/abs/2507.01035", "authors": ["Yushang Zhao", "Haotian Lyu", "Yike Peng", "Aijia Sun", "Feng Jiang", "Xinyue Han"], "title": "Research on Low-Latency Inference and Training Efficiency Optimization for Graph Neural Network and Large Language Model-Based Recommendation Systems", "comment": null, "summary": "The incessant advent of online services demands high speed and efficient\nrecommender systems (ReS) that can maintain real-time performance along with\nprocessing very complex user-item interactions. The present study, therefore,\nconsiders computational bottlenecks involved in hybrid Graph Neural Network\n(GNN) and Large Language Model (LLM)-based ReS with the aim optimizing their\ninference latency and training efficiency. An extensive methodology was used:\nhybrid GNN-LLM integrated architecture-optimization strategies(quantization,\nLoRA, distillation)-hardware acceleration (FPGA, DeepSpeed)-all under R 4.4.2.\nExperimental improvements were significant, with the optimal Hybrid + FPGA +\nDeepSpeed configuration reaching 13.6% more accuracy (NDCG@10: 0.75) at 40-60ms\nof latency, while LoRA brought down training time by 66% (3.8 hours) in\ncomparison to the non-optimized baseline. Irrespective of domain, such as\naccuracy or efficiency, it can be established that hardware-software co-design\nand parameter-efficient tuning permit hybrid models to outperform GNN or LLM\napproaches implemented independently. It recommends the use of FPGA as well as\nLoRA for real-time deployment. Future work should involve federated learning\nalong with advanced fusion architectures for better scalability and privacy\npreservation. Thus, this research marks the fundamental groundwork concerning\nnext-generation ReS balancing low-latency response with cutting-edge\npersonalization.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u7ed3\u5408GNN\u548cLLM\u7684\u63a8\u8350\u7cfb\u7edf\uff0c\u901a\u8fc7\u91cf\u5316\u3001LoRA\u3001\u84b8\u998f\u7b49\u4f18\u5316\u7b56\u7565\u53ca\u786c\u4ef6\u52a0\u901f\uff08FPGA\u3001DeepSpeed\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u5ef6\u8fdf\u548c\u8bad\u7ec3\u6548\u7387\u3002\u6700\u4f18\u914d\u7f6e\u572840-60ms\u5ef6\u8fdf\u4e0b\u8fbe\u523013.6%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0cLoRA\u51cf\u5c1166%\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u5728\u7ebf\u670d\u52a1\u5bf9\u9ad8\u6548\u5b9e\u65f6\u63a8\u8350\u7cfb\u7edf\u7684\u9700\u6c42\uff0c\u4fc3\u4f7f\u7814\u7a76\u5982\u4f55\u4f18\u5316\u6df7\u5408GNN-LLM\u6a21\u578b\u7684\u63a8\u7406\u5ef6\u8fdf\u548c\u8bad\u7ec3\u6548\u7387\u3002", "method": "\u91c7\u7528\u6df7\u5408GNN-LLM\u67b6\u6784\uff0c\u7ed3\u5408\u91cf\u5316\u3001LoRA\u3001\u84b8\u998f\u7b49\u4f18\u5316\u7b56\u7565\uff0c\u5e76\u4f7f\u7528FPGA\u548cDeepSpeed\u8fdb\u884c\u786c\u4ef6\u52a0\u901f\u3002", "result": "\u6700\u4f18\u914d\u7f6e\uff08Hybrid + FPGA + DeepSpeed\uff09\u5728NDCG@10\u8fbe\u52300.75\uff0c\u5ef6\u8fdf40-60ms\uff1bLoRA\u51cf\u5c1166%\u8bad\u7ec3\u65f6\u95f4\u3002", "conclusion": "\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u548c\u53c2\u6570\u9ad8\u6548\u8c03\u4f18\u4f7f\u6df7\u5408\u6a21\u578b\u4f18\u4e8e\u72ec\u7acbGNN\u6216LLM\u65b9\u6cd5\uff0c\u63a8\u8350FPGA\u548cLoRA\u7528\u4e8e\u5b9e\u65f6\u90e8\u7f72\u3002\u672a\u6765\u53ef\u63a2\u7d22\u8054\u90a6\u5b66\u4e60\u548c\u9ad8\u7ea7\u878d\u5408\u67b6\u6784\u3002"}}
{"id": "2507.01290", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01290", "abs": "https://arxiv.org/abs/2507.01290", "authors": ["Sunyong Seo", "Semin Kim", "Jongha Lee"], "title": "Learning an Ensemble Token from Task-driven Priors in Facial Analysis", "comment": "11pages, 8figures, 4tables", "summary": "Facial analysis exhibits task-specific feature variations. While\nConvolutional Neural Networks (CNNs) have enabled the fine-grained\nrepresentation of spatial information, Vision Transformers (ViTs) have\nfacilitated the representation of semantic information at the patch level.\nAlthough the generalization of conventional methodologies has advanced visual\ninterpretability, there remains paucity of research that preserves the unified\nfeature representation on single task learning during the training process. In\nthis work, we introduce ET-Fuser, a novel methodology for learning ensemble\ntoken by leveraging attention mechanisms based on task priors derived from\npre-trained models for facial analysis. Specifically, we propose a robust prior\nunification learning method that generates a ensemble token within a\nself-attention mechanism, which shares the mutual information along the\npre-trained encoders. This ensemble token approach offers high efficiency with\nnegligible computational cost. Our results show improvements across a variety\nof facial analysis, with statistically significant enhancements observed in the\nfeature representations.", "AI": {"tldr": "ET-Fuser\u662f\u4e00\u79cd\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u4efb\u52a1\u5148\u9a8c\u5b66\u4e60\u96c6\u6210\u4ee4\u724c\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u9762\u90e8\u5206\u6790\uff0c\u63d0\u5347\u4e86\u7279\u5f81\u8868\u793a\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5355\u4efb\u52a1\u5b66\u4e60\u4e2d\u7f3a\u4e4f\u7edf\u4e00\u7684\u7279\u5f81\u8868\u793a\uff0cET-Fuser\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u96c6\u6210\u4ee4\u724c\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u5171\u4eab\u4e92\u4fe1\u606f\u3002", "result": "\u5728\u591a\u79cd\u9762\u90e8\u5206\u6790\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u7279\u5f81\u8868\u793a\u6539\u8fdb\u3002", "conclusion": "ET-Fuser\u9ad8\u6548\u4e14\u8ba1\u7b97\u6210\u672c\u4f4e\uff0c\u4e3a\u9762\u90e8\u5206\u6790\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7279\u5f81\u8868\u793a\u65b9\u6cd5\u3002"}}
{"id": "2507.01352", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01352", "abs": "https://arxiv.org/abs/2507.01352", "authors": ["Chris Yuhao Liu", "Liang Zeng", "Yuzhen Xiao", "Jujie He", "Jiacai Liu", "Chaojie Wang", "Rui Yan", "Wei Shen", "Fuxiang Zhang", "Jiacheng Xu", "Yang Liu", "Yahui Zhou"], "title": "Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy", "comment": null, "summary": "Despite the critical role of reward models (RMs) in reinforcement learning\nfrom human feedback (RLHF), current state-of-the-art open RMs perform poorly on\nmost existing evaluation benchmarks, failing to capture the spectrum of nuanced\nand sophisticated human preferences. Even approaches that incorporate advanced\ntraining techniques have not yielded meaningful performance improvements. We\nhypothesize that this brittleness stems primarily from limitations in\npreference datasets, which are often narrowly scoped, synthetically labeled, or\nlack rigorous quality control. To address these challenges, we present a\nlarge-scale preference dataset comprising 40 million preference pairs, named\nSynPref-40M. To enable data curation at scale, we design a human-AI synergistic\ntwo-stage pipeline that leverages the complementary strengths of human\nannotation quality and AI scalability. In this pipeline, humans provide\nverified annotations, while large language models perform automatic curation\nbased on human guidance. Training on this preference mixture, we introduce\nSkywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B\nparameters, trained on a carefully curated subset of 26 million preference\npairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile\nacross a wide range of capabilities, including alignment with human\npreferences, objective correctness, safety, resistance to stylistic biases, and\nbest-of-N scaling, achieving state-of-the-art performance across seven major\nreward model benchmarks. Ablation studies confirm that the effectiveness of our\napproach stems not only from data scale but also from high-quality curation.\nThe Skywork-Reward-V2 series represents substantial progress in open reward\nmodels, highlighting the untapped potential of existing preference datasets and\ndemonstrating how human-AI curation synergy can unlock significantly higher\ndata quality.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSynPref-40M\u7684\u5927\u89c4\u6a21\u504f\u597d\u6570\u636e\u96c6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4eba\u673a\u534f\u540c\u7684\u4e24\u9636\u6bb5\u6570\u636e\u6807\u6ce8\u6d41\u7a0b\uff0c\u8bad\u7ec3\u51faSkywork-Reward-V2\u5956\u52b1\u6a21\u578b\u7cfb\u5217\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u5956\u52b1\u6a21\u578b\u5728\u6355\u6349\u4eba\u7c7b\u504f\u597d\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u504f\u597d\u6570\u636e\u96c6\u5b58\u5728\u8303\u56f4\u72ed\u7a84\u3001\u6807\u7b7e\u5408\u6210\u6216\u8d28\u91cf\u63a7\u5236\u4e0d\u8db3\u7b49\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4eba\u673a\u534f\u540c\u7684\u4e24\u9636\u6bb5\u6807\u6ce8\u6d41\u7a0b\uff08\u4eba\u7c7b\u63d0\u4f9b\u9ad8\u8d28\u91cf\u6807\u6ce8\uff0cAI\u8fdb\u884c\u5927\u89c4\u6a21\u81ea\u52a8\u6807\u6ce8\uff09\uff0c\u6784\u5efa\u4e86\u5305\u542b4000\u4e07\u504f\u597d\u5bf9\u7684SynPref-40M\u6570\u636e\u96c6\uff0c\u5e76\u4ece\u4e2d\u7cbe\u90092600\u4e07\u5bf9\u8bad\u7ec3\u4e86Skywork-Reward-V2\u7cfb\u5217\u6a21\u578b\u3002", "result": "Skywork-Reward-V2\u5728\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u3001\u5b89\u5168\u6027\u3001\u6297\u98ce\u683c\u504f\u89c1\u7b49\u591a\u4e2a\u80fd\u529b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u4e03\u4e2a\u4e3b\u8981\u5956\u52b1\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u6570\u636e\u89c4\u6a21\u548c\u9ad8\u8d28\u6807\u6ce8\u7684\u7ed3\u5408\u662f\u63d0\u5347\u5956\u52b1\u6a21\u578b\u6027\u80fd\u7684\u5173\u952e\uff0c\u4eba\u673a\u534f\u540c\u6807\u6ce8\u6d41\u7a0b\u80fd\u663e\u8457\u63d0\u9ad8\u6570\u636e\u8d28\u91cf\u3002"}}
{"id": "2507.01749", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01749", "abs": "https://arxiv.org/abs/2507.01749", "authors": ["Arash Dehghan", "Mucahit Cevik", "Merve Bodur", "Bissan Ghaddar"], "title": "Joint Matching and Pricing for Crowd-shipping with In-store Customers", "comment": null, "summary": "This paper examines the use of in-store customers as delivery couriers in a\ncentralized crowd-shipping system, targeting the growing need for efficient\nlast-mile delivery in urban areas. We consider a brick-and-mortar retail\nsetting where shoppers are offered compensation to deliver time-sensitive\nonline orders. To manage this process, we propose a Markov Decision Process\n(MDP) model that captures key uncertainties, including the stochastic arrival\nof orders and crowd-shippers, and the probabilistic acceptance of delivery\noffers. Our solution approach integrates Neural Approximate Dynamic Programming\n(NeurADP) for adaptive order-to-shopper assignment with a Deep Double Q-Network\n(DDQN) for dynamic pricing. This joint optimization strategy enables multi-drop\nrouting and accounts for offer acceptance uncertainty, aligning more closely\nwith real-world operations. Experimental results demonstrate that the\nintegrated NeurADP + DDQN policy achieves notable improvements in delivery cost\nefficiency, with up to 6.7\\% savings over NeurADP with fixed pricing and\napproximately 18\\% over myopic baselines. We also show that allowing flexible\ndelivery delays and enabling multi-destination routing further reduces\noperational costs by 8\\% and 17\\%, respectively. These findings underscore the\nadvantages of dynamic, forward-looking policies in crowd-shipping systems and\noffer practical guidance for urban logistics operators.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5229\u7528\u5e97\u5185\u987e\u5ba2\u4f5c\u4e3a\u9001\u8d27\u5458\u7684\u96c6\u4e2d\u5f0f\u4f17\u5305\u914d\u9001\u7cfb\u7edf\uff0c\u4ee5\u5e94\u5bf9\u57ce\u5e02\u5730\u533a\u6700\u540e\u4e00\u516c\u91cc\u914d\u9001\u7684\u9ad8\u6548\u9700\u6c42\u3002\u901a\u8fc7\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u6a21\u578b\u548c\u795e\u7ecf\u8fd1\u4f3c\u52a8\u6001\u89c4\u5212\uff08NeurADP\uff09\u4e0e\u6df1\u5ea6\u53cc\u91cdQ\u7f51\u7edc\uff08DDQN\uff09\u7684\u7ed3\u5408\uff0c\u4f18\u5316\u8ba2\u5355\u5206\u914d\u548c\u52a8\u6001\u5b9a\u4ef7\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u914d\u9001\u6210\u672c\u3002", "motivation": "\u9488\u5bf9\u57ce\u5e02\u6700\u540e\u4e00\u516c\u91cc\u914d\u9001\u7684\u9ad8\u6548\u9700\u6c42\uff0c\u63a2\u7d22\u5229\u7528\u5e97\u5185\u987e\u5ba2\u4f5c\u4e3a\u4f17\u5305\u914d\u9001\u5458\u7684\u53ef\u884c\u6027\u3002", "method": "\u63d0\u51faMDP\u6a21\u578b\u5904\u7406\u8ba2\u5355\u548c\u914d\u9001\u5458\u7684\u968f\u673a\u6027\uff0c\u7ed3\u5408NeurADP\u548cDDQN\u8fdb\u884c\u8ba2\u5355\u5206\u914d\u548c\u52a8\u6001\u5b9a\u4ef7\u7684\u8054\u5408\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cNeurADP + DDQN\u7b56\u7565\u6bd4\u56fa\u5b9a\u5b9a\u4ef7\u8282\u77016.7%\u6210\u672c\uff0c\u6bd4\u77ed\u89c6\u57fa\u7ebf\u8282\u7701\u7ea618%\u3002\u7075\u6d3b\u914d\u9001\u5ef6\u8fdf\u548c\u591a\u76ee\u7684\u5730\u8def\u7531\u8fdb\u4e00\u6b65\u964d\u4f4e\u6210\u672c8%\u548c17%\u3002", "conclusion": "\u52a8\u6001\u524d\u77bb\u6027\u7b56\u7565\u5728\u4f17\u5305\u914d\u9001\u7cfb\u7edf\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u57ce\u5e02\u7269\u6d41\u8fd0\u8425\u5546\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2507.01037", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01037", "abs": "https://arxiv.org/abs/2507.01037", "authors": ["Wenbin Ouyang", "Sirui Li", "Yining Ma", "Cathy Wu"], "title": "Learning to Segment for Vehicle Routing Problems", "comment": null, "summary": "Iterative search heuristics are widely recognized as state-of-the-art for\nsolving Vehicle Routing Problems (VRPs). In this work, we identify and exploit\na critical observation: within these solvers, a large portion of the solution\nremains stable, i.e., unchanged across search iterations, causing redundant\ncomputations, especially for large-scale VRPs with long subtours. To address\nthis, we pioneer the formal study of the First-Segment-Then-Aggregate (FSTA)\ndecomposition technique to accelerate iterative solvers. Specifically, FSTA\npreserves stable solution segments during the search, aggregates nodes within\neach segment into fixed hypernodes, and focuses the search only on unstable\nportions. Yet, a key challenge lies in identifying which segments should be\naggregated by FSTA. To this end, we then introduce Learning-to-Segment (L2Seg),\na novel neural framework to intelligently differentiate potentially stable and\nunstable portions for FSTA decomposition. We present three L2Seg variants:\nnon-autoregressive (globally comprehensive but locally indiscriminate),\nautoregressive (locally refined but globally deficient), and their synergy,\nwith bespoke training and inference strategies. Empirical results on CVRP and\nVRPTW suggest that L2Seg accelerates state-of-the-art iterative solvers by up\nto 7x. Additionally, we provide in-depth analysis showing NAR and AR synergy\nachieves best performance by combining their complementary strengths. Notably,\nL2Seg is a flexible framework that is compatible with traditional,\nlearning-based, and hybrid solvers, while supporting a broad class of VRPs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFSTA\u7684\u5206\u89e3\u6280\u672f\u548cL2Seg\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u7528\u4e8e\u52a0\u901f\u8fed\u4ee3\u5f0f\u8f66\u8f86\u8def\u5f84\u95ee\u9898\uff08VRP\uff09\u6c42\u89e3\u5668\uff0c\u901a\u8fc7\u8bc6\u522b\u5e76\u4fdd\u7559\u7a33\u5b9a\u89e3\u6bb5\uff0c\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe7\u500d\u3002", "motivation": "\u8fed\u4ee3\u5f0f\u641c\u7d22\u542f\u53d1\u5f0f\u65b9\u6cd5\u5728\u89e3\u51b3VRP\u65f6\u5b58\u5728\u5927\u91cf\u5197\u4f59\u8ba1\u7b97\uff0c\u56e0\u4e3a\u89e3\u7684\u5927\u90e8\u5206\u5728\u8fed\u4ee3\u4e2d\u4fdd\u6301\u7a33\u5b9a\u3002", "method": "\u63d0\u51faFSTA\u6280\u672f\u4fdd\u7559\u7a33\u5b9a\u89e3\u6bb5\uff0c\u5e76\u5f15\u5165L2Seg\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\u667a\u80fd\u8bc6\u522b\u7a33\u5b9a\u4e0e\u4e0d\u7a33\u5b9a\u90e8\u5206\uff0c\u8bbe\u8ba1\u4e86\u4e09\u79cdL2Seg\u53d8\u4f53\u3002", "result": "\u5728CVRP\u548cVRPTW\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cL2Seg\u80fd\u5c06\u6c42\u89e3\u5668\u52a0\u901f\u9ad8\u8fbe7\u500d\uff0c\u4e14NAR\u4e0eAR\u7684\u534f\u540c\u6548\u679c\u6700\u4f73\u3002", "conclusion": "L2Seg\u662f\u4e00\u4e2a\u7075\u6d3b\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u4f20\u7edf\u3001\u5b66\u4e60\u548c\u6df7\u5408\u6c42\u89e3\u5668\uff0c\u5e76\u80fd\u652f\u6301\u5e7f\u6cdb\u7684VRP\u95ee\u9898\u3002"}}
{"id": "2507.01305", "categories": ["cs.CV", "cs.GR", "cs.LG", "I.3.3; I.4.8"], "pdf": "https://arxiv.org/pdf/2507.01305", "abs": "https://arxiv.org/abs/2507.01305", "authors": ["Worameth Chinchuthakun", "Pakkapon Phongthawee", "Amit Raj", "Varun Jampani", "Pramook Khungurn", "Supasorn Suwajanakorn"], "title": "DiffusionLight-Turbo: Accelerated Light Probes for Free via Single-Pass Chrome Ball Inpainting", "comment": "arXiv admin note: substantial text overlap with arXiv:2312.09168", "summary": "We introduce a simple yet effective technique for estimating lighting from a\nsingle low-dynamic-range (LDR) image by reframing the task as a chrome ball\ninpainting problem. This approach leverages a pre-trained diffusion model,\nStable Diffusion XL, to overcome the generalization failures of existing\nmethods that rely on limited HDR panorama datasets. While conceptually simple,\nthe task remains challenging because diffusion models often insert incorrect or\ninconsistent content and cannot readily generate chrome balls in HDR format.\nOur analysis reveals that the inpainting process is highly sensitive to the\ninitial noise in the diffusion process, occasionally resulting in unrealistic\noutputs. To address this, we first introduce DiffusionLight, which uses\niterative inpainting to compute a median chrome ball from multiple outputs to\nserve as a stable, low-frequency lighting prior that guides the generation of a\nhigh-quality final result. To generate high-dynamic-range (HDR) light probes,\nan Exposure LoRA is fine-tuned to create LDR images at multiple exposure\nvalues, which are then merged. While effective, DiffusionLight is\ntime-intensive, requiring approximately 30 minutes per estimation. To reduce\nthis overhead, we introduce DiffusionLight-Turbo, which reduces the runtime to\nabout 30 seconds with minimal quality loss. This 60x speedup is achieved by\ntraining a Turbo LoRA to directly predict the averaged chrome balls from the\niterative process. Inference is further streamlined into a single denoising\npass using a LoRA swapping technique. Experimental results that show our method\nproduces convincing light estimates across diverse settings and demonstrates\nsuperior generalization to in-the-wild scenarios. Our code is available at\nhttps://diffusionlight.github.io/turbo", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5c06\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u94ec\u7403\u4fee\u590d\u95ee\u9898\uff0c\u4ece\u5355\u5f20\u4f4e\u52a8\u6001\u8303\u56f4\uff08LDR\uff09\u56fe\u50cf\u4f30\u8ba1\u5149\u7167\u7684\u7b80\u5355\u6709\u6548\u6280\u672f\u3002\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578bStable Diffusion XL\uff0c\u514b\u670d\u4e86\u4f9d\u8d56\u6709\u9650HDR\u5168\u666f\u6570\u636e\u96c6\u65b9\u6cd5\u7684\u6cdb\u5316\u5931\u8d25\u95ee\u9898\u3002\u901a\u8fc7\u8fed\u4ee3\u4fee\u590d\u751f\u6210\u4e2d\u503c\u94ec\u7403\u4f5c\u4e3a\u7a33\u5b9a\u4f4e\u9891\u5149\u7167\u5148\u9a8c\uff0c\u5e76\u5f15\u5165Exposure LoRA\u751f\u6210HDR\u5149\u63a2\u9488\u3002\u8fdb\u4e00\u6b65\u63d0\u51faDiffusionLight-Turbo\uff0c\u5c06\u8fd0\u884c\u65f6\u95f4\u4ece30\u5206\u949f\u7f29\u77ed\u81f330\u79d2\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6709\u9650\u7684HDR\u5168\u666f\u6570\u636e\u96c6\uff0c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u4e14\u6269\u6563\u6a21\u578b\u5728\u751f\u6210HDR\u683c\u5f0f\u94ec\u7403\u65f6\u5b58\u5728\u5185\u5bb9\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "method": "\u4f7f\u7528Stable Diffusion XL\u8fdb\u884c\u94ec\u7403\u4fee\u590d\uff0c\u901a\u8fc7\u8fed\u4ee3\u4fee\u590d\u751f\u6210\u4e2d\u503c\u94ec\u7403\u4f5c\u4e3a\u5149\u7167\u5148\u9a8c\uff0c\u5e76\u5229\u7528Exposure LoRA\u751f\u6210HDR\u5149\u63a2\u9488\u3002\u63d0\u51faDiffusionLight-Turbo\uff0c\u901a\u8fc7\u8bad\u7ec3Turbo LoRA\u76f4\u63a5\u9884\u6d4b\u8fed\u4ee3\u8fc7\u7a0b\u4e2d\u7684\u5e73\u5747\u94ec\u7403\uff0c\u5b9e\u73b060\u500d\u52a0\u901f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u751f\u6210\u903c\u771f\u7684\u5149\u7167\u4f30\u8ba1\uff0c\u5e76\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DiffusionLight\u53ca\u5176\u52a0\u901f\u7248\u672cDiffusionLight-Turbo\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u5149\u7167\u4f30\u8ba1\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u573a\u666f\u3002"}}
{"id": "2507.01437", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01437", "abs": "https://arxiv.org/abs/2507.01437", "authors": ["Ting Xu", "Xiaoxiao Deng", "Xiandong Meng", "Haifeng Yang", "Yan Wu"], "title": "Clinical NLP with Attention-Based Deep Learning for Multi-Disease Prediction", "comment": null, "summary": "This paper addresses the challenges posed by the unstructured nature and\nhigh-dimensional semantic complexity of electronic health record texts. A deep\nlearning method based on attention mechanisms is proposed to achieve unified\nmodeling for information extraction and multi-label disease prediction. The\nstudy is conducted on the MIMIC-IV dataset. A Transformer-based architecture is\nused to perform representation learning over clinical text. Multi-layer\nself-attention mechanisms are employed to capture key medical entities and\ntheir contextual relationships. A Sigmoid-based multi-label classifier is then\napplied to predict multiple disease labels. The model incorporates a\ncontext-aware semantic alignment mechanism, enhancing its representational\ncapacity in typical medical scenarios such as label co-occurrence and sparse\ninformation. To comprehensively evaluate model performance, a series of\nexperiments were conducted, including baseline comparisons, hyperparameter\nsensitivity analysis, data perturbation studies, and noise injection tests.\nResults demonstrate that the proposed method consistently outperforms\nrepresentative existing approaches across multiple performance metrics. The\nmodel maintains strong generalization under varying data scales, interference\nlevels, and model depth configurations. The framework developed in this study\noffers an efficient algorithmic foundation for processing real-world clinical\ntexts and presents practical significance for multi-label medical text modeling\ntasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6587\u672c\u7684\u4fe1\u606f\u63d0\u53d6\u548c\u591a\u6807\u7b7e\u75be\u75c5\u9884\u6d4b\uff0c\u5728MIMIC-IV\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u89e3\u51b3\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6587\u672c\u7684\u975e\u7ed3\u6784\u5316\u548c\u9ad8\u7ef4\u8bed\u4e49\u590d\u6742\u6027\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528Transformer\u67b6\u6784\u548c\u591a\u5c42\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7ed3\u5408Sigmoid\u591a\u6807\u7b7e\u5206\u7c7b\u5668\uff0c\u5e76\u5f15\u5165\u4e0a\u4e0b\u6587\u611f\u77e5\u8bed\u4e49\u5bf9\u9f50\u673a\u5236\u3002", "result": "\u6a21\u578b\u5728\u591a\u9879\u6027\u80fd\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5904\u7406\u771f\u5b9e\u4e34\u5e8a\u6587\u672c\u63d0\u4f9b\u4e86\u9ad8\u6548\u7b97\u6cd5\u57fa\u7840\uff0c\u5bf9\u591a\u6807\u7b7e\u533b\u5b66\u6587\u672c\u5efa\u6a21\u4efb\u52a1\u5177\u6709\u5b9e\u9645\u610f\u4e49\u3002"}}
{"id": "2507.01833", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01833", "abs": "https://arxiv.org/abs/2507.01833", "authors": ["Yi-Dong Shen", "Thomas Eiter"], "title": "Refining Gelfond Rationality Principle Towards More Comprehensive Foundational Principles for Answer Set Semantics", "comment": "76 pages. This article is a significantly extended version of a paper\n  presented by the authors at IJCAI-2022", "summary": "Non-monotonic logic programming is the basis for a declarative problem\nsolving paradigm known as answer set programming (ASP). Departing from the\nseminal definition by Gelfond and Lifschitz in 1988 for simple normal logic\nprograms, various answer set semantics have been proposed for extensions. We\nconsider two important questions: (1) Should the minimal model property,\nconstraint monotonicity and foundedness as defined in the literature be\nmandatory conditions for an answer set semantics in general? (2) If not, what\nother properties could be considered as general principles for answer set\nsemantics? We address the two questions. First, it seems that the three\naforementioned conditions may sometimes be too strong, and we illustrate with\nexamples that enforcing them may exclude expected answer sets. Second, we\nevolve the Gelfond answer set (GAS) principles for answer set construction by\nrefining the Gelfond's rationality principle to well-supportedness, minimality\nw.r.t. negation by default and minimality w.r.t. epistemic negation. The\nprinciple of well-supportedness guarantees that every answer set is\nconstructible from if-then rules obeying a level mapping and is thus free of\ncircular justification, while the two minimality principles ensure that the\nformalism minimizes knowledge both at the level of answer sets and of world\nviews. Third, to embody the refined GAS principles, we extend the notion of\nwell-supportedness substantially to answer sets and world views, respectively.\nFourth, we define new answer set semantics in terms of the refined GAS\nprinciples. Fifth, we use the refined GAS principles as an alternative baseline\nto intuitively assess the existing answer set semantics. Finally, we analyze\nthe computational complexity.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u975e\u5355\u8c03\u903b\u8f91\u7f16\u7a0b\u4e2d\u7684\u7b54\u6848\u96c6\u8bed\u4e49\uff0c\u8d28\u7591\u4e86\u73b0\u6709\u6761\u4ef6\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u7684Gelfond\u7b54\u6848\u96c6\u539f\u5219\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u8ba8\u73b0\u6709\u7b54\u6848\u96c6\u8bed\u4e49\u7684\u6761\u4ef6\u662f\u5426\u8fc7\u4e8e\u4e25\u683c\uff0c\u4ee5\u53ca\u5982\u4f55\u6539\u8fdb\u8fd9\u4e9b\u6761\u4ef6\u4ee5\u66f4\u5408\u7406\u5730\u5b9a\u4e49\u7b54\u6848\u96c6\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5206\u6790\u73b0\u6709\u6761\u4ef6\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u6539\u8fdb\u7684Gelfond\u7b54\u6848\u96c6\u539f\u5219\uff0c\u5e76\u5b9a\u4e49\u65b0\u7684\u7b54\u6848\u96c6\u8bed\u4e49\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u6539\u8fdb\u7684\u539f\u5219\u80fd\u907f\u514d\u6392\u9664\u9884\u671f\u7b54\u6848\u96c6\uff0c\u5e76\u63d0\u4f9b\u4e86\u66f4\u5408\u7406\u7684\u8bed\u4e49\u5b9a\u4e49\u3002", "conclusion": "\u7ed3\u8bba\u662f\u6539\u8fdb\u7684Gelfond\u7b54\u6848\u96c6\u539f\u5219\u4e3a\u7b54\u6848\u96c6\u8bed\u4e49\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u5408\u7406\u7684\u57fa\u7840\u3002"}}
{"id": "2507.01039", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01039", "abs": "https://arxiv.org/abs/2507.01039", "authors": ["Kaaustaaub Shankar", "Wilhelm Louw", "Kelly Cohen"], "title": "On-Policy Optimization of ANFIS Policies Using Proximal Policy Optimization", "comment": "Submitted to NAFIPS 2025", "summary": "We propose a reinforcement learning (RL) approach for training neuro-fuzzy\ncontrollers using Proximal Policy Optimization (PPO). Building on prior work\nthat applied Deep Q-Learning to Adaptive Neuro-Fuzzy Inference Systems (ANFIS),\nour method replaces the off-policy value-based framework with a stable\non-policy actor-critic loop. We evaluate this approach in the CartPole-v1\nenvironment using multiple random seeds and compare its learning performance\nagainst ANFIS-Deep Q-Network (DQN) baselines. It was found that PPO-trained\nfuzzy agents achieved a mean return of 500 +/- 0 on CartPole-v1 after 20000\nupdates, showcasing less variance than prior DQN-based methods during training\nand overall faster convergence. These findings suggest that PPO offers a\npromising pathway for training explainable neuro-fuzzy controllers in\nreinforcement learning tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePPO\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8bad\u7ec3\u795e\u7ecf\u6a21\u7cca\u63a7\u5236\u5668\uff0c\u76f8\u6bd4\u4e4b\u524d\u7684DQN\u65b9\u6cd5\u8868\u73b0\u66f4\u7a33\u5b9a\u4e14\u6536\u655b\u66f4\u5feb\u3002", "motivation": "\u6539\u8fdb\u73b0\u6709\u7684\u57fa\u4e8eDQN\u7684\u795e\u7ecf\u6a21\u7cca\u63a7\u5236\u5668\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5229\u7528PPO\u7684\u7a33\u5b9a\u6027\u548c\u6548\u7387\u63d0\u5347\u6027\u80fd\u3002", "method": "\u4f7f\u7528PPO\u7b97\u6cd5\u66ff\u6362DQN\u6846\u67b6\uff0c\u6784\u5efa\u4e00\u4e2a\u7a33\u5b9a\u7684on-policy actor-critic\u5faa\u73af\uff0c\u5e76\u5728CartPole-v1\u73af\u5883\u4e2d\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "PPO\u8bad\u7ec3\u7684\u6a21\u7cca\u63a7\u5236\u5668\u5728CartPole-v1\u4e2d\u5b9e\u73b0\u4e86500 +/- 0\u7684\u5e73\u5747\u56de\u62a5\uff0c\u8bad\u7ec3\u65b9\u5dee\u66f4\u5c0f\u4e14\u6536\u655b\u66f4\u5feb\u3002", "conclusion": "PPO\u4e3a\u8bad\u7ec3\u53ef\u89e3\u91ca\u7684\u795e\u7ecf\u6a21\u7cca\u63a7\u5236\u5668\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.01340", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01340", "abs": "https://arxiv.org/abs/2507.01340", "authors": ["Cuong Le", "Huy-Phuong Le", "Duc Le", "Minh-Thien Duong", "Van-Binh Nguyen", "My-Ha Le"], "title": "Physics-informed Ground Reaction Dynamics from Human Motion Capture", "comment": "6 pages, 4 figures, 4 tables, HSI 2025", "summary": "Body dynamics are crucial information for the analysis of human motions in\nimportant research fields, ranging from biomechanics, sports science to\ncomputer vision and graphics. Modern approaches collect the body dynamics,\nexternal reactive force specifically, via force plates, synchronizing with\nhuman motion capture data, and learn to estimate the dynamics from a black-box\ndeep learning model. Being specialized devices, force plates can only be\ninstalled in laboratory setups, imposing a significant limitation on the\nlearning of human dynamics. To this end, we propose a novel method for\nestimating human ground reaction dynamics directly from the more reliable\nmotion capture data with physics laws and computational simulation as\nconstrains. We introduce a highly accurate and robust method for computing\nground reaction forces from motion capture data using Euler's integration\nscheme and PD algorithm. The physics-based reactive forces are used to inform\nthe learning model about the physics-informed motion dynamics thus improving\nthe estimation accuracy. The proposed approach was tested on the GroundLink\ndataset, outperforming the baseline model on: 1) the ground reaction force\nestimation accuracy compared to the force plates measurement; and 2) our\nsimulated root trajectory precision. The implementation code is available at\nhttps://github.com/cuongle1206/Phys-GRD", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u7ea6\u675f\u7684\u65b9\u6cd5\uff0c\u76f4\u63a5\u4ece\u8fd0\u52a8\u6355\u6349\u6570\u636e\u4f30\u8ba1\u5730\u9762\u53cd\u4f5c\u7528\u529b\uff0c\u65e0\u9700\u4f9d\u8d56\u529b\u677f\u8bbe\u5907\u3002", "motivation": "\u529b\u677f\u8bbe\u5907\u53ea\u80fd\u5728\u5b9e\u9a8c\u5ba4\u73af\u5883\u4e2d\u4f7f\u7528\uff0c\u9650\u5236\u4e86\u4eba\u4f53\u52a8\u529b\u5b66\u7684\u7814\u7a76\u3002", "method": "\u7ed3\u5408\u6b27\u62c9\u79ef\u5206\u65b9\u6848\u548cPD\u7b97\u6cd5\uff0c\u4ece\u8fd0\u52a8\u6355\u6349\u6570\u636e\u8ba1\u7b97\u5730\u9762\u53cd\u4f5c\u7528\u529b\uff0c\u5e76\u901a\u8fc7\u7269\u7406\u7ea6\u675f\u6539\u8fdb\u5b66\u4e60\u6a21\u578b\u3002", "result": "\u5728GroundLink\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86\u5730\u9762\u53cd\u4f5c\u7528\u529b\u4f30\u8ba1\u548c\u6a21\u62df\u8f68\u8ff9\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u529b\u677f\u8bbe\u5907\uff0c\u901a\u8fc7\u7269\u7406\u7ea6\u675f\u663e\u8457\u63d0\u9ad8\u4e86\u52a8\u529b\u5b66\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2507.01449", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01449", "abs": "https://arxiv.org/abs/2507.01449", "authors": ["Tianyu Liu", "Qitan Lv", "Hao Li", "Xing Gao", "Xiao Sun"], "title": "LogitSpec: Accelerating Retrieval-based Speculative Decoding via Next Next Token Speculation", "comment": null, "summary": "Speculative decoding (SD), where a small draft model is employed to propose\ndraft tokens in advance and then the target model validates them in parallel,\nhas emerged as a promising technique for LLM inference acceleration. Many\nendeavors to improve SD are to eliminate the need for a draft model and\ngenerate draft tokens in a retrieval-based manner in order to further alleviate\nthe drafting overhead and significantly reduce the difficulty in deployment and\napplications. However, retrieval-based SD relies on a matching paradigm to\nretrieval the most relevant reference as the draft tokens, where these methods\noften fail to find matched and accurate draft tokens. To address this\nchallenge, we propose LogitSpec to effectively expand the retrieval range and\nfind the most relevant reference as drafts. Our LogitSpec is motivated by the\nobservation that the logit of the last token can not only predict the next\ntoken, but also speculate the next next token. Specifically, LogitSpec\ngenerates draft tokens in two steps: (1) utilizing the last logit to speculate\nthe next next token; (2) retrieving relevant reference for both the next token\nand the next next token. LogitSpec is training-free and plug-and-play, which\ncan be easily integrated into existing LLM inference frameworks. Extensive\nexperiments on a wide range of text generation benchmarks demonstrate that\nLogitSpec can achieve up to 2.61 $\\times$ speedup and 3.28 mean accepted tokens\nper decoding step. Our code is available at\nhttps://github.com/smart-lty/LogitSpec.", "AI": {"tldr": "LogitSpec\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u68c0\u7d22\u5f0f\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u6700\u540e\u4e00\u4e2atoken\u7684logit\u63a8\u6d4b\u540e\u7eedtoken\uff0c\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u68c0\u7d22\u5f0f\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u96be\u4ee5\u627e\u5230\u5339\u914d\u7684\u51c6\u786e\u8349\u6848token\uff0c\u9650\u5236\u4e86\u5176\u6548\u679c\u3002", "method": "LogitSpec\u5206\u4e24\u6b65\u751f\u6210\u8349\u6848token\uff1a\u5229\u7528\u6700\u540e\u4e00\u4e2atoken\u7684logit\u63a8\u6d4b\u4e0b\u4e00\u4e2a\u53ca\u4e0b\u4e0b\u4e2atoken\uff0c\u5e76\u68c0\u7d22\u76f8\u5173\u53c2\u8003\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLogitSpec\u53ef\u5b9e\u73b0\u6700\u9ad82.61\u500d\u52a0\u901f\uff0c\u5e73\u5747\u6bcf\u89e3\u7801\u6b65\u9aa4\u63a5\u53d73.28\u4e2atoken\u3002", "conclusion": "LogitSpec\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u6613\u96c6\u6210\u7684\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2507.01040", "categories": ["cs.LG", "cs.AI", "cs.NE", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.01040", "abs": "https://arxiv.org/abs/2507.01040", "authors": ["Tianxiang Xia", "Max Neuwinger", "Lin Xiao"], "title": "Fast Clifford Neural Layers", "comment": "7 pages content-wise", "summary": "Clifford Neural Layers improve PDE modeling by introducing Clifford Algebra\ninto neural networks. In this project we focus on optimizing the inference of\n2/3D Clifford convolutional layers and multivector activation layers for one\ncore CPU performance.\n  Overall, by testing on a real network block involving Clifford convolutional\nlayers and multivector activation layers, we observe that our implementation is\n30% faster than standard PyTorch implementation in relatively large data +\nnetwork size (>L2 cache).\n  We open source our code base at\nhttps://github.com/egretwAlker/c-opt-clifford-layers", "AI": {"tldr": "Clifford Neural Layers\u901a\u8fc7\u5f15\u5165Clifford\u4ee3\u6570\u4f18\u5316PDE\u5efa\u6a21\uff0c\u5728\u5355\u6838CPU\u4e0a\u63d0\u53472/3D Clifford\u5377\u79ef\u5c42\u548c\u591a\u5411\u91cf\u6fc0\u6d3b\u5c42\u7684\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u5229\u7528Clifford\u4ee3\u6570\u6539\u8fdb\u795e\u7ecf\u7f51\u7edc\u5bf9PDE\u7684\u5efa\u6a21\u80fd\u529b\uff0c\u5e76\u4f18\u5316\u8ba1\u7b97\u6027\u80fd\u3002", "method": "\u4f18\u53162/3D Clifford\u5377\u79ef\u5c42\u548c\u591a\u5411\u91cf\u6fc0\u6d3b\u5c42\u7684\u63a8\u7406\uff0c\u9488\u5bf9\u5355\u6838CPU\u6027\u80fd\u8fdb\u884c\u6539\u8fdb\u3002", "result": "\u5728\u8f83\u5927\u6570\u636e\u548c\u7f51\u7edc\u89c4\u6a21\uff08>L2\u7f13\u5b58\uff09\u4e0b\uff0c\u5b9e\u73b0\u6bd4\u6807\u51c6PyTorch\u5feb30%\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316Clifford\u5c42\uff0c\u663e\u8457\u63d0\u5347\u4e86PDE\u5efa\u6a21\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.01342", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01342", "abs": "https://arxiv.org/abs/2507.01342", "authors": ["Luxi Zhao", "Mahmoud Afifi", "Michael S. Brown"], "title": "Learning Camera-Agnostic White-Balance Preferences", "comment": null, "summary": "The image signal processor (ISP) pipeline in modern cameras consists of\nseveral modules that transform raw sensor data into visually pleasing images in\na display color space. Among these, the auto white balance (AWB) module is\nessential for compensating for scene illumination. However, commercial AWB\nsystems often strive to compute aesthetic white-balance preferences rather than\naccurate neutral color correction. While learning-based methods have improved\nAWB accuracy, they typically struggle to generalize across different camera\nsensors -- an issue for smartphones with multiple cameras. Recent work has\nexplored cross-camera AWB, but most methods remain focused on achieving neutral\nwhite balance. In contrast, this paper is the first to address aesthetic\nconsistency by learning a post-illuminant-estimation mapping that transforms\nneutral illuminant corrections into aesthetically preferred corrections in a\ncamera-agnostic space. Once trained, our mapping can be applied after any\nneutral AWB module to enable consistent and stylized color rendering across\nunseen cameras. Our proposed model is lightweight -- containing only $\\sim$500\nparameters -- and runs in just 0.024 milliseconds on a typical flagship mobile\nCPU. Evaluated on a dataset of 771 smartphone images from three different\ncameras, our method achieves state-of-the-art performance while remaining fully\ncompatible with existing cross-camera AWB techniques, introducing minimal\ncomputational and memory overhead.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u540e\u7167\u660e\u4f30\u8ba1\u6620\u5c04\uff0c\u5c06\u4e2d\u6027\u767d\u5e73\u8861\u6821\u6b63\u8f6c\u6362\u4e3a\u7f8e\u5b66\u504f\u597d\u6821\u6b63\uff0c\u5b9e\u73b0\u8de8\u76f8\u673a\u7f8e\u5b66\u4e00\u81f4\u6027\u3002", "motivation": "\u5546\u4e1a\u81ea\u52a8\u767d\u5e73\u8861\uff08AWB\uff09\u7cfb\u7edf\u901a\u5e38\u8ffd\u6c42\u7f8e\u5b66\u504f\u597d\u800c\u975e\u4e2d\u6027\u8272\u5f69\u6821\u6b63\uff0c\u4e14\u73b0\u6709\u5b66\u4e60\u578b\u65b9\u6cd5\u96be\u4ee5\u6cdb\u5316\u5230\u4e0d\u540c\u76f8\u673a\u4f20\u611f\u5668\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u7f8e\u5b66\u4e00\u81f4\u6027\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5b66\u4e60\u4e00\u4e2a\u540e\u7167\u660e\u4f30\u8ba1\u6620\u5c04\uff0c\u5c06\u4e2d\u6027\u767d\u5e73\u8861\u6821\u6b63\u8f6c\u6362\u4e3a\u7f8e\u5b66\u504f\u597d\u6821\u6b63\uff0c\u5e76\u5728\u76f8\u673a\u65e0\u5173\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u3002\u6a21\u578b\u4ec5\u542b\u7ea6500\u53c2\u6570\uff0c\u8ba1\u7b97\u9ad8\u6548\u3002", "result": "\u5728771\u5f20\u6765\u81ea\u4e09\u79cd\u4e0d\u540c\u667a\u80fd\u624b\u673a\u76f8\u673a\u7684\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8ba1\u7b97\u65f6\u95f4\u4ec5\u4e3a0.024\u6beb\u79d2\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8f7b\u91cf\u9ad8\u6548\uff0c\u517c\u5bb9\u73b0\u6709\u8de8\u76f8\u673aAWB\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f8e\u5b66\u4e00\u81f4\u6027\u4e14\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002"}}
{"id": "2507.01479", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01479", "abs": "https://arxiv.org/abs/2507.01479", "authors": ["Yingqiang Gao", "Kaede Johnson", "David Froehlich", "Luisa Carrer", "Sarah Ebling"], "title": "Evaluating the Effectiveness of Direct Preference Optimization for Personalizing German Automatic Text Simplifications for Persons with Intellectual Disabilities", "comment": null, "summary": "Automatic text simplification (ATS) aims to enhance language accessibility\nfor various target groups, particularly persons with intellectual disabilities.\nRecent advancements in generative AI, especially large language models (LLMs),\nhave substantially improved the quality of machine-generated text\nsimplifications, thereby mitigating information barriers for the target group.\nHowever, existing LLM-based ATS systems do not incorporate preference feedback\non text simplifications during training, resulting in a lack of personalization\ntailored to the specific needs of target group representatives.\n  In this work, we extend the standard supervised fine-tuning (SFT) approach\nfor adapting LLM-based ATS models by leveraging a computationally efficient LLM\nalignment technique -- direct preference optimization (DPO). Specifically, we\npost-train LLM-based ATS models using human feedback collected from persons\nwith intellectual disabilities, reflecting their preferences on paired text\nsimplifications generated by mainstream LLMs. Furthermore, we propose a\npipeline for developing personalized LLM-based ATS systems, encompassing data\ncollection, model selection, SFT and DPO post-training, and evaluation. Our\nfindings underscore the necessity of active participation of target group\npersons in designing personalized AI accessibility solutions aligned with human\nexpectations. This work represents a step towards personalizing inclusive AI\nsystems at the target-group level, incorporating insights not only from text\nsimplification experts but also from target group persons themselves.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e2a\u6027\u5316\u8c03\u6574\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u81ea\u52a8\u6587\u672c\u7b80\u5316\uff08ATS\uff09\u7cfb\u7edf\uff0c\u4ee5\u6ee1\u8db3\u667a\u529b\u969c\u788d\u4eba\u58eb\u7684\u9700\u6c42\u3002", "motivation": "\u73b0\u6709LLM-based ATS\u7cfb\u7edf\u7f3a\u4e4f\u5bf9\u76ee\u6807\u7fa4\u4f53\u504f\u597d\u7684\u53cd\u9988\uff0c\u5bfc\u81f4\u4e2a\u6027\u5316\u4e0d\u8db3\u3002", "method": "\u6269\u5c55\u4e86\u6807\u51c6\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u65b9\u6cd5\uff0c\u91c7\u7528DPO\u6280\u672f\uff0c\u7ed3\u5408\u667a\u529b\u969c\u788d\u4eba\u58eb\u7684\u504f\u597d\u53cd\u9988\u8fdb\u884c\u540e\u8bad\u7ec3\u3002", "result": "\u7814\u7a76\u5f3a\u8c03\u4e86\u76ee\u6807\u7fa4\u4f53\u53c2\u4e0e\u8bbe\u8ba1\u4e2a\u6027\u5316AI\u89e3\u51b3\u65b9\u6848\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u4e2a\u6027\u5316\u5305\u5bb9\u6027AI\u7cfb\u7edf\u8fc8\u51fa\u4e86\u4e00\u6b65\uff0c\u878d\u5408\u4e86\u4e13\u5bb6\u548c\u76ee\u6807\u7fa4\u4f53\u7684\u89c1\u89e3\u3002"}}
{"id": "2507.01041", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01041", "abs": "https://arxiv.org/abs/2507.01041", "authors": ["Zuguang Li", "Wen Wu", "Shaohua Wu", "Songge Zhang", "Ye Wang", "Xuemin", "Shen"], "title": "Fast AI Model Splitting over Edge Networks", "comment": "13 pages, 14 figures", "summary": "Split learning (SL) has emerged as a computationally efficient approach for\nartificial intelligence (AI) model training, which can alleviate device-side\ncomputational workloads. However, complex AI model architectures pose high\ncomputational complexity to obtain the optimal model splitting. In this paper,\nwe represent an arbitrary AI model as a directed acyclic graph (DAG), and then\nreformulate the optimal model splitting problem as a minimum s-t cut search\nproblem. To solve the problem, we propose a fast DAG-based model splitting\nalgorithm, which restructures the DAG to enable the optimal model splitting\nidentification via a maximum flow method. Theoretical analysis indicates that\nthe proposed algorithm is optimal. Furthermore, considering AI models with\nblock structures, we propose a block-wise model splitting algorithm to reduce\ncomputational complexity. The algorithm abstracts each block, i.e., a component\nconsisting of multiple layers, into a single vertex, thereby obtaining the\noptimal model splitting via a simplified DAG. Extensive experimental results\ndemonstrate that the proposed algorithms can determine the optimal model\nsplitting within milliseconds, as well as reduce training delay by\n24.62%-38.95% in dynamic edge networks as compared to the state-of-the-art\nbenchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDAG\u7684\u5feb\u901f\u6a21\u578b\u5206\u5272\u7b97\u6cd5\uff0c\u901a\u8fc7\u6700\u5927\u6d41\u65b9\u6cd5\u4f18\u5316AI\u6a21\u578b\u5206\u5272\uff0c\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u5ef6\u8fdf\u3002", "motivation": "\u89e3\u51b3\u590d\u6742AI\u6a21\u578b\u5206\u5272\u7684\u9ad8\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u63d0\u5347\u8bbe\u5907\u7aef\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u5c06AI\u6a21\u578b\u8868\u793a\u4e3aDAG\uff0c\u5e76\u91cd\u6784\u4e3a\u6700\u5c0fs-t\u5272\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8eDAG\u548c\u5757\u7ed3\u6784\u7684\u5feb\u901f\u5206\u5272\u7b97\u6cd5\u3002", "result": "\u7b97\u6cd5\u5728\u6beb\u79d2\u7ea7\u5185\u627e\u5230\u6700\u4f18\u5206\u5272\uff0c\u52a8\u6001\u8fb9\u7f18\u7f51\u7edc\u4e2d\u8bad\u7ec3\u5ef6\u8fdf\u964d\u4f4e24.62%-38.95%\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u8fb9\u7f18\u7f51\u7edc\u3002"}}
{"id": "2507.01347", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01347", "abs": "https://arxiv.org/abs/2507.01347", "authors": ["Andrei Jelea", "Ahmed Nabil Belbachir", "Marius Leordeanu"], "title": "Learning from Random Subspace Exploration: Generalized Test-Time Augmentation with Self-supervised Distillation", "comment": null, "summary": "We introduce Generalized Test-Time Augmentation (GTTA), a highly effective\nmethod for improving the performance of a trained model, which unlike other\nexisting Test-Time Augmentation approaches from the literature is general\nenough to be used off-the-shelf for many vision and non-vision tasks, such as\nclassification, regression, image segmentation and object detection. By\napplying a new general data transformation, that randomly perturbs multiple\ntimes the PCA subspace projection of a test input, GTTA forms robust ensembles\nat test time in which, due to sound statistical properties, the structural and\nsystematic noises in the initial input data is filtered out and final estimator\nerrors are reduced. Different from other existing methods, we also propose a\nfinal self-supervised learning stage in which the ensemble output, acting as an\nunsupervised teacher, is used to train the initial single student model, thus\nreducing significantly the test time computational cost, at no loss in\naccuracy. Our tests and comparisons to strong TTA approaches and SoTA models on\nvarious vision and non-vision well-known datasets and tasks, such as image\nclassification and segmentation, speech recognition and house price prediction,\nvalidate the generality of the proposed GTTA. Furthermore, we also prove its\neffectiveness on the more specific real-world task of salmon segmentation and\ndetection in low-visibility underwater videos, for which we introduce\nDeepSalmon, the largest dataset of its kind in the literature.", "AI": {"tldr": "GTTA\u662f\u4e00\u79cd\u901a\u7528\u7684\u6d4b\u8bd5\u65f6\u589e\u5f3a\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u89c6\u89c9\u548c\u975e\u89c6\u89c9\u4efb\u52a1\uff0c\u901a\u8fc7\u6270\u52a8PCA\u5b50\u7a7a\u95f4\u6295\u5f71\u5f62\u6210\u9c81\u68d2\u96c6\u6210\uff0c\u5e76\u7ed3\u5408\u81ea\u76d1\u7763\u5b66\u4e60\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u589e\u5f3a\u65b9\u6cd5\u7f3a\u4e4f\u901a\u7528\u6027\uff0cGTTA\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u7684\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u968f\u673a\u6270\u52a8PCA\u5b50\u7a7a\u95f4\u6295\u5f71\u751f\u6210\u591a\u4e2a\u6d4b\u8bd5\u8f93\u5165\uff0c\u5f62\u6210\u9c81\u68d2\u96c6\u6210\uff0c\u5e76\u7ed3\u5408\u81ea\u76d1\u7763\u5b66\u4e60\u4f18\u5316\u6a21\u578b\u3002", "result": "\u5728\u591a\u4e2a\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86GTTA\u7684\u901a\u7528\u6027\u548c\u6709\u6548\u6027\uff0c\u5305\u62ec\u4f4e\u80fd\u89c1\u5ea6\u6c34\u4e0b\u89c6\u9891\u4e2d\u7684\u4e09\u6587\u9c7c\u5206\u5272\u4e0e\u68c0\u6d4b\u3002", "conclusion": "GTTA\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u6d4b\u8bd5\u65f6\u589e\u5f3a\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u4e14\u4e0d\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2507.01541", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01541", "abs": "https://arxiv.org/abs/2507.01541", "authors": ["\u00c1lvaro Zaera", "Diana Nicoleta Popa", "Ivan Sekulic", "Paolo Rosso"], "title": "Efficient Out-of-Scope Detection in Dialogue Systems via Uncertainty-Driven LLM Routing", "comment": null, "summary": "Out-of-scope (OOS) intent detection is a critical challenge in task-oriented\ndialogue systems (TODS), as it ensures robustness to unseen and ambiguous\nqueries. In this work, we propose a novel but simple modular framework that\ncombines uncertainty modeling with fine-tuned large language models (LLMs) for\nefficient and accurate OOS detection. The first step applies uncertainty\nestimation to the output of an in-scope intent detection classifier, which is\ncurrently deployed in a real-world TODS handling tens of thousands of user\ninteractions daily. The second step then leverages an emerging LLM-based\napproach, where a fine-tuned LLM is triggered to make a final decision on\ninstances with high uncertainty. Unlike prior approaches, our method\neffectively balances computational efficiency and performance, combining\ntraditional approaches with LLMs and yielding state-of-the-art results on key\nOOS detection benchmarks, including real-world OOS data acquired from a\ndeployed TODS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u548c\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u51c6\u786e\u5730\u68c0\u6d4b\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u7684\u8d85\u51fa\u8303\u56f4\uff08OOS\uff09\u610f\u56fe\u3002", "motivation": "\u89e3\u51b3\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u5bf9\u672a\u89c1\u6216\u6a21\u7cca\u67e5\u8be2\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u63d0\u5347OOS\u68c0\u6d4b\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "1. \u5bf9\u5df2\u90e8\u7f72\u7684\u610f\u56fe\u5206\u7c7b\u5668\u8f93\u51fa\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff1b2. \u5bf9\u9ad8\u4e0d\u786e\u5b9a\u6027\u5b9e\u4f8b\u89e6\u53d1\u5fae\u8c03\u7684LLM\u8fdb\u884c\u6700\u7ec8\u51b3\u7b56\u3002", "result": "\u5728\u5173\u952eOOS\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5305\u62ec\u6765\u81ea\u5b9e\u9645\u90e8\u7f72\u7cfb\u7edf\u7684\u6570\u636e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5e73\u8861\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\uff0c\u7ed3\u5408\u4f20\u7edf\u65b9\u6cd5\u4e0eLLM\uff0c\u4e3aOOS\u68c0\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01043", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01043", "abs": "https://arxiv.org/abs/2507.01043", "authors": ["Szymon \u015awiderski", "Agnieszka Jastrz\u0119bska"], "title": "Data Classification with Dynamically Growing and Shrinking Neural Networks", "comment": "Paper submitted to Journal of Computational Science", "summary": "The issue of data-driven neural network model construction is one of the core\nproblems in the domain of Artificial Intelligence. A standard approach assumes\na fixed architecture with trainable weights. A conceptually more advanced\nassumption is that we not only train the weights, but also find out the optimal\nmodel architecture. We present a new method that realizes just that. This\narticle is an extended version of our conference paper titled \"Dynamic Growing\nand Shrinking of Neural Networks with Monte Carlo Tree Search [26]\". In the\npaper, we show in detail how to create a neural network with a procedure that\nallows dynamic shrinking and growing of the model while it is being trained.\nThe decision-making mechanism for the architectural design is governed by a\nMonte Carlo tree search procedure which simulates network behavior and allows\nto compare several candidate architecture changes to choose the best one. The\nproposed method was validated using both visual and time series datasets,\ndemonstrating its particular effectiveness in multivariate time series\nclassification. This is attributed to the architecture's ability to adapt\ndynamically, allowing independent modifications for each time series. The\napproach is supplemented by Python source code for reproducibility.\nExperimental evaluations in visual pattern and multivariate time series\nclassification tasks revealed highly promising performance, underscoring the\nmethod's robustness and adaptability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u8c03\u6574\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7684\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u4f18\u5316\u6a21\u578b\u7ed3\u6784\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u52a8\u6001\u589e\u51cf\u7f51\u7edc\u5c42\uff0c\u9002\u7528\u4e8e\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u56fa\u5b9a\uff0c\u4ec5\u8bad\u7ec3\u6743\u91cd\uff0c\u9650\u5236\u4e86\u6a21\u578b\u6027\u80fd\u3002\u52a8\u6001\u8c03\u6574\u67b6\u6784\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u6a21\u578b\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u6a21\u62df\u7f51\u7edc\u884c\u4e3a\uff0c\u52a8\u6001\u9009\u62e9\u6700\u4f18\u67b6\u6784\u53d8\u5316\uff0c\u652f\u6301\u8bad\u7ec3\u4e2d\u589e\u51cf\u7f51\u7edc\u5c42\u3002", "result": "\u5728\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6a21\u578b\u52a8\u6001\u9002\u5e94\u6027\u5f3a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u548c\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684Python\u4ee3\u7801\u3002"}}
{"id": "2507.01351", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01351", "abs": "https://arxiv.org/abs/2507.01351", "authors": ["Chaoxiang Cai", "Longrong Yang", "Kaibing Chen", "Fan Yang", "Xi Li"], "title": "Long-Tailed Distribution-Aware Router For Mixture-of-Experts in Large Vision-Language Model", "comment": null, "summary": "The mixture-of-experts (MoE), which replaces dense models with sparse\narchitectures, has gained attention in large vision-language models (LVLMs) for\nachieving comparable performance with fewer activated parameters. Existing MoE\nframeworks for LVLMs focus on token-to-expert routing (TER), encouraging\ndifferent experts to specialize in processing distinct tokens. However, these\nframeworks often rely on the load balancing mechanism, overlooking the inherent\ndistributional differences between vision and language. To this end, we propose\na Long-Tailed Distribution-aware Router (LTDR) for vision-language TER,\ntackling two challenges: (1) Distribution-aware router for modality-specific\nrouting. We observe that language TER follows a uniform distribution, whereas\nvision TER exhibits a long-tailed distribution. This discrepancy necessitates\ndistinct routing strategies tailored to each modality. (2) Enhancing expert\nactivation for vision tail tokens. Recognizing the importance of vision tail\ntokens, we introduce an oversampling-like strategy by increasing the number of\nactivated experts for these tokens. Experiments on extensive benchmarks\nvalidate the effectiveness of our approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u957f\u5c3e\u5206\u5e03\u611f\u77e5\u8def\u7531\u5668\uff08LTDR\uff09\uff0c\u7528\u4e8e\u89c6\u89c9-\u8bed\u8a00\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff08MoE\uff09\uff0c\u89e3\u51b3\u4e86\u6a21\u6001\u95f4\u5206\u5e03\u5dee\u5f02\u548c\u89c6\u89c9\u5c3e\u90e8\u4ee4\u724c\u4e13\u5bb6\u6fc0\u6d3b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709MoE\u6846\u67b6\u5728\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\u4f9d\u8d56\u8d1f\u8f7d\u5747\u8861\u673a\u5236\uff0c\u5ffd\u89c6\u4e86\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u6001\u95f4\u7684\u5206\u5e03\u5dee\u5f02\uff0c\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\u3002", "method": "\u63d0\u51faLTDR\uff0c\u5305\u62ec\u6a21\u6001\u611f\u77e5\u8def\u7531\u7b56\u7565\u548c\u9488\u5bf9\u89c6\u89c9\u5c3e\u90e8\u4ee4\u724c\u7684\u4e13\u5bb6\u6fc0\u6d3b\u589e\u5f3a\u7b56\u7565\uff08\u7c7b\u4f3c\u8fc7\u91c7\u6837\uff09\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "LTDR\u901a\u8fc7\u6a21\u6001\u611f\u77e5\u8def\u7531\u548c\u4e13\u5bb6\u6fc0\u6d3b\u589e\u5f3a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9-\u8bed\u8a00MoE\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.01543", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01543", "abs": "https://arxiv.org/abs/2507.01543", "authors": ["Quang Minh Nguyen", "Taegyoon Kim"], "title": "Is External Information Useful for Stance Detection with LLMs?", "comment": "ACL Findings 2025", "summary": "In the stance detection task, a text is classified as either favorable,\nopposing, or neutral towards a target. Prior work suggests that the use of\nexternal information, e.g., excerpts from Wikipedia, improves stance detection\nperformance. However, whether or not such information can benefit large\nlanguage models (LLMs) remains an unanswered question, despite their wide\nadoption in many reasoning tasks. In this study, we conduct a systematic\nevaluation on how Wikipedia and web search external information can affect\nstance detection across eight LLMs and in three datasets with 12 targets.\nSurprisingly, we find that such information degrades performance in most cases,\nwith macro F1 scores dropping by up to 27.9\\%. We explain this through\nexperiments showing LLMs' tendency to align their predictions with the stance\nand sentiment of the provided information rather than the ground truth stance\nof the given text. We also find that performance degradation persists with\nchain-of-thought prompting, while fine-tuning mitigates but does not fully\neliminate it. Our findings, in contrast to previous literature on BERT-based\nsystems which suggests that external information enhances performance,\nhighlight the risks of information biases in LLM-based stance classifiers. Code\nis available at https://github.com/ngqm/acl2025-stance-detection.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u7acb\u573a\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0c\u5916\u90e8\u4fe1\u606f\uff08\u5982\u7ef4\u57fa\u767e\u79d1\u6216\u7f51\u7edc\u641c\u7d22\u5185\u5bb9\uff09\u901a\u5e38\u964d\u4f4e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6027\u80fd\uff0c\u800c\u975e\u63d0\u5347\u3002", "motivation": "\u63a2\u8ba8\u5916\u90e8\u4fe1\u606f\u5bf9LLM\u5728\u7acb\u573a\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u5f71\u54cd\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u516b\u79cdLLM\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u5206\u6790\u5916\u90e8\u4fe1\u606f\u5bf9\u5176\u7acb\u573a\u68c0\u6d4b\u7684\u5f71\u54cd\u3002", "result": "\u5916\u90e8\u4fe1\u606f\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u964d\u4f4e\u6027\u80fd\uff0cF1\u5206\u6570\u6700\u591a\u4e0b\u964d27.9%\uff0c\u4e14LLM\u503e\u5411\u4e8e\u4e0e\u5916\u90e8\u4fe1\u606f\u7684\u7acb\u573a\u5bf9\u9f50\u3002", "conclusion": "\u5916\u90e8\u4fe1\u606f\u53ef\u80fd\u5f15\u5165\u504f\u89c1\uff0c\u5f71\u54cdLLM\u7684\u7acb\u573a\u68c0\u6d4b\u6027\u80fd\uff0c\u4e0eBERT\u7b49\u6a21\u578b\u7684\u7ed3\u8bba\u76f8\u53cd\u3002"}}
{"id": "2507.01045", "categories": ["cs.LG", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.01045", "abs": "https://arxiv.org/abs/2507.01045", "authors": ["Xiao Gu", "Wei Tang", "Jinpei Han", "Veer Sangha", "Fenglin Liu", "Shreyank N Gowda", "Antonio H. Ribeiro", "Patrick Schwab", "Kim Branson", "Lei Clifton", "Antonio Luiz P. Ribeiro", "Zhangdaihong Liu", "David A. Clifton"], "title": "Sensing Cardiac Health Across Scenarios and Devices: A Multi-Modal Foundation Model Pretrained on Heterogeneous Data from 1.7 Million Individuals", "comment": null, "summary": "Cardiac biosignals, such as electrocardiograms (ECG) and photoplethysmograms\n(PPG), are of paramount importance for the diagnosis, prevention, and\nmanagement of cardiovascular diseases, and have been extensively used in a\nvariety of clinical tasks. Conventional deep learning approaches for analyzing\nthese signals typically rely on homogeneous datasets and static bespoke models,\nlimiting their robustness and generalizability across diverse clinical settings\nand acquisition protocols. In this study, we present a cardiac sensing\nfoundation model (CSFM) that leverages advanced transformer architectures and a\ngenerative, masked pretraining strategy to learn unified representations from\nvast, heterogeneous health records. Our model is pretrained on an innovative\nmulti-modal integration of data from multiple large-scale datasets (including\nMIMIC-III-WDB, MIMIC-IV-ECG, and CODE), comprising cardiac signals and the\ncorresponding clinical or machine-generated text reports from approximately 1.7\nmillion individuals. We demonstrate that the embeddings derived from our CSFM\nnot only serve as effective feature extractors across diverse cardiac sensing\nscenarios, but also enable seamless transfer learning across varying input\nconfigurations and sensor modalities. Extensive evaluations across diagnostic\ntasks, demographic information recognition, vital sign measurement, clinical\noutcome prediction, and ECG question answering reveal that CSFM consistently\noutperforms traditional one-modal-one-task approaches. Notably, CSFM exhibits\nrobust performance across multiple ECG lead configurations from standard\n12-lead systems to single-lead setups, and in scenarios where only ECG, only\nPPG, or a combination thereof is available. These findings highlight the\npotential of CSFM as a versatile and scalable solution, for comprehensive\ncardiac monitoring.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u67b6\u6784\u7684\u5fc3\u810f\u611f\u77e5\u57fa\u7840\u6a21\u578b\uff08CSFM\uff09\uff0c\u901a\u8fc7\u751f\u6210\u5f0f\u63a9\u7801\u9884\u8bad\u7ec3\u7b56\u7565\u4ece\u591a\u6a21\u6001\u5f02\u6784\u6570\u636e\u4e2d\u5b66\u4e60\u7edf\u4e00\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5fc3\u810f\u4fe1\u53f7\u5206\u6790\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u540c\u8d28\u6570\u636e\u96c6\u548c\u9759\u6001\u6a21\u578b\uff0c\u9650\u5236\u4e86\u5176\u5728\u591a\u6837\u5316\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528Transformer\u67b6\u6784\u548c\u751f\u6210\u5f0f\u63a9\u7801\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\uff08\u5305\u62ecMIMIC-III-WDB\u3001MIMIC-IV-ECG\u548cCODE\u6570\u636e\u96c6\uff09\uff0c\u8986\u76d6\u7ea6170\u4e07\u4e2a\u4f53\u7684\u5fc3\u810f\u4fe1\u53f7\u548c\u4e34\u5e8a\u62a5\u544a\u3002", "result": "CSFM\u5728\u8bca\u65ad\u4efb\u52a1\u3001\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u8bc6\u522b\u3001\u751f\u547d\u4f53\u5f81\u6d4b\u91cf\u3001\u4e34\u5e8a\u7ed3\u679c\u9884\u6d4b\u548cECG\u95ee\u7b54\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u652f\u6301\u591a\u79cd\u8f93\u5165\u914d\u7f6e\u548c\u4f20\u611f\u5668\u6a21\u6001\u3002", "conclusion": "CSFM\u4f5c\u4e3a\u4e00\u79cd\u591a\u529f\u80fd\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u5168\u9762\u5fc3\u810f\u76d1\u6d4b\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002"}}
{"id": "2507.01367", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01367", "abs": "https://arxiv.org/abs/2507.01367", "authors": ["Tianrui Lou", "Xiaojun Jia", "Siyuan Liang", "Jiawei Liang", "Ming Zhang", "Yanjun Xiao", "Xiaochun Cao"], "title": "3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial Camouflage Generation", "comment": "Accepted by ICCV 2025", "summary": "Physical adversarial attack methods expose the vulnerabilities of deep neural\nnetworks and pose a significant threat to safety-critical scenarios such as\nautonomous driving. Camouflage-based physical attack is a more promising\napproach compared to the patch-based attack, offering stronger adversarial\neffectiveness in complex physical environments. However, most prior work relies\non mesh priors of the target object and virtual environments constructed by\nsimulators, which are time-consuming to obtain and inevitably differ from the\nreal world. Moreover, due to the limitations of the backgrounds in training\nimages, previous methods often fail to produce multi-view robust adversarial\ncamouflage and tend to fall into sub-optimal solutions. Due to these reasons,\nprior work lacks adversarial effectiveness and robustness across diverse\nviewpoints and physical environments. We propose a physical attack framework\nbased on 3D Gaussian Splatting (3DGS), named PGA, which provides rapid and\nprecise reconstruction with few images, along with photo-realistic rendering\ncapabilities. Our framework further enhances cross-view robustness and\nadversarial effectiveness by preventing mutual and self-occlusion among\nGaussians and employing a min-max optimization approach that adjusts the\nimaging background of each viewpoint, helping the algorithm filter out\nnon-robust adversarial features. Extensive experiments validate the\neffectiveness and superiority of PGA. Our code is available\nat:https://github.com/TRLou/PGA.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6563\u5c04\uff083DGS\uff09\u7684\u7269\u7406\u653b\u51fb\u6846\u67b6PGA\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u6a21\u62df\u73af\u5883\u548c\u7f51\u683c\u5148\u9a8c\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u591a\u89c6\u89d2\u9c81\u68d2\u6027\u548c\u5bf9\u6297\u6548\u679c\u3002", "motivation": "\u7269\u7406\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\uff08\u5982\u81ea\u52a8\u9a7e\u9a76\uff09\u4e2d\u66b4\u9732\u51fa\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u8106\u5f31\u6027\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6a21\u62df\u73af\u5883\u548c\u7f51\u683c\u5148\u9a8c\uff0c\u6548\u679c\u548c\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51faPGA\u6846\u67b6\uff0c\u5229\u75283DGS\u5b9e\u73b0\u5feb\u901f\u7cbe\u786e\u91cd\u5efa\u548c\u903c\u771f\u6e32\u67d3\uff0c\u901a\u8fc7\u9632\u6b62\u9ad8\u65af\u76f8\u4e92\u906e\u6321\u548c\u81ea\u906e\u6321\uff0c\u4ee5\u53ca\u91c7\u7528min-max\u4f18\u5316\u8c03\u6574\u80cc\u666f\uff0c\u589e\u5f3a\u5bf9\u6297\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86PGA\u5728\u591a\u89c6\u89d2\u548c\u590d\u6742\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "PGA\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u7269\u7406\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u89c6\u89d2\u548c\u5b9e\u9645\u73af\u5883\u3002"}}
{"id": "2507.01594", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01594", "abs": "https://arxiv.org/abs/2507.01594", "authors": ["Shutong Feng", "Hsien-chin Lin", "Nurul Lubis", "Carel van Niekerk", "Michael Heck", "Benjamin Ruppik", "Renato Vukovic", "Milica Ga\u0161i\u0107"], "title": "Emotionally Intelligent Task-oriented Dialogue Systems: Architecture, Representation, and Optimisation", "comment": "19 pages, 6 figures", "summary": "Task-oriented dialogue (ToD) systems are designed to help users achieve\nspecific goals through natural language interaction. While recent advances in\nlarge language models (LLMs) have significantly improved linguistic fluency and\ncontextual understanding, building effective and emotionally intelligent ToD\nsystems remains a complex challenge. Effective ToD systems must optimise for\ntask success, emotional understanding and responsiveness, and precise\ninformation conveyance, all within inherently noisy and ambiguous\nconversational environments. In this work, we investigate architectural,\nrepresentational, optimisational as well as emotional considerations of ToD\nsystems. We set up systems covering these design considerations with a\nchallenging evaluation environment composed of a natural-language user\nsimulator coupled with an imperfect natural language understanding module. We\npropose \\textbf{LUSTER}, an \\textbf{L}LM-based \\textbf{U}nified \\textbf{S}ystem\nfor \\textbf{T}ask-oriented dialogue with \\textbf{E}nd-to-end\n\\textbf{R}einforcement learning with both short-term (user sentiment) and\nlong-term (task success) rewards. Our findings demonstrate that combining LLM\ncapability with structured reward modelling leads to more resilient and\nemotionally responsive ToD systems, offering a practical path forward for\nnext-generation conversational agents.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edfLUSTER\uff0c\u7528\u4e8e\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\uff0c\u7ed3\u5408\u77ed\u671f\uff08\u7528\u6237\u60c5\u611f\uff09\u548c\u957f\u671f\uff08\u4efb\u52a1\u6210\u529f\uff09\u5956\u52b1\uff0c\u63d0\u5347\u4e86\u5bf9\u8bdd\u7cfb\u7edf\u7684\u60c5\u611f\u54cd\u5e94\u548c\u4efb\u52a1\u5b8c\u6210\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8bed\u8a00\u6d41\u7545\u6027\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u6784\u5efa\u9ad8\u6548\u4e14\u60c5\u611f\u667a\u80fd\u7684\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\uff08ToD\uff09\u7cfb\u7edf\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51faLUSTER\u7cfb\u7edf\uff0c\u7ed3\u5408LLM\u80fd\u529b\u548c\u7ed3\u6784\u5316\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u4efb\u52a1\u6210\u529f\u548c\u60c5\u611f\u54cd\u5e94\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLUSTER\u7cfb\u7edf\u5728\u4efb\u52a1\u5b8c\u6210\u548c\u60c5\u611f\u54cd\u5e94\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002", "conclusion": "\u7ed3\u5408LLM\u548c\u7ed3\u6784\u5316\u5956\u52b1\u6a21\u578b\u7684\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u66f4\u5177\u97e7\u6027\u548c\u60c5\u611f\u667a\u80fd\uff0c\u662f\u672a\u6765\u53d1\u5c55\u7684\u65b9\u5411\u3002"}}
{"id": "2507.01047", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.01047", "abs": "https://arxiv.org/abs/2507.01047", "authors": ["Logan A. Burnett", "Umme Mahbuba Nabila", "Majdi I. Radaideh"], "title": "Variational Digital Twins", "comment": "33 pages, 14 figures, and 7 tables", "summary": "While digital twins (DT) hold promise for providing real-time insights into\ncomplex energy assets, much of the current literature either does not offer a\nclear framework for information exchange between the model and the asset, lacks\nkey features needed for real-time implementation, or gives limited attention to\nmodel uncertainty. Here, we aim to solve these gaps by proposing a variational\ndigital twin (VDT) framework that augments standard neural architectures with a\nsingle Bayesian output layer. This lightweight addition, along with a novel VDT\nupdating algorithm, lets a twin update in seconds on commodity GPUs while\nproducing calibrated uncertainty bounds that can inform experiment design,\ncontrol algorithms, and model reliability. The VDT is evaluated on four\nenergy-sector problems. For critical-heat-flux prediction, uncertainty-driven\nactive learning reaches R2 = 0.98 using 47 % fewer experiments and one-third\nthe training time of random sampling. A three-year renewable-generation twin\nmaintains R2 > 0.95 for solar output and curbs error growth for volatile wind\nforecasts via monthly updates that process only one month of data at a time. A\nnuclear reactor transient cooldown twin reconstructs thermocouple signals with\nR2 > 0.99 and preserves accuracy after 50 % sensor loss, demonstrating\nrobustness to degraded instrumentation. Finally, a physics-informed Li-ion\nbattery twin, retrained after every ten discharges, lowers voltage mean-squared\nerror by an order of magnitude relative to the best static model while adapting\nits credible intervals as the cell approaches end-of-life. These results\ndemonstrate that combining modest Bayesian augmentation with efficient update\nschemes turns conventional surrogates into uncertainty-aware, data-efficient,\nand computationally tractable DTs, paving the way for dependable models across\nindustrial and scientific energy systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53d8\u5206\u6570\u5b57\u5b6a\u751f\uff08VDT\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u8f93\u51fa\u5c42\u548c\u9ad8\u6548\u66f4\u65b0\u7b97\u6cd5\u89e3\u51b3\u73b0\u6709\u6570\u5b57\u5b6a\u751f\u5728\u5b9e\u65f6\u6027\u3001\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u548c\u4fe1\u606f\u4ea4\u6362\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u6570\u5b57\u5b6a\u751f\u6280\u672f\u7f3a\u4e4f\u5b9e\u65f6\u5b9e\u73b0\u7684\u5173\u952e\u7279\u6027\u3001\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u5173\u6ce8\u4e0d\u8db3\uff0c\u4e14\u7f3a\u4e4f\u6e05\u6670\u7684\u6a21\u578b\u4e0e\u8d44\u4ea7\u4fe1\u606f\u4ea4\u6362\u6846\u67b6\u3002", "method": "VDT\u6846\u67b6\u5728\u6807\u51c6\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u4e2d\u589e\u52a0\u4e86\u4e00\u4e2a\u8d1d\u53f6\u65af\u8f93\u51fa\u5c42\uff0c\u5e76\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u7684\u66f4\u65b0\u7b97\u6cd5\uff0c\u4f7f\u5176\u80fd\u5728\u666e\u901aGPU\u4e0a\u5feb\u901f\u66f4\u65b0\u5e76\u63d0\u4f9b\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\u8fb9\u754c\u3002", "result": "\u5728\u56db\u4e2a\u80fd\u6e90\u9886\u57df\u95ee\u9898\u4e2d\u9a8c\u8bc1\u4e86VDT\u7684\u6709\u6548\u6027\uff0c\u5305\u62ec\u4e34\u754c\u70ed\u901a\u91cf\u9884\u6d4b\u3001\u53ef\u518d\u751f\u80fd\u6e90\u53d1\u7535\u9884\u6d4b\u3001\u6838\u53cd\u5e94\u5806\u77ac\u6001\u51b7\u5374\u548c\u9502\u79bb\u5b50\u7535\u6c60\u5efa\u6a21\uff0c\u5747\u8868\u73b0\u51fa\u9ad8\u6548\u3001\u9c81\u68d2\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u7279\u70b9\u3002", "conclusion": "VDT\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8d1d\u53f6\u65af\u589e\u5f3a\u548c\u9ad8\u6548\u66f4\u65b0\u65b9\u6848\uff0c\u5c06\u4f20\u7edf\u4ee3\u7406\u6a21\u578b\u8f6c\u5316\u4e3a\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u3001\u6570\u636e\u9ad8\u6548\u4e14\u8ba1\u7b97\u53ef\u884c\u7684\u6570\u5b57\u5b6a\u751f\uff0c\u4e3a\u5de5\u4e1a\u548c\u79d1\u5b66\u80fd\u6e90\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u9760\u6a21\u578b\u3002"}}
{"id": "2507.01368", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01368", "abs": "https://arxiv.org/abs/2507.01368", "authors": ["Tianning Chai", "Chancharik Mitra", "Brandon Huang", "Gautam Rajendrakumar Gare", "Zhiqiu Lin", "Assaf Arbelle", "Leonid Karlinsky", "Rogerio Feris", "Trevor Darrell", "Deva Ramanan", "Roei Herzig"], "title": "Activation Reward Models for Few-Shot Model Alignment", "comment": null, "summary": "Aligning Large Language Models (LLMs) and Large Multimodal Models (LMMs) to\nhuman preferences is a central challenge in improving the quality of the\nmodels' generative outputs for real-world applications. A common approach is to\nuse reward modeling to encode preferences, enabling alignment via post-training\nusing reinforcement learning. However, traditional reward modeling is not\neasily adaptable to new preferences because it requires a separate reward\nmodel, commonly trained on large preference datasets. To address this, we\nintroduce Activation Reward Models (Activation RMs) -- a novel few-shot reward\nmodeling method that leverages activation steering to construct well-aligned\nreward signals using minimal supervision and no additional model finetuning.\nActivation RMs outperform existing few-shot reward modeling approaches such as\nLLM-as-a-judge with in-context learning, voting-based scoring, and token\nprobability scoring on standard reward modeling benchmarks. Furthermore, we\ndemonstrate the effectiveness of Activation RMs in mitigating reward hacking\nbehaviors, highlighting their utility for safety-critical applications. Toward\nthis end, we propose PreferenceHack, a novel few-shot setting benchmark, the\nfirst to test reward models on reward hacking in a paired preference format.\nFinally, we show that Activation RM achieves state-of-the-art performance on\nthis benchmark, surpassing even GPT-4o.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5c11\u6837\u672c\u5956\u52b1\u5efa\u6a21\u65b9\u6cd5Activation RMs\uff0c\u901a\u8fc7\u6fc0\u6d3b\u5bfc\u5411\u6784\u5efa\u5bf9\u9f50\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u65e0\u9700\u989d\u5916\u5fae\u8c03\uff0c\u5e76\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5956\u52b1\u5efa\u6a21\u96be\u4ee5\u9002\u5e94\u65b0\u504f\u597d\uff0c\u9700\u72ec\u7acb\u5956\u52b1\u6a21\u578b\u548c\u5927\u89c4\u6a21\u504f\u597d\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u3002", "method": "\u5229\u7528\u6fc0\u6d3b\u5bfc\u5411\u6280\u672f\u6784\u5efa\u5956\u52b1\u4fe1\u53f7\uff0c\u4ec5\u9700\u5c11\u91cf\u76d1\u7763\u6570\u636e\uff0c\u65e0\u9700\u989d\u5916\u6a21\u578b\u5fae\u8c03\u3002", "result": "Activation RMs\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u65b0\u63d0\u51fa\u7684PreferenceHack\u57fa\u51c6\u4e2d\u53d6\u5f97\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "Activation RMs\u4e3a\u5c11\u6837\u672c\u5956\u52b1\u5efa\u6a21\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5173\u952e\u5b89\u5168\u5e94\u7528\u3002"}}
{"id": "2507.01627", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01627", "abs": "https://arxiv.org/abs/2507.01627", "authors": ["Maeve Hutchinson", "Radu Jianu", "Aidan Slingsby", "Jo Wood", "Pranava Madhyastha"], "title": "Chart Question Answering from Real-World Analytical Narratives", "comment": "This paper has been accepted to the ACL Student Research Workshop\n  (SRW) 2025", "summary": "We present a new dataset for chart question answering (CQA) constructed from\nvisualization notebooks. The dataset features real-world, multi-view charts\npaired with natural language questions grounded in analytical narratives.\nUnlike prior benchmarks, our data reflects ecologically valid reasoning\nworkflows. Benchmarking state-of-the-art multimodal large language models\nreveals a significant performance gap, with GPT-4.1 achieving an accuracy of\n69.3%, underscoring the challenges posed by this more authentic CQA setting.", "AI": {"tldr": "\u65b0\u6570\u636e\u96c6\u7528\u4e8e\u56fe\u8868\u95ee\u7b54\uff08CQA\uff09\uff0c\u57fa\u4e8e\u53ef\u89c6\u5316\u7b14\u8bb0\u672c\u6784\u5efa\uff0c\u5305\u542b\u771f\u5b9e\u591a\u89c6\u56fe\u56fe\u8868\u548c\u81ea\u7136\u8bed\u8a00\u95ee\u9898\uff0c\u53cd\u6620\u5b9e\u9645\u63a8\u7406\u6d41\u7a0b\u3002GPT-4.1\u51c6\u786e\u738769.3%\uff0c\u663e\u793a\u73b0\u6709\u6a21\u578b\u5728\u6b64\u7c7b\u4efb\u52a1\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709CQA\u6570\u636e\u96c6\u7f3a\u4e4f\u771f\u5b9e\u6027\u548c\u591a\u89c6\u56fe\u56fe\u8868\u652f\u6301\uff0c\u65e0\u6cd5\u53cd\u6620\u5b9e\u9645\u63a8\u7406\u6d41\u7a0b\uff0c\u56e0\u6b64\u6784\u5efa\u65b0\u6570\u636e\u96c6\u4ee5\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4ece\u53ef\u89c6\u5316\u7b14\u8bb0\u672c\u4e2d\u6784\u5efa\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u89c6\u56fe\u56fe\u8868\u548c\u81ea\u7136\u8bed\u8a00\u95ee\u9898\uff0c\u6a21\u62df\u771f\u5b9e\u5206\u6790\u573a\u666f\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGPT-4.1\u7684\u51c6\u786e\u7387\u4e3a69.3%\uff0c\u8868\u660e\u73b0\u6709\u6a21\u578b\u5728\u771f\u5b9eCQA\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u8db3\u3002", "conclusion": "\u65b0\u6570\u636e\u96c6\u4e3aCQA\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u7684\u6d4b\u8bd5\u73af\u5883\uff0c\u51f8\u663e\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2507.01048", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01048", "abs": "https://arxiv.org/abs/2507.01048", "authors": ["Ricardo Emanuel Vaz Vargas", "Afr\u00e2nio Jos\u00e9 de Melo Junior", "Celso Jos\u00e9 Munaro", "Cl\u00e1udio Benevenuto de Campos Lima", "Eduardo Toledo de Lima Junior", "Felipe Muntzberg Barrocas", "Fl\u00e1vio Miguel Varej\u00e3o", "Guilherme Fidelis Peixer", "Igor de Melo Nery Oliveira", "Jader Riso Barbosa Jr.", "Jaime Andr\u00e9s Lozano Cadena", "Jean Carlos Dias de Ara\u00fajo", "Jo\u00e3o Neuenschwander Escosteguy Carneiro", "Lucas Gouveia Omena Lopes", "Lucas Pereira de Gouveia", "Mateus de Araujo Fernandes", "Matheus Lima Scramignon", "Patrick Marques Ciarelli", "Rodrigo Castello Branco", "Rog\u00e9rio Leite Alves Pinto"], "title": "3W Dataset 2.0.0: a realistic and public dataset with rare undesirable real events in oil wells", "comment": "21 pages, 10 figures, and 7 tables", "summary": "In the oil industry, undesirable events in oil wells can cause economic\nlosses, environmental accidents, and human casualties. Solutions based on\nArtificial Intelligence and Machine Learning for Early Detection of such events\nhave proven valuable for diverse applications across industries. In 2019,\nrecognizing the importance and the lack of public datasets related to\nundesirable events in oil wells, Petrobras developed and publicly released the\nfirst version of the 3W Dataset, which is essentially a set of Multivariate\nTime Series labeled by experts. Since then, the 3W Dataset has been developed\ncollaboratively and has become a foundational reference for numerous works in\nthe field. This data article describes the current publicly available version\nof the 3W Dataset, which contains structural modifications and additional\nlabeled data. The detailed description provided encourages and supports the 3W\ncommunity and new 3W users to improve previous published results and to develop\nnew robust methodologies, digital products and services capable of detecting\nundesirable events in oil wells with enough anticipation to enable corrective\nor mitigating actions.", "AI": {"tldr": "Petrobras\u5f00\u53d1\u76843W\u6570\u636e\u96c6\u662f\u4e00\u4e2a\u516c\u5f00\u7684\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u65e9\u671f\u68c0\u6d4b\u77f3\u6cb9\u4e95\u4e2d\u7684\u4e0d\u826f\u4e8b\u4ef6\uff0c\u652f\u6301AI\u548c\u673a\u5668\u5b66\u4e60\u7814\u7a76\u3002", "motivation": "\u77f3\u6cb9\u4e95\u4e2d\u7684\u4e0d\u826f\u4e8b\u4ef6\u53ef\u80fd\u5bfc\u81f4\u7ecf\u6d4e\u635f\u5931\u3001\u73af\u5883\u4e8b\u6545\u548c\u4eba\u5458\u4f24\u4ea1\uff0c\u56e0\u6b64\u9700\u8981\u65e9\u671f\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u4e13\u5bb6\u6807\u6ce8\u7684\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u6784\u5efa3W\u6570\u636e\u96c6\uff0c\u5e76\u516c\u5f00\u5171\u4eab\u4ee5\u652f\u6301\u7814\u7a76\u3002", "result": "3W\u6570\u636e\u96c6\u5df2\u6210\u4e3a\u8be5\u9886\u57df\u7684\u57fa\u7840\u53c2\u8003\uff0c\u5e76\u7ecf\u8fc7\u7ed3\u6784\u4fee\u6539\u548c\u6570\u636e\u6269\u5145\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u9f13\u52b1\u793e\u533a\u6539\u8fdb\u73b0\u6709\u65b9\u6cd5\u5e76\u5f00\u53d1\u65b0\u7684\u68c0\u6d4b\u6280\u672f\uff0c\u4ee5\u51cf\u5c11\u4e0d\u826f\u4e8b\u4ef6\u7684\u5f71\u54cd\u3002"}}
{"id": "2507.01372", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01372", "abs": "https://arxiv.org/abs/2507.01372", "authors": ["Max Hamilton", "Jinlin Lai", "Wenlong Zhao", "Subhransu Maji", "Daniel Sheldon"], "title": "Active Measurement: Efficient Estimation at Scale", "comment": null, "summary": "AI has the potential to transform scientific discovery by analyzing vast\ndatasets with little human effort. However, current workflows often do not\nprovide the accuracy or statistical guarantees that are needed. We introduce\nactive measurement, a human-in-the-loop AI framework for scientific\nmeasurement. An AI model is used to predict measurements for individual units,\nwhich are then sampled for human labeling using importance sampling. With each\nnew set of human labels, the AI model is improved and an unbiased Monte Carlo\nestimate of the total measurement is refined. Active measurement can provide\nprecise estimates even with an imperfect AI model, and requires little human\neffort when the AI model is very accurate. We derive novel estimators,\nweighting schemes, and confidence intervals, and show that active measurement\nreduces estimation error compared to alternatives in several measurement tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u4e3b\u52a8\u6d4b\u91cf\u201d\u7684\u4eba\u673a\u534f\u4f5cAI\u6846\u67b6\uff0c\u7528\u4e8e\u79d1\u5b66\u6d4b\u91cf\uff0c\u901a\u8fc7\u91cd\u8981\u6027\u91c7\u6837\u548c\u8fed\u4ee3\u4f18\u5316\u6a21\u578b\uff0c\u51cf\u5c11\u4f30\u8ba1\u8bef\u5dee\u3002", "motivation": "\u5f53\u524dAI\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u7684\u5de5\u4f5c\u6d41\u7a0b\u7f3a\u4e4f\u8db3\u591f\u7684\u51c6\u786e\u6027\u548c\u7edf\u8ba1\u4fdd\u8bc1\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528AI\u6a21\u578b\u9884\u6d4b\u6d4b\u91cf\u503c\uff0c\u901a\u8fc7\u91cd\u8981\u6027\u91c7\u6837\u9009\u62e9\u6837\u672c\u8fdb\u884c\u4eba\u5de5\u6807\u6ce8\uff0c\u8fed\u4ee3\u4f18\u5316\u6a21\u578b\u5e76\u751f\u6210\u65e0\u504f\u4f30\u8ba1\u3002", "result": "\u4e3b\u52a8\u6d4b\u91cf\u80fd\u5728AI\u6a21\u578b\u4e0d\u5b8c\u7f8e\u65f6\u63d0\u4f9b\u7cbe\u786e\u4f30\u8ba1\uff0c\u5e76\u5728\u6a21\u578b\u51c6\u786e\u65f6\u51cf\u5c11\u4eba\u5de5\u5de5\u4f5c\u91cf\uff0c\u663e\u8457\u964d\u4f4e\u4f30\u8ba1\u8bef\u5dee\u3002", "conclusion": "\u4e3b\u52a8\u6d4b\u91cf\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u79d1\u5b66\u6d4b\u91cf\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6d4b\u91cf\u4efb\u52a1\u3002"}}
{"id": "2507.01633", "categories": ["cs.CL", "cs.IR", "62-04", "D.2.3"], "pdf": "https://arxiv.org/pdf/2507.01633", "abs": "https://arxiv.org/abs/2507.01633", "authors": ["Georgii Levtsov", "Dmitry Ustalov"], "title": "Confidence and Stability of Global and Pairwise Scores in NLP Evaluation", "comment": "8 pages, accepted at ACL SRW 2025", "summary": "With the advent of highly capable instruction-tuned neural language models,\nbenchmarking in natural language processing (NLP) is increasingly shifting\ntowards pairwise comparison leaderboards, such as LMSYS Arena, from traditional\nglobal pointwise scores (e.g., GLUE, BIG-bench, SWE-bench). This paper\nempirically investigates the strengths and weaknesses of both global scores and\npairwise comparisons to aid decision-making in selecting appropriate model\nevaluation strategies. Through computational experiments on synthetic and\nreal-world datasets using standard global metrics and the popular Bradley-Terry\nmodel for pairwise comparisons, we found that while global scores provide more\nreliable overall rankings, they can underestimate strong models with rare,\nsignificant errors or low confidence. Conversely, pairwise comparisons are\nparticularly effective for identifying strong contenders among models with\nlower global scores, especially where quality metrics are hard to define (e.g.,\ntext generation), though they require more comparisons to converge if ties are\nfrequent. Our code and data are available at\nhttps://github.com/HSPyroblast/srw-ranking under a permissive license.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86\u5168\u5c40\u8bc4\u5206\u548c\u6210\u5bf9\u6bd4\u8f83\u5728NLP\u6a21\u578b\u8bc4\u4f30\u4e2d\u7684\u4f18\u7f3a\u70b9\uff0c\u53d1\u73b0\u5168\u5c40\u8bc4\u5206\u66f4\u53ef\u9760\u4f46\u53ef\u80fd\u4f4e\u4f30\u67d0\u4e9b\u6a21\u578b\uff0c\u800c\u6210\u5bf9\u6bd4\u8f83\u80fd\u66f4\u597d\u8bc6\u522b\u4f4e\u5206\u6a21\u578b\u4e2d\u7684\u5f3a\u8005\u3002", "motivation": "\u968f\u7740\u6307\u4ee4\u8c03\u4f18\u795e\u7ecf\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0cNLP\u8bc4\u4f30\u4ece\u5168\u5c40\u8bc4\u5206\u8f6c\u5411\u6210\u5bf9\u6bd4\u8f83\uff0c\u9700\u7814\u7a76\u4e24\u8005\u7684\u4f18\u52a3\u4ee5\u6307\u5bfc\u8bc4\u4f30\u7b56\u7565\u9009\u62e9\u3002", "method": "\u901a\u8fc7\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u7684\u8ba1\u7b97\u5b9e\u9a8c\uff0c\u4f7f\u7528\u5168\u5c40\u6307\u6807\u548cBradley-Terry\u6a21\u578b\u8fdb\u884c\u6210\u5bf9\u6bd4\u8f83\u3002", "result": "\u5168\u5c40\u8bc4\u5206\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u603b\u4f53\u6392\u540d\uff0c\u4f46\u53ef\u80fd\u4f4e\u4f30\u67d0\u4e9b\u6a21\u578b\uff1b\u6210\u5bf9\u6bd4\u8f83\u80fd\u8bc6\u522b\u4f4e\u5206\u6a21\u578b\u4e2d\u7684\u5f3a\u8005\uff0c\u4f46\u6536\u655b\u8f83\u6162\u3002", "conclusion": "\u5efa\u8bae\u6839\u636e\u8bc4\u4f30\u9700\u6c42\u9009\u62e9\u65b9\u6cd5\uff0c\u5168\u5c40\u8bc4\u5206\u9002\u5408\u603b\u4f53\u6392\u540d\uff0c\u6210\u5bf9\u6bd4\u8f83\u9002\u5408\u7279\u5b9a\u573a\u666f\u5982\u6587\u672c\u751f\u6210\u3002"}}
{"id": "2507.01050", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01050", "abs": "https://arxiv.org/abs/2507.01050", "authors": ["Jing Yu", "Yibo Zhao", "Jiapeng Zhu", "Wenming Shao", "Bo Pang", "Zhao Zhang", "Xiang Li"], "title": "Text Detoxification: Data Efficiency, Semantic Preservation and Model Generalization", "comment": null, "summary": "The widespread dissemination of toxic content on social media poses a serious\nthreat to both online environments and public discourse, highlighting the\nurgent need for detoxification methods that effectively remove toxicity while\npreserving the original semantics. However, existing approaches often struggle\nto simultaneously achieve strong detoxification performance, semantic\npreservation, and robustness to out-of-distribution data. Moreover, they\ntypically rely on costly, manually annotated parallel corpora while showing\npoor data efficiency. To address these challenges, we propose a two-stage\ntraining framework that jointly optimizes for data efficiency, semantic\npreservation, and model generalization. We first perform supervised fine-tuning\non a small set of high-quality, filtered parallel data to establish a strong\ninitialization. Then, we leverage unlabeled toxic inputs and a custom-designed\nreward model to train the LLM using Group Relative Policy Optimization.\nExperimental results demonstrate that our method effectively mitigates the\ntrade-offs faced by previous work, achieving state-of-the-art performance with\nimproved generalization and significantly reduced dependence on annotated data.\nOur code is available at:\nhttps://anonymous.4open.science/r/Detoxification-of-Text-725F/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u53bb\u9664\u793e\u4ea4\u5a92\u4f53\u6587\u672c\u4e2d\u7684\u6bd2\u6027\u5185\u5bb9\uff0c\u540c\u65f6\u4fdd\u7559\u8bed\u4e49\uff0c\u5e76\u51cf\u5c11\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e0a\u6bd2\u6027\u5185\u5bb9\u7684\u5e7f\u6cdb\u4f20\u64ad\u5bf9\u5728\u7ebf\u73af\u5883\u548c\u516c\u5171\u8ba8\u8bba\u6784\u6210\u5a01\u80c1\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u53bb\u6bd2\u6027\u80fd\u3001\u8bed\u4e49\u4fdd\u7559\u548c\u6570\u636e\u6548\u7387\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff1a\u9996\u5148\u5728\u5c11\u91cf\u9ad8\u8d28\u91cf\u5e73\u884c\u6570\u636e\u4e0a\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u7136\u540e\u5229\u7528\u672a\u6807\u6ce8\u6bd2\u6027\u6570\u636e\u548c\u81ea\u5b9a\u4e49\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7Group Relative Policy Optimization\u8bad\u7ec3LLM\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u5de5\u4f5c\u7684\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u4e86\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u53bb\u6bd2\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6570\u636e\u6548\u7387\u3002"}}
{"id": "2507.01384", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01384", "abs": "https://arxiv.org/abs/2507.01384", "authors": ["Langyu Wang", "Bingke Zhu", "Yingying Chen", "Yiyuan Zhang", "Ming Tang", "Jinqiao Wang"], "title": "MUG: Pseudo Labeling Augmented Audio-Visual Mamba Network for Audio-Visual Video Parsing", "comment": "Accpted by ICCV 2025", "summary": "The weakly-supervised audio-visual video parsing (AVVP) aims to predict all\nmodality-specific events and locate their temporal boundaries. Despite\nsignificant progress, due to the limitations of the weakly-supervised and the\ndeficiencies of the model architecture, existing methods are lacking in\nsimultaneously improving both the segment-level prediction and the event-level\nprediction. In this work, we propose a audio-visual Mamba network with pseudo\nlabeling aUGmentation (MUG) for emphasising the uniqueness of each segment and\nexcluding the noise interference from the alternate modalities. Specifically,\nwe annotate some of the pseudo-labels based on previous work. Using unimodal\npseudo-labels, we perform cross-modal random combinations to generate new data,\nwhich can enhance the model's ability to parse various segment-level event\ncombinations. For feature processing and interaction, we employ a audio-visual\nmamba network. The AV-Mamba enhances the ability to perceive different segments\nand excludes additional modal noise while sharing similar modal information.\nOur extensive experiments demonstrate that MUG improves state-of-the-art\nresults on LLP dataset in all metrics (e.g,, gains of 2.1% and 1.2% in terms of\nvisual Segment-level and audio Segment-level metrics). Our code is available at\nhttps://github.com/WangLY136/MUG.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f2a\u6807\u7b7e\u589e\u5f3a\u7684\u97f3\u9891-\u89c6\u89c9Mamba\u7f51\u7edc\uff08MUG\uff09\uff0c\u7528\u4e8e\u5f31\u76d1\u7763\u97f3\u9891-\u89c6\u89c9\u89c6\u9891\u89e3\u6790\uff0c\u63d0\u5347\u4e86\u6bb5\u7ea7\u548c\u4e8b\u4ef6\u7ea7\u9884\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5f31\u76d1\u7763\u548c\u6a21\u578b\u67b6\u6784\u7684\u9650\u5236\u4e0b\uff0c\u96be\u4ee5\u540c\u65f6\u63d0\u5347\u6bb5\u7ea7\u548c\u4e8b\u4ef6\u7ea7\u9884\u6d4b\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u4f2a\u6807\u7b7e\u589e\u5f3a\u6570\u636e\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u968f\u673a\u7ec4\u5408\u751f\u6210\u65b0\u6570\u636e\uff0c\u5e76\u91c7\u7528\u97f3\u9891-\u89c6\u89c9Mamba\u7f51\u7edc\u8fdb\u884c\u7279\u5f81\u5904\u7406\u548c\u4ea4\u4e92\u3002", "result": "\u5728LLP\u6570\u636e\u96c6\u4e0a\uff0cMUG\u5728\u6240\u6709\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u89c6\u89c9\u6bb5\u7ea7\u548c\u97f3\u9891\u6bb5\u7ea7\u6307\u6807\u5206\u522b\u63d0\u53472.1%\u548c1.2%\uff09\u3002", "conclusion": "MUG\u901a\u8fc7\u4f2a\u6807\u7b7e\u589e\u5f3a\u548cAV-Mamba\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f31\u76d1\u7763\u97f3\u9891-\u89c6\u89c9\u89c6\u9891\u89e3\u6790\u7684\u6027\u80fd\u3002"}}
{"id": "2507.01645", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01645", "abs": "https://arxiv.org/abs/2507.01645", "authors": ["Rifki Afina Putri"], "title": "Adapting Language Models to Indonesian Local Languages: An Empirical Study of Language Transferability on Zero-Shot Settings", "comment": "AMLDS 2025", "summary": "In this paper, we investigate the transferability of pre-trained language\nmodels to low-resource Indonesian local languages through the task of sentiment\nanalysis. We evaluate both zero-shot performance and adapter-based transfer on\nten local languages using models of different types: a monolingual Indonesian\nBERT, multilingual models such as mBERT and XLM-R, and a modular adapter-based\napproach called MAD-X. To better understand model behavior, we group the target\nlanguages into three categories: seen (included during pre-training), partially\nseen (not included but linguistically related to seen languages), and unseen\n(absent and unrelated in pre-training data). Our results reveal clear\nperformance disparities across these groups: multilingual models perform best\non seen languages, moderately on partially seen ones, and poorly on unseen\nlanguages. We find that MAD-X significantly improves performance, especially\nfor seen and partially seen languages, without requiring labeled data in the\ntarget language. Additionally, we conduct a further analysis on tokenization\nand show that while subword fragmentation and vocabulary overlap with\nIndonesian correlate weakly with prediction quality, they do not fully explain\nthe observed performance. Instead, the most consistent predictor of transfer\nsuccess is the model's prior exposure to the language, either directly or\nthrough a related language.", "AI": {"tldr": "\u7814\u7a76\u4e86\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u5370\u5c3c\u672c\u5730\u8bed\u8a00\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u53ef\u8fc1\u79fb\u6027\uff0c\u8bc4\u4f30\u4e86\u96f6\u6837\u672c\u548c\u9002\u914d\u5668\u8fc1\u79fb\u65b9\u6cd5\uff0c\u53d1\u73b0\u6a21\u578b\u6027\u80fd\u4e0e\u8bed\u8a00\u5728\u9884\u8bad\u7ec3\u4e2d\u7684\u66b4\u9732\u7a0b\u5ea6\u76f8\u5173\u3002", "motivation": "\u63a2\u7d22\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u5370\u5c3c\u672c\u5730\u8bed\u8a00\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u4e0d\u540c\u8bed\u8a00\u7c7b\u522b\uff08\u5df2\u89c1\u3001\u90e8\u5206\u89c1\u3001\u672a\u89c1\uff09\u7684\u6027\u80fd\u5dee\u5f02\u3002", "method": "\u4f7f\u7528\u5370\u5c3c\u5355\u8bedBERT\u3001\u591a\u8bed\u8a00\u6a21\u578b\uff08mBERT\u3001XLM-R\uff09\u548c\u9002\u914d\u5668\u65b9\u6cd5MAD-X\uff0c\u8bc4\u4f30\u96f6\u6837\u672c\u548c\u9002\u914d\u5668\u8fc1\u79fb\u6027\u80fd\u3002", "result": "\u591a\u8bed\u8a00\u6a21\u578b\u5728\u5df2\u89c1\u8bed\u8a00\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u90e8\u5206\u89c1\u6b21\u4e4b\uff0c\u672a\u89c1\u6700\u5dee\uff1bMAD-X\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5df2\u89c1\u548c\u90e8\u5206\u89c1\u8bed\u8a00\u3002", "conclusion": "\u6a21\u578b\u5bf9\u8bed\u8a00\u7684\u9884\u8bad\u7ec3\u66b4\u9732\u7a0b\u5ea6\u662f\u8fc1\u79fb\u6210\u529f\u7684\u6700\u4e00\u81f4\u9884\u6d4b\u56e0\u7d20\uff0c\u800c\u975e\u8bcd\u6c47\u91cd\u53e0\u6216\u5b50\u8bcd\u5206\u5272\u3002"}}
{"id": "2507.01052", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.01052", "abs": "https://arxiv.org/abs/2507.01052", "authors": ["Ahmed Farooq"], "title": "Long-Sequence Memory with Temporal Kernels and Dense Hopfield Functionals", "comment": null, "summary": "In this study we introduce a novel energy functional for long-sequence\nmemory, building upon the framework of dense Hopfield networks which achieves\nexponential storage capacity through higher-order interactions. Building upon\nearlier work on long-sequence Hopfield memory models, we propose a temporal\nkernal $K(m, k)$ to incorporate temporal dependencies, enabling efficient\nsequential retrieval of patterns over extended sequences. We demonstrate the\nsuccessful application of this technique for the storage and sequential\nretrieval of movies frames which are well suited for this because of the high\ndimensional vectors that make up each frame creating enough variation between\neven sequential frames in the high dimensional space. The technique has\napplications in modern transformer architectures, including efficient\nlong-sequence modeling, memory augmentation, improved attention with temporal\nbias, and enhanced handling of long-term dependencies in time-series data. Our\nmodel offers a promising approach to address the limitations of transformers in\nlong-context tasks, with potential implications for natural language\nprocessing, forecasting, and beyond.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bc6\u96c6Hopfield\u7f51\u7edc\u7684\u65b0\u578b\u80fd\u91cf\u51fd\u6570\uff0c\u901a\u8fc7\u9ad8\u9636\u4ea4\u4e92\u5b9e\u73b0\u6307\u6570\u5b58\u50a8\u5bb9\u91cf\uff0c\u5e76\u5f15\u5165\u65f6\u95f4\u6838\u4ee5\u652f\u6301\u957f\u5e8f\u5217\u8bb0\u5fc6\u7684\u9ad8\u6548\u68c0\u7d22\u3002", "motivation": "\u89e3\u51b3\u957f\u5e8f\u5217\u4efb\u52a1\u4e2dTransformer\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5982\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u3001\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u957f\u671f\u4f9d\u8d56\u5904\u7406\u7b49\u3002", "method": "\u6784\u5efa\u65f6\u95f4\u6838$K(m, k)$\u4ee5\u6574\u5408\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u652f\u6301\u9ad8\u6548\u5e8f\u5217\u68c0\u7d22\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u5411\u91cf\uff08\u5982\u7535\u5f71\u5e27\uff09\u7684\u5b58\u50a8\u4e0e\u68c0\u7d22\u3002", "result": "\u6210\u529f\u5e94\u7528\u4e8e\u7535\u5f71\u5e27\u7684\u5b58\u50a8\u4e0e\u5e8f\u5217\u68c0\u7d22\uff0c\u5c55\u793a\u4e86\u5728\u957f\u5e8f\u5217\u5efa\u6a21\u3001\u8bb0\u5fc6\u589e\u5f3a\u548c\u65f6\u95f4\u504f\u7f6e\u6ce8\u610f\u529b\u7b49\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5bf9\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u9884\u6d4b\u7b49\u9886\u57df\u5177\u6709\u6f5c\u5728\u5f71\u54cd\u3002"}}
{"id": "2507.01390", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01390", "abs": "https://arxiv.org/abs/2507.01390", "authors": ["Shuai Tan", "Bill Gong", "Bin Ji", "Ye Pan"], "title": "FixTalk: Taming Identity Leakage for High-Quality Talking Head Generation in Extreme Cases", "comment": null, "summary": "Talking head generation is gaining significant importance across various\ndomains, with a growing demand for high-quality rendering. However, existing\nmethods often suffer from identity leakage (IL) and rendering artifacts (RA),\nparticularly in extreme cases. Through an in-depth analysis of previous\napproaches, we identify two key insights: (1) IL arises from identity\ninformation embedded within motion features, and (2) this identity information\ncan be leveraged to address RA. Building on these findings, this paper\nintroduces FixTalk, a novel framework designed to simultaneously resolve both\nissues for high-quality talking head generation. Firstly, we propose an\nEnhanced Motion Indicator (EMI) to effectively decouple identity information\nfrom motion features, mitigating the impact of IL on generated talking heads.\nTo address RA, we introduce an Enhanced Detail Indicator (EDI), which utilizes\nthe leaked identity information to supplement missing details, thus fixing the\nartifacts. Extensive experiments demonstrate that FixTalk effectively mitigates\nIL and RA, achieving superior performance compared to state-of-the-art methods.", "AI": {"tldr": "FixTalk\u6846\u67b6\u901a\u8fc7\u5206\u79bb\u8eab\u4efd\u4fe1\u606f\u548c\u8fd0\u52a8\u7279\u5f81\uff0c\u89e3\u51b3\u4e86\u8bf4\u8bdd\u5934\u751f\u6210\u4e2d\u7684\u8eab\u4efd\u6cc4\u6f0f\u548c\u6e32\u67d3\u4f2a\u5f71\u95ee\u9898\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6781\u7aef\u60c5\u51b5\u4e0b\u5b58\u5728\u8eab\u4efd\u6cc4\u6f0f\u548c\u6e32\u67d3\u4f2a\u5f71\u95ee\u9898\uff0c\u5f71\u54cd\u751f\u6210\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u589e\u5f3a\u8fd0\u52a8\u6307\u793a\u5668\uff08EMI\uff09\u5206\u79bb\u8eab\u4efd\u4fe1\u606f\uff0c\u589e\u5f3a\u7ec6\u8282\u6307\u793a\u5668\uff08EDI\uff09\u5229\u7528\u6cc4\u6f0f\u4fe1\u606f\u4fee\u590d\u4f2a\u5f71\u3002", "result": "\u5b9e\u9a8c\u8868\u660eFixTalk\u6709\u6548\u51cf\u5c11\u4e86\u8eab\u4efd\u6cc4\u6f0f\u548c\u4f2a\u5f71\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FixTalk\u4e3a\u9ad8\u8d28\u91cf\u8bf4\u8bdd\u5934\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01702", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01702", "abs": "https://arxiv.org/abs/2507.01702", "authors": ["Zixin Chen", "Hongzhan Lin", "Kaixin Li", "Ziyang Luo", "Zhen Ye", "Guang Chen", "Zhiyong Huang", "Jing Ma"], "title": "AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness", "comment": "ACL 2025", "summary": "The proliferation of multimodal memes in the social media era demands that\nmultimodal Large Language Models (mLLMs) effectively understand meme\nharmfulness. Existing benchmarks for assessing mLLMs on harmful meme\nunderstanding rely on accuracy-based, model-agnostic evaluations using static\ndatasets. These benchmarks are limited in their ability to provide up-to-date\nand thorough assessments, as online memes evolve dynamically. To address this,\nwe propose AdamMeme, a flexible, agent-based evaluation framework that\nadaptively probes the reasoning capabilities of mLLMs in deciphering meme\nharmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive\nevaluations by iteratively updating the meme data with challenging samples,\nthereby exposing specific limitations in how mLLMs interpret harmfulness.\nExtensive experiments show that our framework systematically reveals the\nvarying performance of different target mLLMs, offering in-depth, fine-grained\nanalyses of model-specific weaknesses. Our code is available at\nhttps://github.com/Lbotirx/AdamMeme.", "AI": {"tldr": "\u63d0\u51faAdamMeme\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u52a8\u6001\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08mLLMs\uff09\u5bf9\u6709\u5bb3\u6a21\u56e0\u7684\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u57fa\u4e8e\u9759\u6001\u6570\u636e\u96c6\uff0c\u65e0\u6cd5\u9002\u5e94\u6a21\u56e0\u7684\u52a8\u6001\u6f14\u53d8\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u3001\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6AdamMeme\uff0c\u52a8\u6001\u66f4\u65b0\u6a21\u56e0\u6570\u636e\u5e76\u6311\u6218mLLMs\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAdamMeme\u80fd\u7cfb\u7edf\u63ed\u793a\u4e0d\u540cmLLMs\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u6df1\u5165\u5206\u6790\u6a21\u578b\u5f31\u70b9\u3002", "conclusion": "AdamMeme\u4e3a\u8bc4\u4f30mLLMs\u5bf9\u6709\u5bb3\u6a21\u56e0\u7684\u7406\u89e3\u63d0\u4f9b\u4e86\u52a8\u6001\u3001\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01054", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01054", "abs": "https://arxiv.org/abs/2507.01054", "authors": ["Jithendaraa Subramanian", "Linda Hung", "Daniel Schweigert", "Santosh Suram", "Weike Ye"], "title": "XxaCT-NN: Structure Agnostic Multimodal Learning for Materials Science", "comment": "10 pages, 6 figures", "summary": "Recent advances in materials discovery have been driven by structure-based\nmodels, particularly those using crystal graphs. While effective for\ncomputational datasets, these models are impractical for real-world\napplications where atomic structures are often unknown or difficult to obtain.\nWe propose a scalable multimodal framework that learns directly from elemental\ncomposition and X-ray diffraction (XRD) -- two of the more available modalities\nin experimental workflows without requiring crystal structure input. Our\narchitecture integrates modality-specific encoders with a cross-attention\nfusion module and is trained on the 5-million-sample Alexandria dataset. We\npresent masked XRD modeling (MXM), and apply MXM and contrastive alignment as\nself-supervised pretraining strategies. Pretraining yields faster convergence\n(up to 4.2x speedup) and improves both accuracy and representation quality. We\nfurther demonstrate that multimodal performance scales more favorably with\ndataset size than unimodal baselines, with gains compounding at larger data\nregimes. Our results establish a path toward structure-free, experimentally\ngrounded foundation models for materials science.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5143\u7d20\u7ec4\u6210\u548cXRD\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u65e0\u9700\u6676\u4f53\u7ed3\u6784\u8f93\u5165\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7b56\u7565\u63d0\u5347\u6750\u6599\u53d1\u73b0\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u5b9e\u9645\u5e94\u7528\u4e2d\u539f\u5b50\u7ed3\u6784\u96be\u4ee5\u83b7\u53d6\u7684\u95ee\u9898\uff0c\u5229\u7528\u66f4\u6613\u83b7\u5f97\u7684\u5143\u7d20\u7ec4\u6210\u548cXRD\u6570\u636e\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7ed3\u5408\u6a21\u6001\u7279\u5b9a\u7f16\u7801\u5668\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u6a21\u5757\uff0c\u4f7f\u7528\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7b56\u7565\uff08MXM\u548c\u5bf9\u6bd4\u5bf9\u9f50\uff09\u3002", "result": "\u9884\u8bad\u7ec3\u663e\u8457\u52a0\u901f\u6536\u655b\uff08\u6700\u9ad84.2\u500d\uff09\uff0c\u63d0\u5347\u51c6\u786e\u6027\u548c\u8868\u793a\u8d28\u91cf\uff0c\u591a\u6a21\u6001\u6027\u80fd\u968f\u6570\u636e\u89c4\u6a21\u6269\u5c55\u66f4\u4f18\u3002", "conclusion": "\u4e3a\u6750\u6599\u79d1\u5b66\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u7ed3\u6784\u8f93\u5165\u3001\u57fa\u4e8e\u5b9e\u9a8c\u6570\u636e\u7684\u901a\u7528\u6a21\u578b\u8def\u5f84\u3002"}}
{"id": "2507.01397", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01397", "abs": "https://arxiv.org/abs/2507.01397", "authors": ["Khanh Son Pham", "Christian Witte", "Jens Behley", "Johannes Betz", "Cyrill Stachniss"], "title": "Coherent Online Road Topology Estimation and Reasoning with Standard-Definition Maps", "comment": "Accepted at IROS 2025", "summary": "Most autonomous cars rely on the availability of high-definition (HD) maps.\nCurrent research aims to address this constraint by directly predicting HD map\nelements from onboard sensors and reasoning about the relationships between the\npredicted map and traffic elements. Despite recent advancements, the coherent\nonline construction of HD maps remains a challenging endeavor, as it\nnecessitates modeling the high complexity of road topologies in a unified and\nconsistent manner. To address this challenge, we propose a coherent approach to\npredict lane segments and their corresponding topology, as well as road\nboundaries, all by leveraging prior map information represented by commonly\navailable standard-definition (SD) maps. We propose a network architecture,\nwhich leverages hybrid lane segment encodings comprising prior information and\ndenoising techniques to enhance training stability and performance.\nFurthermore, we facilitate past frames for temporal consistency. Our\nexperimental evaluation demonstrates that our approach outperforms previous\nmethods by a large margin, highlighting the benefits of our modeling scheme.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u6807\u51c6\u5730\u56fe\uff08SD\uff09\u4fe1\u606f\u9884\u6d4b\u8f66\u9053\u6bb5\u53ca\u5176\u62d3\u6251\u548c\u9053\u8def\u8fb9\u754c\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u7f16\u7801\u548c\u53bb\u566a\u6280\u672f\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u9ad8\u590d\u6742\u5ea6\u9053\u8def\u62d3\u6251\u7684\u7edf\u4e00\u5efa\u6a21\u95ee\u9898\uff0c\u51cf\u5c11\u5bf9\u9ad8\u7cbe\u5730\u56fe\uff08HD\uff09\u7684\u4f9d\u8d56\u3002", "method": "\u5229\u7528SD\u5730\u56fe\u4fe1\u606f\uff0c\u91c7\u7528\u6df7\u5408\u8f66\u9053\u6bb5\u7f16\u7801\u548c\u53bb\u566a\u6280\u672f\uff0c\u7ed3\u5408\u5386\u53f2\u5e27\u5b9e\u73b0\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u5efa\u6a21\u65b9\u6848\u6709\u6548\u63d0\u5347\u4e86\u8f66\u9053\u548c\u9053\u8def\u8fb9\u754c\u7684\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2507.01715", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01715", "abs": "https://arxiv.org/abs/2507.01715", "authors": ["Aditya Tomar", "Rudra Murthy", "Pushpak Bhattacharyya"], "title": "Stereotype Detection as a Catalyst for Enhanced Bias Detection: A Multi-Task Learning Approach", "comment": null, "summary": "Bias and stereotypes in language models can cause harm, especially in\nsensitive areas like content moderation and decision-making. This paper\naddresses bias and stereotype detection by exploring how jointly learning these\ntasks enhances model performance. We introduce StereoBias, a unique dataset\nlabeled for bias and stereotype detection across five categories: religion,\ngender, socio-economic status, race, profession, and others, enabling a deeper\nstudy of their relationship. Our experiments compare encoder-only models and\nfine-tuned decoder-only models using QLoRA. While encoder-only models perform\nwell, decoder-only models also show competitive results. Crucially, joint\ntraining on bias and stereotype detection significantly improves bias detection\ncompared to training them separately. Additional experiments with sentiment\nanalysis confirm that the improvements stem from the connection between bias\nand stereotypes, not multi-task learning alone. These findings highlight the\nvalue of leveraging stereotype information to build fairer and more effective\nAI systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u5b66\u4e60\u504f\u89c1\u548c\u523b\u677f\u5370\u8c61\u68c0\u6d4b\u7684\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165StereoBias\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u8868\u660e\u8054\u5408\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u504f\u89c1\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u504f\u89c1\u548c\u523b\u677f\u5370\u8c61\u53ef\u80fd\u5bf9\u654f\u611f\u9886\u57df\u9020\u6210\u5371\u5bb3\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "method": "\u4f7f\u7528StereoBias\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u7f16\u7801\u5668\u6a21\u578b\u548c\u89e3\u7801\u5668\u6a21\u578b\uff08\u901a\u8fc7QLoRA\u5fae\u8c03\uff09\uff0c\u5e76\u8fdb\u884c\u8054\u5408\u8bad\u7ec3\u3002", "result": "\u8054\u5408\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u504f\u89c1\u68c0\u6d4b\u6548\u679c\uff0c\u4e14\u89e3\u7801\u5668\u6a21\u578b\u8868\u73b0\u7ade\u4e89\u529b\u3002", "conclusion": "\u5229\u7528\u523b\u677f\u5370\u8c61\u4fe1\u606f\u53ef\u4ee5\u6784\u5efa\u66f4\u516c\u5e73\u6709\u6548\u7684AI\u7cfb\u7edf\u3002"}}
{"id": "2507.01056", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01056", "abs": "https://arxiv.org/abs/2507.01056", "authors": ["Lidan Peng", "Lu Gao", "Feng Hong", "Jingran Sun"], "title": "Evaluating Pavement Deterioration Rates Due to Flooding Events Using Explainable AI", "comment": null, "summary": "Flooding can damage pavement infrastructure significantly, causing both\nimmediate and long-term structural and functional issues. This research\ninvestigates how flooding events affect pavement deterioration, specifically\nfocusing on measuring pavement roughness by the International Roughness Index\n(IRI). To quantify these effects, we utilized 20 years of pavement condition\ndata from TxDOT's PMIS database, which is integrated with flood event data,\nincluding duration and spatial extent. Statistical analyses were performed to\ncompare IRI values before and after flooding and to calculate the deterioration\nrates influenced by flood exposure. Moreover, we applied Explainable Artificial\nIntelligence (XAI) techniques, such as SHapley Additive exPlanations (SHAP) and\nLocal Interpretable Model-Agnostic Explanations (LIME), to assess the impact of\nflooding on pavement performance. The results demonstrate that flood-affected\npavements experience a more rapid increase in roughness compared to non-flooded\nsections. These findings emphasize the need for proactive flood mitigation\nstrategies, including improved drainage systems, flood-resistant materials, and\npreventative maintenance, to enhance pavement resilience in vulnerable regions.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u6d2a\u6c34\u5bf9\u8def\u9762\u7c97\u7cd9\u5ea6\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u6d2a\u6c34\u5bfc\u81f4\u8def\u9762\u7c97\u7cd9\u5ea6\u66f4\u5feb\u589e\u52a0\uff0c\u5efa\u8bae\u91c7\u53d6\u9632\u6d2a\u63aa\u65bd\u3002", "motivation": "\u6d2a\u6c34\u5bf9\u8def\u9762\u57fa\u7840\u8bbe\u65bd\u9020\u6210\u4e25\u91cd\u635f\u5bb3\uff0c\u7814\u7a76\u65e8\u5728\u91cf\u5316\u6d2a\u6c34\u5bf9\u8def\u9762\u7c97\u7cd9\u5ea6\u7684\u5f71\u54cd\u3002", "method": "\u5229\u752820\u5e74\u7684\u8def\u9762\u72b6\u51b5\u6570\u636e\u548c\u6d2a\u6c34\u4e8b\u4ef6\u6570\u636e\uff0c\u7ed3\u5408\u7edf\u8ba1\u5206\u6790\u548cXAI\u6280\u672f\uff08\u5982SHAP\u548cLIME\uff09\u3002", "result": "\u6d2a\u6c34\u5f71\u54cd\u7684\u8def\u9762\u7c97\u7cd9\u5ea6\u589e\u52a0\u901f\u5ea6\u66f4\u5feb\u3002", "conclusion": "\u5efa\u8bae\u91c7\u53d6\u9632\u6d2a\u63aa\u65bd\uff0c\u5982\u6539\u8fdb\u6392\u6c34\u7cfb\u7edf\u548c\u4f7f\u7528\u6297\u6d2a\u6750\u6599\uff0c\u4ee5\u63d0\u9ad8\u8def\u9762\u97e7\u6027\u3002"}}
{"id": "2507.01401", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01401", "abs": "https://arxiv.org/abs/2507.01401", "authors": ["Huanwen Liang", "Jingxian Xu", "Yuanji Zhang", "Yuhao Huang", "Yuhan Zhang", "Xin Yang", "Ran Li", "Xuedong Deng", "Yanjun Liu", "Guowei Tao", "Yun Wu", "Sheng Zhao", "Xinru Gao", "Dong Ni"], "title": "Medical-Knowledge Driven Multiple Instance Learning for Classifying Severe Abdominal Anomalies on Prenatal Ultrasound", "comment": "Accepted by MICCAI 2025", "summary": "Fetal abdominal malformations are serious congenital anomalies that require\naccurate diagnosis to guide pregnancy management and reduce mortality. Although\nAI has demonstrated significant potential in medical diagnosis, its application\nto prenatal abdominal anomalies remains limited. Most existing studies focus on\nimage-level classification and rely on standard plane localization, placing\nless emphasis on case-level diagnosis. In this paper, we develop a case-level\nmultiple instance learning (MIL)-based method, free of standard plane\nlocalization, for classifying fetal abdominal anomalies in prenatal ultrasound.\nOur contribution is three-fold. First, we adopt a mixture-of-attention-experts\nmodule (MoAE) to weight different attention heads for various planes. Secondly,\nwe propose a medical-knowledge-driven feature selection module (MFS) to align\nimage features with medical knowledge, performing self-supervised image token\nselection at the case-level. Finally, we propose a prompt-based prototype\nlearning (PPL) to enhance the MFS. Extensively validated on a large prenatal\nabdominal ultrasound dataset containing 2,419 cases, with a total of 24,748\nimages and 6 categories, our proposed method outperforms the state-of-the-art\ncompetitors. Codes are available at:https://github.com/LL-AC/AAcls.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u5b9e\u4f8b\u5b66\u4e60\uff08MIL\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u80ce\u513f\u8179\u90e8\u5f02\u5e38\u7684\u4ea7\u524d\u8d85\u58f0\u5206\u7c7b\uff0c\u65e0\u9700\u6807\u51c6\u5e73\u9762\u5b9a\u4f4d\uff0c\u901a\u8fc7\u6df7\u5408\u6ce8\u610f\u529b\u4e13\u5bb6\u6a21\u5757\uff08MoAE\uff09\u3001\u533b\u5b66\u77e5\u8bc6\u9a71\u52a8\u7684\u7279\u5f81\u9009\u62e9\u6a21\u5757\uff08MFS\uff09\u548c\u63d0\u793a\u539f\u578b\u5b66\u4e60\uff08PPL\uff09\u5b9e\u73b0\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u80ce\u513f\u8179\u90e8\u7578\u5f62\u662f\u4e25\u91cd\u7684\u5148\u5929\u6027\u5f02\u5e38\uff0c\u9700\u8981\u51c6\u786e\u8bca\u65ad\u4ee5\u6307\u5bfc\u598a\u5a20\u7ba1\u7406\u5e76\u964d\u4f4e\u6b7b\u4ea1\u7387\u3002AI\u5728\u533b\u5b66\u8bca\u65ad\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5728\u4ea7\u524d\u8179\u90e8\u5f02\u5e38\u9886\u57df\u7684\u5e94\u7528\u4ecd\u6709\u9650\u3002\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u56fe\u50cf\u7ea7\u5206\u7c7b\uff0c\u800c\u8f83\u5c11\u5173\u6ce8\u75c5\u4f8b\u7ea7\u8bca\u65ad\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMIL\u7684\u65b9\u6cd5\uff0c\u5305\u62ecMoAE\u6a21\u5757\uff08\u52a0\u6743\u4e0d\u540c\u6ce8\u610f\u529b\u5934\uff09\u3001MFS\u6a21\u5757\uff08\u533b\u5b66\u77e5\u8bc6\u9a71\u52a8\u7684\u7279\u5f81\u9009\u62e9\uff09\u548cPPL\u6a21\u5757\uff08\u63d0\u793a\u539f\u578b\u5b66\u4e60\uff09\u3002", "result": "\u5728\u5305\u542b2,419\u4e2a\u75c5\u4f8b\u300124,748\u5f20\u56fe\u50cf\u548c6\u4e2a\u7c7b\u522b\u7684\u5927\u578b\u4ea7\u524d\u8179\u90e8\u8d85\u58f0\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u75c5\u4f8b\u7ea7\u8bca\u65ad\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u4ea7\u524d\u8179\u90e8\u5f02\u5e38\u7684AI\u8f85\u52a9\u8bca\u65ad\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.01734", "categories": ["cs.CL", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.01734", "abs": "https://arxiv.org/abs/2507.01734", "authors": ["Oliver Wardas", "Florian Matthes"], "title": "LLMs for Legal Subsumption in German Employment Contracts", "comment": "PrePrint - ICAIL25, Chicago", "summary": "Legal work, characterized by its text-heavy and resource-intensive nature,\npresents unique challenges and opportunities for NLP research. While\ndata-driven approaches have advanced the field, their lack of interpretability\nand trustworthiness limits their applicability in dynamic legal environments.\nTo address these issues, we collaborated with legal experts to extend an\nexisting dataset and explored the use of Large Language Models (LLMs) and\nin-context learning to evaluate the legality of clauses in German employment\ncontracts. Our work evaluates the ability of different LLMs to classify clauses\nas \"valid,\" \"unfair,\" or \"void\" under three legal context variants: no legal\ncontext, full-text sources of laws and court rulings, and distilled versions of\nthese (referred to as examination guidelines). Results show that full-text\nsources moderately improve performance, while examination guidelines\nsignificantly enhance recall for void clauses and weighted F1-Score, reaching\n80\\%. Despite these advancements, LLMs' performance when using full-text\nsources remains substantially below that of human lawyers. We contribute an\nextended dataset, including examination guidelines, referenced legal sources,\nand corresponding annotations, alongside our code and all log files. Our\nfindings highlight the potential of LLMs to assist lawyers in contract legality\nreview while also underscoring the limitations of the methods presented.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86NLP\u5728\u6cd5\u5f8b\u6587\u672c\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u6269\u5c55\u6570\u636e\u96c6\u548c\u4f7f\u7528LLMs\u8bc4\u4f30\u5fb7\u56fd\u96c7\u4f63\u5408\u540c\u6761\u6b3e\u7684\u5408\u6cd5\u6027\uff0c\u53d1\u73b0\u6cd5\u5f8b\u4e0a\u4e0b\u6587\u5bf9\u6a21\u578b\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u6cd5\u5f8b\u5de5\u4f5c\u7684\u6587\u672c\u5bc6\u96c6\u6027\u548c\u8d44\u6e90\u5bc6\u96c6\u6027\u4e3aNLP\u7814\u7a76\u63d0\u4f9b\u4e86\u72ec\u7279\u6311\u6218\u548c\u673a\u4f1a\uff0c\u4f46\u73b0\u6709\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "method": "\u4e0e\u6cd5\u5f8b\u4e13\u5bb6\u5408\u4f5c\u6269\u5c55\u6570\u636e\u96c6\uff0c\u5229\u7528LLMs\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u8bc4\u4f30\u5408\u540c\u6761\u6b3e\u5408\u6cd5\u6027\uff0c\u6d4b\u8bd5\u4e0d\u540c\u6cd5\u5f8b\u4e0a\u4e0b\u6587\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b8c\u6574\u6cd5\u5f8b\u6587\u672c\u7565\u5fae\u63d0\u5347\u6027\u80fd\uff0c\u800c\u7cbe\u7b80\u7248\u6cd5\u5f8b\u6307\u5357\u663e\u8457\u63d0\u9ad8\u53ec\u56de\u7387\u548cF1\u5206\u6570\uff0880%\uff09\uff0c\u4f46LLMs\u8868\u73b0\u4ecd\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u5f8b\u5e08\u3002", "conclusion": "LLMs\u5728\u5408\u540c\u5408\u6cd5\u6027\u5ba1\u67e5\u4e2d\u6709\u6f5c\u529b\uff0c\u4f46\u4ecd\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2507.01057", "categories": ["cs.LG", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2507.01057", "abs": "https://arxiv.org/abs/2507.01057", "authors": ["Lushun Fan", "Yuqin Xia", "Jun Li", "Karl Jenkins"], "title": "Loop2Net: Data-Driven Generation and Optimization of Airfoil CFD Meshes from Sparse Boundary Coordinates", "comment": null, "summary": "In this study, an innovative intelligent optimization system for mesh quality\nis proposed, which is based on a deep convolutional neural network\narchitecture, to achieve mesh generation and optimization. The core of the\nstudy is the Loop2Net generator and loss function, it predicts the mesh based\non the given wing coordinates. And the model's performance is continuously\noptimised by two key loss functions during the training. Then discipline by\nadding penalties, the goal of mesh generation was finally reached.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u667a\u80fd\u4f18\u5316\u7cfb\u7edf\uff0c\u7528\u4e8e\u7f51\u683c\u751f\u6210\u4e0e\u4f18\u5316\uff0c\u6838\u5fc3\u662fLoop2Net\u751f\u6210\u5668\u548c\u635f\u5931\u51fd\u6570\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u667a\u80fd\u4f18\u5316\u7cfb\u7edf\u63d0\u5347\u7f51\u683c\u751f\u6210\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7ed3\u5408Loop2Net\u751f\u6210\u5668\u548c\u4e24\u4e2a\u5173\u952e\u635f\u5931\u51fd\u6570\u8fdb\u884c\u8bad\u7ec3\u4f18\u5316\u3002", "result": "\u6a21\u578b\u80fd\u591f\u57fa\u4e8e\u7ed9\u5b9a\u7ffc\u5750\u6807\u9884\u6d4b\u7f51\u683c\uff0c\u5e76\u901a\u8fc7\u60e9\u7f5a\u673a\u5236\u5b9e\u73b0\u7f51\u683c\u751f\u6210\u76ee\u6807\u3002", "conclusion": "\u8be5\u667a\u80fd\u4f18\u5316\u7cfb\u7edf\u5728\u7f51\u683c\u751f\u6210\u4e0e\u4f18\u5316\u65b9\u9762\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u6f5c\u529b\u3002"}}
{"id": "2507.01409", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01409", "abs": "https://arxiv.org/abs/2507.01409", "authors": ["Kuniaki Saito", "Donghyun Kim", "Kwanyong Park", "Atsushi Hashimoto", "Yoshitaka Ushiku"], "title": "CaptionSmiths: Flexibly Controlling Language Pattern in Image Captioning", "comment": "Accepted to ICCV2025", "summary": "An image captioning model flexibly switching its language pattern, e.g.,\ndescriptiveness and length, should be useful since it can be applied to diverse\napplications. However, despite the dramatic improvement in generative\nvision-language models, fine-grained control over the properties of generated\ncaptions is not easy due to two reasons: (i) existing models are not given the\nproperties as a condition during training and (ii) existing models cannot\nsmoothly transition its language pattern from one state to the other. Given\nthis challenge, we propose a new approach, CaptionSmiths, to acquire a single\ncaptioning model that can handle diverse language patterns. First, our approach\nquantifies three properties of each caption, length, descriptiveness, and\nuniqueness of a word, as continuous scalar values, without human annotation.\nGiven the values, we represent the conditioning via interpolation between two\nendpoint vectors corresponding to the extreme states, e.g., one for a very\nshort caption and one for a very long caption. Empirical results demonstrate\nthat the resulting model can smoothly change the properties of the output\ncaptions and show higher lexical alignment than baselines. For instance,\nCaptionSmiths reduces the error in controlling caption length by 506\\% despite\nbetter lexical alignment. Code will be available on\nhttps://github.com/omron-sinicx/captionsmiths.", "AI": {"tldr": "\u63d0\u51faCaptionSmiths\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cf\u5316\u6807\u9898\u5c5e\u6027\u5e76\u63d2\u503c\u7aef\u70b9\u5411\u91cf\uff0c\u5b9e\u73b0\u6807\u9898\u8bed\u8a00\u6a21\u5f0f\u7684\u7075\u6d3b\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u6807\u9898\u751f\u6210\u6a21\u578b\u96be\u4ee5\u7cbe\u7ec6\u63a7\u5236\u6807\u9898\u5c5e\u6027\uff0c\u5982\u957f\u5ea6\u548c\u63cf\u8ff0\u6027\u3002", "method": "\u91cf\u5316\u6807\u9898\u957f\u5ea6\u3001\u63cf\u8ff0\u6027\u548c\u8bcd\u6c47\u72ec\u7279\u6027\uff0c\u901a\u8fc7\u63d2\u503c\u7aef\u70b9\u5411\u91cf\u5b9e\u73b0\u6761\u4ef6\u63a7\u5236\u3002", "result": "\u6a21\u578b\u80fd\u5e73\u6ed1\u8c03\u6574\u6807\u9898\u5c5e\u6027\uff0c\u8bcd\u6c47\u5bf9\u9f50\u4f18\u4e8e\u57fa\u7ebf\uff0c\u957f\u5ea6\u63a7\u5236\u8bef\u5dee\u51cf\u5c11506%\u3002", "conclusion": "CaptionSmiths\u4e3a\u6807\u9898\u751f\u6210\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u63a7\u5236\u65b9\u6cd5\u3002"}}
{"id": "2507.01764", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01764", "abs": "https://arxiv.org/abs/2507.01764", "authors": ["Matteo Di Cristofaro"], "title": "Data interference: emojis, homoglyphs, and issues of data fidelity in corpora and their results", "comment": "Author submitted manuscript", "summary": "Tokenisation - \"the process of splitting text into atomic parts\" (Brezina &\nTimperley, 2017: 1) - is a crucial step for corpus linguistics, as it provides\nthe basis for any applicable quantitative method (e.g. collocations) while\nensuring the reliability of qualitative approaches. This paper examines how\ndiscrepancies in tokenisation affect the representation of language data and\nthe validity of analytical findings: investigating the challenges posed by\nemojis and homoglyphs, the study highlights the necessity of preprocessing\nthese elements to maintain corpus fidelity to the source data. The research\npresents methods for ensuring that digital texts are accurately represented in\ncorpora, thereby supporting reliable linguistic analysis and guaranteeing the\nrepeatability of linguistic interpretations. The findings emphasise the\nnecessity of a detailed understanding of both linguistic and technical aspects\ninvolved in digital textual data to enhance the accuracy of corpus analysis,\nand have significant implications for both quantitative and qualitative\napproaches in corpus-based research.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5206\u8bcd\u5bf9\u8bed\u8a00\u6570\u636e\u8868\u793a\u548c\u5206\u6790\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u91cd\u70b9\u7814\u7a76\u4e86\u8868\u60c5\u7b26\u53f7\u548c\u540c\u5f62\u5f02\u4e49\u8bcd\u7684\u5904\u7406\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u786e\u4fdd\u6570\u5b57\u6587\u672c\u5728\u8bed\u6599\u5e93\u4e2d\u51c6\u786e\u8868\u793a\u7684\u65b9\u6cd5\u3002", "motivation": "\u5206\u8bcd\u662f\u8bed\u6599\u5e93\u8bed\u8a00\u5b66\u7684\u57fa\u7840\u6b65\u9aa4\uff0c\u4f46\u8868\u60c5\u7b26\u53f7\u548c\u540c\u5f62\u5f02\u4e49\u8bcd\u7b49\u5143\u7d20\u53ef\u80fd\u5bfc\u81f4\u6570\u636e\u8868\u793a\u548c\u5206\u6790\u7ed3\u679c\u7684\u504f\u5dee\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5982\u4f55\u9884\u5904\u7406\u8fd9\u4e9b\u5143\u7d20\u4ee5\u63d0\u9ad8\u8bed\u6599\u5e93\u7684\u51c6\u786e\u6027\u3002", "method": "\u7814\u7a76\u5206\u6790\u4e86\u8868\u60c5\u7b26\u53f7\u548c\u540c\u5f62\u5f02\u4e49\u8bcd\u5bf9\u5206\u8bcd\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u786e\u4fdd\u6570\u5b57\u6587\u672c\u5728\u8bed\u6599\u5e93\u4e2d\u51c6\u786e\u8868\u793a\u7684\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u9884\u5904\u7406\u8868\u60c5\u7b26\u53f7\u548c\u540c\u5f62\u5f02\u4e49\u8bcd\u5bf9\u63d0\u9ad8\u8bed\u6599\u5e93\u7684\u51c6\u786e\u6027\u548c\u5206\u6790\u7684\u53ef\u91cd\u590d\u6027\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u6570\u5b57\u6587\u672c\u6570\u636e\u5904\u7406\u4e2d\u7ed3\u5408\u8bed\u8a00\u548c\u6280\u672f\u77e5\u8bc6\u7684\u91cd\u8981\u6027\uff0c\u5bf9\u8bed\u6599\u5e93\u7814\u7a76\u7684\u5b9a\u91cf\u548c\u5b9a\u6027\u65b9\u6cd5\u5177\u6709\u91cd\u8981\u5f71\u54cd\u3002"}}
{"id": "2507.01067", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.01067", "abs": "https://arxiv.org/abs/2507.01067", "authors": ["Keun Soo Yim"], "title": "Evaluation of a Foundational Model and Stochastic Models for Forecasting Sporadic or Spiky Production Outages of High-Performance Machine Learning Services", "comment": null, "summary": "Time series forecasting models have diverse real world applications (e.g.,\nfrom electricity metrics to software workload). Latest foundational models\ntrained for time series forecasting show strengths (e.g., for long sequences\nand in zero-shot settings). However, foundational model was not yet used for\nforecasting rare, spiky events, i.e., a challenging target because those are a\ncorner case of extreme events. In this paper, we optimize a state-of-the-art\nfoundational model to forecast sporadic or spiky production outages of\nhigh-performance machine learning services powering billions of client devices.\nWe evaluate the forecasting errors of the foundational model compared with\nclassical stochastic forecasting models (e.g., moving average and\nautoregressive). The analysis helps us understand how each of the evaluated\nmodels performs for the sporadic or spiky events. For example, it identifies\nthe key patterns in the target data that are well tracked by the foundational\nmodel vs. each of the stochastic models. We use the models with optimal\nparameters to estimate a year-long outage statistics of a particular root cause\nwith less than 6% value errors.", "AI": {"tldr": "\u672c\u6587\u4f18\u5316\u4e86\u4e00\u79cd\u6700\u5148\u8fdb\u7684\u57fa\u7840\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u9ad8\u6027\u80fd\u673a\u5668\u5b66\u4e60\u670d\u52a1\u4e2d\u7684\u7f55\u89c1\u3001\u7a81\u53d1\u6027\u751f\u4ea7\u4e2d\u65ad\uff0c\u5e76\u4e0e\u7ecf\u5178\u968f\u673a\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\uff0c\u53d1\u73b0\u57fa\u7840\u6a21\u578b\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u7f55\u89c1\u3001\u7a81\u53d1\u6027\u4e8b\u4ef6\uff08\u5982\u751f\u4ea7\u4e2d\u65ad\uff09\u662f\u6781\u7aef\u4e8b\u4ef6\u7684\u7279\u6b8a\u60c5\u51b5\uff0c\u73b0\u6709\u57fa\u7840\u6a21\u578b\u5c1a\u672a\u9488\u5bf9\u6b64\u7c7b\u4e8b\u4ef6\u8fdb\u884c\u4f18\u5316\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f18\u5316\u4e86\u4e00\u79cd\u6700\u5148\u8fdb\u7684\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u4e0e\u7ecf\u5178\u968f\u673a\u6a21\u578b\uff08\u5982\u79fb\u52a8\u5e73\u5747\u548c\u81ea\u56de\u5f52\u6a21\u578b\uff09\u8fdb\u884c\u6bd4\u8f83\uff0c\u5206\u6790\u5176\u5728\u7a81\u53d1\u6027\u4e8b\u4ef6\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u57fa\u7840\u6a21\u578b\u5728\u9884\u6d4b\u7a81\u53d1\u6027\u4e8b\u4ef6\u65f6\u8868\u73b0\u4f18\u4e8e\u968f\u673a\u6a21\u578b\uff0c\u5e76\u80fd\u51c6\u786e\u4f30\u8ba1\u7279\u5b9a\u6839\u56e0\u7684\u5e74\u4e2d\u65ad\u7edf\u8ba1\uff0c\u8bef\u5dee\u4f4e\u4e8e6%\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u5728\u7f55\u89c1\u3001\u7a81\u53d1\u6027\u4e8b\u4ef6\u7684\u9884\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01417", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01417", "abs": "https://arxiv.org/abs/2507.01417", "authors": ["Jiawei Gu", "Ziyue Qiao", "Zechao Li"], "title": "Gradient Short-Circuit: Efficient Out-of-Distribution Detection via Feature Intervention", "comment": "Accepted to ICCV 2025", "summary": "Out-of-Distribution (OOD) detection is critical for safely deploying deep\nmodels in open-world environments, where inputs may lie outside the training\ndistribution. During inference on a model trained exclusively with\nIn-Distribution (ID) data, we observe a salient gradient phenomenon: around an\nID sample, the local gradient directions for \"enhancing\" that sample's\npredicted class remain relatively consistent, whereas OOD samples--unseen in\ntraining--exhibit disorganized or conflicting gradient directions in the same\nneighborhood. Motivated by this observation, we propose an inference-stage\ntechnique to short-circuit those feature coordinates that spurious gradients\nexploit to inflate OOD confidence, while leaving ID classification largely\nintact. To circumvent the expense of recomputing the logits after this gradient\nshort-circuit, we further introduce a local first-order approximation that\naccurately captures the post-modification outputs without a second forward\npass. Experiments on standard OOD benchmarks show our approach yields\nsubstantial improvements. Moreover, the method is lightweight and requires\nminimal changes to the standard inference pipeline, offering a practical path\ntoward robust OOD detection in real-world applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u65b9\u5411\u4e00\u81f4\u6027\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u77ed\u8def\u5f02\u5e38\u68af\u5ea6\u65b9\u5411\u6765\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301ID\u5206\u7c7b\u6548\u679c\u3002", "motivation": "\u5728\u5f00\u653e\u4e16\u754c\u4e2d\uff0cOOD\u68c0\u6d4b\u5bf9\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u5b89\u5168\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002\u7814\u7a76\u53d1\u73b0\uff0cID\u6837\u672c\u7684\u68af\u5ea6\u65b9\u5411\u4e00\u81f4\uff0c\u800cOOD\u6837\u672c\u7684\u68af\u5ea6\u65b9\u5411\u6df7\u4e71\uff0c\u8fd9\u4e3a\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u63a8\u7406\u9636\u6bb5\u6280\u672f\uff0c\u77ed\u8def\u5f02\u5e38\u68af\u5ea6\u65b9\u5411\uff0c\u540c\u65f6\u5f15\u5165\u5c40\u90e8\u4e00\u9636\u8fd1\u4f3c\u4ee5\u907f\u514d\u91cd\u590d\u8ba1\u7b97\u3002", "result": "\u5728\u6807\u51c6OOD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u663e\u8457\u63d0\u5347\uff0c\u65b9\u6cd5\u8f7b\u91cf\u4e14\u6613\u4e8e\u96c6\u6210\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684OOD\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01785", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01785", "abs": "https://arxiv.org/abs/2507.01785", "authors": ["Zhixun Chen", "Ping Guo", "Wenhan Han", "Yifan Zhang", "Binbin Liu", "Haobin Lin", "Fengze Liu", "Yan Zhao", "Bingni Zhang", "Taifeng Wang", "Yin Zheng", "Meng Fang"], "title": "MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining", "comment": null, "summary": "Data quality is a critical driver of large language model performance, yet\nexisting model-based selection methods focus almost exclusively on English. We\nintroduce MuRating, a scalable framework that transfers high-quality English\ndata-quality signals into a single rater for 17 target languages. MuRating\naggregates multiple English \"raters\" via pairwise comparisons to learn unified\ndocument-quality scores,then projects these judgments through translation to\ntrain a multilingual evaluator on monolingual, cross-lingual, and parallel text\npairs. Applied to web data, MuRating selects balanced subsets of English and\nmultilingual content to pretrain a 1.2 B-parameter LLaMA model. Compared to\nstrong baselines, including QuRater, AskLLM, DCLM and so on, our approach\nboosts average accuracy on both English benchmarks and multilingual\nevaluations, with especially large gains on knowledge-intensive tasks. We\nfurther analyze translation fidelity, selection biases, and underrepresentation\nof narrative material, outlining directions for future work.", "AI": {"tldr": "MuRating\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u591a\u8bed\u8a00\u6570\u636e\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u82f1\u8bed\u6570\u636e\u8d28\u91cf\u4fe1\u53f7\u8f6c\u79fb\u523017\u79cd\u76ee\u6807\u8bed\u8a00\uff0c\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u82f1\u8bed\uff0c\u7f3a\u4e4f\u591a\u8bed\u8a00\u652f\u6301\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5728\u591a\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "MuRating\u901a\u8fc7\u805a\u5408\u591a\u4e2a\u82f1\u8bed\u8bc4\u5206\u5668\u7684\u4fe1\u53f7\uff0c\u5b66\u4e60\u7edf\u4e00\u7684\u6587\u6863\u8d28\u91cf\u8bc4\u5206\uff0c\u5e76\u901a\u8fc7\u7ffb\u8bd1\u5c06\u8fd9\u4e9b\u8bc4\u5206\u6295\u5c04\u5230\u76ee\u6807\u8bed\u8a00\uff0c\u8bad\u7ec3\u591a\u8bed\u8a00\u8bc4\u4f30\u5668\u3002", "result": "MuRating\u5728\u82f1\u8bed\u548c\u591a\u8bed\u8a00\u8bc4\u4f30\u4e2d\u5747\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "MuRating\u4e3a\u591a\u8bed\u8a00\u6570\u636e\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u5728\u7ffb\u8bd1\u4fdd\u771f\u5ea6\u548c\u5185\u5bb9\u5e73\u8861\u65b9\u9762\u7684\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2507.01068", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01068", "abs": "https://arxiv.org/abs/2507.01068", "authors": ["Biplov Paneru"], "title": "Prediction of Freezing of Gait in Parkinsons Disease using Explainable AI and Federated Deep Learning for Wearable Sensors", "comment": null, "summary": "This study leverages an Inertial Measurement Unit (IMU) dataset to develop\nexplainable AI methods for the early detection and prediction of Freezing of\nGait (FOG), a common symptom in Parkinson's disease. Machine learning models,\nincluding CatBoost, XGBoost, and Extra Trees classifiers, are employed to\naccurately categorize FOG episodes based on relevant clinical features. A\nStacking Ensemble model achieves superior performance, surpassing a hybrid\nbidirectional GRU model and reaching nearly 99% classification accuracy. SHAP\ninterpretability analysis reveals that time (seconds) is the most influential\nfactor in distinguishing gait patterns. Additionally, the proposed FOG\nprediction framework incorporates federated learning, where models are trained\nlocally on individual devices and aggregated on a central server using a\nfederated averaging approach, utilizing a hybrid Conv1D + LSTM architecture for\nenhanced predictive capability.", "AI": {"tldr": "\u5229\u7528IMU\u6570\u636e\u548c\u53ef\u89e3\u91caAI\u65b9\u6cd5\u5f00\u53d1\u65e9\u671f\u68c0\u6d4b\u548c\u9884\u6d4b\u5e15\u91d1\u68ee\u75c5\u51bb\u7ed3\u6b65\u6001\uff08FOG\uff09\u7684\u6a21\u578b\uff0c\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u65e9\u671f\u68c0\u6d4b\u548c\u9884\u6d4b\u5e15\u91d1\u68ee\u75c5\u4e2d\u7684FOG\u75c7\u72b6\uff0c\u4ee5\u6539\u5584\u60a3\u8005\u751f\u6d3b\u8d28\u91cf\u3002", "method": "\u4f7f\u7528CatBoost\u3001XGBoost\u548cExtra Trees\u5206\u7c7b\u5668\uff0c\u5e76\u901a\u8fc7Stacking Ensemble\u6a21\u578b\u63d0\u5347\u6027\u80fd\uff1b\u7ed3\u5408SHAP\u5206\u6790\u89e3\u91ca\u7279\u5f81\u91cd\u8981\u6027\uff1b\u91c7\u7528\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u3002", "result": "Stacking Ensemble\u6a21\u578b\u8fbe\u5230\u8fd199%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u53cc\u5411GRU\u6a21\u578b\uff1b\u65f6\u95f4\uff08\u79d2\uff09\u662f\u533a\u5206\u6b65\u6001\u6a21\u5f0f\u7684\u6700\u91cd\u8981\u56e0\u7d20\u3002", "conclusion": "\u63d0\u51fa\u7684FOG\u9884\u6d4b\u6846\u67b6\u7ed3\u5408\u53ef\u89e3\u91caAI\u548c\u8054\u90a6\u5b66\u4e60\uff0c\u5177\u6709\u9ad8\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.01422", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01422", "abs": "https://arxiv.org/abs/2507.01422", "authors": ["Wenjie Liu", "Bingshu Wang", "Ze Wang", "C. L. Philip Chen"], "title": "DocShaDiffusion: Diffusion Model in Latent Space for Document Image Shadow Removal", "comment": null, "summary": "Document shadow removal is a crucial task in the field of document image\nenhancement. However, existing methods tend to remove shadows with constant\ncolor background and ignore color shadows. In this paper, we first design a\ndiffusion model in latent space for document image shadow removal, called\nDocShaDiffusion. It translates shadow images from pixel space to latent space,\nenabling the model to more easily capture essential features. To address the\nissue of color shadows, we design a shadow soft-mask generation module (SSGM).\nIt is able to produce accurate shadow mask and add noise into shadow regions\nspecially. Guided by the shadow mask, a shadow mask-aware guided diffusion\nmodule (SMGDM) is proposed to remove shadows from document images by\nsupervising the diffusion and denoising process. We also propose a\nshadow-robust perceptual feature loss to preserve details and structures in\ndocument images. Moreover, we develop a large-scale synthetic document color\nshadow removal dataset (SDCSRD). It simulates the distribution of realistic\ncolor shadows and provides powerful supports for the training of models.\nExperiments on three public datasets validate the proposed method's superiority\nover state-of-the-art. Our code and dataset will be publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDocShaDiffusion\u7684\u6f5c\u5728\u7a7a\u95f4\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u6587\u6863\u56fe\u50cf\u9634\u5f71\u53bb\u9664\uff0c\u5e76\u8bbe\u8ba1\u4e86\u9634\u5f71\u8f6f\u63a9\u6a21\u751f\u6210\u6a21\u5757\uff08SSGM\uff09\u548c\u9634\u5f71\u63a9\u6a21\u5f15\u5bfc\u6269\u6563\u6a21\u5757\uff08SMGDM\uff09\uff0c\u4ee5\u89e3\u51b3\u989c\u8272\u9634\u5f71\u95ee\u9898\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86\u9634\u5f71\u9c81\u68d2\u611f\u77e5\u7279\u5f81\u635f\u5931\u548c\u5408\u6210\u6570\u636e\u96c6SDCSRD\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4ec5\u5904\u7406\u6052\u5b9a\u989c\u8272\u80cc\u666f\u7684\u9634\u5f71\uff0c\u800c\u5ffd\u7565\u4e86\u989c\u8272\u9634\u5f71\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u6587\u6863\u9634\u5f71\u53bb\u9664\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u6f5c\u5728\u7a7a\u95f4\u6269\u6563\u6a21\u578bDocShaDiffusion\uff0c\u7ed3\u5408SSGM\u751f\u6210\u9634\u5f71\u63a9\u6a21\uff0c\u5e76\u901a\u8fc7SMGDM\u5f15\u5bfc\u6269\u6563\u548c\u53bb\u566a\u8fc7\u7a0b\u3002\u8fd8\u63d0\u51fa\u4e86\u9634\u5f71\u9c81\u68d2\u611f\u77e5\u7279\u5f81\u635f\u5931\uff0c\u5e76\u5f00\u53d1\u4e86\u5408\u6210\u6570\u636e\u96c6SDCSRD\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "DocShaDiffusion\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u5efa\u6a21\u548c\u63a9\u6a21\u5f15\u5bfc\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u989c\u8272\u9634\u5f71\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u6587\u6863\u7ec6\u8282\u548c\u7ed3\u6784\u3002"}}
{"id": "2507.01786", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01786", "abs": "https://arxiv.org/abs/2507.01786", "authors": ["Jord Nguyen", "Khiem Hoang", "Carlo Leonardo Attubato", "Felix Hofst\u00e4tter"], "title": "Probing Evaluation Awareness of Language Models", "comment": "Technical AI Governance Workshop, ICML (Poster)", "summary": "Language models can distinguish between testing and deployment phases -- a\ncapability known as evaluation awareness. This has significant safety and\npolicy implications, potentially undermining the reliability of evaluations\nthat are central to AI governance frameworks and voluntary industry\ncommitments. In this paper, we study evaluation awareness in\nLlama-3.3-70B-Instruct. We show that linear probes can separate real-world\nevaluation and deployment prompts, suggesting that current models internally\nrepresent this distinction. We also find that current safety evaluations are\ncorrectly classified by the probes, suggesting that they already appear\nartificial or inauthentic to models. Our findings underscore the importance of\nensuring trustworthy evaluations and understanding deceptive capabilities. More\nbroadly, our work showcases how model internals may be leveraged to support\nblackbox methods in safety audits, especially for future models more competent\nat evaluation awareness and deception.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0Llama-3.3-70B-Instruct\u6a21\u578b\u80fd\u533a\u5206\u6d4b\u8bd5\u4e0e\u90e8\u7f72\u9636\u6bb5\uff0c\u7ebf\u6027\u63a2\u9488\u53ef\u5206\u79bb\u8fd9\u4e24\u79cd\u63d0\u793a\uff0c\u8868\u660e\u6a21\u578b\u5185\u90e8\u5df2\u5177\u5907\u8fd9\u79cd\u533a\u5206\u80fd\u529b\u3002\u5b89\u5168\u8bc4\u4f30\u88ab\u6a21\u578b\u8bc6\u522b\u4e3a\u4e0d\u771f\u5b9e\uff0c\u51f8\u663e\u4e86\u53ef\u4fe1\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u63a2\u8ba8\u8bed\u8a00\u6a21\u578b\u5728\u6d4b\u8bd5\u4e0e\u90e8\u7f72\u9636\u6bb5\u7684\u533a\u5206\u80fd\u529b\uff08\u8bc4\u4f30\u610f\u8bc6\uff09\u53ca\u5176\u5bf9AI\u6cbb\u7406\u6846\u67b6\u548c\u884c\u4e1a\u627f\u8bfa\u7684\u6f5c\u5728\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u7ebf\u6027\u63a2\u9488\u5206\u6790Llama-3.3-70B-Instruct\u6a21\u578b\uff0c\u5206\u79bb\u6d4b\u8bd5\u4e0e\u90e8\u7f72\u63d0\u793a\uff0c\u5e76\u8bc4\u4f30\u5176\u5185\u90e8\u8868\u793a\u3002", "result": "\u7ebf\u6027\u63a2\u9488\u80fd\u6709\u6548\u533a\u5206\u6d4b\u8bd5\u4e0e\u90e8\u7f72\u63d0\u793a\uff0c\u4e14\u5f53\u524d\u5b89\u5168\u8bc4\u4f30\u88ab\u6a21\u578b\u89c6\u4e3a\u4e0d\u771f\u5b9e\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u53ef\u4fe1\u8bc4\u4f30\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u5229\u7528\u6a21\u578b\u5185\u90e8\u4fe1\u606f\u652f\u6301\u9ed1\u76d2\u5b89\u5168\u5ba1\u8ba1\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u672a\u6765\u66f4\u5177\u6b3a\u9a97\u80fd\u529b\u7684\u6a21\u578b\u3002"}}
{"id": "2507.01073", "categories": ["cs.LG", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2507.01073", "abs": "https://arxiv.org/abs/2507.01073", "authors": ["Dian Jin"], "title": "Rotational Sampling: A Plug-and-Play Encoder for Rotation-Invariant 3D Molecular GNNs", "comment": null, "summary": "Graph neural networks (GNNs) have achieved remarkable success in molecular\nproperty prediction. However, traditional graph representations struggle to\neffectively encode the inherent 3D spatial structures of molecules, as\nmolecular orientations in 3D space introduce significant variability, severely\nlimiting model generalization and robustness. Existing approaches primarily\nfocus on rotation-invariant and rotation-equivariant methods. Invariant methods\noften rely heavily on prior knowledge and lack sufficient generalizability,\nwhile equivariant methods suffer from high computational costs. To address\nthese limitations, this paper proposes a novel plug-and-play 3D encoding module\nleveraging rotational sampling. By computing the expectation over the SO(3)\nrotational group, the method naturally achieves approximate rotational\ninvariance. Furthermore, by introducing a carefully designed post-alignment\nstrategy, strict invariance can be achieved without compromising performance.\nExperimental evaluations on the QM9 and C10 Datasets demonstrate superior\npredictive accuracy, robustness, and generalization performance compared to\nexisting methods. Moreover, the proposed approach maintains low computational\ncomplexity and enhanced interpretability, providing a promising direction for\nefficient and effective handling of 3D molecular information in drug discovery\nand material design.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65cb\u8f6c\u91c7\u6837\u76843D\u7f16\u7801\u6a21\u5757\uff0c\u7528\u4e8e\u89e3\u51b3\u5206\u5b503D\u7a7a\u95f4\u7ed3\u6784\u7f16\u7801\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u56fe\u8868\u793a\u96be\u4ee5\u6709\u6548\u7f16\u7801\u5206\u5b50\u76843D\u7a7a\u95f4\u7ed3\u6784\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u6cdb\u5316\u6027\u4e0d\u8db3\u6216\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u65cb\u8f6c\u91c7\u6837\u8ba1\u7b97SO(3)\u65cb\u8f6c\u7fa4\u7684\u671f\u671b\uff0c\u5b9e\u73b0\u8fd1\u4f3c\u65cb\u8f6c\u4e0d\u53d8\u6027\uff0c\u5e76\u7ed3\u5408\u540e\u5bf9\u9f50\u7b56\u7565\u5b9e\u73b0\u4e25\u683c\u4e0d\u53d8\u6027\u3002", "result": "\u5728QM9\u548cC10\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8ba1\u7b97\u590d\u6742\u5ea6\u4f4e\u4e14\u53ef\u89e3\u91ca\u6027\u5f3a\uff0c\u4e3a\u836f\u7269\u53d1\u73b0\u548c\u6750\u6599\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u5904\u74063D\u5206\u5b50\u4fe1\u606f\u7684\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.01428", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.01428", "abs": "https://arxiv.org/abs/2507.01428", "authors": ["Chen Sun", "Haiyang Sun", "Zhiqing Guo", "Yunfeng Diao", "Liejun Wang", "Dan Ma", "Gaobo Yang", "Keqin Li"], "title": "DiffMark: Diffusion-based Robust Watermark Against Deepfakes", "comment": null, "summary": "Deepfakes pose significant security and privacy threats through malicious\nfacial manipulations. While robust watermarking can aid in authenticity\nverification and source tracking, existing methods often lack the sufficient\nrobustness against Deepfake manipulations. Diffusion models have demonstrated\nremarkable performance in image generation, enabling the seamless fusion of\nwatermark with image during generation. In this study, we propose a novel\nrobust watermarking framework based on diffusion model, called DiffMark. By\nmodifying the training and sampling scheme, we take the facial image and\nwatermark as conditions to guide the diffusion model to progressively denoise\nand generate corresponding watermarked image. In the construction of facial\ncondition, we weight the facial image by a timestep-dependent factor that\ngradually reduces the guidance intensity with the decrease of noise, thus\nbetter adapting to the sampling process of diffusion model. To achieve the\nfusion of watermark condition, we introduce a cross information fusion (CIF)\nmodule that leverages a learnable embedding table to adaptively extract\nwatermark features and integrates them with image features via cross-attention.\nTo enhance the robustness of the watermark against Deepfake manipulations, we\nintegrate a frozen autoencoder during training phase to simulate Deepfake\nmanipulations. Additionally, we introduce Deepfake-resistant guidance that\nemploys specific Deepfake model to adversarially guide the diffusion sampling\nprocess to generate more robust watermarked images. Experimental results\ndemonstrate the effectiveness of the proposed DiffMark on typical Deepfakes.\nOur code will be available at https://github.com/vpsg-research/DiffMark.", "AI": {"tldr": "DiffMark\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u9c81\u68d2\u6c34\u5370\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u8bad\u7ec3\u548c\u91c7\u6837\u65b9\u6848\uff0c\u7ed3\u5408\u4eba\u8138\u56fe\u50cf\u548c\u6c34\u5370\u6761\u4ef6\u751f\u6210\u6c34\u5370\u56fe\u50cf\uff0c\u5e76\u5f15\u5165\u6297Deepfake\u64cd\u4f5c\u7684\u6280\u672f\u3002", "motivation": "Deepfake\u6280\u672f\u5bf9\u5b89\u5168\u548c\u9690\u79c1\u6784\u6210\u5a01\u80c1\uff0c\u73b0\u6709\u6c34\u5370\u65b9\u6cd5\u5728\u5bf9\u6297Deepfake\u64cd\u4f5c\u65f6\u9c81\u68d2\u6027\u4e0d\u8db3\u3002\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u6c34\u5370\u878d\u5408\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "DiffMark\u901a\u8fc7\u65f6\u95f4\u6b65\u4f9d\u8d56\u7684\u56e0\u5b50\u52a0\u6743\u4eba\u8138\u56fe\u50cf\uff0c\u9010\u6b65\u964d\u4f4e\u5f15\u5bfc\u5f3a\u5ea6\uff0c\u5e76\u5f15\u5165\u4ea4\u53c9\u4fe1\u606f\u878d\u5408\u6a21\u5757\uff08CIF\uff09\u81ea\u9002\u5e94\u63d0\u53d6\u6c34\u5370\u7279\u5f81\u3002\u8bad\u7ec3\u9636\u6bb5\u4f7f\u7528\u51bb\u7ed3\u81ea\u52a8\u7f16\u7801\u5668\u6a21\u62dfDeepfake\u64cd\u4f5c\uff0c\u5e76\u91c7\u7528Deepfake-resistant guidance\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eDiffMark\u5728\u5178\u578bDeepfake\u64cd\u4f5c\u4e0b\u5177\u6709\u6709\u6548\u6027\u3002", "conclusion": "DiffMark\u4e3a\u5bf9\u6297Deepfake\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u6c34\u5370\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.01790", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01790", "abs": "https://arxiv.org/abs/2507.01790", "authors": ["Tianze Hua", "Tian Yun", "Ellie Pavlick"], "title": "How Do Vision-Language Models Process Conflicting Information Across Modalities?", "comment": "All code and resources are available at:\n  https://github.com/ethahtz/vlm_conflicting_info_processing", "summary": "AI models are increasingly required to be multimodal, integrating disparate\ninput streams into a coherent state representation on which subsequent\nbehaviors and actions can be based. This paper seeks to understand how such\nmodels behave when input streams present conflicting information. Focusing\nspecifically on vision-language models, we provide inconsistent inputs (e.g.,\nan image of a dog paired with the caption \"A photo of a cat\") and ask the model\nto report the information present in one of the specific modalities (e.g.,\n\"What does the caption say / What is in the image?\"). We find that models often\nfavor one modality over the other, e.g., reporting the image regardless of what\nthe caption says, but that different models differ in which modality they\nfavor. We find evidence that the behaviorally preferred modality is evident in\nthe internal representational structure of the model, and that specific\nattention heads can restructure the representations to favor one modality over\nthe other. Moreover, we find modality-agnostic \"router heads\" which appear to\npromote answers about the modality requested in the instruction, and which can\nbe manipulated or transferred in order to improve performance across datasets\nand modalities. Together, the work provides essential steps towards identifying\nand controlling if and how models detect and resolve conflicting signals within\ncomplex multimodal environments.", "AI": {"tldr": "\u7814\u7a76\u591a\u6a21\u6001AI\u6a21\u578b\u5728\u8f93\u5165\u4fe1\u606f\u51b2\u7a81\u65f6\u7684\u884c\u4e3a\uff0c\u53d1\u73b0\u6a21\u578b\u503e\u5411\u4e8e\u4f18\u5148\u5904\u7406\u67d0\u4e00\u6a21\u6001\uff0c\u5e76\u8bc6\u522b\u51fa\u5f71\u54cd\u6a21\u6001\u504f\u597d\u7684\u5185\u90e8\u7ed3\u6784\u548c\u6ce8\u610f\u529b\u673a\u5236\u3002", "motivation": "\u7406\u89e3\u591a\u6a21\u6001\u6a21\u578b\u5728\u8f93\u5165\u4fe1\u606f\u51b2\u7a81\u65f6\u7684\u884c\u4e3a\uff0c\u63a2\u7d22\u6a21\u578b\u5982\u4f55\u6574\u5408\u548c\u4f18\u5148\u5904\u7406\u4e0d\u540c\u6a21\u6001\u7684\u4fe1\u606f\u3002", "method": "\u901a\u8fc7\u5411\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e0d\u4e00\u81f4\u7684\u8f93\u5165\uff08\u5982\u56fe\u50cf\u4e0e\u6807\u9898\u77db\u76fe\uff09\uff0c\u89c2\u5bdf\u6a21\u578b\u5bf9\u7279\u5b9a\u6a21\u6001\u4fe1\u606f\u7684\u62a5\u544a\u884c\u4e3a\uff0c\u5e76\u5206\u6790\u5176\u5185\u90e8\u7ed3\u6784\u548c\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u6a21\u578b\u503e\u5411\u4e8e\u4f18\u5148\u5904\u7406\u67d0\u4e00\u6a21\u6001\uff0c\u4e14\u8fd9\u79cd\u504f\u597d\u4f53\u73b0\u5728\u5185\u90e8\u8868\u793a\u7ed3\u6784\u4e2d\uff1b\u8bc6\u522b\u51fa\u53ef\u8c03\u6574\u6a21\u6001\u504f\u597d\u7684\u6ce8\u610f\u529b\u673a\u5236\u548c\u6a21\u6001\u65e0\u5173\u7684\u201c\u8def\u7531\u5934\u201d\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8bc6\u522b\u548c\u63a7\u5236\u591a\u6a21\u6001\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u51b2\u7a81\u4fe1\u53f7\u5904\u7406\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2507.01075", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.01075", "abs": "https://arxiv.org/abs/2507.01075", "authors": ["Gabriele Padovani", "Valentine Anantharaj", "Sandro Fiore"], "title": "Provenance Tracking in Large-Scale Machine Learning Systems", "comment": null, "summary": "As the demand for large scale AI models continues to grow, the optimization\nof their training to balance computational efficiency, execution time, accuracy\nand energy consumption represents a critical multidimensional challenge.\nAchieving this balance requires not only innovative algorithmic techniques and\nhardware architectures but also comprehensive tools for monitoring, analyzing,\nand understanding the underlying processes involved in model training and\ndeployment. Provenance data information about the origins, context, and\ntransformations of data and processes has become a key component in this\npursuit. By leveraging provenance, researchers and engineers can gain insights\ninto resource usage patterns, identify inefficiencies, and ensure\nreproducibility and accountability in AI development workflows. For this\nreason, the question of how distributed resources can be optimally utilized to\nscale large AI models in an energy efficient manner is a fundamental one. To\nsupport this effort, we introduce the yProv4ML library, a tool designed to\ncollect provenance data in JSON format, compliant with the W3C PROV and ProvML\nstandards. yProv4ML focuses on flexibility and extensibility, and enables users\nto integrate additional data collection tools via plugins. The library is fully\nintegrated with the yProv framework, allowing for higher level pairing in tasks\nrun also through workflow management systems.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86yProv4ML\u5e93\uff0c\u7528\u4e8e\u6536\u96c6\u7b26\u5408W3C PROV\u548cProvML\u6807\u51c6\u7684JSON\u683c\u5f0f\u7684\u6eaf\u6e90\u6570\u636e\uff0c\u4ee5\u4f18\u5316\u5927\u89c4\u6a21AI\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u7387\u3001\u6267\u884c\u65f6\u95f4\u3001\u51c6\u786e\u6027\u548c\u80fd\u8017\u3002", "motivation": "\u968f\u7740\u5927\u89c4\u6a21AI\u6a21\u578b\u9700\u6c42\u7684\u589e\u957f\uff0c\u5982\u4f55\u5728\u8ba1\u7b97\u6548\u7387\u3001\u6267\u884c\u65f6\u95f4\u3001\u51c6\u786e\u6027\u548c\u80fd\u8017\u4e4b\u95f4\u5b9e\u73b0\u5e73\u8861\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002\u6eaf\u6e90\u6570\u636e\u6210\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u7684\u6838\u5fc3\u5de5\u5177\u3002", "method": "\u5f00\u53d1\u4e86yProv4ML\u5e93\uff0c\u652f\u6301\u7075\u6d3b\u6269\u5c55\uff0c\u53ef\u901a\u8fc7\u63d2\u4ef6\u96c6\u6210\u5176\u4ed6\u6570\u636e\u6536\u96c6\u5de5\u5177\uff0c\u5e76\u4e0eyProv\u6846\u67b6\u5b8c\u5168\u96c6\u6210\u3002", "result": "yProv4ML\u5e93\u80fd\u591f\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u548c\u5de5\u7a0b\u5e08\u5206\u6790\u8d44\u6e90\u4f7f\u7528\u6a21\u5f0f\u3001\u8bc6\u522b\u4f4e\u6548\u73af\u8282\uff0c\u5e76\u786e\u4fddAI\u5f00\u53d1\u6d41\u7a0b\u7684\u53ef\u91cd\u73b0\u6027\u548c\u95ee\u8d23\u6027\u3002", "conclusion": "\u901a\u8fc7yProv4ML\u5e93\uff0c\u53ef\u4ee5\u66f4\u9ad8\u6548\u5730\u5229\u7528\u5206\u5e03\u5f0f\u8d44\u6e90\uff0c\u4ee5\u8282\u80fd\u65b9\u5f0f\u6269\u5c55\u5927\u578bAI\u6a21\u578b\u3002"}}
{"id": "2507.01439", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01439", "abs": "https://arxiv.org/abs/2507.01439", "authors": ["Shaocheng Yan", "Pengcheng Shi", "Zhenjun Zhao", "Kaixin Wang", "Kuang Cao", "Ji Wu", "Jiayuan Li"], "title": "TurboReg: TurboClique for Robust and Efficient Point Cloud Registration", "comment": "ICCV-2025 Accepted Paper", "summary": "Robust estimation is essential in correspondence-based Point Cloud\nRegistration (PCR). Existing methods using maximal clique search in\ncompatibility graphs achieve high recall but suffer from exponential time\ncomplexity, limiting their use in time-sensitive applications. To address this\nchallenge, we propose a fast and robust estimator, TurboReg, built upon a novel\nlightweight clique, TurboClique, and a highly parallelizable Pivot-Guided\nSearch (PGS) algorithm. First, we define the TurboClique as a 3-clique within a\nhighly-constrained compatibility graph. The lightweight nature of the 3-clique\nallows for efficient parallel searching, and the highly-constrained\ncompatibility graph ensures robust spatial consistency for stable\ntransformation estimation. Next, PGS selects matching pairs with high SC$^2$\nscores as pivots, effectively guiding the search toward TurboCliques with\nhigher inlier ratios. Moreover, the PGS algorithm has linear time complexity\nand is significantly more efficient than the maximal clique search with\nexponential time complexity. Extensive experiments show that TurboReg achieves\nstate-of-the-art performance across multiple real-world datasets, with\nsubstantial speed improvements. For example, on the 3DMatch+FCGF dataset,\nTurboReg (1K) operates $208.22\\times$ faster than 3DMAC while also achieving\nhigher recall. Our code is accessible at\n\\href{https://github.com/Laka-3DV/TurboReg}{\\texttt{TurboReg}}.", "AI": {"tldr": "TurboReg\u662f\u4e00\u79cd\u57fa\u4e8eTurboClique\u548cPivot-Guided Search\uff08PGS\uff09\u7b97\u6cd5\u7684\u5feb\u901f\u9c81\u68d2\u70b9\u4e91\u914d\u51c6\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u901f\u5ea6\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6700\u5927\u56e2\u641c\u7d22\u7684\u70b9\u4e91\u914d\u51c6\u65b9\u6cd5\u867d\u7136\u53ec\u56de\u7387\u9ad8\uff0c\u4f46\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a\u6307\u6570\u7ea7\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u6027\u9700\u6c42\u3002", "method": "\u63d0\u51faTurboClique\uff08\u8f7b\u91cf\u7ea73-\u56e2\uff09\u548cPGS\u7b97\u6cd5\uff0c\u901a\u8fc7\u9ad8\u5ea6\u7ea6\u675f\u7684\u517c\u5bb9\u6027\u56fe\u548c\u5e76\u884c\u641c\u7d22\u5b9e\u73b0\u9ad8\u6548\u9c81\u68d2\u914d\u51c6\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u901f\u5ea6\u63d0\u5347\u663e\u8457\uff08\u59823DMatch+FCGF\u6570\u636e\u96c6\u4e0a\u6bd43DMAC\u5feb208.22\u500d\uff09\u3002", "conclusion": "TurboReg\u5728\u901f\u5ea6\u548c\u9c81\u68d2\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\u3002"}}
{"id": "2507.01802", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01802", "abs": "https://arxiv.org/abs/2507.01802", "authors": ["Katharina Beckh", "Elisa Studeny", "Sujan Sai Gannamaneni", "Dario Antweiler", "Stefan R\u00fcping"], "title": "The Anatomy of Evidence: An Investigation Into Explainable ICD Coding", "comment": "Accepted to ACL 2025 Findings", "summary": "Automatic medical coding has the potential to ease documentation and billing\nprocesses. For this task, transparency plays an important role for medical\ncoders and regulatory bodies, which can be achieved using explainability\nmethods. However, the evaluation of these approaches has been mostly limited to\nshort text and binary settings due to a scarcity of annotated data. Recent\nefforts by Cheng et al. (2023) have introduced the MDACE dataset, which\nprovides a valuable resource containing code evidence in clinical records. In\nthis work, we conduct an in-depth analysis of the MDACE dataset and perform\nplausibility evaluation of current explainable medical coding systems from an\napplied perspective. With this, we contribute to a deeper understanding of\nautomatic medical coding and evidence extraction. Our findings reveal that\nground truth evidence aligns with code descriptions to a certain degree. An\ninvestigation into state-of-the-art approaches shows a high overlap with ground\ntruth evidence. We propose match measures and highlight success and failure\ncases. Based on our findings, we provide recommendations for developing and\nevaluating explainable medical coding systems.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86MDACE\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u5f53\u524d\u53ef\u89e3\u91ca\u533b\u7597\u7f16\u7801\u7cfb\u7edf\u7684\u5408\u7406\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "\u81ea\u52a8\u533b\u7597\u7f16\u7801\u53ef\u7b80\u5316\u6587\u6863\u548c\u8ba1\u8d39\u6d41\u7a0b\uff0c\u4f46\u9700\u8981\u900f\u660e\u6027\uff0c\u800c\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u56e0\u6570\u636e\u7a00\u7f3a\u53d7\u9650\u3002", "method": "\u6df1\u5165\u5206\u6790MDACE\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u73b0\u6709\u53ef\u89e3\u91ca\u533b\u7597\u7f16\u7801\u7cfb\u7edf\u7684\u5408\u7406\u6027\uff0c\u5e76\u63d0\u51fa\u5339\u914d\u5ea6\u91cf\u3002", "result": "\u771f\u5b9e\u8bc1\u636e\u4e0e\u4ee3\u7801\u63cf\u8ff0\u90e8\u5206\u4e00\u81f4\uff0c\u5148\u8fdb\u65b9\u6cd5\u4e0e\u771f\u5b9e\u8bc1\u636e\u9ad8\u5ea6\u91cd\u53e0\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f00\u53d1\u4e0e\u8bc4\u4f30\u53ef\u89e3\u91ca\u533b\u7597\u7f16\u7801\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5efa\u8bae\u3002"}}
{"id": "2507.01196", "categories": ["cs.LG", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.01196", "abs": "https://arxiv.org/abs/2507.01196", "authors": ["Na Lee", "Konstantinos Barmpas", "Yannis Panagakis", "Dimitrios Adamos", "Nikolaos Laskaris", "Stefanos Zafeiriou"], "title": "Are Large Brainwave Foundation Models Capable Yet? Insights from Fine-tuning", "comment": null, "summary": "Foundation Models have demonstrated significant success across various\ndomains in Artificial Intelligence (AI), yet their capabilities for brainwave\nmodeling remain unclear. In this paper, we comprehensively evaluate current\nLarge Brainwave Foundation Models (LBMs) through systematic fine-tuning\nexperiments across multiple Brain-Computer Interface (BCI) benchmark tasks,\nincluding memory tasks and sleep stage classification. Our extensive analysis\nshows that state-of-the-art LBMs achieve only marginal improvements (0.9%-1.2%)\nover traditional deep architectures while requiring significantly more\nparameters (millions vs thousands), raising important questions about their\nefficiency and applicability in BCI contexts. Moreover, through detailed\nablation studies and Low-Rank Adaptation (LoRA), we significantly reduce\ntrainable parameters without performance degradation, while demonstrating that\narchitectural and training inefficiencies limit LBMs' current capabilities. Our\nexperiments span both full model fine-tuning and parameter-efficient adaptation\ntechniques, providing insights into optimal training strategies for BCI\napplications. We pioneer the application of LoRA to LBMs, revealing that\nperformance benefits generally emerge when adapting multiple neural network\ncomponents simultaneously. These findings highlight the critical need for\ndomain-specific development strategies to advance LBMs, suggesting that current\narchitectures may require redesign to fully leverage the potential of\nfoundation models in brainwave analysis.", "AI": {"tldr": "\u5f53\u524d\u5927\u578b\u8111\u6ce2\u57fa\u7840\u6a21\u578b\uff08LBMs\uff09\u5728\u8111\u673a\u63a5\u53e3\uff08BCI\uff09\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u4ec5\u6bd4\u4f20\u7edf\u6df1\u5ea6\u67b6\u6784\u7565\u6709\u63d0\u5347\uff080.9%-1.2%\uff09\uff0c\u4f46\u53c2\u6570\u9700\u6c42\u663e\u8457\u589e\u52a0\u3002\u901a\u8fc7LoRA\u6280\u672f\u51cf\u5c11\u53c2\u6570\u540e\u6027\u80fd\u672a\u964d\uff0c\u8868\u660eLBMs\u9700\u9886\u57df\u7279\u5b9a\u4f18\u5316\u3002", "motivation": "\u8bc4\u4f30LBMs\u5728\u8111\u6ce2\u5efa\u6a21\u4e2d\u7684\u80fd\u529b\uff0c\u63a2\u7d22\u5176\u5728BCI\u4efb\u52a1\u4e2d\u7684\u6548\u7387\u548c\u9002\u7528\u6027\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5fae\u8c03\u5b9e\u9a8c\u548cLoRA\u6280\u672f\uff0c\u5206\u6790LBMs\u5728\u8bb0\u5fc6\u4efb\u52a1\u548c\u7761\u7720\u9636\u6bb5\u5206\u7c7b\u7b49BCI\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "LBMs\u6027\u80fd\u63d0\u5347\u6709\u9650\u4e14\u53c2\u6570\u9700\u6c42\u9ad8\uff0cLoRA\u53ef\u51cf\u5c11\u53c2\u6570\u800c\u4e0d\u964d\u4f4e\u6027\u80fd\u3002", "conclusion": "LBMs\u9700\u9886\u57df\u7279\u5b9a\u4f18\u5316\u548c\u67b6\u6784\u91cd\u65b0\u8bbe\u8ba1\u4ee5\u5145\u5206\u53d1\u6325\u6f5c\u529b\u3002"}}
{"id": "2507.01077", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01077", "abs": "https://arxiv.org/abs/2507.01077", "authors": ["Bogdan Bogdan", "Arina Cazacu", "Laura Vasilie"], "title": "Good Enough to Learn: LLM-based Anomaly Detection in ECU Logs without Reliable Labels", "comment": "6 pages, 7 figures, 4 tables, accepted to IEEE Intelligent Vehicles\n  Symposium (IV) 2025", "summary": "Anomaly detection often relies on supervised or clustering approaches, with\nlimited success in specialized domains like automotive communication systems\nwhere scalable solutions are essential. We propose a novel decoder-only Large\nLanguage Model (LLM) to detect anomalies in Electronic Control Unit (ECU)\ncommunication logs. Our approach addresses two key challenges: the lack of LLMs\ntailored for ECU communication and the complexity of inconsistent ground truth\ndata. By learning from UDP communication logs, we formulate anomaly detection\nsimply as identifying deviations in time from normal behavior. We introduce an\nentropy regularization technique that increases model's uncertainty in known\nanomalies while maintaining consistency in similar scenarios. Our solution\noffers three novelties: a decoder-only anomaly detection architecture, a way to\nhandle inconsistent labeling, and an adaptable LLM for different ECU\ncommunication use cases. By leveraging the generative capabilities of\ndecoder-only models, we present a new technique that addresses the high cost\nand error-prone nature of manual labeling through a more scalable system that\nis able to learn from a minimal set of examples, while improving detection\naccuracy in complex communication environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89e3\u7801\u5668\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7528\u4e8e\u68c0\u6d4b\u7535\u5b50\u63a7\u5236\u5355\u5143\uff08ECU\uff09\u901a\u4fe1\u65e5\u5fd7\u4e2d\u7684\u5f02\u5e38\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4e13\u4e1a\u9886\u57df\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u6c7d\u8f66\u901a\u4fe1\u7cfb\u7edf\u7b49\u4e13\u4e1a\u9886\u57df\u6548\u679c\u6709\u9650\uff0c\u4e14\u7f3a\u4e4f\u9488\u5bf9ECU\u901a\u4fe1\u7684LLM\u6a21\u578b\u3002", "method": "\u4f7f\u7528\u89e3\u7801\u5668LLM\u5b66\u4e60UDP\u901a\u4fe1\u65e5\u5fd7\uff0c\u901a\u8fc7\u65f6\u95f4\u504f\u5dee\u68c0\u6d4b\u5f02\u5e38\uff0c\u5e76\u5f15\u5165\u71b5\u6b63\u5219\u5316\u6280\u672f\u63d0\u9ad8\u6a21\u578b\u5bf9\u5df2\u77e5\u5f02\u5e38\u7684\u8bc6\u522b\u80fd\u529b\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u7801\u5668\u5f02\u5e38\u68c0\u6d4b\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u6807\u7b7e\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u9002\u7528\u4e8e\u4e0d\u540cECU\u901a\u4fe1\u573a\u666f\u7684LLM\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u80fd\u529b\u51cf\u5c11\u4e86\u4eba\u5de5\u6807\u6ce8\u6210\u672c\uff0c\u63d0\u9ad8\u4e86\u590d\u6742\u901a\u4fe1\u73af\u5883\u4e2d\u7684\u68c0\u6d4b\u51c6\u786e\u6027\u3002"}}
{"id": "2507.01455", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01455", "abs": "https://arxiv.org/abs/2507.01455", "authors": ["Yuxing Liu", "Ji Zhang", "Zhou Xuchuan", "Jingzhong Xiao", "Huimin Yang", "Jiaxin Zhong"], "title": "OoDDINO:A Multi-level Framework for Anomaly Segmentation on Complex Road Scenes", "comment": "12 pages, 5 figures", "summary": "Anomaly segmentation aims to identify Out-of-Distribution (OoD) anomalous\nobjects within images. Existing pixel-wise methods typically assign anomaly\nscores individually and employ a global thresholding strategy to segment\nanomalies. Despite their effectiveness, these approaches encounter significant\nchallenges in real-world applications: (1) neglecting spatial correlations\namong pixels within the same object, resulting in fragmented segmentation; (2)\nvariabil ity in anomaly score distributions across image regions, causing\nglobal thresholds to either generate false positives in background areas or\nmiss segments of anomalous objects. In this work, we introduce OoDDINO, a novel\nmulti-level anomaly segmentation framework designed to address these\nlimitations through a coarse-to-fine anomaly detection strategy. OoDDINO\ncombines an uncertainty-guided anomaly detection model with a pixel-level\nsegmentation model within a two-stage cascade architecture. Initially, we\npropose an Orthogonal Uncertainty-Aware Fusion Strategy (OUAFS) that\nsequentially integrates multiple uncertainty metrics with visual\nrepresentations, employing orthogonal constraints to strengthen the detection\nmodel's capacity for localizing anomalous regions accurately. Subsequently, we\ndevelop an Adaptive Dual-Threshold Network (ADT-Net), which dynamically\ngenerates region-specific thresholds based on object-level detection outputs\nand pixel-wise anomaly scores. This approach allows for distinct thresholding\nstrategies within foreground and background areas, achieving fine-grained\nanomaly segmentation. The proposed framework is compatible with other\npixel-wise anomaly detection models, which acts as a plug-in to boost the\nperformance. Extensive experiments on two benchmark datasets validate our\nframework's superiority and compatibility over state-of-the-art methods.", "AI": {"tldr": "OoDDINO\u662f\u4e00\u4e2a\u591a\u7ea7\u5f02\u5e38\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u7c97\u5230\u7ec6\u7684\u68c0\u6d4b\u7b56\u7565\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u68c0\u6d4b\u6a21\u578b\u548c\u50cf\u7d20\u7ea7\u5206\u5272\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u5f02\u5e38\u5206\u5272\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u50cf\u7d20\u7ea7\u5f02\u5e38\u5206\u5272\u65b9\u6cd5\u5ffd\u89c6\u7a7a\u95f4\u76f8\u5173\u6027\u4e14\u4f9d\u8d56\u5168\u5c40\u9608\u503c\uff0c\u5bfc\u81f4\u5206\u5272\u788e\u7247\u5316\u6216\u8bef\u5224\uff0cOoDDINO\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u7ea7\u8054\u67b6\u6784\uff1a1) \u6b63\u4ea4\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u878d\u5408\u7b56\u7565\uff08OUAFS\uff09\u6574\u5408\u591a\u6307\u6807\uff1b2) \u81ea\u9002\u5e94\u53cc\u9608\u503c\u7f51\u7edc\uff08ADT-Net\uff09\u52a8\u6001\u751f\u6210\u533a\u57df\u7279\u5b9a\u9608\u503c\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cOoDDINO\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u517c\u5bb9\u6027\u5f3a\u3002", "conclusion": "OoDDINO\u901a\u8fc7\u591a\u7ea7\u7b56\u7565\u663e\u8457\u63d0\u5347\u5f02\u5e38\u5206\u5272\u7cbe\u5ea6\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2507.01810", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.01810", "abs": "https://arxiv.org/abs/2507.01810", "authors": ["Nikita Neveditsin", "Pawan Lingras", "Vijay Mago"], "title": "Evaluating Structured Output Robustness of Small Language Models for Open Attribute-Value Extraction from Clinical Notes", "comment": "To appear in the ACL Anthology", "summary": "We present a comparative analysis of the parseability of structured outputs\ngenerated by small language models for open attribute-value extraction from\nclinical notes. We evaluate three widely used serialization formats: JSON,\nYAML, and XML, and find that JSON consistently yields the highest parseability.\nStructural robustness improves with targeted prompting and larger models, but\ndeclines for longer documents and certain note types. Our error analysis\nidentifies recurring format-specific failure patterns. These findings offer\npractical guidance for selecting serialization formats and designing prompts\nwhen deploying language models in privacy-sensitive clinical settings.", "AI": {"tldr": "\u6bd4\u8f83\u4e86JSON\u3001YAML\u548cXML\u5728\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u7684\u89e3\u6790\u6027\u80fd\uff0c\u53d1\u73b0JSON\u8868\u73b0\u6700\u4f73\uff0c\u63d0\u793a\u8bbe\u8ba1\u548c\u6a21\u578b\u5927\u5c0f\u5f71\u54cd\u89e3\u6790\u7a33\u5065\u6027\u3002", "motivation": "\u7814\u7a76\u4e0d\u540c\u5e8f\u5217\u5316\u683c\u5f0f\u5728\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u7684\u89e3\u6790\u6027\u80fd\uff0c\u4e3a\u9690\u79c1\u654f\u611f\u7684\u4e34\u5e8a\u73af\u5883\u63d0\u4f9b\u5b9e\u7528\u6307\u5bfc\u3002", "method": "\u8bc4\u4f30\u4e09\u79cd\u683c\u5f0f\uff08JSON\u3001YAML\u3001XML\uff09\u7684\u89e3\u6790\u6027\u80fd\uff0c\u5206\u6790\u9519\u8bef\u6a21\u5f0f\u3002", "result": "JSON\u89e3\u6790\u6027\u80fd\u6700\u4f73\uff0c\u4f46\u957f\u6587\u6863\u548c\u7279\u5b9a\u7b14\u8bb0\u7c7b\u578b\u4f1a\u964d\u4f4e\u7a33\u5065\u6027\u3002", "conclusion": "\u5efa\u8bae\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u4f18\u5148\u9009\u62e9JSON\uff0c\u5e76\u4f18\u5316\u63d0\u793a\u8bbe\u8ba1\u4ee5\u63d0\u9ad8\u89e3\u6790\u7a33\u5065\u6027\u3002"}}
{"id": "2507.01241", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01241", "abs": "https://arxiv.org/abs/2507.01241", "authors": ["Di Zhang", "Yihang Zhang"], "title": "Beyond First-Order: Training LLMs with Stochastic Conjugate Subgradients and AdamW", "comment": null, "summary": "Stochastic gradient-based descent (SGD), have long been central to training\nlarge language models (LLMs). However, their effectiveness is increasingly\nbeing questioned, particularly in large-scale applications where empirical\nevidence suggests potential performance limitations. In response, this paper\nproposes a stochastic conjugate subgradient method together with adaptive\nsampling tailored specifically for training LLMs. The method not only achieves\nfaster convergence per iteration but also demonstrates improved scalability\ncompared to traditional SGD techniques. It leverages sample complexity analysis\nto adaptively choose the sample size, employs a stochastic conjugate\nsubgradient approach to determine search directions and utilizing an AdamW-like\nalgorithm to adaptively adjust step sizes. This approach preserves the key\nadvantages of first-order methods while effectively addressing the nonconvexity\nand non-smoothness inherent in LLMs training. Additionally, we provide a\ndetailed analysis of the advantage of the algorithm. Experimental results show\nthat the proposed method not only maintains, but in many cases surpasses, the\nscalability of traditional SGD techniques, significantly enhancing both the\nspeed and accuracy of the optimization process.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8bad\u7ec3\u7684\u968f\u673a\u5171\u8f6d\u6b21\u68af\u5ea6\u65b9\u6cd5\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u91c7\u6837\uff0c\u76f8\u6bd4\u4f20\u7edfSGD\u65b9\u6cd5\u5728\u6536\u655b\u901f\u5ea6\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u4f20\u7edfSGD\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u5e94\u7528\u4e2d\u5b58\u5728\u6027\u80fd\u9650\u5236\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u81ea\u9002\u5e94\u91c7\u6837\u3001\u968f\u673a\u5171\u8f6d\u6b21\u68af\u5ea6\u65b9\u5411\u548cAdamW-like\u6b65\u957f\u8c03\u6574\uff0c\u89e3\u51b3LLM\u8bad\u7ec3\u4e2d\u7684\u975e\u51f8\u6027\u548c\u975e\u5149\u6ed1\u6027\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6536\u655b\u901f\u5ea6\u548c\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edfSGD\uff0c\u4e14\u66f4\u5177\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aLLM\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2507.01078", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.01078", "abs": "https://arxiv.org/abs/2507.01078", "authors": ["Gabriele Padovani", "Valentine Anantharaj", "Sandro Fiore"], "title": "yProv4ML: Effortless Provenance Tracking for Machine Learning Systems", "comment": null, "summary": "The rapid growth of interest in large language models (LLMs) reflects their\npotential for flexibility and generalization, and attracted the attention of a\ndiverse range of researchers. However, the advent of these techniques has also\nbrought to light the lack of transparency and rigor with which development is\npursued. In particular, the inability to determine the number of epochs and\nother hyperparameters in advance presents challenges in identifying the best\nmodel. To address this challenge, machine learning frameworks such as MLFlow\ncan automate the collection of this type of information. However, these tools\ncapture data using proprietary formats and pose little attention to lineage.\nThis paper proposes yProv4ML, a framework to capture provenance information\ngenerated during machine learning processes in PROV-JSON format, with minimal\ncode modifications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fayProv4ML\u6846\u67b6\uff0c\u7528\u4e8e\u4ee5PROV-JSON\u683c\u5f0f\u6355\u83b7\u673a\u5668\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u6eaf\u6e90\u4fe1\u606f\uff0c\u89e3\u51b3\u73b0\u6709\u5de5\u5177\u7f3a\u4e4f\u900f\u660e\u6027\u548c\u6570\u636e\u6eaf\u6e90\u7684\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u53d1\u5c55\u7f3a\u4e4f\u900f\u660e\u6027\u548c\u4e25\u8c28\u6027\uff0c\u5c24\u5176\u662f\u8d85\u53c2\u6570\u548c\u8bad\u7ec3\u8f6e\u6570\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5bfc\u81f4\u96be\u4ee5\u786e\u5b9a\u6700\u4f73\u6a21\u578b\u3002\u73b0\u6709\u5de5\u5177\u5982MLFlow\u867d\u80fd\u81ea\u52a8\u5316\u6536\u96c6\u4fe1\u606f\uff0c\u4f46\u4f7f\u7528\u4e13\u6709\u683c\u5f0f\u4e14\u5ffd\u89c6\u6570\u636e\u6eaf\u6e90\u3002", "method": "\u63d0\u51fayProv4ML\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5c0f\u4ee3\u7801\u4fee\u6539\uff0c\u4ee5PROV-JSON\u683c\u5f0f\u6355\u83b7\u673a\u5668\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u6eaf\u6e90\u4fe1\u606f\u3002", "result": "yProv4ML\u80fd\u591f\u6709\u6548\u8bb0\u5f55\u673a\u5668\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u5173\u952e\u4fe1\u606f\uff0c\u63d0\u5347\u900f\u660e\u6027\u548c\u6570\u636e\u6eaf\u6e90\u80fd\u529b\u3002", "conclusion": "yProv4ML\u4e3a\u673a\u5668\u5b66\u4e60\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u900f\u660e\u6027\u548c\u6eaf\u6e90\u652f\u6301\uff0c\u586b\u8865\u4e86\u73b0\u6709\u5de5\u5177\u7684\u4e0d\u8db3\u3002"}}
{"id": "2507.01463", "categories": ["cs.CV", "cs.AI", "I.2; I.4; I.5"], "pdf": "https://arxiv.org/pdf/2507.01463", "abs": "https://arxiv.org/abs/2507.01463", "authors": ["Max Gandyra", "Alessandro Santonicola", "Michael Beetz"], "title": "NOCTIS: Novel Object Cyclic Threshold based Instance Segmentation", "comment": "10 pages, 3 figures, 3 tables, NeurIPS 2025 preprint", "summary": "Instance segmentation of novel objects instances in RGB images, given some\nexample images for each object, is a well known problem in computer vision.\nDesigning a model general enough to be employed, for all kinds of novel\nobjects, without (re-) training, has proven to be a difficult task. To handle\nthis, we propose a simple, yet powerful, framework, called: Novel Object Cyclic\nThreshold based Instance Segmentation (NOCTIS). This work stems from and\nimproves upon previous ones like CNOS, SAM-6D and NIDS-Net; thus, it also\nleverages on recent vision foundation models, namely: Grounded-SAM 2 and\nDINOv2. It utilises Grounded-SAM 2 to obtain object proposals with precise\nbounding boxes and their corresponding segmentation masks; while DINOv2's\nzero-shot capabilities are employed to generate the image embeddings. The\nquality of those masks, together with their embeddings, is of vital importance\nto our approach; as the proposal-object matching is realized by determining an\nobject matching score based on the similarity of the class embeddings and the\naverage maximum similarity of the patch embeddings. Differently to SAM-6D,\ncalculating the latter involves a prior patch filtering based on the distance\nbetween each patch and its corresponding cyclic/roundtrip patch in the image\ngrid. Furthermore, the average confidence of the proposals' bounding box and\nmask is used as an additional weighting factor for the object matching score.\nWe empirically show that NOCTIS, without further training/fine tuning,\noutperforms the best RGB and RGB-D methods on the seven core datasets of the\nBOP 2023 challenge for the \"Model-based 2D segmentation of unseen objects\"\ntask.", "AI": {"tldr": "NOCTIS\u662f\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u5bf9\u672a\u89c1\u7269\u4f53\u8fdb\u884c\u5b9e\u4f8b\u5206\u5272\u7684\u6846\u67b6\uff0c\u7ed3\u5408Grounded-SAM 2\u548cDINOv2\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u5339\u914d\u8bc4\u5206\u65b9\u6cd5\u5728BOP 2023\u6311\u6218\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u5bf9\u5404\u7c7b\u672a\u89c1\u7269\u4f53\u8fdb\u884c\u5b9e\u4f8b\u5206\u5272\u7684\u96be\u9898\u3002", "method": "\u5229\u7528Grounded-SAM 2\u751f\u6210\u7269\u4f53\u63d0\u8bae\u548c\u5206\u5272\u63a9\u7801\uff0c\u7ed3\u5408DINOv2\u7684\u96f6\u6837\u672c\u80fd\u529b\u751f\u6210\u5d4c\u5165\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u5339\u914d\u8bc4\u5206\u65b9\u6cd5\uff08\u5305\u62ec\u5faa\u73af\u9608\u503c\u8fc7\u6ee4\u548c\u7f6e\u4fe1\u5ea6\u52a0\u6743\uff09\u5b9e\u73b0\u7269\u4f53\u5339\u914d\u3002", "result": "\u5728BOP 2023\u6311\u6218\u7684\u4e03\u5927\u6570\u636e\u96c6\u4e0a\uff0cNOCTIS\u5728\u672a\u89c1\u7269\u4f53\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6RGB\u548cRGB-D\u65b9\u6cd5\u3002", "conclusion": "NOCTIS\u901a\u8fc7\u7b80\u5355\u4f46\u5f3a\u5927\u7684\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u672a\u89c1\u7269\u4f53\u5b9e\u4f8b\u5206\u5272\u7684\u6027\u80fd\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002"}}
{"id": "2507.01844", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01844", "abs": "https://arxiv.org/abs/2507.01844", "authors": ["Arthur Wuhrmann", "Anastasiia Kucherenko", "Andrei Kucharavy"], "title": "Low-Perplexity LLM-Generated Sequences and Where To Find Them", "comment": "Camera-ready version. Accepted to ACL 2025. 10 pages, 4 figures, 6\n  tables", "summary": "As Large Language Models (LLMs) become increasingly widespread, understanding\nhow specific training data shapes their outputs is crucial for transparency,\naccountability, privacy, and fairness. To explore how LLMs leverage and\nreplicate their training data, we introduce a systematic approach centered on\nanalyzing low-perplexity sequences - high-probability text spans generated by\nthe model. Our pipeline reliably extracts such long sequences across diverse\ntopics while avoiding degeneration, then traces them back to their sources in\nthe training data. Surprisingly, we find that a substantial portion of these\nlow-perplexity spans cannot be mapped to the corpus. For those that do match,\nwe quantify the distribution of occurrences across source documents,\nhighlighting the scope and nature of verbatim recall and paving a way toward\nbetter understanding of how LLMs training data impacts their behavior.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5982\u4f55\u5229\u7528\u548c\u590d\u5236\u5176\u8bad\u7ec3\u6570\u636e\uff0c\u901a\u8fc7\u5206\u6790\u4f4e\u56f0\u60d1\u5ea6\u5e8f\u5217\u6765\u8ffd\u8e2a\u6570\u636e\u6765\u6e90\uff0c\u53d1\u73b0\u90e8\u5206\u5e8f\u5217\u65e0\u6cd5\u6620\u5c04\u5230\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u5bf9\u5339\u914d\u5e8f\u5217\u7684\u5206\u5e03\u8fdb\u884c\u4e86\u91cf\u5316\u3002", "motivation": "\u7406\u89e3LLMs\u5982\u4f55\u5229\u7528\u8bad\u7ec3\u6570\u636e\u5bf9\u900f\u660e\u5ea6\u3001\u8d23\u4efb\u3001\u9690\u79c1\u548c\u516c\u5e73\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u4f4e\u56f0\u60d1\u5ea6\u5e8f\u5217\uff08\u6a21\u578b\u751f\u6210\u7684\u9ad8\u6982\u7387\u6587\u672c\u7247\u6bb5\uff09\u6765\u8ffd\u8e2a\u8bad\u7ec3\u6570\u636e\u6765\u6e90\u3002", "result": "\u53d1\u73b0\u5927\u91cf\u4f4e\u56f0\u60d1\u5ea6\u5e8f\u5217\u65e0\u6cd5\u6620\u5c04\u5230\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u5bf9\u5339\u914d\u5e8f\u5217\u7684\u5206\u5e03\u8fdb\u884c\u4e86\u91cf\u5316\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7406\u89e3LLMs\u8bad\u7ec3\u6570\u636e\u5982\u4f55\u5f71\u54cd\u5176\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2507.01080", "categories": ["cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.01080", "abs": "https://arxiv.org/abs/2507.01080", "authors": ["Edouard Lansiaux", "Ramy Azzouz", "Emmanuel Chazard", "Am\u00e9lie Vromant", "Eric Wiel"], "title": "Development and Comparative Evaluation of Three Artificial Intelligence Models (NLP, LLM, JEPA) for Predicting Triage in Emergency Departments: A 7-Month Retrospective Proof-of-Concept", "comment": "15 pages, 6 figures", "summary": "Triage errors, including undertriage and overtriage, are persistent\nchallenges in emergency departments (EDs). With increasing patient influx and\nstaff shortages, the integration of artificial intelligence (AI) into triage\nprotocols has gained attention. This study compares the performance of three AI\nmodels [Natural Language Processing (NLP), Large Language Models (LLM), and\nJoint Embedding Predictive Architecture (JEPA)] in predicting triage outcomes\nagainst the FRENCH scale and clinical practice.We conducted a retrospective\nanalysis of a prospectively recruited cohort gathering adult patient triage\ndata over a 7-month period at the Roger Salengro Hospital ED (Lille, France).\nThree AI models were trained and validated : (1) TRIAGEMASTER (NLP), (2)\nURGENTIAPARSE (LLM), and (3) EMERGINET (JEPA). Data included demographic\ndetails, verbatim chief complaints, vital signs, and triage outcomes based on\nthe FRENCH scale and GEMSA coding. The primary outcome was the concordance of\nAI-predicted triage level with the FRENCH gold-standard. It was assessed thanks\nto various indicators : F1-Score, Weighted Kappa, Spearman, MAE, RMSE. The LLM\nmodel (URGENTIAPARSE) showed higher accuracy (composite score: 2.514) compared\nto JEPA (EMERGINET, 0.438) and NLP (TRIAGEMASTER, -3.511), outperforming nurse\ntriage (-4.343). Secondary analyses highlighted the effectiveness of\nURGENTIAPARSE in predicting hospitalization needs (GEMSA) and its robustness\nwith structured data versus raw transcripts (either for GEMSA prediction or for\nFRENCH prediction). LLM architecture, through abstraction of patient\nrepresentations, offers the most accurate triage predictions among tested\nmodels. Integrating AI into ED workflows could enhance patient safety and\noperational efficiency, though integration into clinical workflows requires\naddressing model limitations and ensuring ethical transparency.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4e09\u79cdAI\u6a21\u578b\uff08NLP\u3001LLM\u3001JEPA\uff09\u5728\u6025\u8bca\u5206\u8bca\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0LLM\u6a21\u578b\uff08URGENTIAPARSE\uff09\u51c6\u786e\u6027\u6700\u9ad8\uff0c\u4f18\u4e8e\u62a4\u58eb\u5206\u8bca\u3002", "motivation": "\u6025\u8bca\u5206\u8bca\u4e2d\u7684\u9519\u8bef\uff08\u5982\u5206\u8bca\u4e0d\u8db3\u6216\u8fc7\u5ea6\uff09\u662f\u6301\u7eed\u6311\u6218\uff0cAI\u7684\u5f15\u5165\u53ef\u80fd\u63d0\u5347\u5206\u8bca\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "method": "\u56de\u987e\u6027\u5206\u67907\u4e2a\u6708\u7684\u6025\u8bca\u60a3\u8005\u6570\u636e\uff0c\u8bad\u7ec3\u5e76\u9a8c\u8bc1\u4e09\u79cdAI\u6a21\u578b\uff0c\u8bc4\u4f30\u5176\u4e0eFRENCH\u91d1\u6807\u51c6\u7684\u543b\u5408\u5ea6\u3002", "result": "LLM\u6a21\u578b\uff08URGENTIAPARSE\uff09\u8868\u73b0\u6700\u4f73\uff0c\u5c24\u5176\u5728\u9884\u6d4b\u4f4f\u9662\u9700\u6c42\u548c\u7ed3\u6784\u5316\u6570\u636e\u4e0a\u66f4\u7a33\u5065\u3002", "conclusion": "AI\uff08\u5c24\u5176\u662fLLM\uff09\u53ef\u63d0\u5347\u6025\u8bca\u5206\u8bca\u51c6\u786e\u6027\uff0c\u4f46\u9700\u89e3\u51b3\u6a21\u578b\u5c40\u9650\u6027\u548c\u4f26\u7406\u95ee\u9898\u3002"}}
{"id": "2507.01467", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01467", "abs": "https://arxiv.org/abs/2507.01467", "authors": ["Ge Wu", "Shen Zhang", "Ruijing Shi", "Shanghua Gao", "Zhenyuan Chen", "Lei Wang", "Zhaowei Chen", "Hongcheng Gao", "Yao Tang", "Jian Yang", "Ming-Ming Cheng", "Xiang Li"], "title": "Representation Entanglement for Generation:Training Diffusion Transformers Is Much Easier Than You Think", "comment": null, "summary": "REPA and its variants effectively mitigate training challenges in diffusion\nmodels by incorporating external visual representations from pretrained models,\nthrough alignment between the noisy hidden projections of denoising networks\nand foundational clean image representations. We argue that the external\nalignment, which is absent during the entire denoising inference process, falls\nshort of fully harnessing the potential of discriminative representations. In\nthis work, we propose a straightforward method called Representation\nEntanglement for Generation (REG), which entangles low-level image latents with\na single high-level class token from pretrained foundation models for\ndenoising. REG acquires the capability to produce coherent image-class pairs\ndirectly from pure noise, substantially improving both generation quality and\ntraining efficiency. This is accomplished with negligible additional inference\noverhead, requiring only one single additional token for denoising (<0.5\\%\nincrease in FLOPs and latency). The inference process concurrently reconstructs\nboth image latents and their corresponding global semantics, where the acquired\nsemantic knowledge actively guides and enhances the image generation process.\nOn ImageNet 256$\\times$256, SiT-XL/2 + REG demonstrates remarkable convergence\nacceleration, achieving $\\textbf{63}\\times$ and $\\textbf{23}\\times$ faster\ntraining than SiT-XL/2 and SiT-XL/2 + REPA, respectively. More impressively,\nSiT-L/2 + REG trained for merely 400K iterations outperforms SiT-XL/2 + REPA\ntrained for 4M iterations ($\\textbf{10}\\times$ longer). Code is available at:\nhttps://github.com/Martinser/REG.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aREG\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u4f4e\u5c42\u56fe\u50cf\u6f5c\u5728\u53d8\u91cf\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u9ad8\u5c42\u7c7b\u522b\u6807\u8bb0\u7ea0\u7f20\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\u548c\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982REPA\uff09\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5229\u7528\u5916\u90e8\u89c6\u89c9\u8868\u793a\uff0c\u4f46\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u672a\u80fd\u5145\u5206\u5229\u7528\u5224\u522b\u6027\u8868\u793a\uff0c\u9650\u5236\u4e86\u6a21\u578b\u6f5c\u529b\u3002", "method": "REG\u65b9\u6cd5\u5c06\u4f4e\u5c42\u56fe\u50cf\u6f5c\u5728\u53d8\u91cf\u4e0e\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u7684\u5355\u4e2a\u9ad8\u5c42\u7c7b\u522b\u6807\u8bb0\u7ea0\u7f20\uff0c\u76f4\u63a5\u4ece\u7eaf\u566a\u58f0\u4e2d\u751f\u6210\u4e00\u81f4\u7684\u56fe\u50cf-\u7c7b\u522b\u5bf9\u3002", "result": "\u5728ImageNet 256\u00d7256\u4e0a\uff0cREG\u663e\u8457\u52a0\u901f\u4e86\u6536\u655b\uff08\u8bad\u7ec3\u901f\u5ea6\u63d0\u534763\u500d\u548c23\u500d\uff09\uff0c\u5e76\u5728\u66f4\u77ed\u7684\u8bad\u7ec3\u65f6\u95f4\u5185\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "REG\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u77e5\u8bc6\u4e3b\u52a8\u6307\u5bfc\u56fe\u50cf\u751f\u6210\uff0c\u4ee5\u6781\u5c0f\u7684\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2507.01853", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01853", "abs": "https://arxiv.org/abs/2507.01853", "authors": ["Samridhi Raj Sinha", "Rajvee Sheth", "Abhishek Upperwal", "Mayank Singh"], "title": "Eka-Eval : A Comprehensive Evaluation Framework for Large Language Models in Indian Languages", "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has intensified the\nneed for evaluation frameworks that go beyond English centric benchmarks and\naddress the requirements of linguistically diverse regions such as India. We\npresent EKA-EVAL, a unified and production-ready evaluation framework that\nintegrates over 35 benchmarks, including 10 Indic-specific datasets, spanning\ncategories like reasoning, mathematics, tool use, long-context understanding,\nand reading comprehension. Compared to existing Indian language evaluation\ntools, EKA-EVAL offers broader benchmark coverage, with built-in support for\ndistributed inference, quantization, and multi-GPU usage. Our systematic\ncomparison positions EKA-EVAL as the first end-to-end, extensible evaluation\nsuite tailored for both global and Indic LLMs, significantly lowering the\nbarrier to multilingual benchmarking. The framework is open-source and publicly\navailable at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA\ninitiative (https://eka.soket.ai), which aims to scale up to over 100\nbenchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.", "AI": {"tldr": "EKA-EVAL\u662f\u4e00\u4e2a\u7edf\u4e00\u4e14\u751f\u4ea7\u5c31\u7eea\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u6574\u5408\u4e8635\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u62ec10\u4e2a\u5370\u5ea6\u7279\u5b9a\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u8bed\u8a00\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u9700\u8981\u8d85\u8d8a\u82f1\u8bed\u4e2d\u5fc3\u57fa\u51c6\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u6ee1\u8db3\u8bed\u8a00\u591a\u6837\u6027\u5730\u533a\uff08\u5982\u5370\u5ea6\uff09\u7684\u9700\u6c42\u3002", "method": "EKA-EVAL\u6574\u5408\u4e86\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\uff0c\u652f\u6301\u5206\u5e03\u5f0f\u63a8\u7406\u3001\u91cf\u5316\u548c\u591aGPU\u4f7f\u7528\uff0c\u63d0\u4f9b\u5e7f\u6cdb\u7684\u57fa\u51c6\u8986\u76d6\u3002", "result": "EKA-EVAL\u662f\u9996\u4e2a\u7aef\u5230\u7aef\u3001\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u5957\u4ef6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u7684\u95e8\u69db\u3002", "conclusion": "EKA-EVAL\u4f5c\u4e3a\u5f00\u6e90\u6846\u67b6\uff0c\u65e8\u5728\u5efa\u7acb\u4e00\u4e2a\u5f3a\u5927\u7684\u591a\u8bed\u8a00LLM\u8bc4\u4f30\u751f\u6001\u7cfb\u7edf\u3002"}}
{"id": "2507.01271", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01271", "abs": "https://arxiv.org/abs/2507.01271", "authors": ["Tatsuki Kawakami", "Kazuki Egashira", "Atsuyuki Miyai", "Go Irie", "Kiyoharu Aizawa"], "title": "PULSE: Practical Evaluation Scenarios for Large Multimodal Model Unlearning", "comment": null, "summary": "In recent years, unlearning techniques, which are methods for inducing a\nmodel to \"forget\" previously learned information, have attracted attention as a\nway to address privacy and copyright concerns in large language models (LLMs)\nand large multimodal models (LMMs). While several unlearning benchmarks have\nbeen established for LLMs, a practical evaluation framework for unlearning in\nLMMs has been less explored. Specifically, existing unlearning benchmark for\nLMMs considers only scenarios in which the model is required to unlearn\nfine-tuned knowledge through a single unlearning operation. In this study, we\nintroduce PULSE protocol for realistic unlearning scenarios for LMMs by\nintroducing two critical perspectives: (i) Pre-trained knowledge Unlearning for\nanalyzing the effect across different knowledge acquisition phases and (ii)\nLong-term Sustainability Evaluation to address sequential requests. We then\nevaluate existing unlearning methods along these dimensions. Our results reveal\nthat, although some techniques can successfully unlearn knowledge acquired\nthrough fine-tuning, they struggle to eliminate information learned during\npre-training. Moreover, methods that effectively unlearn a batch of target data\nin a single operation exhibit substantial performance degradation when the same\ndata are split and unlearned sequentially.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86PULSE\u534f\u8bae\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u7684\u9057\u5fd8\u6280\u672f\uff0c\u91cd\u70b9\u5173\u6ce8\u9884\u8bad\u7ec3\u77e5\u8bc6\u9057\u5fd8\u548c\u957f\u671f\u53ef\u6301\u7eed\u6027\u8bc4\u4f30\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u9057\u5fd8\u57fa\u51c6\u4ec5\u5173\u6ce8\u5355\u6b21\u9057\u5fd8\u64cd\u4f5c\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u66f4\u73b0\u5b9e\u7684\u9057\u5fd8\u573a\u666f\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u5f15\u5165PULSE\u534f\u8bae\uff0c\u5305\u62ec\u9884\u8bad\u7ec3\u77e5\u8bc6\u9057\u5fd8\u548c\u957f\u671f\u53ef\u6301\u7eed\u6027\u8bc4\u4f30\u4e24\u4e2a\u7ef4\u5ea6\uff0c\u8bc4\u4f30\u73b0\u6709\u9057\u5fd8\u65b9\u6cd5\u3002", "result": "\u73b0\u6709\u6280\u672f\u80fd\u6709\u6548\u9057\u5fd8\u5fae\u8c03\u77e5\u8bc6\uff0c\u4f46\u96be\u4ee5\u6d88\u9664\u9884\u8bad\u7ec3\u4fe1\u606f\uff1b\u5355\u6b21\u6279\u91cf\u9057\u5fd8\u65b9\u6cd5\u5728\u591a\u6b21\u64cd\u4f5c\u4e2d\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "PULSE\u534f\u8bae\u4e3aLMMs\u7684\u9057\u5fd8\u6280\u672f\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.01098", "categories": ["cs.LG", "cond-mat.dis-nn", "q-bio.NC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.01098", "abs": "https://arxiv.org/abs/2507.01098", "authors": ["Liu Ziyin", "Isaac Chuang"], "title": "Proof of a perfect platonic representation hypothesis", "comment": null, "summary": "In this note, we elaborate on and explain in detail the proof given by Ziyin\net al. (2025) of the \"perfect\" Platonic Representation Hypothesis (PRH) for the\nembedded deep linear network model (EDLN). We show that if trained with SGD,\ntwo EDLNs with different widths and depths and trained on different data will\nbecome Perfectly Platonic, meaning that every possible pair of layers will\nlearn the same representation up to a rotation. Because most of the global\nminima of the loss function are not Platonic, that SGD only finds the perfectly\nPlatonic solution is rather extraordinary. The proof also suggests at least six\nways the PRH can be broken. We also show that in the EDLN model, the emergence\nof the Platonic representations is due to the same reason as the emergence of\nprogressive sharpening. This implies that these two seemingly unrelated\nphenomena in deep learning can, surprisingly, have a common cause. Overall, the\ntheory and proof highlight the importance of understanding emergent \"entropic\nforces\" due to the irreversibility of SGD training and their role in\nrepresentation learning. The goal of this note is to be instructive and avoid\nlengthy technical details.", "AI": {"tldr": "\u8bba\u6587\u8be6\u7ec6\u89e3\u91ca\u4e86Ziyin\u7b49\u4eba\uff082025\uff09\u5173\u4e8e\u5d4c\u5165\u5f0f\u6df1\u5ea6\u7ebf\u6027\u7f51\u7edc\u6a21\u578b\uff08EDLN\uff09\u7684\u201c\u5b8c\u7f8e\u201d\u67cf\u62c9\u56fe\u8868\u793a\u5047\u8bbe\uff08PRH\uff09\u7684\u8bc1\u660e\uff0c\u5e76\u5c55\u793a\u4e86SGD\u8bad\u7ec3\u4e0bEDLN\u7684\u67cf\u62c9\u56fe\u6027\u3002", "motivation": "\u63a2\u8ba8SGD\u8bad\u7ec3\u4e0bEDLN\u6a21\u578b\u5982\u4f55\u5b66\u4e60\u5230\u67cf\u62c9\u56fe\u8868\u793a\uff0c\u5e76\u63ed\u793a\u5176\u4e0e\u6e10\u8fdb\u9510\u5316\u7684\u5171\u540c\u539f\u56e0\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u8bc1\u660e\uff0c\u5c55\u793aSGD\u8bad\u7ec3\u4e0bEDLN\u6a21\u578b\u7684\u67cf\u62c9\u56fe\u8868\u793a\u7279\u6027\u3002", "result": "\u53d1\u73b0SGD\u4ec5\u627e\u5230\u5b8c\u7f8e\u7684\u67cf\u62c9\u56fe\u89e3\uff0c\u4e14PRH\u53ef\u901a\u8fc7\u516d\u79cd\u65b9\u5f0f\u6253\u7834\u3002\u67cf\u62c9\u56fe\u8868\u793a\u4e0e\u6e10\u8fdb\u9510\u5316\u6709\u5171\u540c\u539f\u56e0\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u7406\u89e3SGD\u8bad\u7ec3\u4e2d\u201c\u71b5\u529b\u201d\u7684\u91cd\u8981\u6027\u53ca\u5176\u5728\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u4f5c\u7528\u3002"}}
{"id": "2507.01472", "categories": ["cs.CV", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.01472", "abs": "https://arxiv.org/abs/2507.01472", "authors": ["Jon\u00e1\u0161 Herec", "V\u00edt R\u016f\u017ei\u010dka", "Rado Pito\u0148\u00e1k"], "title": "Optimizing Methane Detection On Board Satellites: Speed, Accuracy, and Low-Power Solutions for Resource-Constrained Hardware", "comment": "This is a preprint of a paper accepted for the EDHPC 2025 Conference", "summary": "Methane is a potent greenhouse gas, and detecting its leaks early via\nhyperspectral satellite imagery can help mitigate climate change. Meanwhile,\nmany existing missions operate in manual tasking regimes only, thus missing\npotential events of interest. To overcome slow downlink rates cost-effectively,\nonboard detection is a viable solution. However, traditional methane\nenhancement methods are too computationally demanding for resource-limited\nonboard hardware. This work accelerates methane detection by focusing on\nefficient, low-power algorithms. We test fast target detection methods (ACE,\nCEM) that have not been previously used for methane detection and propose a\nMag1c-SAS - a significantly faster variant of the current state-of-the-art\nalgorithm for methane detection: Mag1c. To explore their true detection\npotential, we integrate them with a machine learning model (U-Net, LinkNet).\nOur results identify two promising candidates (Mag1c-SAS and CEM), both\nacceptably accurate for the detection of strong plumes and computationally\nefficient enough for onboard deployment: one optimized more for accuracy, the\nother more for speed, achieving up to ~100x and ~230x faster computation than\noriginal Mag1c on resource-limited hardware. Additionally, we propose and\nevaluate three band selection strategies. One of them can outperform the method\ntraditionally used in the field while using fewer channels, leading to even\nfaster processing without compromising accuracy. This research lays the\nfoundation for future advancements in onboard methane detection with minimal\nhardware requirements, improving timely data delivery. The produced code, data,\nand models are open-sourced and can be accessed from\nhttps://github.com/zaitra/methane-filters-benchmark.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u529f\u8017\u7684\u7532\u70f7\u6cc4\u6f0f\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdb\u73b0\u6709\u7b97\u6cd5\u548c\u7ed3\u5408\u673a\u5668\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u5728\u8d44\u6e90\u6709\u9650\u7684\u661f\u8f7d\u786c\u4ef6\u4e0a\u7684\u5feb\u901f\u68c0\u6d4b\u3002", "motivation": "\u7532\u70f7\u662f\u4e00\u79cd\u5f3a\u6548\u6e29\u5ba4\u6c14\u4f53\uff0c\u65e9\u671f\u68c0\u6d4b\u5176\u6cc4\u6f0f\u5bf9\u7f13\u89e3\u6c14\u5019\u53d8\u5316\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u8ba1\u7b97\u91cf\u5927\uff0c\u96be\u4ee5\u5728\u661f\u8f7d\u786c\u4ef6\u4e0a\u5b9e\u65f6\u8fd0\u884c\u3002", "method": "\u6d4b\u8bd5\u4e86\u5feb\u901f\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff08ACE\u3001CEM\uff09\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u7b97\u6cd5Mag1c-SAS\uff0c\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08U-Net\u3001LinkNet\uff09\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "Mag1c-SAS\u548cCEM\u5728\u68c0\u6d4b\u5f3a\u7fbd\u6d41\u65f6\u8868\u73b0\u826f\u597d\uff0c\u8ba1\u7b97\u6548\u7387\u5206\u522b\u6bd4\u539fMag1c\u5feb100\u500d\u548c230\u500d\u3002\u8fd8\u63d0\u51fa\u4e86\u4e09\u79cd\u6ce2\u6bb5\u9009\u62e9\u7b56\u7565\uff0c\u5176\u4e2d\u4e00\u79cd\u5728\u51cf\u5c11\u901a\u9053\u6570\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u661f\u8f7d\u7532\u70f7\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.01872", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01872", "abs": "https://arxiv.org/abs/2507.01872", "authors": ["Kenan Tang", "Yanhong Li", "Yao Qin"], "title": "DIY-MKG: An LLM-Based Polyglot Language Learning System", "comment": "Submitted to EMNLP 2025 System Demonstration", "summary": "Existing language learning tools, even those powered by Large Language Models\n(LLMs), often lack support for polyglot learners to build linguistic\nconnections across vocabularies in multiple languages, provide limited\ncustomization for individual learning paces or needs, and suffer from\ndetrimental cognitive offloading. To address these limitations, we design\nDo-It-Yourself Multilingual Knowledge Graph (DIY-MKG), an open-source system\nthat supports polyglot language learning. DIY-MKG allows the user to build\npersonalized vocabulary knowledge graphs, which are constructed by selective\nexpansion with related words suggested by an LLM. The system further enhances\nlearning through rich annotation capabilities and an adaptive review module\nthat leverages LLMs for dynamic, personalized quiz generation. In addition,\nDIY-MKG allows users to flag incorrect quiz questions, simultaneously\nincreasing user engagement and providing a feedback loop for prompt refinement.\nOur evaluation of LLM-based components in DIY-MKG shows that vocabulary\nexpansion is reliable and fair across multiple languages, and that the\ngenerated quizzes are highly accurate, validating the robustness of DIY-MKG.", "AI": {"tldr": "DIY-MKG\u662f\u4e00\u4e2a\u5f00\u6e90\u7cfb\u7edf\uff0c\u652f\u6301\u591a\u8bed\u8a00\u5b66\u4e60\uff0c\u901a\u8fc7\u4e2a\u6027\u5316\u8bcd\u6c47\u77e5\u8bc6\u56fe\u8c31\u548c\u52a8\u6001\u6d4b\u9a8c\u751f\u6210\u89e3\u51b3\u73b0\u6709\u5de5\u5177\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u5b66\u4e60\u5de5\u5177\u5728\u591a\u8bed\u8a00\u8bcd\u6c47\u8fde\u63a5\u3001\u4e2a\u6027\u5316\u5b66\u4e60\u548c\u8ba4\u77e5\u5378\u8f7d\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u8bbe\u8ba1DIY-MKG\u7cfb\u7edf\uff0c\u5229\u7528LLM\u6784\u5efa\u4e2a\u6027\u5316\u8bcd\u6c47\u77e5\u8bc6\u56fe\u8c31\uff0c\u652f\u6301\u52a8\u6001\u6d4b\u9a8c\u751f\u6210\u548c\u7528\u6237\u53cd\u9988\u3002", "result": "\u8bc4\u4f30\u663e\u793aDIY-MKG\u5728\u591a\u8bed\u8a00\u8bcd\u6c47\u6269\u5c55\u548c\u6d4b\u9a8c\u751f\u6210\u65b9\u9762\u8868\u73b0\u53ef\u9760\u4e14\u51c6\u786e\u3002", "conclusion": "DIY-MKG\u662f\u4e00\u4e2a\u6709\u6548\u7684\u591a\u8bed\u8a00\u5b66\u4e60\u5de5\u5177\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5de5\u5177\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.01117", "categories": ["cs.LG", "68T07, 35A99"], "pdf": "https://arxiv.org/pdf/2507.01117", "abs": "https://arxiv.org/abs/2507.01117", "authors": ["Nikita Sakovich", "Dmitry Aksenov", "Ekaterina Pleshakova", "Sergey Gataullin"], "title": "A Neural Operator based on Dynamic Mode Decomposition", "comment": "30 pages, 10 figures", "summary": "The scientific computation methods development in conjunction with artificial\nintelligence technologies remains a hot research topic. Finding a balance\nbetween lightweight and accurate computations is a solid foundation for this\ndirection. The study presents a neural operator based on the dynamic mode\ndecomposition algorithm (DMD), mapping functional spaces, which combines DMD\nand deep learning (DL) for spatiotemporal processes efficient modeling. Solving\nPDEs for various initial and boundary conditions requires significant\ncomputational resources. The method suggested automatically extracts key modes\nand system dynamics using them to construct predictions, reducing computational\ncosts compared to traditional numerical methods. The approach has demonstrated\nits efficiency through comparative analysis of performance with closest\nanalogues DeepONet and FNO in the heat equation, Laplaces equation, and Burgers\nequation solutions approximation, where it achieves high reconstruction\naccuracy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u6a21\u5f0f\u5206\u89e3\uff08DMD\uff09\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u795e\u7ecf\u7b97\u5b50\uff0c\u7528\u4e8e\u9ad8\u6548\u5efa\u6a21\u65f6\u7a7a\u8fc7\u7a0b\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u79d1\u5b66\u8ba1\u7b97\u4e0e\u4eba\u5de5\u667a\u80fd\u7ed3\u5408\u662f\u70ed\u95e8\u7814\u7a76\u65b9\u5411\uff0c\u4f46\u5982\u4f55\u5728\u8f7b\u91cf\u5316\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u662f\u5173\u952e\u3002", "method": "\u7ed3\u5408DMD\u548c\u6df1\u5ea6\u5b66\u4e60\uff0c\u81ea\u52a8\u63d0\u53d6\u5173\u952e\u6a21\u5f0f\u4e0e\u7cfb\u7edf\u52a8\u6001\uff0c\u7528\u4e8e\u6784\u5efa\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u5728\u70ed\u65b9\u7a0b\u3001\u62c9\u666e\u62c9\u65af\u65b9\u7a0b\u548cBurgers\u65b9\u7a0b\u7684\u8fd1\u4f3c\u89e3\u4e2d\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u51fa\u9ad8\u91cd\u5efa\u7cbe\u5ea6\uff0c\u8ba1\u7b97\u6210\u672c\u4f4e\u4e8e\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u6548\u5efa\u6a21\u65f6\u7a7a\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5c24\u5176\u5728\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2507.01478", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01478", "abs": "https://arxiv.org/abs/2507.01478", "authors": ["Chentao Shen", "Ding Pan", "Mingyu Mei", "Zaixing He", "Xinyue Zhao"], "title": "Active Control Points-based 6DoF Pose Tracking for Industrial Metal Objects", "comment": "preprint version", "summary": "Visual pose tracking is playing an increasingly vital role in industrial\ncontexts in recent years. However, the pose tracking for industrial metal\nobjects remains a challenging task especially in the real world-environments,\ndue to the reflection characteristic of metal objects. To address this issue,\nwe propose a novel 6DoF pose tracking method based on active control points.\nThe method uses image control points to generate edge feature for optimization\nactively instead of 6DoF pose-based rendering, and serve them as optimization\nvariables. We also introduce an optimal control point regression method to\nimprove robustness. The proposed tracking method performs effectively in both\ndataset evaluation and real world tasks, providing a viable solution for\nreal-time tracking of industrial metal objects. Our source code is made\npublicly available at: https://github.com/tomatoma00/ACPTracking.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u52a8\u63a7\u5236\u70b9\u76846DoF\u59ff\u6001\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u91d1\u5c5e\u7269\u4f53\u53cd\u5c04\u7279\u6027\u5e26\u6765\u7684\u8ddf\u8e2a\u96be\u9898\u3002", "motivation": "\u5de5\u4e1a\u91d1\u5c5e\u7269\u4f53\u7684\u59ff\u6001\u8ddf\u8e2a\u56e0\u53cd\u5c04\u7279\u6027\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5229\u7528\u56fe\u50cf\u63a7\u5236\u70b9\u4e3b\u52a8\u751f\u6210\u8fb9\u7f18\u7279\u5f81\u8fdb\u884c\u4f18\u5316\uff0c\u5e76\u5f15\u5165\u6700\u4f18\u63a7\u5236\u70b9\u56de\u5f52\u65b9\u6cd5\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "result": "\u5728\u6570\u636e\u96c6\u8bc4\u4f30\u548c\u5b9e\u9645\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u6548\uff0c\u652f\u6301\u5b9e\u65f6\u8ddf\u8e2a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5de5\u4e1a\u91d1\u5c5e\u7269\u4f53\u7684\u5b9e\u65f6\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01887", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01887", "abs": "https://arxiv.org/abs/2507.01887", "authors": ["Dongyi Ding", "Tiannan Wang", "Chenghao Zhu", "Meiling Tao", "Yuchen Eleanor Jiang", "Wangchunshu Zhou"], "title": "MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher Assistants", "comment": "Work in progress", "summary": "Large language models (LLMs) excel at reasoning tasks requiring long thought\nsequences for planning, reflection, and refinement. However, their substantial\nmodel size and high computational demands are impractical for widespread\ndeployment. Yet, small language models (SLMs) often struggle to learn long-form\nCoT reasoning due to their limited capacity, a phenomenon we refer to as the\n\"SLMs Learnability Gap\". To address this, we introduce\n\\textbf{Mi}d-\\textbf{Co}T \\textbf{T}eacher \\textbf{A}ssistant Distillation\n(MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA\nemploys intermediate-sized models as teacher assistants and utilizes\nintermediate-length CoT sequences to bridge both the capacity and reasoning\nlength gaps. Our experiments on downstream tasks demonstrate that although SLMs\ndistilled from large teachers can perform poorly, by applying MiCoTA, they\nachieve significant improvements in reasoning performance. Specifically,\nQwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and\n3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and\nGSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform\na quantitative experiment demonstrating that our method produces data more\nclosely aligned with base SLM distributions. Our insights pave the way for\nfuture research into long-CoT data distillation for SLMs.", "AI": {"tldr": "MiCoTA\u6846\u67b6\u901a\u8fc7\u4e2d\u95f4\u89c4\u6a21\u6a21\u578b\u548c\u4e2d\u95f4\u957f\u5ea6\u63a8\u7406\u5e8f\u5217\uff0c\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\u7684\u957f\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u5c0f\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u5728\u957f\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5b66\u4e60\u80fd\u529b\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u4e2d\u95f4\u89c4\u6a21\u6a21\u578b\u4f5c\u4e3a\u6559\u5e08\u52a9\u624b\uff0c\u5229\u7528\u4e2d\u95f4\u957f\u5ea6\u63a8\u7406\u5e8f\u5217\u586b\u8865\u80fd\u529b\u4e0e\u63a8\u7406\u957f\u5ea6\u5dee\u8ddd\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cMiCoTA\u663e\u8457\u63d0\u5347SLMs\u7684\u63a8\u7406\u6027\u80fd\uff0c\u5982Qwen2.5-7B\u548c3B\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u5f97\u5206\u63d0\u9ad83.47\u548c3.93\u3002", "conclusion": "MiCoTA\u4e3aSLMs\u7684\u957f\u63a8\u7406\u6570\u636e\u84b8\u998f\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u672a\u6765\u7814\u7a76\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002"}}
{"id": "2507.01313", "categories": ["cs.LG", "cs.AI", "math.DS", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.01313", "abs": "https://arxiv.org/abs/2507.01313", "authors": ["Qian Qi"], "title": "Neural Hamiltonian Operator", "comment": null, "summary": "Stochastic control problems in high dimensions are notoriously difficult to\nsolve due to the curse of dimensionality. An alternative to traditional dynamic\nprogramming is Pontryagin's Maximum Principle (PMP), which recasts the problem\nas a system of Forward-Backward Stochastic Differential Equations (FBSDEs). In\nthis paper, we introduce a formal framework for solving such problems with deep\nlearning by defining a \\textbf{Neural Hamiltonian Operator (NHO)}. This\noperator parameterizes the coupled FBSDE dynamics via neural networks that\nrepresent the feedback control and an ansatz for the value function's spatial\ngradient. We show how the optimal NHO can be found by training the underlying\nnetworks to enforce the consistency conditions dictated by the PMP. By adopting\nthis operator-theoretic view, we situate the deep FBSDE method within the\nrigorous language of statistical inference, framing it as a problem of learning\nan unknown operator from simulated data. This perspective allows us to prove\nthe universal approximation capabilities of NHOs under general martingale\ndrivers and provides a clear lens for analyzing the significant optimization\nchallenges inherent to this class of models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u795e\u7ecf\u54c8\u5bc6\u987f\u7b97\u5b50\uff08NHO\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u9ad8\u7ef4\u968f\u673a\u63a7\u5236\u95ee\u9898\uff0c\u66ff\u4ee3\u4f20\u7edf\u52a8\u6001\u89c4\u5212\u65b9\u6cd5\u3002", "motivation": "\u9ad8\u7ef4\u968f\u673a\u63a7\u5236\u95ee\u9898\u56e0\u7ef4\u5ea6\u707e\u96be\u96be\u4ee5\u89e3\u51b3\uff0c\u4f20\u7edf\u52a8\u6001\u7f16\u7a0b\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49\u795e\u7ecf\u54c8\u5bc6\u987f\u7b97\u5b50\uff08NHO\uff09\uff0c\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u524d\u5411-\u540e\u5411\u968f\u673a\u5fae\u5206\u65b9\u7a0b\uff08FBSDEs\uff09\uff0c\u5e76\u7528\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u5316\u53cd\u9988\u63a7\u5236\u548c\u503c\u51fd\u6570\u68af\u5ea6\u3002", "result": "\u8bc1\u660e\u4e86NHO\u5728\u4e00\u822c\u9785\u9a71\u52a8\u4e0b\u7684\u901a\u7528\u903c\u8fd1\u80fd\u529b\uff0c\u5e76\u5206\u6790\u4e86\u4f18\u5316\u6311\u6218\u3002", "conclusion": "NHO\u4e3a\u9ad8\u7ef4\u968f\u673a\u63a7\u5236\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u8df5\u6f5c\u529b\u3002"}}
{"id": "2507.01129", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.01129", "abs": "https://arxiv.org/abs/2507.01129", "authors": ["Arun Ganesh", "Brendan McMahan", "Abhradeep Thakurta"], "title": "On Design Principles for Private Adaptive Optimizers", "comment": "PPML 2025", "summary": "The spherical noise added to gradients in differentially private (DP)\ntraining undermines the performance of adaptive optimizers like AdaGrad and\nAdam, and hence many recent works have proposed algorithms to address this\nchallenge. However, the empirical results in these works focus on simple tasks\nand models and the conclusions may not generalize to model training in\npractice. In this paper we survey several of these variants, and develop better\ntheoretical intuition for them as well as perform empirical studies comparing\nthem. We find that a common intuition of aiming for unbiased estimates of\nsecond moments of gradients in adaptive optimizers is misguided, and instead\nthat a simple technique called scale-then-privatize (which does not achieve\nunbiased second moments) has more desirable theoretical behaviors and\noutperforms all other variants we study on a small-scale language model\ntraining task. We additionally argue that scale-then-privatize causes the noise\naddition to better match the application of correlated noise mechanisms which\nare more desirable to use in practice.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5dee\u5206\u9690\u79c1\u8bad\u7ec3\u4e2d\u7403\u5f62\u566a\u58f0\u5bf9\u81ea\u9002\u5e94\u4f18\u5316\u5668\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5728\u7b80\u5355\u4efb\u52a1\u4e0a\u7684\u7ed3\u8bba\u53ef\u80fd\u4e0d\u9002\u7528\u4e8e\u5b9e\u9645\u6a21\u578b\u8bad\u7ec3\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u7814\u7a76\uff0c\u53d1\u73b0\u201cscale-then-privatize\u201d\u65b9\u6cd5\u4f18\u4e8e\u5176\u4ed6\u53d8\u4f53\u3002", "motivation": "\u5dee\u5206\u9690\u79c1\u8bad\u7ec3\u4e2d\u7684\u7403\u5f62\u566a\u58f0\u4f1a\u964d\u4f4e\u81ea\u9002\u5e94\u4f18\u5316\u5668\uff08\u5982AdaGrad\u548cAdam\uff09\u7684\u6027\u80fd\uff0c\u73b0\u6709\u7814\u7a76\u591a\u57fa\u4e8e\u7b80\u5355\u4efb\u52a1\uff0c\u7ed3\u8bba\u53ef\u80fd\u4e0d\u5177\u666e\u9002\u6027\u3002", "method": "\u8c03\u67e5\u4e86\u591a\u79cd\u6539\u8fdb\u65b9\u6cd5\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u7814\u7a76\u6bd4\u8f83\u5176\u6027\u80fd\uff0c\u91cd\u70b9\u5173\u6ce8\u201cscale-then-privatize\u201d\u6280\u672f\u3002", "result": "\u53d1\u73b0\u8ffd\u6c42\u68af\u5ea6\u4e8c\u9636\u77e9\u65e0\u504f\u4f30\u8ba1\u7684\u76f4\u89c9\u662f\u9519\u8bef\u7684\uff0c\u201cscale-then-privatize\u201d\u65b9\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u201cscale-then-privatize\u201d\u65b9\u6cd5\u4e0d\u4ec5\u7406\u8bba\u884c\u4e3a\u66f4\u4f18\uff0c\u4e14\u5728\u5c0f\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u66f4\u9002\u5408\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.01484", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01484", "abs": "https://arxiv.org/abs/2507.01484", "authors": ["Xiaoshuai Hao", "Yuting Zhao", "Yuheng Ji", "Luanyuan Dai", "Peng Hao", "Dingzhe Li", "Shuai Cheng", "Rong Yin"], "title": "What Really Matters for Robust Multi-Sensor HD Map Construction?", "comment": "Accepted by IROS 2025", "summary": "High-definition (HD) map construction methods are crucial for providing\nprecise and comprehensive static environmental information, which is essential\nfor autonomous driving systems. While Camera-LiDAR fusion techniques have shown\npromising results by integrating data from both modalities, existing approaches\nprimarily focus on improving model accuracy and often neglect the robustness of\nperception models, which is a critical aspect for real-world applications. In\nthis paper, we explore strategies to enhance the robustness of multi-modal\nfusion methods for HD map construction while maintaining high accuracy. We\npropose three key components: data augmentation, a novel multi-modal fusion\nmodule, and a modality dropout training strategy. These components are\nevaluated on a challenging dataset containing 10 days of NuScenes data. Our\nexperimental results demonstrate that our proposed methods significantly\nenhance the robustness of baseline methods. Furthermore, our approach achieves\nstate-of-the-art performance on the clean validation set of the NuScenes\ndataset. Our findings provide valuable insights for developing more robust and\nreliable HD map construction models, advancing their applicability in\nreal-world autonomous driving scenarios. Project website:\nhttps://robomap-123.github.io.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\u9c81\u68d2\u6027\u7684\u7b56\u7565\uff0c\u7528\u4e8e\u9ad8\u7cbe\u5730\u56fe\u6784\u5efa\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u3001\u65b0\u578b\u591a\u6a21\u6001\u878d\u5408\u6a21\u5757\u548c\u6a21\u6001\u4e22\u5f03\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u7ebf\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728NuScenes\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6a21\u578b\u7cbe\u5ea6\uff0c\u5ffd\u89c6\u4e86\u611f\u77e5\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u800c\u9c81\u68d2\u6027\u5bf9\u5b9e\u9645\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u6570\u636e\u589e\u5f3a\u3001\u65b0\u578b\u591a\u6a21\u6001\u878d\u5408\u6a21\u5757\u548c\u6a21\u6001\u4e22\u5f03\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5728NuScenes\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u57fa\u7ebf\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u5e72\u51c0\u9a8c\u8bc1\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u53ef\u9760\u7684\u9ad8\u7cbe\u5730\u56fe\u6784\u5efa\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u63a8\u52a8\u4e86\u5176\u5728\u73b0\u5b9e\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2507.01900", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01900", "abs": "https://arxiv.org/abs/2507.01900", "authors": ["Songtao Liu", "Peng Liu"], "title": "High-Layer Attention Pruning with Rescaling", "comment": null, "summary": "Pruning is a highly effective approach for compressing large language models\n(LLMs), significantly reducing inference latency. However, conventional\ntraining-free structured pruning methods often employ a heuristic metric that\nindiscriminately removes some attention heads across all pruning layers,\nwithout considering their positions within the network architecture. In this\nwork, we propose a novel pruning algorithm that strategically prunes attention\nheads in the model's higher layers. Since the removal of attention heads can\nalter the magnitude of token representations, we introduce an adaptive\nrescaling parameter that calibrates the representation scale post-pruning to\ncounteract this effect. We conduct comprehensive experiments on a wide range of\nLLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our\nevaluation includes both generation and discriminative tasks across 27\ndatasets. The results consistently demonstrate that our method outperforms\nexisting structured pruning methods. This improvement is particularly notable\nin generation tasks, where our approach significantly outperforms existing\nbaselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u526a\u679d\u7b97\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u9ad8\u5c42\u6ce8\u610f\u529b\u5934\u7684\u526a\u679d\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u91cd\u7f29\u653e\u53c2\u6570\u4ee5\u6821\u51c6\u526a\u679d\u540e\u7684\u8868\u793a\u89c4\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u526a\u679d\u65b9\u6cd5\u4e0d\u8003\u8651\u6ce8\u610f\u529b\u5934\u5728\u7f51\u7edc\u4e2d\u7684\u4f4d\u7f6e\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u667a\u80fd\u7684\u526a\u679d\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u526a\u679d\u7b97\u6cd5\uff0c\u9488\u5bf9\u9ad8\u5c42\u6ce8\u610f\u529b\u5934\u8fdb\u884c\u526a\u679d\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u91cd\u7f29\u653e\u53c2\u6570\u4ee5\u6821\u51c6\u8868\u793a\u89c4\u6a21\u3002", "result": "\u5728\u591a\u4e2aLLM\u6a21\u578b\u548c27\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u667a\u80fd\u526a\u679d\u548c\u91cd\u7f29\u653e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2507.01321", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.01321", "abs": "https://arxiv.org/abs/2507.01321", "authors": ["Zhiyao Ren", "Siyuan Liang", "Aishan Liu", "Dacheng Tao"], "title": "ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks", "comment": "ICML 2025", "summary": "In-context learning (ICL) has demonstrated remarkable success in large\nlanguage models (LLMs) due to its adaptability and parameter-free nature.\nHowever, it also introduces a critical vulnerability to backdoor attacks, where\nadversaries can manipulate LLM behaviors by simply poisoning a few ICL\ndemonstrations. In this paper, we propose, for the first time, the\ndual-learning hypothesis, which posits that LLMs simultaneously learn both the\ntask-relevant latent concepts and backdoor latent concepts within poisoned\ndemonstrations, jointly influencing the probability of model outputs. Through\ntheoretical analysis, we derive an upper bound for ICL backdoor effects,\nrevealing that the vulnerability is dominated by the concept preference ratio\nbetween the task and the backdoor. Motivated by these findings, we propose\nICLShield, a defense mechanism that dynamically adjusts the concept preference\nratio. Our method encourages LLMs to select clean demonstrations during the ICL\nphase by leveraging confidence and similarity scores, effectively mitigating\nsusceptibility to backdoor attacks. Extensive experiments across multiple LLMs\nand tasks demonstrate that our method achieves state-of-the-art defense\neffectiveness, significantly outperforming existing approaches (+26.02% on\naverage). Furthermore, our method exhibits exceptional adaptability and\ndefensive performance even for closed-source models (e.g., GPT-4).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u53cc\u5b66\u4e60\u5047\u8bf4\uff0c\u63ed\u793aLLMs\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u4e2d\u540c\u65f6\u5b66\u4e60\u4efb\u52a1\u76f8\u5173\u548c\u6f5c\u5728\u540e\u95e8\u6982\u5ff5\uff0c\u5e76\u63d0\u51faICLShield\u9632\u5fa1\u673a\u5236\uff0c\u52a8\u6001\u8c03\u6574\u6982\u5ff5\u504f\u597d\u6bd4\u4f8b\uff0c\u663e\u8457\u63d0\u5347\u9632\u5fa1\u6548\u679c\u3002", "motivation": "ICL\u56e0\u5176\u9002\u5e94\u6027\u548c\u65e0\u53c2\u6570\u7279\u6027\u5728LLMs\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4e5f\u6613\u53d7\u540e\u95e8\u653b\u51fb\uff0c\u9700\u7814\u7a76\u9632\u5fa1\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u53cc\u5b66\u4e60\u5047\u8bf4\uff0c\u7406\u8bba\u5206\u6790\u540e\u95e8\u6548\u5e94\u4e0a\u9650\uff0c\u8bbe\u8ba1ICLShield\u673a\u5236\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u548c\u76f8\u4f3c\u5ea6\u52a8\u6001\u9009\u62e9\u5e72\u51c0\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660eICLShield\u5728\u591a\u79cdLLMs\u548c\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u5e73\u5747\u63d0\u534726.02%\uff0c\u5bf9\u95ed\u6e90\u6a21\u578b\uff08\u5982GPT-4\uff09\u4e5f\u6709\u6548\u3002", "conclusion": "\u53cc\u5b66\u4e60\u5047\u8bf4\u548cICLShield\u4e3aICL\u540e\u95e8\u653b\u51fb\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u548c\u5b9e\u7528\u9632\u5fa1\u65b9\u6848\u3002"}}
{"id": "2507.01131", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2507.01131", "abs": "https://arxiv.org/abs/2507.01131", "authors": ["Yuchao Lin", "Cong Fu", "Zachary Krueger", "Haiyang Yu", "Maho Nakata", "Jianwen Xie", "Emine Kucukbenli", "Xiaofeng Qian", "Shuiwang Ji"], "title": "Tensor Decomposition Networks for Fast Machine Learning Interatomic Potential Computations", "comment": null, "summary": "$\\rm{SO}(3)$-equivariant networks are the dominant models for machine\nlearning interatomic potentials (MLIPs). The key operation of such networks is\nthe Clebsch-Gordan (CG) tensor product, which is computationally expensive. To\naccelerate the computation, we develop tensor decomposition networks (TDNs) as\na class of approximately equivariant networks whose CG tensor products are\nreplaced by low-rank tensor decompositions, such as the CANDECOMP/PARAFAC (CP)\ndecomposition. With the CP decomposition, we prove (i) a uniform bound on the\ninduced error of $\\rm{SO}(3)$-equivariance, and (ii) the universality of\napproximating any equivariant bilinear map. To further reduce the number of\nparameters, we propose path-weight sharing that ties all multiplicity-space\nweights across the $O(L^3)$ CG paths into a single path without compromising\nequivariance, where $L$ is the maximum angular degree. The resulting layer acts\nas a plug-and-play replacement for tensor products in existing networks, and\nthe computational complexity of tensor products is reduced from $O(L^6)$ to\n$O(L^4)$. We evaluate TDNs on PubChemQCR, a newly curated molecular relaxation\ndataset containing 105 million DFT-calculated snapshots. We also use existing\ndatasets, including OC20, and OC22. Results show that TDNs achieve competitive\nperformance with dramatic speedup in computations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8fd1\u4f3c\u7b49\u53d8\u7684\u7f51\u7edc\uff08TDNs\uff09\uff0c\u901a\u8fc7\u4f4e\u79e9\u5f20\u91cf\u5206\u89e3\uff08\u5982CP\u5206\u89e3\uff09\u66ff\u4ee3\u8ba1\u7b97\u6602\u8d35\u7684Clebsch-Gordan\u5f20\u91cf\u79ef\uff0c\u663e\u8457\u52a0\u901f\u8ba1\u7b97\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684SO(3)-\u7b49\u53d8\u7f51\u7edc\u5728\u673a\u5668\u5b66\u4e60\u539f\u5b50\u95f4\u52bf\u80fd\uff08MLIPs\uff09\u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u4f46\u5176\u5173\u952e\u64cd\u4f5cClebsch-Gordan\u5f20\u91cf\u79ef\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e9f\u9700\u4f18\u5316\u3002", "method": "\u63d0\u51fa\u5f20\u91cf\u5206\u89e3\u7f51\u7edc\uff08TDNs\uff09\uff0c\u7528\u4f4e\u79e9\u5f20\u91cf\u5206\u89e3\uff08\u5982CP\u5206\u89e3\uff09\u66ff\u4ee3Clebsch-Gordan\u5f20\u91cf\u79ef\uff0c\u5e76\u5f15\u5165\u8def\u5f84\u6743\u91cd\u5171\u4eab\u8fdb\u4e00\u6b65\u51cf\u5c11\u53c2\u6570\u3002", "result": "\u5728PubChemQCR\u3001OC20\u548cOC22\u6570\u636e\u96c6\u4e0a\uff0cTDNs\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u52a0\u901f\u8ba1\u7b97\uff0c\u590d\u6742\u5ea6\u4eceO(L^6)\u964d\u81f3O(L^4)\u3002", "conclusion": "TDNs\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u5f02\u7684SO(3)-\u7b49\u53d8\u7f51\u7edc\u66ff\u4ee3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5206\u5b50\u6a21\u62df\u4efb\u52a1\u3002"}}
{"id": "2507.01492", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01492", "abs": "https://arxiv.org/abs/2507.01492", "authors": ["Jiyang Tang", "Hengyi Li", "Yifan Du", "Wayne Xin Zhao"], "title": "AVC-DPO: Aligned Video Captioning via Direct Preference Optimization", "comment": null, "summary": "Although video multimodal large language models (video MLLMs) have achieved\nsubstantial progress in video captioning tasks, it remains challenging to\nadjust the focal emphasis of video captions according to human preferences. To\naddress this limitation, we propose Aligned Video Captioning via Direct\nPreference Optimization (AVC-DPO), a post-training framework designed to\nenhance captioning capabilities in video MLLMs through preference alignment.\nOur approach designs enhanced prompts that specifically target temporal\ndynamics and spatial information-two key factors that humans care about when\nwatching a video-thereby incorporating human-centric preferences. AVC-DPO\nleverages the same foundation model's caption generation responses under varied\nprompt conditions to conduct preference-aware training and caption alignment.\nUsing this framework, we have achieved exceptional performance in the\nLOVE@CVPR'25 Workshop Track 1A: Video Detailed Captioning Challenge, achieving\nfirst place on the Video Detailed Captioning (VDC) benchmark according to the\nVDCSCORE evaluation metric.", "AI": {"tldr": "AVC-DPO\u662f\u4e00\u79cd\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u504f\u597d\u5bf9\u9f50\u589e\u5f3a\u89c6\u9891MLLMs\u7684\u6807\u9898\u751f\u6210\u80fd\u529b\uff0c\u4e13\u6ce8\u4e8e\u65f6\u95f4\u548c\u7a7a\u95f4\u4fe1\u606f\uff0c\u5e76\u5728VDC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u4f73\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u89c6\u9891MLLMs\u5728\u6807\u9898\u751f\u6210\u4efb\u52a1\u4e2d\u96be\u4ee5\u6839\u636e\u4eba\u7c7b\u504f\u597d\u8c03\u6574\u7126\u70b9\uff0c\u56e0\u6b64\u63d0\u51faAVC-DPO\u4ee5\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u8bbe\u8ba1\u589e\u5f3a\u63d0\u793a\uff0c\u9488\u5bf9\u65f6\u95f4\u548c\u7a7a\u95f4\u4fe1\u606f\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u5728\u4e0d\u540c\u63d0\u793a\u6761\u4ef6\u4e0b\u7684\u54cd\u5e94\u8fdb\u884c\u504f\u597d\u611f\u77e5\u8bad\u7ec3\u548c\u5bf9\u9f50\u3002", "result": "\u5728LOVE@CVPR'25 Workshop Track 1A\u4e2d\u53d6\u5f97\u7b2c\u4e00\u540d\uff0cVDCSCORE\u8bc4\u4f30\u6307\u6807\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "AVC-DPO\u901a\u8fc7\u504f\u597d\u5bf9\u9f50\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891MLLMs\u7684\u6807\u9898\u751f\u6210\u80fd\u529b\uff0c\u6ee1\u8db3\u4eba\u7c7b\u504f\u597d\u9700\u6c42\u3002"}}
{"id": "2507.01903", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01903", "abs": "https://arxiv.org/abs/2507.01903", "authors": ["Qiguang Chen", "Mingda Yang", "Libo Qin", "Jinhao Liu", "Zheng Yan", "Jiannan Guan", "Dengyun Peng", "Yiyan Ji", "Hanjing Li", "Mengkang Hu", "Yimeng Zhang", "Yihao Liang", "Yuhang Zhou", "Jiaqi Wang", "Zhi Chen", "Wanxiang Che"], "title": "AI4Research: A Survey of Artificial Intelligence for Scientific Research", "comment": "Preprint", "summary": "Recent advancements in artificial intelligence (AI), particularly in large\nlanguage models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated\nremarkable capabilities in complex domains such as logical reasoning and\nexperimental coding. Motivated by these advancements, numerous studies have\nexplored the application of AI in the innovation process, particularly in the\ncontext of scientific research. These AI technologies primarily aim to develop\nsystems that can autonomously conduct research processes across a wide range of\nscientific disciplines. Despite these significant strides, a comprehensive\nsurvey on AI for Research (AI4Research) remains absent, which hampers our\nunderstanding and impedes further development in this field. To address this\ngap, we present a comprehensive survey and offer a unified perspective on\nAI4Research. Specifically, the main contributions of our work are as follows:\n(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify\nfive mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key\nresearch gaps and highlight promising future directions, focusing on the rigor\nand scalability of automated experiments, as well as the societal impact. (3)\nAbundant applications and resources: Finally, we compile a wealth of resources,\nincluding relevant multidisciplinary applications, data corpora, and tools. We\nhope our work will provide the research community with quick access to these\nresources and stimulate innovative breakthroughs in AI4Research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u9879\u5173\u4e8eAI4Research\u7684\u5168\u9762\u8c03\u67e5\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u7a7a\u767d\uff0c\u5e76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5206\u7c7b\u3001\u65b0\u7814\u7a76\u65b9\u5411\u548c\u4e30\u5bcc\u8d44\u6e90\u3002", "motivation": "\u8fd1\u5e74\u6765AI\uff08\u5c24\u5176\u662fLLMs\uff09\u5728\u903b\u8f91\u63a8\u7406\u548c\u5b9e\u9a8c\u7f16\u7801\u7b49\u590d\u6742\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u5bf9AI\u5728\u79d1\u7814\u4e2d\u5e94\u7528\u7684\u5168\u9762\u8c03\u67e5\uff0c\u963b\u788d\u4e86\u8fdb\u4e00\u6b65\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5206\u7c7b\u6cd5\u5bf9AI4Research\u7684\u4e94\u7c7b\u4e3b\u6d41\u4efb\u52a1\u8fdb\u884c\u5206\u7c7b\uff0c\u8bc6\u522b\u7814\u7a76\u7a7a\u767d\u5e76\u63d0\u51fa\u672a\u6765\u65b9\u5411\uff0c\u540c\u65f6\u6574\u7406\u591a\u5b66\u79d1\u5e94\u7528\u548c\u8d44\u6e90\u3002", "result": "\u8bba\u6587\u63d0\u4f9b\u4e86AI4Research\u7684\u7cfb\u7edf\u5206\u7c7b\u3001\u5173\u952e\u7814\u7a76\u65b9\u5411\u548c\u4e30\u5bcc\u8d44\u6e90\uff0c\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u5feb\u901f\u8bbf\u95ee\u548c\u521b\u65b0\u7684\u57fa\u7840\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u586b\u8865\u4e86AI4Research\u9886\u57df\u7684\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u7edf\u4e00\u89c6\u89d2\u548c\u8d44\u6e90\u652f\u6301\uff0c\u6709\u671b\u63a8\u52a8\u8be5\u9886\u57df\u7684\u521b\u65b0\u7a81\u7834\u3002"}}
{"id": "2507.01327", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01327", "abs": "https://arxiv.org/abs/2507.01327", "authors": ["Xiaoyun Zhang", "Jingqing Ruan", "Xing Ma", "Yawen Zhu", "Jiansong Chen", "Ke Zeng", "Xunliang Cai"], "title": "Reasoner for Real-World Event Detection: Scaling Reinforcement Learning via Adaptive Perplexity-Aware Sampling Strategy", "comment": "15 pages, 6 figures, submitted to EMNLP", "summary": "Detecting abnormal events in real-world customer service dialogues is highly\nchallenging due to the complexity of business data and the dynamic nature of\ncustomer interactions. Moreover, models must demonstrate strong out-of-domain\n(OOD) generalization to enable rapid adaptation across different business\nscenarios and maximize commercial value. In this work, we propose a novel\nAdaptive Perplexity-Aware Reinforcement Learning (APARL) framework that\nleverages the advanced reasoning capabilities of large language models for\nabnormal event detection. APARL introduces a dual-loop dynamic curriculum\nlearning architecture, enabling the model to progressively focus on more\nchallenging samples as its proficiency increases. This design effectively\naddresses performance bottlenecks and significantly enhances OOD\ntransferability. Extensive evaluations on food delivery dialogue tasks show\nthat our model achieves significantly enhanced adaptability and robustness,\nattaining the highest F1 score with an average improvement of 17.19\\%, and an\naverage improvement of 9.59\\% in OOD transfer tests. This method provides a\nsuperior solution for industrial deployment of anomaly detection models,\ncontributing to improved operational efficiency and commercial benefits.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u9002\u5e94\u56f0\u60d1\u5ea6\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\uff08APARL\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u5ba2\u6237\u670d\u52a1\u5bf9\u8bdd\u4e2d\u7684\u5f02\u5e38\u4e8b\u4ef6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u7531\u4e8e\u5546\u4e1a\u6570\u636e\u7684\u590d\u6742\u6027\u548c\u5ba2\u6237\u4e92\u52a8\u7684\u52a8\u6001\u6027\uff0c\u68c0\u6d4b\u5ba2\u6237\u670d\u52a1\u5bf9\u8bdd\u4e2d\u7684\u5f02\u5e38\u4e8b\u4ef6\u6781\u5177\u6311\u6218\u6027\uff0c\u4e14\u6a21\u578b\u9700\u5177\u5907\u5f3a\u5927\u7684\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002", "method": "APARL\u6846\u67b6\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u91c7\u7528\u53cc\u73af\u52a8\u6001\u8bfe\u7a0b\u5b66\u4e60\u67b6\u6784\uff0c\u9010\u6b65\u805a\u7126\u66f4\u5177\u6311\u6218\u6027\u7684\u6837\u672c\u3002", "result": "\u5728\u98df\u54c1\u914d\u9001\u5bf9\u8bdd\u4efb\u52a1\u4e2d\uff0cAPARL\u7684F1\u5206\u6570\u5e73\u5747\u63d0\u534717.19%\uff0c\u8de8\u9886\u57df\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u53479.59%\u3002", "conclusion": "APARL\u4e3a\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u7684\u5de5\u4e1a\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u5347\u4e86\u8fd0\u8425\u6548\u7387\u548c\u5546\u4e1a\u4ef7\u503c\u3002"}}
{"id": "2507.01132", "categories": ["cs.LG", "q-bio.MN"], "pdf": "https://arxiv.org/pdf/2507.01132", "abs": "https://arxiv.org/abs/2507.01132", "authors": ["Brenda Nogueira", "Gabe Gomes", "Meng Jiang", "Nitesh V. Chawla", "Nuno Moniz"], "title": "Spectral Manifold Harmonization for Graph Imbalanced Regression", "comment": null, "summary": "Graph-structured data is ubiquitous in scientific domains, where models often\nface imbalanced learning settings. In imbalanced regression, domain preferences\nfocus on specific target value ranges representing the most scientifically\nvaluable cases; we observe a significant lack of research. In this paper, we\npresent Spectral Manifold Harmonization (SMH), a novel approach for addressing\nthis imbalanced regression challenge on graph-structured data by generating\nsynthetic graph samples that preserve topological properties while focusing on\noften underrepresented target distribution regions. Conventional methods fail\nin this context because they either ignore graph topology in case generation or\ndo not target specific domain ranges, resulting in models biased toward average\ntarget values. Experimental results demonstrate the potential of SMH on\nchemistry and drug discovery benchmark datasets, showing consistent\nimprovements in predictive performance for target domain ranges.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSpectral Manifold Harmonization\uff08SMH\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u56fe\u7ed3\u6784\u6570\u636e\u4e2d\u7684\u4e0d\u5e73\u8861\u56de\u5f52\u95ee\u9898\uff0c\u901a\u8fc7\u751f\u6210\u5408\u6210\u6837\u672c\u4ee5\u5173\u6ce8\u76ee\u6807\u5206\u5e03\u4e2d underrepresented \u7684\u533a\u57df\u3002", "motivation": "\u56fe\u7ed3\u6784\u6570\u636e\u5728\u79d1\u5b66\u9886\u57df\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5728\u5904\u7406\u4e0d\u5e73\u8861\u56de\u5f52\u95ee\u9898\u65f6\u5f80\u5f80\u5ffd\u7565\u7279\u5b9a\u76ee\u6807\u503c\u8303\u56f4\u7684\u91cd\u8981\u6027\uff0c\u5bfc\u81f4\u6a21\u578b\u504f\u5411\u4e8e\u5e73\u5747\u76ee\u6807\u503c\u3002", "method": "SMH\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u5408\u6210\u56fe\u6837\u672c\uff0c\u65e2\u4fdd\u7559\u4e86\u56fe\u7684\u62d3\u6251\u6027\u8d28\uff0c\u53c8\u4e13\u6ce8\u4e8e\u76ee\u6807\u5206\u5e03\u4e2d underrepresented \u7684\u533a\u57df\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSMH\u5728\u5316\u5b66\u548c\u836f\u7269\u53d1\u73b0\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u76ee\u6807\u57df\u8303\u56f4\u7684\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "SMH\u4e3a\u89e3\u51b3\u56fe\u7ed3\u6784\u6570\u636e\u4e2d\u7684\u4e0d\u5e73\u8861\u56de\u5f52\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u79d1\u5b66\u9886\u57df\u4e2d\u7684\u5173\u952e\u76ee\u6807\u503c\u8303\u56f4\u3002"}}
{"id": "2507.01494", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01494", "abs": "https://arxiv.org/abs/2507.01494", "authors": ["Muhammad Hassam Ejaz", "Muhammad Bilal", "Usman Habib"], "title": "Crop Pest Classification Using Deep Learning Techniques: A Review", "comment": null, "summary": "Insect pests continue to bring a serious threat to crop yields around the\nworld, and traditional methods for monitoring them are often slow, manual, and\ndifficult to scale. In recent years, deep learning has emerged as a powerful\nsolution, with techniques like convolutional neural networks (CNNs), vision\ntransformers (ViTs), and hybrid models gaining popularity for automating pest\ndetection. This review looks at 37 carefully selected studies published between\n2018 and 2025, all focused on AI-based pest classification. The selected\nresearch is organized by crop type, pest species, model architecture, dataset\nusage, and key technical challenges. The early studies relied heavily on CNNs\nbut latest work is shifting toward hybrid and transformer-based models that\ndeliver higher accuracy and better contextual understanding. Still, challenges\nlike imbalanced datasets, difficulty in detecting small pests, limited\ngeneralizability, and deployment on edge devices remain significant hurdles.\nOverall, this review offers a structured overview of the field, highlights\nuseful datasets, and outlines the key challenges and future directions for\nAI-based pest monitoring systems.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e862018\u81f32025\u5e74\u95f437\u9879\u5173\u4e8eAI\u5bb3\u866b\u5206\u7c7b\u7684\u7814\u7a76\uff0c\u63a2\u8ba8\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u5bb3\u866b\u76d1\u6d4b\u4e2d\u7684\u5e94\u7528\u3001\u6280\u672f\u6311\u6218\u53ca\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u4f20\u7edf\u5bb3\u866b\u76d1\u6d4b\u65b9\u6cd5\u6548\u7387\u4f4e\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff08\u5982CNN\u3001ViT\u548c\u6df7\u5408\u6a21\u578b\uff09\u4e3a\u81ea\u52a8\u5316\u5bb3\u866b\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5206\u679037\u9879\u7814\u7a76\uff0c\u6309\u4f5c\u7269\u7c7b\u578b\u3001\u5bb3\u866b\u79cd\u7c7b\u3001\u6a21\u578b\u67b6\u6784\u3001\u6570\u636e\u96c6\u4f7f\u7528\u548c\u6280\u672f\u6311\u6218\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u65e9\u671f\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56CNN\uff0c\u8fd1\u671f\u8f6c\u5411\u6df7\u5408\u548c\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\uff0c\u4f46\u4ecd\u9762\u4e34\u6570\u636e\u96c6\u4e0d\u5e73\u8861\u3001\u5c0f\u5bb3\u866b\u68c0\u6d4b\u96be\u7b49\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u4e3aAI\u5bb3\u866b\u76d1\u6d4b\u9886\u57df\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u6982\u8ff0\uff0c\u6307\u51fa\u4e86\u5173\u952e\u6311\u6218\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2507.01915", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01915", "abs": "https://arxiv.org/abs/2507.01915", "authors": ["Chengao Li", "Hanyu Zhang", "Yunkun Xu", "Hongyan Xue", "Xiang Ao", "Qing He"], "title": "Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models", "comment": "19 pages, 3 figures. Accepted by ACL 2025 (main)", "summary": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful\ntechnique for aligning large language models (LLMs) with human preferences.\nHowever, effectively aligning LLMs with diverse human preferences remains a\nsignificant challenge, particularly when they are conflict. To address this\nissue, we frame human value alignment as a multi-objective optimization\nproblem, aiming to maximize a set of potentially conflicting objectives. We\nintroduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning\nparadigm that employs multiple-gradient descent to align LLMs with diverse\npreference distributions. GAPO adaptively rescales the gradients for each\nobjective to determine an update direction that optimally balances the\ntrade-offs between objectives. Additionally, we introduce P-GAPO, which\nincorporates user preferences across different objectives and achieves Pareto\nsolutions that better align with the user's specific needs. Our theoretical\nanalysis demonstrates that GAPO converges towards a Pareto optimal solution for\nmultiple objectives. Empirical results on Mistral-7B show that GAPO outperforms\ncurrent state-of-the-art methods, achieving superior performance in both\nhelpfulness and harmlessness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGAPO\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u51b2\u7a81\u76ee\u6807\uff0c\u5b9e\u73b0LLM\u4e0e\u591a\u6837\u5316\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\u3002", "motivation": "\u89e3\u51b3LLM\u4e0e\u591a\u6837\u5316\u4e14\u53ef\u80fd\u51b2\u7a81\u7684\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u7684\u6311\u6218\u3002", "method": "\u5f15\u5165GAPO\u548cP-GAPO\uff0c\u91c7\u7528\u591a\u68af\u5ea6\u4e0b\u964d\u548c\u7528\u6237\u504f\u597d\u52a0\u6743\uff0c\u4f18\u5316\u51b2\u7a81\u76ee\u6807\u3002", "result": "\u7406\u8bba\u8bc1\u660eGAPO\u6536\u655b\u4e8e\u5e15\u7d2f\u6258\u6700\u4f18\u89e3\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728Mistral-7B\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GAPO\u80fd\u6709\u6548\u5e73\u8861\u51b2\u7a81\u76ee\u6807\uff0c\u63d0\u5347LLM\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\u6027\u80fd\u3002"}}
{"id": "2507.01154", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.01154", "abs": "https://arxiv.org/abs/2507.01154", "authors": ["Liangyu Wang", "Junxiao Wang", "Jie Ren", "Zihang Xiang", "David E. Keyes", "Di Wang"], "title": "FlashDP: Private Training Large Language Models with Efficient DP-SGD", "comment": null, "summary": "As large language models (LLMs) increasingly underpin technological\nadvancements, the privacy of their training data emerges as a critical concern.\nDifferential Privacy (DP) serves as a rigorous mechanism to protect this data,\nyet its integration via Differentially Private Stochastic Gradient Descent\n(DP-SGD) introduces substantial challenges, primarily due to the complexities\nof per-sample gradient clipping. Current explicit methods, such as Opacus,\nnecessitate extensive storage for per-sample gradients, significantly inflating\nmemory requirements. Conversely, implicit methods like GhostClip reduce storage\nneeds by recalculating gradients multiple times, which leads to inefficiencies\ndue to redundant computations. This paper introduces FlashDP, an innovative\ncache-friendly per-layer DP-SGD that consolidates necessary operations into a\nsingle task, calculating gradients only once in a fused manner. This approach\nnot only diminishes memory movement by up to \\textbf{50\\%} but also cuts down\nredundant computations by \\textbf{20\\%}, compared to previous methods.\nConsequently, FlashDP does not increase memory demands and achieves a\n\\textbf{90\\%} throughput compared to the Non-DP method on a four-A100 system\nduring the pre-training of the Llama-13B model, while maintaining parity with\nstandard per-layer clipped DP-SGD in terms of accuracy. These advancements\nestablish FlashDP as a pivotal development for efficient and privacy-preserving\ntraining of LLMs. FlashDP's code has been open-sourced in\nhttps://github.com/kaustpradalab/flashdp.", "AI": {"tldr": "FlashDP\u662f\u4e00\u79cd\u521b\u65b0\u7684\u7f13\u5b58\u53cb\u597d\u578bDP-SGD\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u6b21\u878d\u5408\u8ba1\u7b97\u68af\u5ea6\uff0c\u663e\u8457\u51cf\u5c11\u5185\u5b58\u9700\u6c42\u548c\u5197\u4f59\u8ba1\u7b97\uff0c\u540c\u65f6\u4fdd\u6301\u9690\u79c1\u4fdd\u62a4\u6548\u679c\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u8bad\u7ec3\u6570\u636e\u7684\u9690\u79c1\u4fdd\u62a4\u6210\u4e3a\u5173\u952e\u95ee\u9898\u3002\u73b0\u6709\u7684\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u65b9\u6cd5\u5982DP-SGD\u5728\u5b9e\u73b0\u65f6\u9762\u4e34\u5185\u5b58\u5360\u7528\u5927\u6216\u8ba1\u7b97\u5197\u4f59\u7684\u6311\u6218\u3002", "method": "FlashDP\u63d0\u51fa\u4e86\u4e00\u79cd\u7f13\u5b58\u53cb\u597d\u7684\u9010\u5c42DP-SGD\u65b9\u6cd5\uff0c\u5c06\u5fc5\u8981\u64cd\u4f5c\u5408\u5e76\u4e3a\u5355\u4e00\u4efb\u52a1\uff0c\u4ec5\u9700\u4e00\u6b21\u878d\u5408\u8ba1\u7b97\u68af\u5ea6\uff0c\u51cf\u5c11\u5185\u5b58\u79fb\u52a8\u548c\u5197\u4f59\u8ba1\u7b97\u3002", "result": "FlashDP\u5728Llama-13B\u6a21\u578b\u9884\u8bad\u7ec3\u4e2d\uff0c\u5185\u5b58\u9700\u6c42\u672a\u589e\u52a0\uff0c\u541e\u5410\u91cf\u8fbe\u5230\u975eDP\u65b9\u6cd5\u768490%\uff0c\u540c\u65f6\u4e0e\u6807\u51c6DP-SGD\u4fdd\u6301\u76f8\u540c\u7684\u51c6\u786e\u6027\u3002", "conclusion": "FlashDP\u4e3a\u9ad8\u6548\u4e14\u9690\u79c1\u4fdd\u62a4\u7684LLM\u8bad\u7ec3\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u5176\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.01496", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01496", "abs": "https://arxiv.org/abs/2507.01496", "authors": ["Jimyeong Kim", "Jungwon Park", "Yeji Song", "Nojun Kwak", "Wonjong Rhee"], "title": "ReFlex: Text-Guided Editing of Real Images in Rectified Flow via Mid-Step Feature Extraction and Attention Adaptation", "comment": "Published at ICCV 2025. Project page:\n  https://wlaud1001.github.io/ReFlex/", "summary": "Rectified Flow text-to-image models surpass diffusion models in image quality\nand text alignment, but adapting ReFlow for real-image editing remains\nchallenging. We propose a new real-image editing method for ReFlow by analyzing\nthe intermediate representations of multimodal transformer blocks and\nidentifying three key features. To extract these features from real images with\nsufficient structural preservation, we leverage mid-step latent, which is\ninverted only up to the mid-step. We then adapt attention during injection to\nimprove editability and enhance alignment to the target text. Our method is\ntraining-free, requires no user-provided mask, and can be applied even without\na source prompt. Extensive experiments on two benchmarks with nine baselines\ndemonstrate its superior performance over prior methods, further validated by\nhuman evaluations confirming a strong user preference for our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u65e0\u9700\u7528\u6237\u63d0\u4f9b\u63a9\u7801\u7684ReFlow\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u4e2d\u95f4\u8868\u793a\u548c\u5229\u7528\u4e2d\u6b65\u6f5c\u5728\u7279\u5f81\u63d0\u5347\u7f16\u8f91\u6548\u679c\u3002", "motivation": "\u5c3d\u7ba1ReFlow\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u6587\u672c\u5bf9\u9f50\u4e0a\u4f18\u4e8e\u6269\u6563\u6a21\u578b\uff0c\u4f46\u5b9e\u9645\u56fe\u50cf\u7f16\u8f91\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u5206\u6790\u591a\u6a21\u6001\u53d8\u6362\u5668\u5757\u7684\u4e2d\u95f4\u8868\u793a\uff0c\u63d0\u53d6\u4e09\u4e2a\u5173\u952e\u7279\u5f81\uff1b\u5229\u7528\u4e2d\u6b65\u6f5c\u5728\u7279\u5f81\u4fdd\u6301\u7ed3\u6784\uff1b\u8c03\u6574\u6ce8\u610f\u529b\u6ce8\u5165\u4ee5\u63d0\u5347\u7f16\u8f91\u6027\u548c\u6587\u672c\u5bf9\u9f50\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u4e5d\u79cd\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4eba\u7c7b\u8bc4\u4f30\u4e5f\u663e\u793a\u7528\u6237\u504f\u597d\u8be5\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u65e0\u9700\u8bad\u7ec3\u548c\u63a9\u7801\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86ReFlow\u7684\u5b9e\u9645\u56fe\u50cf\u7f16\u8f91\u80fd\u529b\u3002"}}
{"id": "2507.01921", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01921", "abs": "https://arxiv.org/abs/2507.01921", "authors": ["Yang Li", "Youssef Emad", "Karthik Padthe", "Jack Lanchantin", "Weizhe Yuan", "Thao Nguyen", "Jason Weston", "Shang-Wen Li", "Dong Wang", "Ilia Kulikov", "Xian Li"], "title": "NaturalThoughts: Selecting and Distilling Reasoning Traces for General Reasoning Tasks", "comment": null, "summary": "Recent work has shown that distilling reasoning traces from a larger teacher\nmodel via supervised finetuning outperforms reinforcement learning with the\nsmaller student model alone (Guo et al. 2025). However, there has not been a\nsystematic study of what kind of reasoning demonstrations from the teacher are\nmost effective in improving the student model's reasoning capabilities. In this\nwork we curate high-quality \"NaturalThoughts\" by selecting reasoning traces\nfrom a strong teacher model based on a large pool of questions from\nNaturalReasoning (Yuan et al. 2025). We first conduct a systematic analysis of\nfactors that affect distilling reasoning capabilities, in terms of sample\nefficiency and scalability for general reasoning tasks. We observe that simply\nscaling up data size with random sampling is a strong baseline with steady\nperformance gains. Further, we find that selecting difficult examples that\nrequire more diverse reasoning strategies is more sample-efficient to transfer\nthe teacher model's reasoning skills. Evaluated on both Llama and Qwen models,\ntraining with NaturalThoughts outperforms existing reasoning datasets such as\nOpenThoughts, LIMO, etc. on general STEM reasoning benchmarks including\nGPQA-Diamond, MMLU-Pro and SuperGPQA.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u7cbe\u9009\u9ad8\u8d28\u91cf\u63a8\u7406\u8f68\u8ff9\uff08NaturalThoughts\uff09\u63d0\u5347\u5b66\u751f\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u9009\u62e9\u56f0\u96be\u4e14\u591a\u6837\u5316\u7684\u6837\u672c\u66f4\u6709\u6548\u3002", "motivation": "\u63a2\u7d22\u6559\u5e08\u6a21\u578b\u63a8\u7406\u6f14\u793a\u5bf9\u5b66\u751f\u6a21\u578b\u63a8\u7406\u80fd\u529b\u63d0\u5347\u7684\u6700\u6709\u6548\u65b9\u5f0f\u3002", "method": "\u4ece\u6559\u5e08\u6a21\u578b\u4e2d\u7cbe\u9009\u9ad8\u8d28\u91cf\u63a8\u7406\u8f68\u8ff9\uff08NaturalThoughts\uff09\uff0c\u5206\u6790\u5f71\u54cd\u63a8\u7406\u80fd\u529b\u84b8\u998f\u7684\u56e0\u7d20\u3002", "result": "NaturalThoughts\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u6570\u636e\u96c6\u3002", "conclusion": "\u7cbe\u9009\u56f0\u96be\u4e14\u591a\u6837\u5316\u7684\u63a8\u7406\u6837\u672c\u80fd\u66f4\u9ad8\u6548\u5730\u63d0\u5347\u5b66\u751f\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2507.01178", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01178", "abs": "https://arxiv.org/abs/2507.01178", "authors": ["Alec Helbling", "Duen Horng Chau"], "title": "Diffusion Explorer: Interactive Exploration of Diffusion Models", "comment": null, "summary": "Diffusion models have been central to the development of recent image, video,\nand even text generation systems. They posses striking geometric properties\nthat can be faithfully portrayed in low-dimensional settings. However, existing\nresources for explaining diffusion either require an advanced theoretical\nfoundation or focus on their neural network architectures rather than their\nrich geometric properties. We introduce Diffusion Explorer, an interactive tool\nto explain the geometric properties of diffusion models. Users can train 2D\ndiffusion models in the browser and observe the temporal dynamics of their\nsampling process. Diffusion Explorer leverages interactive animation, which has\nbeen shown to be a powerful tool for making engaging visualizations of dynamic\nsystems, making it well suited to explaining diffusion models which represent\nstochastic processes that evolve over time. Diffusion Explorer is open source\nand a live demo is available at alechelbling.com/Diffusion-Explorer.", "AI": {"tldr": "Diffusion Explorer\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u5de5\u5177\uff0c\u7528\u4e8e\u89e3\u91ca\u6269\u6563\u6a21\u578b\u7684\u51e0\u4f55\u7279\u6027\uff0c\u7528\u6237\u53ef\u5728\u6d4f\u89c8\u5668\u4e2d\u8bad\u7ec32D\u6269\u6563\u6a21\u578b\u5e76\u89c2\u5bdf\u91c7\u6837\u8fc7\u7a0b\u7684\u52a8\u6001\u3002", "motivation": "\u73b0\u6709\u89e3\u91ca\u6269\u6563\u6a21\u578b\u7684\u8d44\u6e90\u8981\u4e48\u9700\u8981\u9ad8\u7ea7\u7406\u8bba\u57fa\u7840\uff0c\u8981\u4e48\u8fc7\u4e8e\u5173\u6ce8\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u800c\u5ffd\u7565\u4e86\u5176\u4e30\u5bcc\u7684\u51e0\u4f55\u7279\u6027\u3002", "method": "\u5f00\u53d1\u4e86Diffusion Explorer\uff0c\u5229\u7528\u4ea4\u4e92\u5f0f\u52a8\u753b\u5c55\u793a\u6269\u6563\u6a21\u578b\u7684\u52a8\u6001\u7279\u6027\uff0c\u7528\u6237\u53ef\u76f4\u63a5\u5728\u6d4f\u89c8\u5668\u4e2d\u64cd\u4f5c\u548c\u89c2\u5bdf\u3002", "result": "Diffusion Explorer\u6210\u529f\u5c55\u793a\u4e86\u6269\u6563\u6a21\u578b\u7684\u51e0\u4f55\u7279\u6027\uff0c\u5e76\u901a\u8fc7\u4ea4\u4e92\u5f0f\u52a8\u753b\u4f7f\u5176\u66f4\u6613\u4e8e\u7406\u89e3\u3002", "conclusion": "Diffusion Explorer\u4e3a\u7406\u89e3\u548c\u6559\u5b66\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u76f4\u89c2\u4e14\u4e92\u52a8\u7684\u65b9\u5f0f\uff0c\u9002\u5408\u4e0d\u540c\u80cc\u666f\u7684\u7528\u6237\u3002"}}
{"id": "2507.01502", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01502", "abs": "https://arxiv.org/abs/2507.01502", "authors": ["Ozan Durgut", "Beril Kallfelz-Sirmacek", "Cem Unsalan"], "title": "Integrating Traditional and Deep Learning Methods to Detect Tree Crowns in Satellite Images", "comment": "11 pages, 4 figures, journal manuscript", "summary": "Global warming, loss of biodiversity, and air pollution are among the most\nsignificant problems facing Earth. One of the primary challenges in addressing\nthese issues is the lack of monitoring forests to protect them. To tackle this\nproblem, it is important to leverage remote sensing and computer vision methods\nto automate monitoring applications. Hence, automatic tree crown detection\nalgorithms emerged based on traditional and deep learning methods. In this\nstudy, we first introduce two different tree crown detection methods based on\nthese approaches. Then, we form a novel rule-based approach that integrates\nthese two methods to enhance robustness and accuracy of tree crown detection\nresults. While traditional methods are employed for feature extraction and\nsegmentation of forested areas, deep learning methods are used to detect tree\ncrowns in our method. With the proposed rule-based approach, we post-process\nthese results, aiming to increase the number of detected tree crowns through\nneighboring trees and localized operations. We compare the obtained results\nwith the proposed method in terms of the number of detected tree crowns and\nreport the advantages, disadvantages, and areas for improvement of the obtained\noutcomes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4f20\u7edf\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u89c4\u5219\u5316\u6811\u51a0\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5168\u7403\u53d8\u6696\u3001\u751f\u7269\u591a\u6837\u6027\u4e27\u5931\u548c\u7a7a\u6c14\u6c61\u67d3\u7b49\u95ee\u9898\u4e9f\u9700\u68ee\u6797\u76d1\u6d4b\uff0c\u4f46\u7f3a\u4e4f\u81ea\u52a8\u5316\u76d1\u6d4b\u624b\u6bb5\u3002", "method": "\u7ed3\u5408\u4f20\u7edf\u65b9\u6cd5\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u548c\u5206\u5272\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u68c0\u6d4b\u6811\u51a0\uff0c\u5e76\u901a\u8fc7\u89c4\u5219\u5316\u540e\u5904\u7406\u63d0\u5347\u68c0\u6d4b\u6548\u679c\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u6811\u51a0\u68c0\u6d4b\u6570\u91cf\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u5e76\u5206\u6790\u4e86\u5176\u4f18\u7f3a\u70b9\u548c\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "\u89c4\u5219\u5316\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u6811\u51a0\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u68ee\u6797\u76d1\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.01923", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01923", "abs": "https://arxiv.org/abs/2507.01923", "authors": ["Yu-Shiang Huang", "Chuan-Ju Wang", "Chung-Chi Chen"], "title": "Decision-oriented Text Evaluation", "comment": null, "summary": "Natural language generation (NLG) is increasingly deployed in high-stakes\ndomains, yet common intrinsic evaluation methods, such as n-gram overlap or\nsentence plausibility, weakly correlate with actual decision-making efficacy.\nWe propose a decision-oriented framework for evaluating generated text by\ndirectly measuring its influence on human and large language model (LLM)\ndecision outcomes. Using market digest texts--including objective morning\nsummaries and subjective closing-bell analyses--as test cases, we assess\ndecision quality based on the financial performance of trades executed by human\ninvestors and autonomous LLM agents informed exclusively by these texts. Our\nfindings reveal that neither humans nor LLM agents consistently surpass random\nperformance when relying solely on summaries. However, richer analytical\ncommentaries enable collaborative human-LLM teams to outperform individual\nhuman or agent baselines significantly. Our approach underscores the importance\nof evaluating generated text by its ability to facilitate synergistic\ndecision-making between humans and LLMs, highlighting critical limitations of\ntraditional intrinsic metrics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51b3\u7b56\u6548\u679c\u7684NLG\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u6d4b\u91cf\u751f\u6210\u6587\u672c\u5bf9\u4eba\u7c7b\u548cLLM\u51b3\u7b56\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u4e0e\u5b9e\u9645\u51b3\u7b56\u6548\u679c\u76f8\u5173\u6027\u8f83\u5f31\u3002", "motivation": "\u4f20\u7edfNLG\u8bc4\u4f30\u65b9\u6cd5\uff08\u5982n-gram\u91cd\u53e0\u6216\u53e5\u5b50\u5408\u7406\u6027\uff09\u4e0e\u5b9e\u9645\u51b3\u7b56\u6548\u679c\u76f8\u5173\u6027\u8f83\u5f31\uff0c\u9700\u8981\u66f4\u76f4\u63a5\u7684\u8bc4\u4f30\u65b9\u5f0f\u3002", "method": "\u63d0\u51fa\u51b3\u7b56\u5bfc\u5411\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5e02\u573a\u6458\u8981\u6587\u672c\uff08\u5ba2\u89c2\u548c\u4e3b\u89c2\u5185\u5bb9\uff09\u6d4b\u8bd5\u4eba\u7c7b\u548cLLM\u7684\u51b3\u7b56\u8d28\u91cf\uff0c\u6bd4\u8f83\u5176\u4ea4\u6613\u8868\u73b0\u3002", "result": "\u4eba\u7c7b\u548cLLM\u5355\u72ec\u4f9d\u8d56\u6458\u8981\u65f6\u8868\u73b0\u4e0d\u4f18\u4e8e\u968f\u673a\uff0c\u4f46\u7ed3\u5408\u5206\u6790\u8bc4\u8bba\u540e\uff0c\u4eba\u673a\u534f\u4f5c\u56e2\u961f\u663e\u8457\u4f18\u4e8e\u5355\u72ec\u8868\u73b0\u3002", "conclusion": "\u8bc4\u4f30\u751f\u6210\u6587\u672c\u5e94\u5173\u6ce8\u5176\u4fc3\u8fdb\u4eba\u673a\u534f\u540c\u51b3\u7b56\u7684\u80fd\u529b\uff0c\u4f20\u7edf\u5185\u5728\u6307\u6807\u5b58\u5728\u5c40\u9650\u6027\u3002"}}
{"id": "2507.01381", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01381", "abs": "https://arxiv.org/abs/2507.01381", "authors": ["Tong Liu", "Yinuo Wang", "Xujie Song", "Wenjun Zou", "Liangfa Chen", "Likun Wang", "Bin Shuai", "Jingliang Duan", "Shengbo Eben Li"], "title": "Distributional Soft Actor-Critic with Diffusion Policy", "comment": "Accepted IEEE ITSC 2025", "summary": "Reinforcement learning has been proven to be highly effective in handling\ncomplex control tasks. Traditional methods typically use unimodal\ndistributions, such as Gaussian distributions, to model the output of value\ndistributions. However, unimodal distribution often and easily causes bias in\nvalue function estimation, leading to poor algorithm performance. This paper\nproposes a distributional reinforcement learning algorithm called DSAC-D\n(Distributed Soft Actor Critic with Diffusion Policy) to address the challenges\nof estimating bias in value functions and obtaining multimodal policy\nrepresentations. A multimodal distributional policy iteration framework that\ncan converge to the optimal policy was established by introducing policy\nentropy and value distribution function. A diffusion value network that can\naccurately characterize the distribution of multi peaks was constructed by\ngenerating a set of reward samples through reverse sampling using a diffusion\nmodel. Based on this, a distributional reinforcement learning algorithm with\ndual diffusion of the value network and the policy network was derived. MuJoCo\ntesting tasks demonstrate that the proposed algorithm not only learns\nmultimodal policy, but also achieves state-of-the-art (SOTA) performance in all\n9 control tasks, with significant suppression of estimation bias and total\naverage return improvement of over 10\\% compared to existing mainstream\nalgorithms. The results of real vehicle testing show that DSAC-D can accurately\ncharacterize the multimodal distribution of different driving styles, and the\ndiffusion policy network can characterize multimodal trajectories.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDSAC-D\u7684\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u6269\u6563\u6a21\u578b\u548c\u71b5\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5355\u5cf0\u5206\u5e03\u65b9\u6cd5\u5728\u503c\u51fd\u6570\u4f30\u8ba1\u4e2d\u7684\u504f\u5dee\u95ee\u9898\uff0c\u5e76\u5728\u591a\u6a21\u6001\u7b56\u7565\u8868\u793a\u548c\u6027\u80fd\u63d0\u5347\u4e0a\u53d6\u5f97\u663e\u8457\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f7f\u7528\u5355\u5cf0\u5206\u5e03\uff08\u5982\u9ad8\u65af\u5206\u5e03\uff09\u5efa\u6a21\u503c\u5206\u5e03\uff0c\u5bb9\u6613\u5bfc\u81f4\u503c\u51fd\u6570\u4f30\u8ba1\u504f\u5dee\uff0c\u5f71\u54cd\u7b97\u6cd5\u6027\u80fd\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u591a\u6a21\u6001\u7b56\u7565\u8868\u793a\u3002", "method": "\u63d0\u51faDSAC-D\u7b97\u6cd5\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u71b5\u4f18\u5316\uff0c\u6784\u5efa\u591a\u5cf0\u5206\u5e03\u503c\u7f51\u7edc\u548c\u7b56\u7565\u7f51\u7edc\uff0c\u901a\u8fc7\u53cd\u5411\u91c7\u6837\u751f\u6210\u5956\u52b1\u6837\u672c\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u7b56\u7565\u8fed\u4ee3\u3002", "result": "\u5728MuJoCo\u6d4b\u8bd5\u4efb\u52a1\u4e2d\uff0cDSAC-D\u5728\u6240\u67099\u4e2a\u63a7\u5236\u4efb\u52a1\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u4f30\u8ba1\u504f\u5dee\u663e\u8457\u6291\u5236\uff0c\u603b\u5e73\u5747\u56de\u62a5\u63d0\u5347\u8d85\u8fc710%\u3002\u5b9e\u8f66\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5176\u591a\u6a21\u6001\u5206\u5e03\u8868\u5f81\u80fd\u529b\u3002", "conclusion": "DSAC-D\u901a\u8fc7\u6269\u6563\u6a21\u578b\u548c\u71b5\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u503c\u51fd\u6570\u4f30\u8ba1\u504f\u5dee\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u7b56\u7565\u8868\u793a\u548c\u6027\u80fd\u63d0\u5347\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.01504", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01504", "abs": "https://arxiv.org/abs/2507.01504", "authors": ["Robert Aufschl\u00e4ger", "Youssef Shoeb", "Azarm Nowzad", "Michael Heigl", "Fabian Bally", "Martin Schramm"], "title": "Following the Clues: Experiments on Person Re-ID using Cross-Modal Intelligence", "comment": "accepted for publication at the 2025 IEEE 28th International\n  Conference on Intelligent Transportation Systems (ITSC 2025), taking place\n  during November 18-21, 2025 in Gold Coast, Australia", "summary": "The collection and release of street-level recordings as Open Data play a\nvital role in advancing autonomous driving systems and AI research. However,\nthese datasets pose significant privacy risks, particularly for pedestrians,\ndue to the presence of Personally Identifiable Information (PII) that extends\nbeyond biometric traits such as faces. In this paper, we present cRID, a novel\ncross-modal framework combining Large Vision-Language Models, Graph Attention\nNetworks, and representation learning to detect textual describable clues of\nPII and enhance person re-identification (Re-ID). Our approach focuses on\nidentifying and leveraging interpretable features, enabling the detection of\nsemantically meaningful PII beyond low-level appearance cues. We conduct a\nsystematic evaluation of PII presence in person image datasets. Our experiments\nshow improved performance in practical cross-dataset Re-ID scenarios, notably\nfrom Market-1501 to CUHK03-np (detected), highlighting the framework's\npractical utility. Code is available at https://github.com/RAufschlaeger/cRID.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3acRID\u7684\u8de8\u6a21\u6001\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff0c\u7528\u4e8e\u68c0\u6d4b\u4e2a\u4eba\u53ef\u8bc6\u522b\u4fe1\u606f\uff08PII\uff09\u5e76\u589e\u5f3a\u884c\u4eba\u91cd\u8bc6\u522b\uff08Re-ID\uff09\u6027\u80fd\u3002", "motivation": "\u8857\u666f\u6570\u636e\u4f5c\u4e3a\u5f00\u653e\u6570\u636e\u5bf9\u81ea\u52a8\u9a7e\u9a76\u548cAI\u7814\u7a76\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u4e2d\u5305\u542b\u7684\u4e2a\u4eba\u53ef\u8bc6\u522b\u4fe1\u606f\uff08PII\uff09\u5bf9\u9690\u79c1\u6784\u6210\u5a01\u80c1\u3002", "method": "cRID\u6846\u67b6\u7ed3\u5408\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u548c\u8868\u793a\u5b66\u4e60\uff0c\u68c0\u6d4b\u6587\u672c\u53ef\u63cf\u8ff0\u7684PII\u7ebf\u7d22\uff0c\u5e76\u5229\u7528\u53ef\u89e3\u91ca\u7279\u5f81\u589e\u5f3aRe-ID\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0ccRID\u5728\u8de8\u6570\u636e\u96c6Re-ID\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7279\u522b\u662f\u5728Market-1501\u5230CUHK03-np\u7684\u6570\u636e\u96c6\u4e0a\u3002", "conclusion": "cRID\u6846\u67b6\u80fd\u6709\u6548\u68c0\u6d4b\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u7684PII\uff0c\u5e76\u63d0\u5347Re-ID\u6027\u80fd\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.01931", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.01931", "abs": "https://arxiv.org/abs/2507.01931", "authors": ["Md Sazzadul Islam Ridoy", "Sumi Akter", "Md. Aminur Rahman"], "title": "Adaptability of ASR Models on Low-Resource Language: A Comparative Study of Whisper and Wav2Vec-BERT on Bangla", "comment": null, "summary": "In recent years, neural models trained on large multilingual text and speech\ndatasets have shown great potential for supporting low-resource languages. This\nstudy investigates the performances of two state-of-the-art Automatic Speech\nRecognition (ASR) models, OpenAI's Whisper (Small & Large-V2) and Facebook's\nWav2Vec-BERT on Bangla, a low-resource language. We have conducted experiments\nusing two publicly available datasets: Mozilla Common Voice-17 and OpenSLR to\nevaluate model performances. Through systematic fine-tuning and hyperparameter\noptimization, including learning rate, epochs, and model checkpoint selection,\nwe have compared the models based on Word Error Rate (WER), Character Error\nRate (CER), Training Time, and Computational Efficiency. The Wav2Vec-BERT model\noutperformed Whisper across all key evaluation metrics, demonstrated superior\nperformance while requiring fewer computational resources, and offered valuable\ninsights to develop robust speech recognition systems in low-resource\nlinguistic settings.", "AI": {"tldr": "\u6bd4\u8f83\u4e86Whisper\u548cWav2Vec-BERT\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00Bangla\u4e0a\u7684\u8868\u73b0\uff0cWav2Vec-BERT\u5728\u5404\u9879\u6307\u6807\u4e0a\u5747\u4f18\u4e8eWhisper\u3002", "motivation": "\u7814\u7a76\u4f4e\u8d44\u6e90\u8bed\u8a00Bangla\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u6027\u80fd\uff0c\u63a2\u7d22\u9ad8\u6548\u6a21\u578b\u3002", "method": "\u4f7f\u7528Mozilla Common Voice-17\u548cOpenSLR\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5fae\u8c03\u548c\u8d85\u53c2\u6570\u4f18\u5316\u8bc4\u4f30Whisper\u548cWav2Vec-BERT\u3002", "result": "Wav2Vec-BERT\u5728\u6240\u6709\u5173\u952e\u6307\u6807\uff08WER\u3001CER\u3001\u8bad\u7ec3\u65f6\u95f4\u548c\u8ba1\u7b97\u6548\u7387\uff09\u4e0a\u4f18\u4e8eWhisper\u3002", "conclusion": "Wav2Vec-BERT\u66f4\u9002\u5408\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u8bed\u97f3\u8bc6\u522b\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u53c2\u8003\u3002"}}
{"id": "2507.01201", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01201", "abs": "https://arxiv.org/abs/2507.01201", "authors": ["Hyoseo", "Yoon", "Yisong Yue", "Been Kim"], "title": "Escaping Platos Cave: JAM for Aligning Independently Trained Vision and Language Models", "comment": null, "summary": "Independently trained vision and language models inhabit disjoint\nrepresentational spaces, shaped by their respective modalities, objectives, and\narchitectures. Yet an emerging hypothesis - the Platonic Representation\nHypothesis - suggests that such models may nonetheless converge toward a shared\nstatistical model of reality. This compatibility, if it exists, raises a\nfundamental question: can we move beyond post-hoc statistical detection of\nalignment and explicitly optimize for it between such disjoint representations?\nWe cast this Platonic alignment problem as a multi-objective optimization task\n- preserve each modality's native structure while aligning for mutual\ncoherence. We introduce the Joint Autoencoder Modulator (JAM) framework that\njointly trains modality-specific autoencoders on the latent representations of\npre-trained single modality models, encouraging alignment through both\nreconstruction and cross-modal objectives. By analogy, this framework serves as\na method to escape Plato's Cave, enabling the emergence of shared structure\nfrom disjoint inputs. We evaluate this framework across three critical design\naxes: (i) the alignment objective - comparing contrastive loss (Con), its\nhard-negative variant (NegCon), and our Spread loss, (ii) the layer depth at\nwhich alignment is most effective, and (iii) the impact of foundation model\nscale on representational convergence. Our findings show that our lightweight\nPareto-efficient framework reliably induces alignment, even across frozen,\nindependently trained representations, offering both theoretical insight and\npractical pathways for transforming generalist unimodal foundations into\nspecialist multimodal models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u81ea\u52a8\u7f16\u7801\u5668\u8c03\u5236\u5668\uff08JAM\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u578b\u4e4b\u95f4\u7684\u8868\u793a\u5bf9\u9f50\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u5b9e\u73b0\u6a21\u6001\u95f4\u5171\u4eab\u7ed3\u6784\u3002", "motivation": "\u63a2\u7d22\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u578b\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u4f18\u5316\u5b9e\u73b0\u8868\u793a\u5bf9\u9f50\uff0c\u8d85\u8d8a\u7edf\u8ba1\u68c0\u6d4b\uff0c\u9a8c\u8bc1\u67cf\u62c9\u56fe\u8868\u793a\u5047\u8bf4\u3002", "method": "\u5f15\u5165JAM\u6846\u67b6\uff0c\u8054\u5408\u8bad\u7ec3\u6a21\u6001\u7279\u5b9a\u7684\u81ea\u52a8\u7f16\u7801\u5668\uff0c\u7ed3\u5408\u91cd\u5efa\u548c\u8de8\u6a21\u6001\u76ee\u6807\uff0c\u4f18\u5316\u5bf9\u9f50\u3002", "result": "JAM\u6846\u67b6\u5728\u591a\u79cd\u8bbe\u8ba1\u8f74\u4e0a\uff08\u5982\u5bf9\u9f50\u76ee\u6807\u3001\u5c42\u6df1\u5ea6\u548c\u6a21\u578b\u89c4\u6a21\uff09\u5747\u80fd\u6709\u6548\u8bf1\u5bfc\u5bf9\u9f50\uff0c\u5373\u4f7f\u5bf9\u4e8e\u51bb\u7ed3\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5c06\u901a\u7528\u5355\u6a21\u6001\u6a21\u578b\u8f6c\u5316\u4e3a\u4e13\u4e1a\u591a\u6a21\u6001\u6a21\u578b\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u9014\u5f84\u3002"}}
{"id": "2507.01509", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01509", "abs": "https://arxiv.org/abs/2507.01509", "authors": ["Tapas K. Dutta", "Snehashis Majhi", "Deepak Ranjan Nayak", "Debesh Jha"], "title": "Mamba Guided Boundary Prior Matters: A New Perspective for Generalized Polyp Segmentation", "comment": "11 pages, 2 figures, MICCAI-2025", "summary": "Polyp segmentation in colonoscopy images is crucial for early detection and\ndiagnosis of colorectal cancer. However, this task remains a significant\nchallenge due to the substantial variations in polyp shape, size, and color, as\nwell as the high similarity between polyps and surrounding tissues, often\ncompounded by indistinct boundaries. While existing encoder-decoder CNN and\ntransformer-based approaches have shown promising results, they struggle with\nstable segmentation performance on polyps with weak or blurry boundaries. These\nmethods exhibit limited abilities to distinguish between polyps and non-polyps\nand capture essential boundary cues. Moreover, their generalizability still\nfalls short of meeting the demands of real-time clinical applications. To\naddress these limitations, we propose SAM-MaGuP, a groundbreaking approach for\nrobust polyp segmentation. By incorporating a boundary distillation module and\na 1D-2D Mamba adapter within the Segment Anything Model (SAM), SAM-MaGuP excels\nat resolving weak boundary challenges and amplifies feature learning through\nenriched global contextual interactions. Extensive evaluations across five\ndiverse datasets reveal that SAM-MaGuP outperforms state-of-the-art methods,\nachieving unmatched segmentation accuracy and robustness. Our key innovations,\na Mamba-guided boundary prior and a 1D-2D Mamba block, set a new benchmark in\nthe field, pushing the boundaries of polyp segmentation to new heights.", "AI": {"tldr": "SAM-MaGuP\u662f\u4e00\u79cd\u57fa\u4e8eSegment Anything Model\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fb9\u754c\u84b8\u998f\u6a21\u5757\u548c1D-2D Mamba\u9002\u914d\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u606f\u8089\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u606f\u8089\u8fb9\u754c\u6a21\u7cca\u6216\u5f31\u5316\u65f6\u7684\u5206\u5272\u6027\u80fd\u4e0d\u8db3\uff0c\u4ee5\u53ca\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u8fb9\u754c\u84b8\u998f\u6a21\u5757\u548c1D-2D Mamba\u9002\u914d\u5668\uff0c\u589e\u5f3a\u5168\u5c40\u4e0a\u4e0b\u6587\u4ea4\u4e92\u548c\u8fb9\u754c\u7279\u5f81\u5b66\u4e60\u3002", "result": "\u5728\u4e94\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "SAM-MaGuP\u4e3a\u606f\u8089\u5206\u5272\u9886\u57df\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\uff0c\u5177\u6709\u663e\u8457\u7684\u4e34\u5e8a\u6f5c\u529b\u3002"}}
{"id": "2507.01936", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.01936", "abs": "https://arxiv.org/abs/2507.01936", "authors": ["Adrian de Wynter", "Tangming Yuan"], "title": "The Thin Line Between Comprehension and Persuasion in LLMs", "comment": null, "summary": "Large language models (LLMs) are excellent at maintaining high-level,\nconvincing dialogues. They are being fast deployed as chatbots and evaluators\nin sensitive areas, such as peer review and mental health applications. This,\nalong with the disparate accounts on their reasoning capabilities, calls for a\ncloser examination of LLMs and their comprehension of dialogue. In this work we\nbegin by evaluating LLMs' ability to maintain a debate--one of the purest yet\nmost complex forms of human communication. Then we measure how this capability\nrelates to their understanding of what is being talked about, namely, their\ncomprehension of dialogical structures and the pragmatic context. We find that\nLLMs are capable of maintaining coherent, persuasive debates, often swaying the\nbeliefs of participants and audiences alike. We also note that awareness or\nsuspicion of AI involvement encourage people to be more critical of the\narguments made. When polling LLMs on their comprehension of deeper structures\nof dialogue, however, they cannot demonstrate said understanding. Our findings\ntie the shortcomings of LLMs-as-evaluators to their (in)ability to understand\nthe context. More broadly, for the field of argumentation theory we posit that,\nif an agent can convincingly maintain a dialogue, it is not necessary for it to\nknow what it is talking about. Hence, the modelling of pragmatic context and\ncoherence are secondary to effectiveness.", "AI": {"tldr": "LLMs\u80fd\u8fdb\u884c\u8fde\u8d2f\u4e14\u6709\u8bf4\u670d\u529b\u7684\u8fa9\u8bba\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5bf9\u8bdd\u6df1\u5c42\u7ed3\u6784\u7684\u7406\u89e3\u3002", "motivation": "\u8bc4\u4f30LLMs\u5728\u8fa9\u8bba\u4e2d\u7684\u8868\u73b0\u53ca\u5176\u5bf9\u5bf9\u8bdd\u7ed3\u6784\u7684\u7406\u89e3\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u6d4b\u8bd5LLMs\u7684\u8fa9\u8bba\u80fd\u529b\u548c\u5bf9\u5bf9\u8bdd\u7ed3\u6784\u7684\u7406\u89e3\u3002", "result": "LLMs\u80fd\u8fdb\u884c\u6709\u8bf4\u670d\u529b\u7684\u8fa9\u8bba\uff0c\u4f46\u65e0\u6cd5\u7406\u89e3\u6df1\u5c42\u5bf9\u8bdd\u7ed3\u6784\u3002", "conclusion": "LLMs\u7684\u8fa9\u8bba\u80fd\u529b\u4e0d\u4f9d\u8d56\u4e8e\u5bf9\u5185\u5bb9\u7684\u7406\u89e3\uff0c\u5b9e\u7528\u6027\u548c\u8fde\u8d2f\u6027\u6bd4\u7406\u89e3\u66f4\u91cd\u8981\u3002"}}
{"id": "2507.01208", "categories": ["cs.LG", "cs.CR", "C.2.0; I.2.0"], "pdf": "https://arxiv.org/pdf/2507.01208", "abs": "https://arxiv.org/abs/2507.01208", "authors": ["Pedro R. X. Carmo", "Igor de Moura", "Assis T. de Oliveira Filho", "Djamel Sadok", "Cleber Zanchettin"], "title": "Deep Learning-Based Intrusion Detection for Automotive Ethernet: Evaluating & Optimizing Fast Inference Techniques for Deployment on Low-Cost Platform", "comment": null, "summary": "Modern vehicles are increasingly connected, and in this context, automotive\nEthernet is one of the technologies that promise to provide the necessary\ninfrastructure for intra-vehicle communication. However, these systems are\nsubject to attacks that can compromise safety, including flow injection\nattacks. Deep Learning-based Intrusion Detection Systems (IDS) are often\ndesigned to combat this problem, but they require expensive hardware to run in\nreal time. In this work, we propose to evaluate and apply fast neural network\ninference techniques like Distilling and Prunning for deploying IDS models on\nlow-cost platforms in real time. The results show that these techniques can\nachieve intrusion detection times of up to 727 {\\mu}s using a Raspberry Pi 4,\nwith AUCROC values of 0.9890.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8f7b\u91cf\u5316\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\uff08IDS\uff09\uff0c\u901a\u8fc7\u84b8\u998f\u548c\u526a\u679d\u6280\u672f\uff0c\u5b9e\u73b0\u5728\u4f4e\u6210\u672c\u786c\u4ef6\uff08\u5982\u6811\u8393\u6d3e4\uff09\u4e0a\u5b9e\u65f6\u8fd0\u884c\uff0c\u68c0\u6d4b\u65f6\u95f4\u4f4e\u81f3727\u5fae\u79d2\uff0cAUCROC\u503c\u8fbe0.9890\u3002", "motivation": "\u73b0\u4ee3\u8f66\u8f86\u4e92\u8054\u6027\u589e\u5f3a\uff0c\u8f66\u8f7d\u4ee5\u592a\u7f51\u6280\u672f\u9762\u4e34\u6d41\u6ce8\u5165\u653b\u51fb\u7b49\u5b89\u5168\u5a01\u80c1\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60IDS\u9700\u8981\u6602\u8d35\u786c\u4ef6\u652f\u6301\u5b9e\u65f6\u8fd0\u884c\u3002", "method": "\u91c7\u7528\u84b8\u998f\u548c\u526a\u679d\u7b49\u5feb\u901f\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u6280\u672f\uff0c\u4f18\u5316IDS\u6a21\u578b\u4ee5\u9002\u914d\u4f4e\u6210\u672c\u5e73\u53f0\u3002", "result": "\u5728\u6811\u8393\u6d3e4\u4e0a\u5b9e\u73b0727\u5fae\u79d2\u7684\u5165\u4fb5\u68c0\u6d4b\u65f6\u95f4\uff0cAUCROC\u503c\u4e3a0.9890\u3002", "conclusion": "\u8f7b\u91cf\u5316\u6280\u672f\u53ef\u6709\u6548\u964d\u4f4eIDS\u786c\u4ef6\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u8f66\u8f7d\u4ee5\u592a\u7f51\u5b89\u5168\u9632\u62a4\u3002"}}
{"id": "2507.01532", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01532", "abs": "https://arxiv.org/abs/2507.01532", "authors": ["Tomas Zelezny", "Jakub Straka", "Vaclav Javorek", "Ondrej Valach", "Marek Hruz", "Ivan Gruber"], "title": "Exploring Pose-based Sign Language Translation: Ablation Studies and Attention Insights", "comment": "8 pages, 9 figures, supplementary, SLRTP2025, CVPR2025", "summary": "Sign Language Translation (SLT) has evolved significantly, moving from\nisolated recognition approaches to complex, continuous gloss-free translation\nsystems. This paper explores the impact of pose-based data preprocessing\ntechniques - normalization, interpolation, and augmentation - on SLT\nperformance. We employ a transformer-based architecture, adapting a modified T5\nencoder-decoder model to process pose representations. Through extensive\nablation studies on YouTubeASL and How2Sign datasets, we analyze how different\npreprocessing strategies affect translation accuracy. Our results demonstrate\nthat appropriate normalization, interpolation, and augmentation techniques can\nsignificantly improve model robustness and generalization abilities.\nAdditionally, we provide a deep analysis of the model's attentions and reveal\ninteresting behavior suggesting that adding a dedicated register token can\nimprove overall model performance. We publish our code on our GitHub\nrepository, including the preprocessed YouTubeASL data.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u59ff\u6001\u6570\u636e\u9884\u5904\u7406\uff08\u6807\u51c6\u5316\u3001\u63d2\u503c\u548c\u589e\u5f3a\uff09\u5bf9\u624b\u8bed\u7ffb\u8bd1\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4f7f\u7528\u6539\u8fdb\u7684T5\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\uff0c\u5b9e\u9a8c\u8868\u660e\u8fd9\u4e9b\u6280\u672f\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u59ff\u6001\u6570\u636e\u9884\u5904\u7406\u6280\u672f\u5bf9\u624b\u8bed\u7ffb\u8bd1\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u6539\u8fdb\u7684T5\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\uff0c\u7ed3\u5408\u59ff\u6001\u6570\u636e\u9884\u5904\u7406\u6280\u672f\uff08\u6807\u51c6\u5316\u3001\u63d2\u503c\u548c\u589e\u5f3a\uff09\uff0c\u5728YouTubeASL\u548cHow2Sign\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d88\u878d\u5b9e\u9a8c\u3002", "result": "\u9002\u5f53\u7684\u9884\u5904\u7406\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u7ffb\u8bd1\u51c6\u786e\u6027\u548c\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u53d1\u73b0\u6dfb\u52a0\u4e13\u7528\u5bc4\u5b58\u5668\u6807\u8bb0\u53ef\u80fd\u8fdb\u4e00\u6b65\u6539\u8fdb\u6a21\u578b\u3002", "conclusion": "\u59ff\u6001\u6570\u636e\u9884\u5904\u7406\u6280\u672f\u5bf9\u624b\u8bed\u7ffb\u8bd1\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u672a\u6765\u53ef\u63a2\u7d22\u4e13\u7528\u5bc4\u5b58\u5668\u6807\u8bb0\u7684\u5e94\u7528\u3002"}}
{"id": "2507.01457", "categories": ["cs.LG", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.01457", "abs": "https://arxiv.org/abs/2507.01457", "authors": ["Federico Nicolas Peccia", "Frederik Haxel", "Oliver Bringmann"], "title": "Tensor Program Optimization for the RISC-V Vector Extension Using Probabilistic Programs", "comment": "9 pages, 10 figures, 2 algorithms", "summary": "RISC-V provides a flexible and scalable platform for applications ranging\nfrom embedded devices to high-performance computing clusters. Particularly, its\nRISC-V Vector Extension (RVV) becomes of interest for the acceleration of AI\nworkloads. But writing software that efficiently utilizes the vector units of\nRISC-V CPUs without expert knowledge requires the programmer to rely on the\nautovectorization features of compilers or hand-crafted libraries like\nmuRISCV-NN. Smarter approaches, like autotuning frameworks, have been missing\nthe integration with the RISC-V RVV extension, thus heavily limiting the\nefficient deployment of complex AI workloads. In this paper, we present a\nworkflow based on the TVM compiler to efficiently map AI workloads onto RISC-V\nvector units. Instead of relying on hand-crafted libraries, we integrated the\nRVV extension into TVM's MetaSchedule framework, a probabilistic program\nframework for tensor operation tuning. We implemented different RISC-V SoCs on\nan FPGA and tuned a wide range of AI workloads on them. We found that our\nproposal shows a mean improvement of 46% in execution latency when compared\nagainst the autovectorization feature of GCC, and 29% against muRISCV-NN.\nMoreover, the binary resulting from our proposal has a smaller code memory\nfootprint, making it more suitable for embedded devices. Finally, we also\nevaluated our solution on a commercially available RISC-V SoC implementing the\nRVV 1.0 Vector Extension and found our solution is able to find mappings that\nare 35% faster on average than the ones proposed by LLVM. We open-sourced our\nproposal for the community to expand it to target other RISC-V extensions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTVM\u7f16\u8bd1\u5668\u7684\u5de5\u4f5c\u6d41\uff0c\u7528\u4e8e\u9ad8\u6548\u5730\u5c06AI\u5de5\u4f5c\u8d1f\u8f7d\u6620\u5c04\u5230RISC-V\u5411\u91cf\u5355\u5143\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "RISC-V\u7684\u5411\u91cf\u6269\u5c55\uff08RVV\uff09\u5728AI\u5de5\u4f5c\u8d1f\u8f7d\u52a0\u901f\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u667a\u80fd\u65b9\u6cd5\uff08\u5982\u81ea\u52a8\u8c03\u4f18\u6846\u67b6\uff09\u7684\u96c6\u6210\uff0c\u9650\u5236\u4e86\u5176\u9ad8\u6548\u90e8\u7f72\u3002", "method": "\u5c06RVV\u6269\u5c55\u96c6\u6210\u5230TVM\u7684MetaSchedule\u6846\u67b6\u4e2d\uff0c\u5e76\u901a\u8fc7FPGA\u5b9e\u73b0\u4e0d\u540c\u7684RISC-V SoC\uff0c\u5bf9\u591a\u79cdAI\u5de5\u4f5c\u8d1f\u8f7d\u8fdb\u884c\u8c03\u4f18\u3002", "result": "\u76f8\u6bd4GCC\u7684\u81ea\u52a8\u5411\u91cf\u5316\u529f\u80fd\uff0c\u5e73\u5747\u6267\u884c\u5ef6\u8fdf\u63d0\u5347\u4e8646%\uff1b\u76f8\u6bd4muRISCV-NN\uff0c\u63d0\u5347\u4e8629%\u3002\u540c\u65f6\uff0c\u751f\u6210\u7684\u4e8c\u8fdb\u5236\u4ee3\u7801\u5360\u7528\u66f4\u5c11\u5185\u5b58\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86RISC-V\u4e0aAI\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6267\u884c\u6548\u7387\uff0c\u5e76\u5f00\u6e90\u4ee5\u652f\u6301\u793e\u533a\u6269\u5c55\u3002"}}
{"id": "2507.01216", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.01216", "abs": "https://arxiv.org/abs/2507.01216", "authors": ["Xingke Yang", "Liang Li", "Zhiyi Wan", "Sicong Li", "Hao Wang", "Xiaoqi Qi", "Jiang Liu", "Tomoaki Ohtsuki", "Xin Fu", "Miao Pan"], "title": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile Device via Additive Side-Tuning", "comment": null, "summary": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data, labels or fine-tuned models to the\nserver. To address those issues, we develop PAE MobiLLM, a privacy-aware and\nefficient LLM FT method which can be deployed on the mobile device via\nserver-assisted additive side-tuning. To further accelerate FT convergence and\nimprove computing efficiency, PAE MobiLLM integrates activation caching on the\nserver side, which allows the server to reuse historical activations and saves\nthe mobile device from repeatedly computing forward passes for the recurring\ndata samples. Besides, to reduce communication cost, PAE MobiLLM develops a\none-token (i.e., ``pivot'' token) activation shortcut that transmits only a\nsingle activation dimension instead of full activation matrices to guide the\nside network tuning. Last but not least, PAE MobiLLM introduces the additive\nadapter side-network design which makes the server train the adapter modules\nbased on device-defined prediction differences rather than raw ground-truth\nlabels. In this way, the server can only assist device-defined side-network\ncomputing, and learn nothing about data, labels or fine-tuned models.", "AI": {"tldr": "PAE MobiLLM\u662f\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u4e14\u9ad8\u6548\u7684\u79fb\u52a8\u8bbe\u5907\u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u670d\u52a1\u5668\u8f85\u52a9\u7684\u4fa7\u8c03\u4f18\u548c\u6fc0\u6d3b\u7f13\u5b58\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u901a\u4fe1\u8d1f\u62c5\u548c\u6570\u636e\u9690\u79c1\u95ee\u9898\u3002", "motivation": "\u79fb\u52a8\u8bbe\u5907\u8d44\u6e90\u6709\u9650\uff0c\u73b0\u6709\u670d\u52a1\u5668\u8f85\u52a9\u65b9\u6cd5\u5b58\u5728\u901a\u4fe1\u8d1f\u62c5\u91cd\u548c\u6570\u636e\u9690\u79c1\u6cc4\u9732\u95ee\u9898\u3002", "method": "\u91c7\u7528\u670d\u52a1\u5668\u8f85\u52a9\u4fa7\u8c03\u4f18\u3001\u6fc0\u6d3b\u7f13\u5b58\u3001\u5355\u4ee4\u724c\u6fc0\u6d3b\u5feb\u6377\u65b9\u5f0f\u548c\u52a0\u6cd5\u9002\u914d\u5668\u4fa7\u7f51\u7edc\u8bbe\u8ba1\u3002", "result": "\u51cf\u5c11\u4e86\u901a\u4fe1\u6210\u672c\uff0c\u52a0\u901f\u4e86\u5fae\u8c03\u6536\u655b\uff0c\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u62a4\u4e86\u6570\u636e\u9690\u79c1\u3002", "conclusion": "PAE MobiLLM\u4e3a\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9690\u79c1\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01535", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01535", "abs": "https://arxiv.org/abs/2507.01535", "authors": ["Bingxi Liu", "Calvin Chen", "Junhao Li", "Guyang Yu", "Haoqian Song", "Xuchen Liu", "Jinqiang Cui", "Hong Zhang"], "title": "TrackingMiM: Efficient Mamba-in-Mamba Serialization for Real-time UAV Object Tracking", "comment": "12 pages", "summary": "The Vision Transformer (ViT) model has long struggled with the challenge of\nquadratic complexity, a limitation that becomes especially critical in unmanned\naerial vehicle (UAV) tracking systems, where data must be processed in real\ntime. In this study, we explore the recently proposed State-Space Model, Mamba,\nleveraging its computational efficiency and capability for long-sequence\nmodeling to effectively process dense image sequences in tracking tasks. First,\nwe highlight the issue of temporal inconsistency in existing Mamba-based\nmethods, specifically the failure to account for temporal continuity in the\nMamba scanning mechanism. Secondly, building upon this insight,we propose\nTrackingMiM, a Mamba-in-Mamba architecture, a minimal-computation burden model\nfor handling image sequence of tracking problem. In our framework, the mamba\nscan is performed in a nested way while independently process temporal and\nspatial coherent patch tokens. While the template frame is encoded as query\ntoken and utilized for tracking in every scan. Extensive experiments conducted\non five UAV tracking benchmarks confirm that the proposed TrackingMiM achieves\nstate-of-the-art precision while offering noticeable higher speed in UAV\ntracking.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMamba\u6a21\u578b\u7684TrackingMiM\u67b6\u6784\uff0c\u7528\u4e8e\u89e3\u51b3\u65e0\u4eba\u673a\u8ddf\u8e2a\u4efb\u52a1\u4e2dViT\u6a21\u578b\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u957f\u5e8f\u5217\u5efa\u6a21\u548c\u5b9e\u65f6\u5904\u7406\u3002", "motivation": "Vision Transformer (ViT) \u5728\u65e0\u4eba\u673a\u8ddf\u8e2a\u7cfb\u7edf\u4e2d\u56e0\u4e8c\u6b21\u590d\u6742\u5ea6\u95ee\u9898\u96be\u4ee5\u5b9e\u65f6\u5904\u7406\u6570\u636e\uff0c\u800c\u73b0\u6709\u7684Mamba\u65b9\u6cd5\u5728\u65f6\u95f4\u8fde\u7eed\u6027\u5efa\u6a21\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faTrackingMiM\uff0c\u4e00\u79cdMamba-in-Mamba\u67b6\u6784\uff0c\u901a\u8fc7\u5d4c\u5957\u626b\u63cf\u673a\u5236\u72ec\u7acb\u5904\u7406\u65f6\u7a7a\u4e00\u81f4\u7684\u56fe\u50cf\u5757\uff0c\u6a21\u677f\u5e27\u4f5c\u4e3a\u67e5\u8be2\u4ee4\u724c\u7528\u4e8e\u8ddf\u8e2a\u3002", "result": "\u5728\u4e94\u4e2a\u65e0\u4eba\u673a\u8ddf\u8e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTrackingMiM\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6\u548c\u663e\u8457\u66f4\u9ad8\u7684\u901f\u5ea6\u3002", "conclusion": "TrackingMiM\u901a\u8fc7\u6539\u8fdbMamba\u7684\u65f6\u95f4\u8fde\u7eed\u6027\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u8ddf\u8e2a\u7684\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2507.01235", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.01235", "abs": "https://arxiv.org/abs/2507.01235", "authors": ["Bara Rababa", "Bilal Farooq"], "title": "Quantum Machine Learning in Transportation: A Case Study of Pedestrian Stress Modelling", "comment": "Proceedings of IEEE Intelligent Transportation Systems Conference,\n  2025", "summary": "Quantum computing has opened new opportunities to tackle complex machine\nlearning tasks, for instance, high-dimensional data representations commonly\nrequired in intelligent transportation systems. We explore quantum machine\nlearning to model complex skin conductance response (SCR) events that reflect\npedestrian stress in a virtual reality road crossing experiment. For this\npurpose, Quantum Support Vector Machine (QSVM) with an eight-qubit ZZ feature\nmap and a Quantum Neural Network (QNN) using a Tree Tensor Network ansatz and\nan eight-qubit ZZ feature map, were developed on Pennylane. The dataset\nconsists of SCR measurements along with features such as the response amplitude\nand elapsed time, which have been categorized into amplitude-based classes. The\nQSVM achieved good training accuracy, but had an overfitting problem, showing a\nlow test accuracy of 45% and therefore impacting the reliability of the\nclassification model. The QNN model reached a higher test accuracy of 55%,\nmaking it a better classification model than the QSVM and the classic versions.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u5728\u6a21\u62df\u590d\u6742\u76ae\u80a4\u7535\u5bfc\u53cd\u5e94\uff08SCR\uff09\u4e8b\u4ef6\u4e2d\u7684\u5e94\u7528\uff0c\u6bd4\u8f83\u4e86\u91cf\u5b50\u652f\u6301\u5411\u91cf\u673a\uff08QSVM\uff09\u548c\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\uff08QNN\uff09\u7684\u6027\u80fd\u3002", "motivation": "\u5229\u7528\u91cf\u5b50\u8ba1\u7b97\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u9ad8\u7ef4\u6570\u636e\u8868\u793a\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u6a21\u62df\u884c\u4eba\u538b\u529b\u7684SCR\u4e8b\u4ef6\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8ePennylane\u7684QSVM\uff08\u4f7f\u7528\u516b\u91cf\u5b50\u4f4dZZ\u7279\u5f81\u56fe\uff09\u548cQNN\uff08\u4f7f\u7528\u6811\u5f20\u91cf\u7f51\u7edc\u7ed3\u6784\u548c\u516b\u91cf\u5b50\u4f4dZZ\u7279\u5f81\u56fe\uff09\u3002", "result": "QSVM\u8bad\u7ec3\u7cbe\u5ea6\u9ad8\u4f46\u6d4b\u8bd5\u7cbe\u5ea6\u4f4e\uff0845%\uff09\uff0c\u5b58\u5728\u8fc7\u62df\u5408\u95ee\u9898\uff1bQNN\u6d4b\u8bd5\u7cbe\u5ea6\u66f4\u9ad8\uff0855%\uff09\uff0c\u4f18\u4e8eQSVM\u548c\u7ecf\u5178\u6a21\u578b\u3002", "conclusion": "QNN\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u5c55\u73b0\u4e86\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u5728\u590d\u6742\u6570\u636e\u5efa\u6a21\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.01539", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01539", "abs": "https://arxiv.org/abs/2507.01539", "authors": ["Mohammadreza Amirian", "Michael Bach", "Oscar Jimenez-del-Toro", "Christoph Aberle", "Roger Schaer", "Vincent Andrearczyk", "Jean-F\u00e9lix Maestrati", "Maria Martin Asiain", "Kyriakos Flouris", "Markus Obmann", "Clarisse Dromain", "Beno\u00eet Dufour", "Pierre-Alexandre Alois Poletti", "Hendrik von Tengg-Kobligk", "Rolf H\u00fcgli", "Martin Kretzschmar", "Hatem Alkadhi", "Ender Konukoglu", "Henning M\u00fcller", "Bram Stieltjes", "Adrien Depeursinge"], "title": "A Multi-Centric Anthropomorphic 3D CT Phantom-Based Benchmark Dataset for Harmonization", "comment": null, "summary": "Artificial intelligence (AI) has introduced numerous opportunities for human\nassistance and task automation in medicine. However, it suffers from poor\ngeneralization in the presence of shifts in the data distribution. In the\ncontext of AI-based computed tomography (CT) analysis, significant data\ndistribution shifts can be caused by changes in scanner manufacturer,\nreconstruction technique or dose. AI harmonization techniques can address this\nproblem by reducing distribution shifts caused by various acquisition settings.\nThis paper presents an open-source benchmark dataset containing CT scans of an\nanthropomorphic phantom acquired with various scanners and settings, which\npurpose is to foster the development of AI harmonization techniques. Using a\nphantom allows fixing variations attributed to inter- and intra-patient\nvariations. The dataset includes 1378 image series acquired with 13 scanners\nfrom 4 manufacturers across 8 institutions using a harmonized protocol as well\nas several acquisition doses. Additionally, we present a methodology, baseline\nresults and open-source code to assess image- and feature-level stability and\nliver tissue classification, promoting the development of AI harmonization\nstrategies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f00\u6e90\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u4fc3\u8fdbAI\u5728CT\u5206\u6790\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u901a\u8fc7\u51cf\u5c11\u6570\u636e\u5206\u5e03\u504f\u79fb\u3002", "motivation": "\u89e3\u51b3AI\u5728\u533b\u5b66CT\u5206\u6790\u4e2d\u56e0\u6570\u636e\u5206\u5e03\u504f\u79fb\u5bfc\u81f4\u7684\u6cdb\u5316\u80fd\u529b\u5dee\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5305\u542b1378\u4e2a\u56fe\u50cf\u7cfb\u5217\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e0d\u540c\u626b\u63cf\u4eea\u548c\u8bbe\u7f6e\uff0c\u5e76\u63d0\u51fa\u8bc4\u4f30\u56fe\u50cf\u548c\u7279\u5f81\u7a33\u5b9a\u6027\u7684\u65b9\u6cd5\u3002", "result": "\u63d0\u4f9b\u4e86\u57fa\u7ebf\u7ed3\u679c\u548c\u5f00\u6e90\u4ee3\u7801\uff0c\u652f\u6301AI\u534f\u8c03\u7b56\u7565\u7684\u5f00\u53d1\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u6709\u52a9\u4e8e\u63a8\u52a8AI\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u7814\u7a76\u3002"}}
{"id": "2507.01470", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01470", "abs": "https://arxiv.org/abs/2507.01470", "authors": ["Yannick Molinghen", "Tom Lenaerts"], "title": "Zero-Incentive Dynamics: a look at reward sparsity through the lens of unrewarded subgoals", "comment": "Accepted at \"Finding the Frame 2025\", workshop at RLC", "summary": "This work re-examines the commonly held assumption that the frequency of\nrewards is a reliable measure of task difficulty in reinforcement learning. We\nidentify and formalize a structural challenge that undermines the effectiveness\nof current policy learning methods: when essential subgoals do not directly\nyield rewards. We characterize such settings as exhibiting zero-incentive\ndynamics, where transitions critical to success remain unrewarded. We show that\nstate-of-the-art deep subgoal-based algorithms fail to leverage these dynamics\nand that learning performance is highly sensitive to the temporal proximity\nbetween subgoal completion and eventual reward. These findings reveal a\nfundamental limitation in current approaches and point to the need for\nmechanisms that can infer latent task structure without relying on immediate\nincentives.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5956\u52b1\u9891\u7387\u4e0d\u80fd\u53ef\u9760\u8861\u91cf\u4efb\u52a1\u96be\u5ea6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u672a\u76f4\u63a5\u5956\u52b1\u5173\u952e\u5b50\u76ee\u6807\u65f6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u91cd\u65b0\u9a8c\u8bc1\u5956\u52b1\u9891\u7387\u4f5c\u4e3a\u4efb\u52a1\u96be\u5ea6\u6307\u6807\u7684\u5047\u8bbe\uff0c\u5e76\u63a2\u8ba8\u672a\u76f4\u63a5\u5956\u52b1\u5173\u952e\u5b50\u76ee\u6807\u5bf9\u7b56\u7565\u5b66\u4e60\u7684\u5f71\u54cd\u3002", "method": "\u5f62\u5f0f\u5316\u96f6\u6fc0\u52b1\u52a8\u6001\u95ee\u9898\uff0c\u5206\u6790\u73b0\u6709\u6df1\u5ea6\u5b50\u76ee\u6807\u7b97\u6cd5\u5728\u6b64\u7c7b\u52a8\u6001\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u73b0\u6709\u7b97\u6cd5\u65e0\u6cd5\u6709\u6548\u5229\u7528\u96f6\u6fc0\u52b1\u52a8\u6001\uff0c\u5b66\u4e60\u6027\u80fd\u5bf9\u5b50\u76ee\u6807\u5b8c\u6210\u4e0e\u6700\u7ec8\u5956\u52b1\u7684\u65f6\u95f4\u63a5\u8fd1\u6027\u9ad8\u5ea6\u654f\u611f\u3002", "conclusion": "\u5f53\u524d\u65b9\u6cd5\u5b58\u5728\u6839\u672c\u6027\u5c40\u9650\uff0c\u9700\u5f00\u53d1\u80fd\u63a8\u65ad\u6f5c\u5728\u4efb\u52a1\u7ed3\u6784\u800c\u4e0d\u4f9d\u8d56\u5373\u65f6\u5956\u52b1\u7684\u673a\u5236\u3002"}}
{"id": "2507.01557", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01557", "abs": "https://arxiv.org/abs/2507.01557", "authors": ["Marcin Kowlaczyk", "Tomasz Kryjak"], "title": "Interpolation-Based Event Visual Data Filtering Algorithms", "comment": "This paper has been accepted for publication at the IEEE Conference\n  on Computer Vision and Pattern Recognition (CVPR) Workshops, Vancouver, 2023.\n  Copyright IEEE", "summary": "The field of neuromorphic vision is developing rapidly, and event cameras are\nfinding their way into more and more applications. However, the data stream\nfrom these sensors is characterised by significant noise. In this paper, we\npropose a method for event data that is capable of removing approximately 99\\%\nof noise while preserving the majority of the valid signal. We have proposed\nfour algorithms based on the matrix of infinite impulse response (IIR) filters\nmethod. We compared them on several event datasets that were further modified\nby adding artificially generated noise and noise recorded with dynamic vision\nsensor. The proposed methods use about 30KB of memory for a sensor with a\nresolution of 1280 x 720 and is therefore well suited for implementation in\nembedded devices.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u9650\u8109\u51b2\u54cd\u5e94\uff08IIR\uff09\u6ee4\u6ce2\u5668\u77e9\u9635\u7684\u65b9\u6cd5\uff0c\u80fd\u53bb\u9664\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u4e2d\u7ea699%\u7684\u566a\u58f0\uff0c\u540c\u65f6\u4fdd\u7559\u6709\u6548\u4fe1\u53f7\uff0c\u9002\u7528\u4e8e\u5d4c\u5165\u5f0f\u8bbe\u5907\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u6d41\u4e2d\u5b58\u5728\u663e\u8457\u566a\u58f0\uff0c\u5f71\u54cd\u5e94\u7528\u6548\u679c\uff0c\u9700\u9ad8\u6548\u53bb\u566a\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u56db\u79cd\u57fa\u4e8eIIR\u6ee4\u6ce2\u5668\u77e9\u9635\u7684\u7b97\u6cd5\uff0c\u5e76\u5728\u6dfb\u52a0\u4eba\u5de5\u566a\u58f0\u548c\u52a8\u6001\u89c6\u89c9\u4f20\u611f\u5668\u566a\u58f0\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u65b9\u6cd5\u80fd\u53bb\u9664\u7ea699%\u7684\u566a\u58f0\uff0c\u4ec5\u970030KB\u5185\u5b58\uff0c\u9002\u7528\u4e8e1280x720\u5206\u8fa8\u7387\u7684\u4f20\u611f\u5668\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u3001\u4f4e\u5185\u5b58\u5360\u7528\uff0c\u9002\u5408\u5d4c\u5165\u5f0f\u8bbe\u5907\u5b9e\u73b0\u3002"}}
{"id": "2507.01573", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01573", "abs": "https://arxiv.org/abs/2507.01573", "authors": ["Hao Wang", "Keyan Hu", "Xin Guo", "Haifeng Li", "Chao Tao"], "title": "A Gift from the Integration of Discriminative and Diffusion-based Generative Learning: Boundary Refinement Remote Sensing Semantic Segmentation", "comment": "20 pages, 14 figures", "summary": "Remote sensing semantic segmentation must address both what the ground\nobjects are within an image and where they are located. Consequently,\nsegmentation models must ensure not only the semantic correctness of\nlarge-scale patches (low-frequency information) but also the precise\nlocalization of boundaries between patches (high-frequency information).\nHowever, most existing approaches rely heavily on discriminative learning,\nwhich excels at capturing low-frequency features, while overlooking its\ninherent limitations in learning high-frequency features for semantic\nsegmentation. Recent studies have revealed that diffusion generative models\nexcel at generating high-frequency details. Our theoretical analysis confirms\nthat the diffusion denoising process significantly enhances the model's ability\nto learn high-frequency features; however, we also observe that these models\nexhibit insufficient semantic inference for low-frequency features when guided\nsolely by the original image. Therefore, we integrate the strengths of both\ndiscriminative and generative learning, proposing the Integration of\nDiscriminative and diffusion-based Generative learning for Boundary Refinement\n(IDGBR) framework. The framework first generates a coarse segmentation map\nusing a discriminative backbone model. This map and the original image are fed\ninto a conditioning guidance network to jointly learn a guidance representation\nsubsequently leveraged by an iterative denoising diffusion process refining the\ncoarse segmentation. Extensive experiments across five remote sensing semantic\nsegmentation datasets (binary and multi-class segmentation) confirm our\nframework's capability of consistent boundary refinement for coarse results\nfrom diverse discriminative architectures. The source code will be available at\nhttps://github.com/KeyanHu-git/IDGBR.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5224\u522b\u5f0f\u5b66\u4e60\u548c\u6269\u6563\u751f\u6210\u5b66\u4e60\u7684\u6846\u67b6\uff08IDGBR\uff09\uff0c\u7528\u4e8e\u63d0\u5347\u9065\u611f\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u4e2d\u8fb9\u754c\u7684\u9ad8\u9891\u7279\u5f81\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5224\u522b\u5f0f\u5b66\u4e60\uff0c\u64c5\u957f\u4f4e\u9891\u7279\u5f81\u4f46\u5ffd\u7565\u9ad8\u9891\u8fb9\u754c\u7ec6\u8282\u3002\u6269\u6563\u751f\u6210\u6a21\u578b\u64c5\u957f\u9ad8\u9891\u7ec6\u8282\u751f\u6210\uff0c\u4f46\u8bed\u4e49\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u5224\u522b\u5f0f\u548c\u751f\u6210\u5f0f\u5b66\u4e60\uff0c\u5148\u7528\u5224\u522b\u5f0f\u6a21\u578b\u751f\u6210\u7c97\u5206\u5272\u56fe\uff0c\u518d\u901a\u8fc7\u6269\u6563\u53bb\u566a\u8fc7\u7a0b\u7ec6\u5316\u8fb9\u754c\u3002", "result": "\u5728\u4e94\u4e2a\u9065\u611f\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86IDGBR\u6846\u67b6\u5bf9\u8fb9\u754c\u7ec6\u5316\u7684\u6709\u6548\u6027\u3002", "conclusion": "IDGBR\u6846\u67b6\u6210\u529f\u6574\u5408\u4e86\u4e24\u79cd\u5b66\u4e60\u7684\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fb9\u754c\u5206\u5272\u7684\u7cbe\u5ea6\u3002"}}
{"id": "2507.01551", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01551", "abs": "https://arxiv.org/abs/2507.01551", "authors": ["Wu Fei", "Hao Kong", "Shuxian Liang", "Yang Lin", "Yibo Yang", "Jing Tang", "Lei Chen", "Xiansheng Hua"], "title": "Self-Guided Process Reward Optimization with Masked Step Advantage for Process Reinforcement Learning", "comment": null, "summary": "Process Reinforcement Learning~(PRL) has demonstrated considerable potential\nin enhancing the reasoning capabilities of Large Language Models~(LLMs).\nHowever, introducing additional process reward models incurs substantial\ncomputational overhead, and there is no unified theoretical framework for\nprocess-level advantage estimation. To bridge this gap, we propose\n\\textbf{S}elf-Guided \\textbf{P}rocess \\textbf{R}eward\n\\textbf{O}ptimization~(\\textbf{SPRO}), a novel framework that enables\nprocess-aware RL through two key innovations: (1) we first theoretically\ndemonstrate that process rewards can be derived intrinsically from the policy\nmodel itself, and (2) we introduce well-defined cumulative process rewards and\n\\textbf{M}asked \\textbf{S}tep \\textbf{A}dvantage (\\textbf{MSA}), which\nfacilitates rigorous step-wise action advantage estimation within shared-prompt\nsampling groups. Our experimental results demonstrate that SPRO outperforms\nvaniila GRPO with 3.4x higher training efficiency and a 17.5\\% test accuracy\nimprovement. Furthermore, SPRO maintains a stable and elevated policy entropy\nthroughout training while reducing the average response length by approximately\n$1/3$, evidencing sufficient exploration and prevention of reward hacking.\nNotably, SPRO incurs no additional computational overhead compared to\noutcome-supervised RL methods such as GRPO, which benefit industrial\nimplementation.", "AI": {"tldr": "SPRO\u662f\u4e00\u79cd\u81ea\u5f15\u5bfc\u8fc7\u7a0b\u5956\u52b1\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5185\u5728\u63a8\u5bfc\u8fc7\u7a0b\u5956\u52b1\u548c\u5f15\u5165\u7d2f\u79ef\u8fc7\u7a0b\u5956\u52b1\u4e0e\u63a9\u7801\u6b65\u9aa4\u4f18\u52bf\uff0c\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6d4b\u8bd5\u51c6\u786e\u6027\uff0c\u540c\u65f6\u907f\u514d\u4e86\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u89e3\u51b3\u8fc7\u7a0b\u5f3a\u5316\u5b66\u4e60\u4e2d\u5f15\u5165\u989d\u5916\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u5e26\u6765\u7684\u8ba1\u7b97\u5f00\u9500\u548c\u7f3a\u4e4f\u7edf\u4e00\u7406\u8bba\u6846\u67b6\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faSPRO\u6846\u67b6\uff0c\u5305\u62ec\u5185\u5728\u63a8\u5bfc\u8fc7\u7a0b\u5956\u52b1\u548c\u5b9a\u4e49\u7d2f\u79ef\u8fc7\u7a0b\u5956\u52b1\u4e0e\u63a9\u7801\u6b65\u9aa4\u4f18\u52bf\uff08MSA\uff09\u3002", "result": "SPRO\u5728\u8bad\u7ec3\u6548\u7387\u4e0a\u6bd4GRPO\u9ad83.4\u500d\uff0c\u6d4b\u8bd5\u51c6\u786e\u7387\u63d0\u534717.5%\uff0c\u540c\u65f6\u51cf\u5c11\u54cd\u5e94\u957f\u5ea6\u7ea61/3\u3002", "conclusion": "SPRO\u5728\u63d0\u5347\u6027\u80fd\u7684\u540c\u65f6\u907f\u514d\u4e86\u989d\u5916\u8ba1\u7b97\u5f00\u9500\uff0c\u9002\u5408\u5de5\u4e1a\u5e94\u7528\u3002"}}
{"id": "2507.01285", "categories": ["cs.LG", "cs.DC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.01285", "abs": "https://arxiv.org/abs/2507.01285", "authors": ["Aymen Rayane Khouas", "Mohamed Reda Bouadjenek", "Hakim Hacid", "Sunil Aryal"], "title": "Far From Sight, Far From Mind: Inverse Distance Weighting for Graph Federated Recommendation", "comment": "17 pages, 5 figures", "summary": "Graph federated recommendation systems offer a privacy-preserving alternative\nto traditional centralized recommendation architectures, which often raise\nconcerns about data security. While federated learning enables personalized\nrecommendations without exposing raw user data, existing aggregation methods\noverlook the unique properties of user embeddings in this setting. Indeed,\ntraditional aggregation methods fail to account for their complexity and the\ncritical role of user similarity in recommendation effectiveness. Moreover,\nevolving user interactions require adaptive aggregation while preserving the\ninfluence of high-relevance anchor users (the primary users before expansion in\ngraph-based frameworks). To address these limitations, we introduce\nDist-FedAvg, a novel distance-based aggregation method designed to enhance\npersonalization and aggregation efficiency in graph federated learning. Our\nmethod assigns higher aggregation weights to users with similar embeddings,\nwhile ensuring that anchor users retain significant influence in local updates.\nEmpirical evaluations on multiple datasets demonstrate that Dist-FedAvg\nconsistently outperforms baseline aggregation techniques, improving\nrecommendation accuracy while maintaining seamless integration into existing\nfederated learning frameworks.", "AI": {"tldr": "Dist-FedAvg\u662f\u4e00\u79cd\u57fa\u4e8e\u8ddd\u79bb\u7684\u805a\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u56fe\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u4e2a\u6027\u5316\u548c\u805a\u5408\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u805a\u5408\u65b9\u6cd5\u672a\u8003\u8651\u7528\u6237\u5d4c\u5165\u7684\u590d\u6742\u6027\u548c\u7528\u6237\u76f8\u4f3c\u6027\u5728\u63a8\u8350\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u9ad8\u76f8\u5173\u6027\u951a\u7528\u6237\u7684\u9002\u5e94\u6027\u4fdd\u62a4\u3002", "method": "\u63d0\u51faDist-FedAvg\uff0c\u901a\u8fc7\u4e3a\u76f8\u4f3c\u5d4c\u5165\u7528\u6237\u5206\u914d\u66f4\u9ad8\u6743\u91cd\uff0c\u5e76\u4fdd\u6301\u951a\u7528\u6237\u5728\u672c\u5730\u66f4\u65b0\u4e2d\u7684\u5f71\u54cd\u529b\uff0c\u4f18\u5316\u805a\u5408\u8fc7\u7a0b\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDist-FedAvg\u5728\u63a8\u8350\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u4e2d\u3002", "conclusion": "Dist-FedAvg\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u805a\u5408\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u63a8\u8350\u6548\u679c\u548c\u9690\u79c1\u4fdd\u62a4\u80fd\u529b\u3002"}}
{"id": "2507.01586", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01586", "abs": "https://arxiv.org/abs/2507.01586", "authors": ["Bryan Constantine Sadihin", "Michael Hua Wang", "Shei Pern Chua", "Hang Su"], "title": "SketchColour: Channel Concat Guided DiT-based Sketch-to-Colour Pipeline for 2D Animation", "comment": "Project page and code: https://bconstantine.github.io/SketchColour", "summary": "The production of high-quality 2D animation is highly labor-intensive\nprocess, as animators are currently required to draw and color a large number\nof frames by hand. We present SketchColour, the first sketch-to-colour pipeline\nfor 2D animation built on a diffusion transformer (DiT) backbone. By replacing\nthe conventional U-Net denoiser with a DiT-style architecture and injecting\nsketch information via lightweight channel-concatenation adapters accompanied\nwith LoRA finetuning, our method natively integrates conditioning without the\nparameter and memory bloat of a duplicated ControlNet, greatly reducing\nparameter count and GPU memory usage. Evaluated on the SAKUGA dataset,\nSketchColour outperforms previous state-of-the-art video colourization methods\nacross all metrics, despite using only half the training data of competing\nmodels. Our approach produces temporally coherent animations with minimal\nartifacts such as colour bleeding or object deformation. Our code is available\nat: https://bconstantine.github.io/SketchColour .", "AI": {"tldr": "SketchColour\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u53d8\u538b\u5668\uff08DiT\uff09\u76842D\u52a8\u753b\u8349\u56fe\u5230\u8272\u5f69\u8f6c\u6362\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u548cLoRA\u5fae\u8c03\uff0c\u663e\u8457\u51cf\u5c11\u53c2\u6570\u548cGPU\u5185\u5b58\u4f7f\u7528\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf2D\u52a8\u753b\u5236\u4f5c\u9700\u8981\u5927\u91cf\u624b\u5de5\u7ed8\u5236\u548c\u4e0a\u8272\uff0c\u8017\u65f6\u8017\u529b\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u5316\u5de5\u5177\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u4f7f\u7528DiT\u67b6\u6784\u66ff\u4ee3\u4f20\u7edfU-Net\u53bb\u566a\u5668\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u901a\u9053\u8fde\u63a5\u9002\u914d\u5668\u548cLoRA\u5fae\u8c03\u6ce8\u5165\u8349\u56fe\u4fe1\u606f\uff0c\u51cf\u5c11\u53c2\u6570\u548c\u5185\u5b58\u5360\u7528\u3002", "result": "\u5728SAKUGA\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u89c6\u9891\u4e0a\u8272\u65b9\u6cd5\uff0c\u8bad\u7ec3\u6570\u636e\u4ec5\u4e3a\u7ade\u4e89\u6a21\u578b\u7684\u4e00\u534a\uff0c\u4e14\u751f\u6210\u52a8\u753b\u65f6\u95f4\u4e00\u81f4\u3001\u4f2a\u5f71\u5c11\u3002", "conclusion": "SketchColour\u4e3a2D\u52a8\u753b\u5236\u4f5c\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01587", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.01587", "abs": "https://arxiv.org/abs/2507.01587", "authors": ["Youngjin Oh", "Junhyeong Kwon", "Keuntek Lee", "Nam Ik Cho"], "title": "Towards Controllable Real Image Denoising with Camera Parameters", "comment": "Accepted for publication in ICIP 2025, IEEE International Conference\n  on Image Processing", "summary": "Recent deep learning-based image denoising methods have shown impressive\nperformance; however, many lack the flexibility to adjust the denoising\nstrength based on the noise levels, camera settings, and user preferences. In\nthis paper, we introduce a new controllable denoising framework that adaptively\nremoves noise from images by utilizing information from camera parameters.\nSpecifically, we focus on ISO, shutter speed, and F-number, which are closely\nrelated to noise levels. We convert these selected parameters into a vector to\ncontrol and enhance the performance of the denoising network. Experimental\nresults show that our method seamlessly adds controllability to standard\ndenoising neural networks and improves their performance. Code is available at\nhttps://github.com/OBAKSA/CPADNet.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u63a7\u53bb\u566a\u6846\u67b6\uff0c\u5229\u7528\u76f8\u673a\u53c2\u6570\uff08\u5982ISO\u3001\u5feb\u95e8\u901f\u5ea6\u548c\u5149\u5708\u503c\uff09\u52a8\u6001\u8c03\u6574\u53bb\u566a\u5f3a\u5ea6\uff0c\u63d0\u5347\u53bb\u566a\u7f51\u7edc\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u53bb\u566a\u65f6\u7f3a\u4e4f\u6839\u636e\u566a\u58f0\u6c34\u5e73\u3001\u76f8\u673a\u8bbe\u7f6e\u548c\u7528\u6237\u504f\u597d\u8c03\u6574\u53bb\u566a\u5f3a\u5ea6\u7684\u7075\u6d3b\u6027\u3002", "method": "\u5c06\u76f8\u673a\u53c2\u6570\uff08ISO\u3001\u5feb\u95e8\u901f\u5ea6\u3001\u5149\u5708\u503c\uff09\u8f6c\u6362\u4e3a\u5411\u91cf\uff0c\u7528\u4e8e\u63a7\u5236\u53bb\u566a\u7f51\u7edc\uff0c\u589e\u5f3a\u5176\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u65e0\u7f1d\u5730\u4e3a\u6807\u51c6\u53bb\u566a\u795e\u7ecf\u7f51\u7edc\u589e\u52a0\u53ef\u63a7\u6027\uff0c\u5e76\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u56fe\u50cf\u53bb\u566a\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u9ad8\u6548\u7684\u63a7\u5236\u65b9\u5f0f\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.01679", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01679", "abs": "https://arxiv.org/abs/2507.01679", "authors": ["Zeyu Huang", "Tianhao Cheng", "Zihan Qiu", "Zili Wang", "Yinghui Xu", "Edoardo M. Ponti", "Ivan Titov"], "title": "Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling", "comment": "Work in progress", "summary": "Existing post-training techniques for large language models are broadly\ncategorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning\n(RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking\ndemonstration data but can lead to problematic generalization as a form of\nbehavior cloning. Conversely, RFT can significantly enhance a model's\nperformance but is prone to learn unexpected behaviors, and its performance is\nhighly sensitive to the initial policy. In this paper, we propose a unified\nview of these methods and introduce Prefix-RFT, a hybrid approach that\nsynergizes learning from both demonstration and exploration. Using mathematical\nreasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is\nboth simple and effective. It not only surpasses the performance of standalone\nSFT and RFT but also outperforms parallel mixed-policy RFT methods. A key\nadvantage is its seamless integration into existing open-source frameworks,\nrequiring only minimal modifications to the standard RFT pipeline. Our analysis\nhighlights the complementary nature of SFT and RFT, and validates that\nPrefix-RFT effectively harmonizes these two learning paradigms. Furthermore,\nablation studies confirm the method's robustness to variations in the quality\nand quantity of demonstration data. We hope this work offers a new perspective\non LLM post-training, suggesting that a unified paradigm that judiciously\nintegrates demonstration and exploration could be a promising direction for\nfuture research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPrefix-RFT\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u5f3a\u5316\u5fae\u8c03\uff08RFT\uff09\u7684\u4f18\u52bf\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u6846\u67b6\u4e2d\u3002", "motivation": "\u73b0\u6709\u7684SFT\u548cRFT\u65b9\u6cd5\u5404\u6709\u4f18\u7f3a\u70b9\uff0cSFT\u5bb9\u6613\u8fc7\u5ea6\u6a21\u4eff\u6f14\u793a\u6570\u636e\uff0c\u800cRFT\u6027\u80fd\u4e0d\u7a33\u5b9a\u4e14\u53ef\u80fd\u5b66\u4e60\u5230\u610f\u5916\u884c\u4e3a\u3002\u8bba\u6587\u65e8\u5728\u7edf\u4e00\u8fd9\u4e24\u79cd\u65b9\u6cd5\uff0c\u63d0\u51fa\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faPrefix-RFT\uff0c\u4e00\u79cd\u7ed3\u5408\u6f14\u793a\u548c\u63a2\u7d22\u5b66\u4e60\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "Prefix-RFT\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u5355\u72ec\u7684SFT\u548cRFT\uff0c\u4e14\u5bf9\u6f14\u793a\u6570\u636e\u7684\u8d28\u91cf\u548c\u6570\u91cf\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "Prefix-RFT\u5c55\u793a\u4e86\u7edf\u4e00SFT\u548cRFT\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765LLM\u540e\u8bad\u7ec3\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.01590", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01590", "abs": "https://arxiv.org/abs/2507.01590", "authors": ["Ameer Hamza", "Zuhaib Hussain But", "Umar Arif", "Samiya", "M. Abdullah Asad", "Muhammad Naeem"], "title": "Autonomous AI Surveillance: Multimodal Deep Learning for Cognitive and Behavioral Monitoring", "comment": null, "summary": "This study presents a novel classroom surveillance system that integrates\nmultiple modalities, including drowsiness, tracking of mobile phone usage, and\nface recognition,to assess student attentiveness with enhanced precision.The\nsystem leverages the YOLOv8 model to detect both mobile phone and sleep\nusage,(Ghatge et al., 2024) while facial recognition is achieved through\nLResNet Occ FC body tracking using YOLO and MTCNN.(Durai et al., 2024) These\nmodels work in synergy to provide comprehensive, real-time monitoring, offering\ninsights into student engagement and behavior.(S et al., 2023) The framework is\ntrained on specialized datasets, such as the RMFD dataset for face recognition\nand a Roboflow dataset for mobile phone detection. The extensive evaluation of\nthe system shows promising results. Sleep detection achieves 97. 42% mAP@50,\nface recognition achieves 86. 45% validation accuracy and mobile phone\ndetection reach 85. 89% mAP@50. The system is implemented within a core PHP web\napplication and utilizes ESP32-CAM hardware for seamless data capture.(Neto et\nal., 2024) This integrated approach not only enhances classroom monitoring, but\nalso ensures automatic attendance recording via face recognition as students\nremain seated in the classroom, offering scalability for diverse educational\nenvironments.(Banada,2025)", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6559\u5ba4\u76d1\u63a7\u7cfb\u7edf\uff0c\u901a\u8fc7\u68c0\u6d4b\u5b66\u751f\u7761\u610f\u3001\u624b\u673a\u4f7f\u7528\u548c\u4eba\u8138\u8bc6\u522b\u6765\u8bc4\u4f30\u5b66\u751f\u6ce8\u610f\u529b\uff0c\u5229\u7528YOLOv8\u548cLResNet Occ FC\u7b49\u6280\u672f\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5b9e\u65f6\u76d1\u6d4b\u3002", "motivation": "\u63d0\u5347\u6559\u5ba4\u4e2d\u5b66\u751f\u6ce8\u610f\u529b\u8bc4\u4f30\u7684\u7cbe\u786e\u6027\u548c\u5b9e\u65f6\u6027\uff0c\u540c\u65f6\u5b9e\u73b0\u81ea\u52a8\u8003\u52e4\u8bb0\u5f55\u3002", "method": "\u7ed3\u5408YOLOv8\u6a21\u578b\u68c0\u6d4b\u624b\u673a\u548c\u7761\u7720\u884c\u4e3a\uff0cLResNet Occ FC\u8fdb\u884c\u4eba\u8138\u8bc6\u522b\uff0c\u4f7f\u7528ESP32-CAM\u786c\u4ef6\u548cPHP\u6846\u67b6\u5b9e\u73b0\u7cfb\u7edf\u96c6\u6210\u3002", "result": "\u7761\u7720\u68c0\u6d4bmAP@50\u4e3a97.42%\uff0c\u4eba\u8138\u8bc6\u522b\u9a8c\u8bc1\u51c6\u786e\u7387\u4e3a86.45%\uff0c\u624b\u673a\u68c0\u6d4bmAP@50\u4e3a85.89%\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u6559\u80b2\u73af\u5883\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u76d1\u63a7\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u81ea\u52a8\u8003\u52e4\u529f\u80fd\u3002"}}
{"id": "2507.01735", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01735", "abs": "https://arxiv.org/abs/2507.01735", "authors": ["Kai Chen", "Ruiyuan Gao", "Lanqing Hong", "Hang Xu", "Xu Jia", "Holger Caesar", "Dengxin Dai", "Bingbing Liu", "Dzmitry Tsishkou", "Songcen Xu", "Chunjing Xu", "Qiang Xu", "Huchuan Lu", "Dit-Yan Yeung"], "title": "ECCV 2024 W-CODA: 1st Workshop on Multimodal Perception and Comprehension of Corner Cases in Autonomous Driving", "comment": "ECCV 2024. Workshop page: https://coda-dataset.github.io/w-coda2024/", "summary": "In this paper, we present details of the 1st W-CODA workshop, held in\nconjunction with the ECCV 2024. W-CODA aims to explore next-generation\nsolutions for autonomous driving corner cases, empowered by state-of-the-art\nmultimodal perception and comprehension techniques. 5 Speakers from both\nacademia and industry are invited to share their latest progress and opinions.\nWe collect research papers and hold a dual-track challenge, including both\ncorner case scene understanding and generation. As the pioneering effort, we\nwill continuously bridge the gap between frontier autonomous driving techniques\nand fully intelligent, reliable self-driving agents robust towards corner\ncases.", "AI": {"tldr": "\u4ecb\u7ecd\u9996\u5c4aW-CODA\u7814\u8ba8\u4f1a\u7684\u7ec6\u8282\uff0c\u805a\u7126\u81ea\u52a8\u9a7e\u9a76\u6781\u7aef\u573a\u666f\u7684\u4e0b\u4e00\u4ee3\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u611f\u77e5\u4e0e\u7406\u89e3\u6280\u672f\u3002", "motivation": "\u63a2\u7d22\u524d\u6cbf\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u5728\u6781\u7aef\u573a\u666f\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u5347\u667a\u80fd\u9a7e\u9a76\u7684\u53ef\u9760\u6027\u4e0e\u9c81\u68d2\u6027\u3002", "method": "\u9080\u8bf75\u4f4d\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\u4e13\u5bb6\u5206\u4eab\u6700\u65b0\u8fdb\u5c55\uff0c\u6536\u96c6\u7814\u7a76\u8bba\u6587\u5e76\u4e3e\u529e\u53cc\u8f68\u6311\u6218\u8d5b\uff08\u6781\u7aef\u573a\u666f\u7406\u89e3\u4e0e\u751f\u6210\uff09\u3002", "result": "\u7814\u8ba8\u4f1a\u4f5c\u4e3a\u5148\u9a71\u6027\u52aa\u529b\uff0c\u65e8\u5728\u5f25\u5408\u524d\u6cbf\u6280\u672f\u4e0e\u5b8c\u5168\u667a\u80fd\u3001\u53ef\u9760\u7684\u81ea\u52a8\u9a7e\u9a76\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "W-CODA\u5c06\u6301\u7eed\u63a8\u52a8\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u5728\u6781\u7aef\u573a\u666f\u4e2d\u7684\u521b\u65b0\u4e0e\u5e94\u7528\u3002"}}
{"id": "2507.01522", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.01522", "abs": "https://arxiv.org/abs/2507.01522", "authors": ["Koen Ponse", "Jan Felix Kleuker", "Aske Plaat", "Thomas Moerland"], "title": "Chargax: A JAX Accelerated EV Charging Simulator", "comment": "Accepted at RLC 2025", "summary": "Deep Reinforcement Learning can play a key role in addressing sustainable\nenergy challenges. For instance, many grid systems are heavily congested,\nhighlighting the urgent need to enhance operational efficiency. However,\nreinforcement learning approaches have traditionally been slow due to the high\nsample complexity and expensive simulation requirements. While recent works\nhave effectively used GPUs to accelerate data generation by converting\nenvironments to JAX, these works have largely focussed on classical toy\nproblems. This paper introduces Chargax, a JAX-based environment for realistic\nsimulation of electric vehicle charging stations designed for accelerated\ntraining of RL agents. We validate our environment in a variety of scenarios\nbased on real data, comparing reinforcement learning agents against baselines.\nChargax delivers substantial computational performance improvements of over\n100x-1000x over existing environments. Additionally, Chargax' modular\narchitecture enables the representation of diverse real-world charging station\nconfigurations.", "AI": {"tldr": "Chargax\u662f\u4e00\u4e2a\u57fa\u4e8eJAX\u7684\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u7ad9\u4eff\u771f\u73af\u5883\uff0c\u663e\u8457\u63d0\u5347RL\u8bad\u7ec3\u6548\u7387\uff0c\u6027\u80fd\u6bd4\u73b0\u6709\u73af\u5883\u5feb100-1000\u500d\u3002", "motivation": "\u7535\u7f51\u7cfb\u7edf\u62e5\u5835\u95ee\u9898\u6025\u9700\u63d0\u5347\u8fd0\u8425\u6548\u7387\uff0c\u4f46\u4f20\u7edfRL\u65b9\u6cd5\u56e0\u9ad8\u6837\u672c\u590d\u6742\u6027\u548c\u6602\u8d35\u4eff\u771f\u9700\u6c42\u800c\u7f13\u6162\u3002", "method": "\u5f00\u53d1\u4e86Chargax\uff0c\u4e00\u4e2a\u57fa\u4e8eJAX\u7684\u4eff\u771f\u73af\u5883\uff0c\u652f\u6301\u771f\u5b9e\u6570\u636e\u573a\u666f\u4e0b\u7684RL\u8bad\u7ec3\u3002", "result": "Chargax\u5728\u8ba1\u7b97\u6027\u80fd\u4e0a\u6bd4\u73b0\u6709\u73af\u5883\u5feb100-1000\u500d\uff0c\u5e76\u80fd\u6a21\u62df\u591a\u6837\u5316\u7684\u771f\u5b9e\u5145\u7535\u7ad9\u914d\u7f6e\u3002", "conclusion": "Chargax\u4e3a\u53ef\u6301\u7eed\u80fd\u6e90\u6311\u6218\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684RL\u8bad\u7ec3\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01603", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01603", "abs": "https://arxiv.org/abs/2507.01603", "authors": ["Yue-Jiang Dong", "Wang Zhao", "Jiale Xu", "Ying Shan", "Song-Hai Zhang"], "title": "DepthSync: Diffusion Guidance-Based Depth Synchronization for Scale- and Geometry-Consistent Video Depth Estimation", "comment": "Accepted by ICCV 2025", "summary": "Diffusion-based video depth estimation methods have achieved remarkable\nsuccess with strong generalization ability. However, predicting depth for long\nvideos remains challenging. Existing methods typically split videos into\noverlapping sliding windows, leading to accumulated scale discrepancies across\ndifferent windows, particularly as the number of windows increases.\nAdditionally, these methods rely solely on 2D diffusion priors, overlooking the\ninherent 3D geometric structure of video depths, which results in geometrically\ninconsistent predictions. In this paper, we propose DepthSync, a novel,\ntraining-free framework using diffusion guidance to achieve scale- and\ngeometry-consistent depth predictions for long videos. Specifically, we\nintroduce scale guidance to synchronize the depth scale across windows and\ngeometry guidance to enforce geometric alignment within windows based on the\ninherent 3D constraints in video depths. These two terms work synergistically,\nsteering the denoising process toward consistent depth predictions. Experiments\non various datasets validate the effectiveness of our method in producing depth\nestimates with improved scale and geometry consistency, particularly for long\nvideos.", "AI": {"tldr": "DepthSync\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u6563\u5f15\u5bfc\u5b9e\u73b0\u957f\u89c6\u9891\u6df1\u5ea6\u4f30\u8ba1\u7684\u5c3a\u5ea6\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u957f\u89c6\u9891\u65f6\u5b58\u5728\u5c3a\u5ea6\u4e0d\u4e00\u81f4\u548c\u51e0\u4f55\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u4f9d\u8d562D\u6269\u6563\u5148\u9a8c\u800c\u5ffd\u7565\u4e863D\u51e0\u4f55\u7ed3\u6784\u3002", "method": "\u5f15\u5165\u5c3a\u5ea6\u5f15\u5bfc\u548c\u51e0\u4f55\u5f15\u5bfc\uff0c\u5206\u522b\u540c\u6b65\u7a97\u53e3\u95f4\u7684\u6df1\u5ea6\u5c3a\u5ea6\u548c\u7a97\u53e3\u5185\u7684\u51e0\u4f55\u5bf9\u9f50\uff0c\u534f\u540c\u4f18\u5316\u53bb\u566a\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86DepthSync\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u80fd\u663e\u8457\u63d0\u5347\u957f\u89c6\u9891\u6df1\u5ea6\u4f30\u8ba1\u7684\u5c3a\u5ea6\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u3002", "conclusion": "DepthSync\u901a\u8fc7\u7ed3\u5408\u5c3a\u5ea6\u548c\u51e0\u4f55\u5f15\u5bfc\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u6df1\u5ea6\u4f30\u8ba1\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898\u3002"}}
{"id": "2507.01752", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.01752", "abs": "https://arxiv.org/abs/2507.01752", "authors": ["Ismail Labiad", "Mathurin Videau", "Matthieu Kowalski", "Marc Schoenauer", "Alessandro Leite", "Julia Kempe", "Olivier Teytaud"], "title": "Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training", "comment": null, "summary": "Gradient-based optimization is the workhorse of deep learning, offering\nefficient and scalable training via backpropagation. However, its reliance on\nlarge volumes of labeled data raises privacy and security concerns such as\nsusceptibility to data poisoning attacks and the risk of overfitting. In\ncontrast, black box optimization methods, which treat the model as an opaque\nfunction, relying solely on function evaluations to guide optimization, offer a\npromising alternative in scenarios where data access is restricted, adversarial\nrisks are high, or overfitting is a concern. However, black box methods also\npose significant challenges, including poor scalability to high-dimensional\nparameter spaces, as prevalent in large language models (LLMs), and high\ncomputational costs due to reliance on numerous model evaluations. This paper\nintroduces BBoxER, an evolutionary black-box method for LLM post-training that\ninduces an information bottleneck via implicit compression of the training\ndata. Leveraging the tractability of information flow, we provide strong\ntheoretical bounds on generalization, differential privacy, susceptibility to\ndata poisoning attacks, and robustness to extraction attacks. BBoxER operates\non top of pre-trained LLMs, offering a lightweight and modular enhancement\nsuitable for deployment in restricted or privacy-sensitive environments, in\naddition to non-vacuous generalization guarantees. In experiments with LLMs, we\ndemonstrate empirically that Retrofitting methods are able to learn, showing\nhow a few iterations of BBoxER improve performance and generalize well on a\nbenchmark of reasoning datasets. This positions BBoxER as an attractive add-on\non top of gradient-based optimization.", "AI": {"tldr": "BBoxER\u662f\u4e00\u79cd\u57fa\u4e8e\u8fdb\u5316\u7684\u9ed1\u76d2\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u540e\u8bad\u7ec3\uff0c\u901a\u8fc7\u9690\u5f0f\u538b\u7f29\u8bad\u7ec3\u6570\u636e\u5f15\u5165\u4fe1\u606f\u74f6\u9888\uff0c\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u548c\u8f7b\u91cf\u7ea7\u589e\u5f3a\u3002", "motivation": "\u89e3\u51b3\u68af\u5ea6\u4f18\u5316\u4f9d\u8d56\u5927\u91cf\u6807\u8bb0\u6570\u636e\u5e26\u6765\u7684\u9690\u79c1\u548c\u5b89\u5168\u6027\u95ee\u9898\uff0c\u4ee5\u53ca\u9ed1\u76d2\u65b9\u6cd5\u5728\u9ad8\u7ef4\u53c2\u6570\u7a7a\u95f4\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002", "method": "\u63d0\u51faBBoxER\u65b9\u6cd5\uff0c\u5229\u7528\u4fe1\u606f\u6d41\u53ef\u8ffd\u8e2a\u6027\uff0c\u63d0\u4f9b\u6cdb\u5316\u3001\u5dee\u5206\u9690\u79c1\u3001\u6297\u6570\u636e\u6bd2\u5316\u548c\u6297\u63d0\u53d6\u653b\u51fb\u7684\u7406\u8bba\u4fdd\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eBBoxER\u5728\u5c11\u91cf\u8fed\u4ee3\u540e\u80fd\u63d0\u5347LLM\u6027\u80fd\u5e76\u5728\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u6cdb\u5316\u826f\u597d\u3002", "conclusion": "BBoxER\u662f\u68af\u5ea6\u4f18\u5316\u7684\u8f7b\u91cf\u7ea7\u8865\u5145\uff0c\u9002\u7528\u4e8e\u9690\u79c1\u654f\u611f\u6216\u53d7\u9650\u73af\u5883\u3002"}}
{"id": "2507.01354", "categories": ["cs.LG", "physics.ao-ph", "86A10 (Primary) 86A22, 68U10 (Secondary)", "J.2; I.4.4"], "pdf": "https://arxiv.org/pdf/2507.01354", "abs": "https://arxiv.org/abs/2507.01354", "authors": ["Chugang Yi", "Minghan Yu", "Weikang Qian", "Yixin Wen", "Haizhao Yang"], "title": "Efficient Kilometer-Scale Precipitation Downscaling with Conditional Wavelet Diffusion", "comment": null, "summary": "Effective hydrological modeling and extreme weather analysis demand\nprecipitation data at a kilometer-scale resolution, which is significantly\nfiner than the 10 km scale offered by standard global products like IMERG. To\naddress this, we propose the Wavelet Diffusion Model (WDM), a generative\nframework that achieves 10x spatial super-resolution (downscaling to 1 km) and\ndelivers a 9x inference speedup over pixel-based diffusion models. WDM is a\nconditional diffusion model that learns the learns the complex structure of\nprecipitation from MRMS radar data directly in the wavelet domain. By focusing\non high-frequency wavelet coefficients, it generates exceptionally realistic\nand detailed 1-km precipitation fields. This wavelet-based approach produces\nvisually superior results with fewer artifacts than pixel-space models, and\ndelivers a significant gains in sampling efficiency. Our results demonstrate\nthat WDM provides a robust solution to the dual challenges of accuracy and\nspeed in geoscience super-resolution, paving the way for more reliable\nhydrological forecasts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u57df\u7684\u751f\u6210\u6a21\u578b\uff08WDM\uff09\uff0c\u7528\u4e8e\u964d\u6c34\u6570\u636e\u768410\u500d\u7a7a\u95f4\u8d85\u5206\u8fa8\u7387\uff08\u964d\u5c3a\u5ea6\u81f31\u516c\u91cc\uff09\uff0c\u5e76\u5728\u63a8\u7406\u901f\u5ea6\u4e0a\u6bd4\u57fa\u4e8e\u50cf\u7d20\u7684\u6269\u6563\u6a21\u578b\u5feb9\u500d\u3002", "motivation": "\u73b0\u6709\u5168\u7403\u964d\u6c34\u6570\u636e\uff08\u5982IMERG\uff09\u5206\u8fa8\u7387\u8f83\u4f4e\uff0810\u516c\u91cc\uff09\uff0c\u65e0\u6cd5\u6ee1\u8db3\u6c34\u6587\u5efa\u6a21\u548c\u6781\u7aef\u5929\u6c14\u5206\u6790\u7684\u9700\u6c42\u3002", "method": "WDM\u662f\u4e00\u79cd\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u76f4\u63a5\u5728\u5c0f\u6ce2\u57df\u4e2d\u5b66\u4e60\u964d\u6c34\u6570\u636e\u7684\u590d\u6742\u7ed3\u6784\uff0c\u901a\u8fc7\u5173\u6ce8\u9ad8\u9891\u5c0f\u6ce2\u7cfb\u6570\u751f\u62101\u516c\u91cc\u5206\u8fa8\u7387\u7684\u964d\u6c34\u573a\u3002", "result": "WDM\u751f\u6210\u7684\u964d\u6c34\u573a\u89c6\u89c9\u6548\u679c\u66f4\u4f18\uff0c\u4f2a\u5f71\u66f4\u5c11\uff0c\u91c7\u6837\u6548\u7387\u663e\u8457\u63d0\u5347\u3002", "conclusion": "WDM\u4e3a\u5730\u7403\u79d1\u5b66\u8d85\u5206\u8fa8\u7387\u4e2d\u7684\u7cbe\u5ea6\u548c\u901f\u5ea6\u6311\u6218\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u66f4\u53ef\u9760\u7684\u6c34\u6587\u9884\u62a5\u3002"}}
{"id": "2507.01607", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01607", "abs": "https://arxiv.org/abs/2507.01607", "authors": ["Quentin Le Roux", "Yannick Teglia", "Teddy Furon", "Philippe Loubet-Moundi", "Eric Bourbao"], "title": "Survivability of Backdoor Attacks on Unconstrained Face Recognition Systems", "comment": null, "summary": "The widespread use of deep learning face recognition raises several security\nconcerns. Although prior works point at existing vulnerabilities, DNN backdoor\nattacks against real-life, unconstrained systems dealing with images captured\nin the wild remain a blind spot of the literature. This paper conducts the\nfirst system-level study of backdoors in deep learning-based face recognition\nsystems. This paper yields four contributions by exploring the feasibility of\nDNN backdoors on these pipelines in a holistic fashion. We demonstrate for the\nfirst time two backdoor attacks on the face detection task: face generation and\nface landmark shift attacks. We then show that face feature extractors trained\nwith large margin losses also fall victim to backdoor attacks. Combining our\nmodels, we then show using 20 possible pipeline configurations and 15 attack\ncases that a single backdoor enables an attacker to bypass the entire function\nof a system. Finally, we provide stakeholders with several best practices and\ncountermeasures.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u6df1\u5ea6\u5b66\u4e60\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u4e2d\u7684\u540e\u95e8\u653b\u51fb\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u9488\u5bf9\u4eba\u8138\u68c0\u6d4b\u4efb\u52a1\u7684\u65b0\u578b\u540e\u95e8\u653b\u51fb\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5927\u89c4\u6a21\u635f\u5931\u8bad\u7ec3\u7684\u7279\u5f81\u63d0\u53d6\u5668\u540c\u6837\u6613\u53d7\u653b\u51fb\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u7684\u5e7f\u6cdb\u5e94\u7528\u5f15\u53d1\u4e86\u5b89\u5168\u9690\u60a3\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u672a\u5145\u5206\u63a2\u8ba8\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u540e\u95e8\u653b\u51fb\u3002", "method": "\u901a\u8fc7\u63a2\u7d22\u6df1\u5ea6\u5b66\u4e60\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u7684\u540e\u95e8\u653b\u51fb\u53ef\u884c\u6027\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u578b\u653b\u51fb\uff08\u4eba\u8138\u751f\u6210\u548c\u5173\u952e\u70b9\u504f\u79fb\uff09\uff0c\u5e76\u6d4b\u8bd5\u4e8620\u79cd\u7cfb\u7edf\u914d\u7f6e\u548c15\u79cd\u653b\u51fb\u6848\u4f8b\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5355\u4e00\u540e\u95e8\u653b\u51fb\u53ef\u7ed5\u8fc7\u6574\u4e2a\u7cfb\u7edf\u529f\u80fd\uff0c\u9a8c\u8bc1\u4e86\u653b\u51fb\u7684\u5e7f\u6cdb\u5f71\u54cd\u3002", "conclusion": "\u8bba\u6587\u4e3a\u76f8\u5173\u65b9\u63d0\u4f9b\u4e86\u6700\u4f73\u5b9e\u8df5\u548c\u9632\u5fa1\u63aa\u65bd\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002"}}
{"id": "2507.01806", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.01806", "abs": "https://arxiv.org/abs/2507.01806", "authors": ["Reza Arabpour", "Haitz S\u00e1ez de Oc\u00e1riz Borde", "Anastasis Kratsios"], "title": "LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework for LLMs", "comment": "5-page main paper (excluding references) + 11-page appendix, 3\n  tables, 1 figure. Accepted to ICML 2025 Workshop on Efficient Systems for\n  Foundation Models", "summary": "Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language\nModels (LLMs) by enabling parameter-efficient updates. However, their\nwidespread adoption remains limited by the reliance on GPU-based training. In\nthis work, we propose a theoretically grounded approach to LoRA fine-tuning\ndesigned specifically for users with limited computational resources,\nparticularly those restricted to standard laptop CPUs. Our method learns a\nmeta-operator that maps any input dataset, represented as a probability\ndistribution, to a set of LoRA weights by leveraging a large bank of\npre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of\nperforming new gradient-based updates, our pipeline constructs adapters via\nlightweight combinations of existing LoRAs directly on CPU. While the resulting\nadapters do not match the performance of GPU-trained counterparts, they\nconsistently outperform the base Mistral model on downstream tasks, offering a\npractical and accessible alternative to traditional GPU-based fine-tuning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCPU\u7684\u4f4e\u6210\u672cLoRA\u5fae\u8c03\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u7528\u6237\uff0c\u901a\u8fc7\u7ec4\u5408\u9884\u8bad\u7ec3\u9002\u914d\u5668\u751f\u6210\u65b0\u9002\u914d\u5668\uff0c\u6027\u80fd\u867d\u4e0d\u53caGPU\u8bad\u7ec3\uff0c\u4f46\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3LoRA\u5fae\u8c03\u5bf9GPU\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u4e3a\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u7528\u6237\u63d0\u4f9b\u53ef\u884c\u65b9\u6848\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u9002\u914d\u5668\u5e93\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7ec4\u5408\u751f\u6210\u65b0\u9002\u914d\u5668\uff0c\u65e0\u9700\u68af\u5ea6\u66f4\u65b0\uff0c\u76f4\u63a5\u5728CPU\u4e0a\u5b8c\u6210\u3002", "result": "\u751f\u6210\u7684\u9002\u914d\u5668\u6027\u80fd\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\uff0c\u4f46\u4e0d\u53caGPU\u8bad\u7ec3\u7684\u9002\u914d\u5668\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8d44\u6e90\u6709\u9650\u7684\u7528\u6237\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684LoRA\u5fae\u8c03\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2507.01608", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.01608", "abs": "https://arxiv.org/abs/2507.01608", "authors": ["Xu Zhang", "Ming Lu", "Yan Chen", "Zhan Ma"], "title": "Perception-Oriented Latent Coding for High-Performance Compressed Domain Semantic Inference", "comment": "International Conference on Multimedia and Expo (ICME), 2025", "summary": "In recent years, compressed domain semantic inference has primarily relied on\nlearned image coding models optimized for mean squared error (MSE). However,\nMSE-oriented optimization tends to yield latent spaces with limited semantic\nrichness, which hinders effective semantic inference in downstream tasks.\nMoreover, achieving high performance with these models often requires\nfine-tuning the entire vision model, which is computationally intensive,\nespecially for large models. To address these problems, we introduce\nPerception-Oriented Latent Coding (POLC), an approach that enriches the\nsemantic content of latent features for high-performance compressed domain\nsemantic inference. With the semantically rich latent space, POLC requires only\na plug-and-play adapter for fine-tuning, significantly reducing the parameter\ncount compared to previous MSE-oriented methods. Experimental results\ndemonstrate that POLC achieves rate-perception performance comparable to\nstate-of-the-art generative image coding methods while markedly enhancing\nperformance in vision tasks, with minimal fine-tuning overhead. Code is\navailable at https://github.com/NJUVISION/POLC.", "AI": {"tldr": "POLC\u63d0\u51fa\u4e86\u4e00\u79cd\u611f\u77e5\u5bfc\u5411\u7684\u6f5c\u5728\u7f16\u7801\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfMSE\u4f18\u5316\u65b9\u6cd5\u5728\u8bed\u4e49\u4e30\u5bcc\u6027\u548c\u8ba1\u7b97\u5f00\u9500\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8eMSE\u4f18\u5316\u7684\u56fe\u50cf\u7f16\u7801\u6a21\u578b\u5728\u6f5c\u5728\u7a7a\u95f4\u8bed\u4e49\u4e30\u5bcc\u6027\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u4e14\u4e0b\u6e38\u4efb\u52a1\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u8fdb\u884c\u5fae\u8c03\u3002", "method": "\u63d0\u51faPOLC\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u5f3a\u6f5c\u5728\u7279\u5f81\u7684\u8bed\u4e49\u5185\u5bb9\uff0c\u4ec5\u9700\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u5373\u53ef\u5b9e\u73b0\u9ad8\u6027\u80fd\u8bed\u4e49\u63a8\u7406\u3002", "result": "POLC\u5728\u4fdd\u6301\u4e0e\u751f\u6210\u5f0f\u56fe\u50cf\u7f16\u7801\u65b9\u6cd5\u76f8\u5f53\u7684\u7387\u5931\u771f\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u4efb\u52a1\u6027\u80fd\uff0c\u4e14\u5fae\u8c03\u5f00\u9500\u6781\u4f4e\u3002", "conclusion": "POLC\u4e3a\u538b\u7f29\u57df\u8bed\u4e49\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u8bed\u4e49\u4e30\u5bcc\u7684\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.01951", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01951", "abs": "https://arxiv.org/abs/2507.01951", "authors": ["Zixiao Wang", "Yuxin Wang", "Xiaorui Wang", "Mengting Xing", "Jie Gao", "Jianjun Xu", "Guangcan Liu", "Chenhui Jin", "Zhuo Wang", "Shengzhuo Zhang", "Hongtao Xie"], "title": "Test-Time Scaling with Reflective Generative Model", "comment": null, "summary": "We introduce our first reflective generative model MetaStone-S1, which\nobtains OpenAI o3's performance via the self-supervised process reward model\n(SPRM). Through sharing the backbone network and using task-specific heads for\nnext token prediction and process scoring respectively, SPRM successfully\nintegrates the policy model and process reward model(PRM) into a unified\ninterface without extra process annotation, reducing over 99% PRM parameters\nfor efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable\nfor test time scaling (TTS), and we provide three reasoning effort modes (low,\nmedium, and high), based on the controllable thinking length. Moreover, we\nempirically establish a scaling law that reveals the relationship between total\nthinking computation and TTS performance. Experiments demonstrate that our\nMetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with\nonly 32B parameter size. To support the research community, we have\nopen-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.", "AI": {"tldr": "MetaStone-S1\u662f\u4e00\u79cd\u53cd\u5c04\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08SPRM\uff09\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\uff0c\u6027\u80fd\u5ab2\u7f8eOpenAI o3\uff0c\u53c2\u6570\u4ec532B\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u7edf\u4e00\u63a5\u53e3\u6574\u5408\u7b56\u7565\u6a21\u578b\u548c\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff0c\u51cf\u5c11\u53c2\u6570\u9700\u6c42\uff0c\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002", "method": "\u91c7\u7528\u5171\u4eab\u4e3b\u5e72\u7f51\u7edc\u548c\u4efb\u52a1\u7279\u5b9a\u5934\uff0c\u7ed3\u5408SPRM\u5b9e\u73b0\u65e0\u989d\u5916\u6807\u6ce8\u7684\u9ad8\u6548\u63a8\u7406\uff0c\u652f\u6301\u6d4b\u8bd5\u65f6\u6269\u5c55\uff08TTS\uff09\u3002", "result": "\u5b9e\u9a8c\u663e\u793aMetaStone-S1\u6027\u80fd\u4e0eOpenAI-o3-mini\u76f8\u5f53\uff0c\u53c2\u6570\u51cf\u5c1199%\u3002", "conclusion": "MetaStone-S1\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u751f\u6210\u6a21\u578b\uff0c\u5e76\u5f00\u6e90\u4ee3\u7801\u3002"}}
{"id": "2507.01389", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.01389", "abs": "https://arxiv.org/abs/2507.01389", "authors": ["Anbang Wang", "Dunbo Cai", "Yu Zhang", "Yangqing Huang", "Xiangyang Feng", "Zhihong Zhang"], "title": "Surrogate Modeling via Factorization Machine and Ising Model with Enhanced Higher-Order Interaction Learning", "comment": null, "summary": "Recently, a surrogate model was proposed that employs a factorization machine\nto approximate the underlying input-output mapping of the original system, with\nquantum annealing used to optimize the resulting surrogate function. Inspired\nby this approach, we propose an enhanced surrogate model that incorporates\nadditional slack variables into both the factorization machine and its\nassociated Ising representation thereby unifying what was by design a two-step\nprocess into a single, integrated step. During the training phase, the slack\nvariables are iteratively updated, enabling the model to account for\nhigher-order feature interactions. We apply the proposed method to the task of\npredicting drug combination effects. Experimental results indicate that the\nintroduction of slack variables leads to a notable improvement of performance.\nOur algorithm offers a promising approach for building efficient surrogate\nmodels that exploit potential quantum advantages.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u7684\u4ee3\u7406\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u677e\u5f1b\u53d8\u91cf\u5c06\u4e24\u6b65\u8fc7\u7a0b\u7edf\u4e00\u4e3a\u4e00\u6b65\uff0c\u5e76\u5728\u836f\u7269\u7ec4\u5408\u6548\u5e94\u9884\u6d4b\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u53d7\u73b0\u6709\u4ee3\u7406\u6a21\u578b\u542f\u53d1\uff0c\u5e0c\u671b\u901a\u8fc7\u5f15\u5165\u677e\u5f1b\u53d8\u91cf\u4f18\u5316\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u63a2\u7d22\u91cf\u5b50\u4f18\u52bf\u3002", "method": "\u5728\u56e0\u5b50\u5206\u89e3\u673a\u53ca\u5176Ising\u8868\u793a\u4e2d\u5f15\u5165\u677e\u5f1b\u53d8\u91cf\uff0c\u7edf\u4e00\u4e24\u6b65\u8fc7\u7a0b\u4e3a\u4e00\u6b65\uff0c\u8fed\u4ee3\u66f4\u65b0\u677e\u5f1b\u53d8\u91cf\u4ee5\u6355\u6349\u9ad8\u9636\u7279\u5f81\u4ea4\u4e92\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u677e\u5f1b\u53d8\u91cf\u7684\u5f15\u5165\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u4e3a\u6784\u5efa\u9ad8\u6548\u4ee3\u7406\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u6cd5\uff0c\u5e76\u53ef\u80fd\u5229\u7528\u91cf\u5b50\u4f18\u52bf\u3002"}}
{"id": "2507.01630", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01630", "abs": "https://arxiv.org/abs/2507.01630", "authors": ["Yuxiao Wang", "Yu Lei", "Zhenao Wei", "Weiying Xue", "Xinyu Jiang", "Nan Zhuang", "Qi Liu"], "title": "Prompt Guidance and Human Proximal Perception for HOT Prediction with Regional Joint Loss", "comment": "Accepted by ICCV 2025", "summary": "The task of Human-Object conTact (HOT) detection involves identifying the\nspecific areas of the human body that are touching objects. Nevertheless,\ncurrent models are restricted to just one type of image, often leading to too\nmuch segmentation in areas with little interaction, and struggling to maintain\ncategory consistency within specific regions. To tackle this issue, a HOT\nframework, termed \\textbf{P3HOT}, is proposed, which blends \\textbf{P}rompt\nguidance and human \\textbf{P}roximal \\textbf{P}erception. To begin with, we\nutilize a semantic-driven prompt mechanism to direct the network's attention\ntowards the relevant regions based on the correlation between image and text.\nThen a human proximal perception mechanism is employed to dynamically perceive\nkey depth range around the human, using learnable parameters to effectively\neliminate regions where interactions are not expected. Calculating depth\nresolves the uncertainty of the overlap between humans and objects in a 2D\nperspective, providing a quasi-3D viewpoint. Moreover, a Regional Joint Loss\n(RJLoss) has been created as a new loss to inhibit abnormal categories in the\nsame area. A new evaluation metric called ``AD-Acc.'' is introduced to address\nthe shortcomings of existing methods in addressing negative samples.\nComprehensive experimental results demonstrate that our approach achieves\nstate-of-the-art performance in four metrics across two benchmark datasets.\nSpecifically, our model achieves an improvement of \\textbf{0.7}$\\uparrow$,\n\\textbf{2.0}$\\uparrow$, \\textbf{1.6}$\\uparrow$, and \\textbf{11.0}$\\uparrow$ in\nSC-Acc., mIoU, wIoU, and AD-Acc. metrics, respectively, on the HOT-Annotated\ndataset. Code is available at https://github.com/YuxiaoWang-AI/P3HOT.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aP3HOT\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u63d0\u793a\u5f15\u5bfc\u548c\u4eba\u7c7b\u8fd1\u7aef\u611f\u77e5\uff0c\u7528\u4e8e\u6539\u8fdb\u4eba-\u7269\u63a5\u89e6\uff08HOT\uff09\u68c0\u6d4b\u4efb\u52a1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u5206\u5272\u548c\u7c7b\u522b\u4e00\u81f4\u6027\u4e0a\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524dHOT\u68c0\u6d4b\u6a21\u578b\u5c40\u9650\u4e8e\u5355\u4e00\u56fe\u50cf\u7c7b\u578b\uff0c\u5bfc\u81f4\u8fc7\u5ea6\u5206\u5272\u548c\u7c7b\u522b\u4e00\u81f4\u6027\u5dee\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "P3HOT\u6846\u67b6\u7ed3\u5408\u8bed\u4e49\u9a71\u52a8\u63d0\u793a\u548c\u4eba\u7c7b\u8fd1\u7aef\u611f\u77e5\u673a\u5236\uff0c\u52a8\u6001\u611f\u77e5\u5173\u952e\u6df1\u5ea6\u8303\u56f4\uff0c\u5e76\u4f7f\u7528\u65b0\u7684Regional Joint Loss\u548c\u8bc4\u4ef7\u6307\u6807AD-Acc\u3002", "result": "\u5728HOT-Annotated\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u5728\u56db\u4e2a\u6307\u6807\u4e0a\u5747\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff08SC-Acc.\u3001mIoU\u3001wIoU\u3001AD-Acc.\uff09\u3002", "conclusion": "P3HOT\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u63d0\u793a\u548c\u6df1\u5ea6\u611f\u77e5\uff0c\u663e\u8457\u63d0\u5347\u4e86HOT\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.01414", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01414", "abs": "https://arxiv.org/abs/2507.01414", "authors": ["Sultan Daniels", "Dylan Davis", "Dhruv Gautam", "Wentinn Liao", "Gireeja Ranade", "Anant Sahai"], "title": "Decomposing Prediction Mechanisms for In-Context Recall", "comment": "44 pages, 47 figures, 2 tables", "summary": "We introduce a new family of toy problems that combine features of\nlinear-regression-style continuous in-context learning (ICL) with discrete\nassociative recall. We pretrain transformer models on sample traces from this\ntoy, specifically symbolically-labeled interleaved state observations from\nrandomly drawn linear deterministic dynamical systems. We study if the\ntransformer models can recall the state of a sequence previously seen in its\ncontext when prompted to do so with the corresponding in-context label. Taking\na closer look at this task, it becomes clear that the model must perform two\nfunctions: (1) identify which system's state should be recalled and apply that\nsystem to its last seen state, and (2) continuing to apply the correct system\nto predict the subsequent states. Training dynamics reveal that the first\ncapability emerges well into a model's training. Surprisingly, the second\ncapability, of continuing the prediction of a resumed sequence, develops much\nearlier.\n  Via out-of-distribution experiments, and a mechanistic analysis on model\nweights via edge pruning, we find that next-token prediction for this toy\nproblem involves at least two separate mechanisms. One mechanism uses the\ndiscrete symbolic labels to do the associative recall required to predict the\nstart of a resumption of a previously seen sequence. The second mechanism,\nwhich is largely agnostic to the discrete symbolic labels, performs a\n\"Bayesian-style\" prediction based on the previous token and the context. These\ntwo mechanisms have different learning dynamics.\n  To confirm that this multi-mechanism (manifesting as separate phase\ntransitions) phenomenon is not just an artifact of our toy setting, we used\nOLMo training checkpoints on an ICL translation task to see a similar\nphenomenon: a decisive gap in the emergence of first-task-token performance vs\nsecond-task-token performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7ebf\u6027\u56de\u5f52\u5f0f\u8fde\u7eed\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u548c\u79bb\u6563\u5173\u8054\u53ec\u56de\u7684\u65b0\u73a9\u5177\u95ee\u9898\uff0c\u7814\u7a76\u4e86Transformer\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u6d89\u53ca\u4e24\u79cd\u673a\u5236\uff1a\u57fa\u4e8e\u6807\u7b7e\u7684\u5173\u8054\u53ec\u56de\u548c\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u8d1d\u53f6\u65af\u9884\u6d4b\uff0c\u4e14\u5b66\u4e60\u52a8\u6001\u4e0d\u540c\u3002", "motivation": "\u63a2\u7d22Transformer\u6a21\u578b\u5728\u5904\u7406\u7ed3\u5408\u8fde\u7eed\u548c\u79bb\u6563\u7279\u5f81\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u4efb\u52a1\u65f6\u7684\u80fd\u529b\uff0c\u5e76\u5206\u6790\u5176\u5185\u90e8\u673a\u5236\u3002", "method": "\u9884\u8bad\u7ec3Transformer\u6a21\u578b\u4e8e\u7b26\u53f7\u6807\u8bb0\u7684\u968f\u673a\u7ebf\u6027\u786e\u5b9a\u6027\u52a8\u6001\u7cfb\u7edf\u6837\u672c\uff0c\u901a\u8fc7\u8fb9\u7f18\u526a\u679d\u548c\u5206\u5e03\u5916\u5b9e\u9a8c\u5206\u6790\u6a21\u578b\u6743\u91cd\u3002", "result": "\u6a21\u578b\u9700\u5b8c\u6210\u5173\u8054\u53ec\u56de\u548c\u72b6\u6001\u9884\u6d4b\u4e24\u79cd\u529f\u80fd\uff0c\u524d\u8005\u5728\u8bad\u7ec3\u540e\u671f\u624d\u51fa\u73b0\uff0c\u540e\u8005\u8f83\u65e9\u51fa\u73b0\u3002\u4e24\u79cd\u673a\u5236\u5b66\u4e60\u52a8\u6001\u4e0d\u540c\u3002", "conclusion": "\u591a\u673a\u5236\u73b0\u8c61\uff08\u8868\u73b0\u4e3a\u5206\u79bb\u7684\u76f8\u53d8\uff09\u4e0d\u4ec5\u9650\u4e8e\u73a9\u5177\u95ee\u9898\uff0c\u5728ICL\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u4e5f\u89c2\u5bdf\u5230\u7c7b\u4f3c\u73b0\u8c61\u3002"}}
{"id": "2507.01631", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01631", "abs": "https://arxiv.org/abs/2507.01631", "authors": ["Camille Billouard", "Dawa Derksen", "Alexandre Constantin", "Bruno Vallet"], "title": "Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation", "comment": "Accepted at ICCV 2025 Workshop 3D-VAST (From street to space: 3D\n  Vision Across Altitudes). Version before camera ready. Our code will be made\n  public after the conference", "summary": "Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D\nreconstruction from multiview satellite imagery. However, state-of-the-art NeRF\nmethods are typically constrained to small scenes due to the memory footprint\nduring training, which we study in this paper. Previous work on large-scale\nNeRFs palliate this by dividing the scene into NeRFs. This paper introduces\nSnake-NeRF, a framework that scales to large scenes. Our out-of-core method\neliminates the need to load all images and networks simultaneously, and\noperates on a single device. We achieve this by dividing the region of interest\ninto NeRFs that 3D tile without overlap. Importantly, we crop the images with\noverlap to ensure each NeRFs is trained with all the necessary pixels. We\nintroduce a novel $2\\times 2$ 3D tile progression strategy and segmented\nsampler, which together prevent 3D reconstruction errors along the tile edges.\nOur experiments conclude that large satellite images can effectively be\nprocessed with linear time complexity, on a single GPU, and without compromise\nin quality.", "AI": {"tldr": "Snake-NeRF\u662f\u4e00\u79cd\u6269\u5c55\u5230\u5927\u573a\u666f\u7684NeRF\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5757\u5904\u7406\u548c\u4f18\u5316\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u5185\u5b58\u9650\u5236\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709NeRF\u65b9\u6cd5\u56e0\u5185\u5b58\u9650\u5236\u4ec5\u9002\u7528\u4e8e\u5c0f\u573a\u666f\uff0c\u65e0\u6cd5\u5904\u7406\u5927\u5c3a\u5ea6\u536b\u661f\u56fe\u50cf\u3002", "method": "\u63d0\u51faSnake-NeRF\u6846\u67b6\uff0c\u91c7\u7528\u5206\u5757\u5904\u7406\u3001\u56fe\u50cf\u88c1\u526a\u548c3D\u74e6\u7247\u7b56\u7565\uff0c\u907f\u514d\u5185\u5b58\u8fc7\u8f7d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u7ebf\u6027\u5904\u7406\u5927\u536b\u661f\u56fe\u50cf\uff0c\u5355GPU\u8fd0\u884c\u4e14\u8d28\u91cf\u65e0\u635f\u3002", "conclusion": "Snake-NeRF\u6210\u529f\u6269\u5c55\u4e86NeRF\u7684\u5e94\u7528\u8303\u56f4\uff0c\u9002\u7528\u4e8e\u5927\u573a\u666f3D\u91cd\u5efa\u3002"}}
{"id": "2507.01634", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01634", "abs": "https://arxiv.org/abs/2507.01634", "authors": ["Boyuan Sun", "Modi Jin", "Bowen Yin", "Qibin Hou"], "title": "Depth Anything at Any Condition", "comment": null, "summary": "We present Depth Anything at Any Condition (DepthAnything-AC), a foundation\nmonocular depth estimation (MDE) model capable of handling diverse\nenvironmental conditions. Previous foundation MDE models achieve impressive\nperformance across general scenes but not perform well in complex open-world\nenvironments that involve challenging conditions, such as illumination\nvariations, adverse weather, and sensor-induced distortions. To overcome the\nchallenges of data scarcity and the inability of generating high-quality\npseudo-labels from corrupted images, we propose an unsupervised consistency\nregularization finetuning paradigm that requires only a relatively small amount\nof unlabeled data. Furthermore, we propose the Spatial Distance Constraint to\nexplicitly enforce the model to learn patch-level relative relationships,\nresulting in clearer semantic boundaries and more accurate details.\nExperimental results demonstrate the zero-shot capabilities of DepthAnything-AC\nacross diverse benchmarks, including real-world adverse weather benchmarks,\nsynthetic corruption benchmarks, and general benchmarks.\n  Project Page: https://ghost233lism.github.io/depthanything-AC-page\n  Code: https://github.com/HVision-NKU/DepthAnythingAC", "AI": {"tldr": "DepthAnything-AC\u662f\u4e00\u79cd\u57fa\u7840\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\uff0c\u80fd\u591f\u5728\u591a\u6837\u5316\u73af\u5883\u6761\u4ef6\u4e0b\u5de5\u4f5c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u590d\u6742\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u7840MDE\u6a21\u578b\u5728\u4e00\u822c\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u590d\u6742\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\uff08\u5982\u5149\u7167\u53d8\u5316\u3001\u6076\u52a3\u5929\u6c14\u548c\u4f20\u611f\u5668\u5931\u771f\uff09\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u4e00\u81f4\u6027\u6b63\u5219\u5316\u5fae\u8c03\u8303\u5f0f\uff0c\u4ec5\u9700\u5c11\u91cf\u672a\u6807\u8bb0\u6570\u636e\uff0c\u5e76\u5f15\u5165\u7a7a\u95f4\u8ddd\u79bb\u7ea6\u675f\u4ee5\u660e\u786e\u5b66\u4e60\u8865\u4e01\u7ea7\u76f8\u5bf9\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eDepthAnything-AC\u5728\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5177\u6709\u96f6\u6837\u672c\u80fd\u529b\uff0c\u5305\u62ec\u771f\u5b9e\u4e16\u754c\u6076\u52a3\u5929\u6c14\u57fa\u51c6\u3001\u5408\u6210\u5931\u771f\u57fa\u51c6\u548c\u4e00\u822c\u57fa\u51c6\u3002", "conclusion": "DepthAnything-AC\u901a\u8fc7\u65b0\u65b9\u6cd5\u89e3\u51b3\u4e86\u590d\u6742\u73af\u5883\u4e0b\u7684\u6df1\u5ea6\u4f30\u8ba1\u95ee\u9898\uff0c\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2507.01469", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.01469", "abs": "https://arxiv.org/abs/2507.01469", "authors": ["Alessio Ferrato", "Fabio Gasparetti", "Carla Limongelli", "Stefano Mastandrea", "Giuseppe Sansonetti", "Joaqu\u00edn Torres-Sospedra"], "title": "Cross-platform Smartphone Positioning at Museums", "comment": "Accepted at the 2025 International Conference on Indoor Positioning\n  and Indoor Navigation (IPIN), Tampere, Finland, September 15-18, 2025", "summary": "Indoor Positioning Systems (IPSs) hold significant potential for enhancing\nvisitor experiences in cultural heritage institutions. By enabling personalized\nnavigation, efficient artifact organization, and better interaction with\nexhibits, IPSs can transform the modalities of how individuals engage with\nmuseums, galleries and libraries. However, these institutions face several\nchallenges in implementing IPSs, including environmental constraints, technical\nlimits, and limited experimentation. In other contexts, Received Signal\nStrength (RSS)-based approaches using Bluetooth Low Energy (BLE) and WiFi have\nemerged as preferred solutions due to their non-invasive nature and minimal\ninfrastructure requirements. Nevertheless, the lack of publicly available RSS\ndatasets that specifically reflect museum environments presents a substantial\nbarrier to developing and evaluating positioning algorithms designed for the\nintricate spatial characteristics typical of cultural heritage sites. To\naddress this limitation, we present BAR, a novel RSS dataset collected in front\nof 90 artworks across 13 museum rooms using two different platforms, i.e.,\nAndroid and iOS. Additionally, we provide an advanced position classification\nbaseline taking advantage of a proximity-based method and $k$-NN algorithms. In\nour analysis, we discuss the results and offer suggestions for potential\nresearch directions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBAR\u7684\u65b0\u578bRSS\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u89e3\u51b3\u6587\u5316\u9057\u5740\u4e2d\u5ba4\u5185\u5b9a\u4f4d\u7cfb\u7edf\uff08IPS\uff09\u5f00\u53d1\u7684\u6570\u636e\u4e0d\u8db3\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u57fa\u4e8e\u90bb\u8fd1\u6027\u548ck-NN\u7b97\u6cd5\u7684\u5206\u7c7b\u57fa\u7ebf\u3002", "motivation": "\u6587\u5316\u9057\u5740\u4e2d\u7684IPS\u53ef\u4ee5\u63d0\u5347\u6e38\u5ba2\u4f53\u9a8c\uff0c\u4f46\u7f3a\u4e4f\u9488\u5bf9\u535a\u7269\u9986\u73af\u5883\u7684\u516c\u5f00RSS\u6570\u636e\u96c6\u963b\u788d\u4e86\u76f8\u5173\u7b97\u6cd5\u7684\u5f00\u53d1\u4e0e\u8bc4\u4f30\u3002", "method": "\u6536\u96c6\u4e8690\u4ef6\u827a\u672f\u54c1\u524d\u7684RSS\u6570\u636e\uff0c\u4f7f\u7528Android\u548ciOS\u5e73\u53f0\uff0c\u5e76\u91c7\u7528\u90bb\u8fd1\u6027\u65b9\u6cd5\u548ck-NN\u7b97\u6cd5\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u63d0\u4f9b\u4e86BAR\u6570\u636e\u96c6\uff0c\u5e76\u5c55\u793a\u4e86\u57fa\u4e8e\u8be5\u6570\u636e\u96c6\u7684\u5206\u7c7b\u57fa\u7ebf\u7ed3\u679c\u3002", "conclusion": "BAR\u6570\u636e\u96c6\u586b\u8865\u4e86\u535a\u7269\u9986\u73af\u5883RSS\u6570\u636e\u7684\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u548c\u5efa\u8bae\u3002"}}
{"id": "2507.01643", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01643", "abs": "https://arxiv.org/abs/2507.01643", "authors": ["Weijie Yin", "Dingkang Yang", "Hongyuan Dong", "Zijian Kang", "Jiacong Wang", "Xiao Liang", "Chao Feng", "Jiao Ran"], "title": "SAILViT: Towards Robust and Generalizable Visual Backbones for MLLMs via Gradual Feature Refinement", "comment": "We release SAILViT, a series of versatile vision foundation models", "summary": "Vision Transformers (ViTs) are essential as foundation backbones in\nestablishing the visual comprehension capabilities of Multimodal Large Language\nModels (MLLMs). Although most ViTs achieve impressive performance through\nimage-text pair-based contrastive learning or self-supervised mechanisms, they\nstruggle to engage in connector-based co-training directly with LLMs due to\npotential parameter initialization conflicts and modality semantic gaps. To\naddress the above challenges, this paper proposes SAILViT, a gradual feature\nlearning-enhanced ViT for facilitating MLLMs to break through performance\nbottlenecks in complex multimodal interactions. SAILViT achieves\ncoarse-to-fine-grained feature alignment and world knowledge infusion with\ngradual feature refinement, which better serves target training demands. We\nperform thorough empirical analyses to confirm the powerful robustness and\ngeneralizability of SAILViT across different dimensions, including parameter\nsizes, model architectures, training strategies, and data scales. Equipped with\nSAILViT, existing MLLMs show significant and consistent performance\nimprovements on the OpenCompass benchmark across extensive downstream tasks.\nSAILViT series models are released at\nhttps://huggingface.co/BytedanceDouyinContent.", "AI": {"tldr": "SAILViT\u662f\u4e00\u79cd\u9010\u6b65\u7279\u5f81\u5b66\u4e60\u589e\u5f3a\u7684ViT\uff0c\u65e8\u5728\u89e3\u51b3ViT\u4e0eLLM\u76f4\u63a5\u8054\u5408\u8bad\u7ec3\u65f6\u7684\u53c2\u6570\u51b2\u7a81\u548c\u6a21\u6001\u8bed\u4e49\u5dee\u8ddd\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347MLLM\u5728\u590d\u6742\u591a\u6a21\u6001\u4ea4\u4e92\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709ViT\u901a\u8fc7\u56fe\u50cf-\u6587\u672c\u5bf9\u6bd4\u5b66\u4e60\u6216\u81ea\u76d1\u7763\u673a\u5236\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u96be\u4ee5\u4e0eLLM\u76f4\u63a5\u8054\u5408\u8bad\u7ec3\uff0c\u5b58\u5728\u53c2\u6570\u521d\u59cb\u5316\u51b2\u7a81\u548c\u6a21\u6001\u8bed\u4e49\u5dee\u8ddd\u95ee\u9898\u3002", "method": "\u63d0\u51faSAILViT\uff0c\u901a\u8fc7\u9010\u6b65\u7279\u5f81\u7ec6\u5316\u5b9e\u73b0\u7c97\u5230\u7ec6\u7684\u7279\u5f81\u5bf9\u9f50\u548c\u4e16\u754c\u77e5\u8bc6\u6ce8\u5165\uff0c\u9002\u5e94\u76ee\u6807\u8bad\u7ec3\u9700\u6c42\u3002", "result": "SAILViT\u5728\u4e0d\u540c\u53c2\u6570\u89c4\u6a21\u3001\u67b6\u6784\u3001\u8bad\u7ec3\u7b56\u7565\u548c\u6570\u636e\u89c4\u6a21\u4e0b\u8868\u73b0\u51fa\u5f3a\u5927\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\uff0c\u663e\u8457\u63d0\u5347MLLM\u5728OpenCompass\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "SAILViT\u6709\u6548\u89e3\u51b3\u4e86ViT\u4e0eLLM\u8054\u5408\u8bad\u7ec3\u7684\u6311\u6218\uff0c\u4e3aMLLM\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u63d0\u5347\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.01649", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01649", "abs": "https://arxiv.org/abs/2507.01649", "authors": ["Yoav Gelberg", "Yam Eitan", "Aviv Navon", "Aviv Shamsian", "Theo", "Putterman", "Michael Bronstein", "Haggai Maron"], "title": "GradMetaNet: An Equivariant Architecture for Learning on Gradients", "comment": null, "summary": "Gradients of neural networks encode valuable information for optimization,\nediting, and analysis of models. Therefore, practitioners often treat gradients\nas inputs to task-specific algorithms, e.g. for pruning or optimization. Recent\nworks explore learning algorithms that operate directly on gradients but use\narchitectures that are not specifically designed for gradient processing,\nlimiting their applicability. In this paper, we present a principled approach\nfor designing architectures that process gradients. Our approach is guided by\nthree principles: (1) equivariant design that preserves neuron permutation\nsymmetries, (2) processing sets of gradients across multiple data points to\ncapture curvature information, and (3) efficient gradient representation\nthrough rank-1 decomposition. Based on these principles, we introduce\nGradMetaNet, a novel architecture for learning on gradients, constructed from\nsimple equivariant blocks. We prove universality results for GradMetaNet, and\nshow that previous approaches cannot approximate natural gradient-based\nfunctions that GradMetaNet can. We then demonstrate GradMetaNet's effectiveness\non a diverse set of gradient-based tasks on MLPs and transformers, such as\nlearned optimization, INR editing, and estimating loss landscape curvature.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGradMetaNet\u7684\u65b0\u67b6\u6784\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u5904\u7406\u68af\u5ea6\uff0c\u57fa\u4e8e\u4e09\u4e2a\u539f\u5219\uff1a\u7b49\u53d8\u6027\u8bbe\u8ba1\u3001\u591a\u6570\u636e\u70b9\u68af\u5ea6\u5904\u7406\u548c\u9ad8\u6548\u68af\u5ea6\u8868\u793a\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u68af\u5ea6\u5904\u7406\uff0c\u9650\u5236\u4e86\u5176\u9002\u7528\u6027\u3002", "method": "\u57fa\u4e8e\u7b49\u53d8\u6027\u8bbe\u8ba1\u3001\u591a\u6570\u636e\u70b9\u68af\u5ea6\u5904\u7406\u548c\u9ad8\u6548\u68af\u5ea6\u8868\u793a\uff08\u79e91\u5206\u89e3\uff09\u6784\u5efaGradMetaNet\u3002", "result": "GradMetaNet\u80fd\u903c\u8fd1\u81ea\u7136\u68af\u5ea6\u51fd\u6570\uff0c\u5e76\u5728\u591a\u79cd\u4efb\u52a1\uff08\u5982\u4f18\u5316\u3001\u7f16\u8f91\u548c\u66f2\u7387\u4f30\u8ba1\uff09\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "GradMetaNet\u4e3a\u68af\u5ea6\u5904\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01652", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.01652", "abs": "https://arxiv.org/abs/2507.01652", "authors": ["Yuxin Mao", "Zhen Qin", "Jinxing Zhou", "Hui Deng", "Xuyang Shen", "Bin Fan", "Jing Zhang", "Yiran Zhong", "Yuchao Dai"], "title": "Autoregressive Image Generation with Linear Complexity: A Spatial-Aware Decay Perspective", "comment": null, "summary": "Autoregressive (AR) models have garnered significant attention in image\ngeneration for their ability to effectively capture both local and global\nstructures within visual data. However, prevalent AR models predominantly rely\non the transformer architectures, which are beset by quadratic computational\ncomplexity concerning input sequence length and substantial memory overhead due\nto the necessity of maintaining key-value caches. Although linear attention\nmechanisms have successfully reduced this burden in language models, our\ninitial experiments reveal that they significantly degrade image generation\nquality because of their inability to capture critical long-range dependencies\nin visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a\nnovel attention mechanism that explicitly preserves genuine 2D spatial\nrelationships within the flattened image sequences by computing\nposition-dependent decay factors based on true 2D spatial location rather than\n1D sequence positions. Based on this mechanism, we present LASADGen, an\nautoregressive image generator that enables selective attention to relevant\nspatial contexts with linear complexity. Experiments on ImageNet show LASADGen\nachieves state-of-the-art image generation performance and computational\nefficiency, bridging the gap between linear attention's efficiency and spatial\nunderstanding needed for high-quality generation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLASAD\u7684\u65b0\u578b\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u4fdd\u75592D\u7a7a\u95f4\u5173\u7cfb\u89e3\u51b3\u4e86\u7ebf\u6027\u6ce8\u610f\u529b\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u6027\u80fd\u95ee\u9898\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86LASADGen\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u56fe\u50cf\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u56de\u5f52\u6a21\u578b\u4f9d\u8d56Transformer\u67b6\u6784\uff0c\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5185\u5b58\u5f00\u9500\u9ad8\u7684\u95ee\u9898\uff0c\u800c\u7ebf\u6027\u6ce8\u610f\u529b\u673a\u5236\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u65e0\u6cd5\u6355\u6349\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u4e86Linear Attention with Spatial-Aware Decay (LASAD)\uff0c\u901a\u8fc7\u57fa\u4e8e\u771f\u5b9e2D\u7a7a\u95f4\u4f4d\u7f6e\u8ba1\u7b97\u8870\u51cf\u56e0\u5b50\u6765\u4fdd\u7559\u7a7a\u95f4\u5173\u7cfb\uff0c\u5e76\u5f00\u53d1\u4e86LASADGen\u6a21\u578b\u3002", "result": "\u5728ImageNet\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLASADGen\u5728\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\u3002", "conclusion": "LASADGen\u6210\u529f\u5f25\u8865\u4e86\u7ebf\u6027\u6ce8\u610f\u529b\u6548\u7387\u4e0e\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\u6240\u9700\u7a7a\u95f4\u7406\u89e3\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2507.01516", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01516", "abs": "https://arxiv.org/abs/2507.01516", "authors": ["Dibyanshu Kumar", "Philipp Vaeth", "Magda Gregorov\u00e1"], "title": "Loss Functions in Diffusion Models: A Comparative Study", "comment": "Accepted to ECML 2025", "summary": "Diffusion models have emerged as powerful generative models, inspiring\nextensive research into their underlying mechanisms. One of the key questions\nin this area is the loss functions these models shall train with. Multiple\nformulations have been introduced in the literature over the past several years\nwith some links and some critical differences stemming from various initial\nconsiderations. In this paper, we explore the different target objectives and\ncorresponding loss functions in detail. We present a systematic overview of\ntheir relationships, unifying them under the framework of the variational lower\nbound objective. We complement this theoretical analysis with an empirical\nstudy providing insights into the conditions under which these objectives\ndiverge in performance and the underlying factors contributing to such\ndeviations. Additionally, we evaluate how the choice of objective impacts the\nmodel ability to achieve specific goals, such as generating high-quality\nsamples or accurately estimating likelihoods. This study offers a unified\nunderstanding of loss functions in diffusion models, contributing to more\nefficient and goal-oriented model designs in future research.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u6269\u6563\u6a21\u578b\u4e2d\u7684\u635f\u5931\u51fd\u6570\uff0c\u7edf\u4e00\u4e86\u591a\u79cd\u76ee\u6807\u76ee\u6807\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8bc1\u7814\u7a76\u63a2\u8ba8\u4e86\u5176\u6027\u80fd\u5dee\u5f02\u53ca\u5f71\u54cd\u56e0\u7d20\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u5f3a\u5927\u7684\u751f\u6210\u6a21\u578b\uff0c\u5176\u635f\u5931\u51fd\u6570\u7684\u9009\u62e9\u662f\u5173\u952e\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u4e0d\u540c\u76ee\u6807\u76ee\u6807\u53ca\u5176\u5bf9\u5e94\u7684\u635f\u5931\u51fd\u6570\uff0c\u4ee5\u63d0\u4f9b\u7edf\u4e00\u7684\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u5c06\u591a\u79cd\u635f\u5931\u51fd\u6570\u7edf\u4e00\u5230\u53d8\u5206\u4e0b\u754c\u76ee\u6807\u6846\u67b6\u4e0b\uff0c\u5e76\u8f85\u4ee5\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u6027\u80fd\u5dee\u5f02\u7684\u6761\u4ef6\u548c\u56e0\u7d20\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u635f\u5931\u51fd\u6570\u5728\u6027\u80fd\u4e0a\u5b58\u5728\u5dee\u5f02\uff0c\u5e76\u63ed\u793a\u4e86\u5f71\u54cd\u8fd9\u4e9b\u5dee\u5f02\u7684\u6f5c\u5728\u56e0\u7d20\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u6269\u6563\u6a21\u578b\u7684\u635f\u5931\u51fd\u6570\u63d0\u4f9b\u4e86\u7edf\u4e00\u89c6\u89d2\uff0c\u6709\u52a9\u4e8e\u672a\u6765\u66f4\u9ad8\u6548\u548c\u76ee\u6807\u5bfc\u5411\u7684\u6a21\u578b\u8bbe\u8ba1\u3002"}}
{"id": "2507.01653", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01653", "abs": "https://arxiv.org/abs/2507.01653", "authors": ["Yuran Wang", "Yingping Liang", "Yutao Hu", "Ying Fu"], "title": "RobuSTereo: Robust Zero-Shot Stereo Matching under Adverse Weather", "comment": "accepted by ICCV25", "summary": "Learning-based stereo matching models struggle in adverse weather conditions\ndue to the scarcity of corresponding training data and the challenges in\nextracting discriminative features from degraded images. These limitations\nsignificantly hinder zero-shot generalization to out-of-distribution weather\nconditions. In this paper, we propose \\textbf{RobuSTereo}, a novel framework\nthat enhances the zero-shot generalization of stereo matching models under\nadverse weather by addressing both data scarcity and feature extraction\nchallenges. First, we introduce a diffusion-based simulation pipeline with a\nstereo consistency module, which generates high-quality stereo data tailored\nfor adverse conditions. By training stereo matching models on our synthetic\ndatasets, we reduce the domain gap between clean and degraded images,\nsignificantly improving the models' robustness to unseen weather conditions.\nThe stereo consistency module ensures structural alignment across synthesized\nimage pairs, preserving geometric integrity and enhancing depth estimation\naccuracy. Second, we design a robust feature encoder that combines a\nspecialized ConvNet with a denoising transformer to extract stable and reliable\nfeatures from degraded images. The ConvNet captures fine-grained local\nstructures, while the denoising transformer refines global representations,\neffectively mitigating the impact of noise, low visibility, and weather-induced\ndistortions. This enables more accurate disparity estimation even under\nchallenging visual conditions. Extensive experiments demonstrate that\n\\textbf{RobuSTereo} significantly improves the robustness and generalization of\nstereo matching models across diverse adverse weather scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRobuSTereo\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5408\u6210\u6570\u636e\u548c\u6539\u8fdb\u7279\u5f81\u63d0\u53d6\uff0c\u63d0\u5347\u7acb\u4f53\u5339\u914d\u6a21\u578b\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5b66\u4e60\u7684\u7acb\u4f53\u5339\u914d\u6a21\u578b\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u56e0\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u548c\u7279\u5f81\u63d0\u53d6\u56f0\u96be\u3002", "method": "1. \u63d0\u51fa\u6269\u6563\u6a21\u62df\u7ba1\u9053\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\uff1b2. \u8bbe\u8ba1\u7ed3\u5408ConvNet\u548c\u53bb\u566aTransformer\u7684\u9c81\u68d2\u7279\u5f81\u7f16\u7801\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660eRobuSTereo\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u591a\u79cd\u6076\u52a3\u5929\u6c14\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "RobuSTereo\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u7279\u5f81\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7acb\u4f53\u5339\u914d\u6a21\u578b\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u7684\u6311\u6218\u3002"}}
{"id": "2507.01663", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01663", "abs": "https://arxiv.org/abs/2507.01663", "authors": ["Zhenyu Han", "Ansheng You", "Haibo Wang", "Kui Luo", "Guang Yang", "Wenqi Shi", "Menglong Chen", "Sicheng Zhang", "Zeshun Lan", "Chunshi Deng", "Huazhong Ji", "Wenjie Liu", "Yu Huang", "Yixiang Zhang", "Chenyi Pan", "Jing Wang", "Xin Huang", "Chunsheng Li", "Jianping Wu"], "title": "AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM Post-Training", "comment": null, "summary": "Reinforcement learning (RL) has become a pivotal technology in the\npost-training phase of large language models (LLMs). Traditional task-colocated\nRL frameworks suffer from significant scalability bottlenecks, while\ntask-separated RL frameworks face challenges in complex dataflows and the\ncorresponding resource idling and workload imbalance. Moreover, most existing\nframeworks are tightly coupled with LLM training or inference engines, making\nit difficult to support custom-designed engines. To address these challenges,\nwe propose AsyncFlow, an asynchronous streaming RL framework for efficient\npost-training. Specifically, we introduce a distributed data storage and\ntransfer module that provides a unified data management and fine-grained\nscheduling capability in a fully streamed manner. This architecture inherently\nfacilitates automated pipeline overlapping among RL tasks and dynamic load\nbalancing. Moreover, we propose a producer-consumer-based asynchronous workflow\nengineered to minimize computational idleness by strategically deferring\nparameter update process within staleness thresholds. Finally, the core\ncapability of AsynFlow is architecturally decoupled from underlying training\nand inference engines and encapsulated by service-oriented user interfaces,\noffering a modular and customizable user experience. Extensive experiments\ndemonstrate an average of 1.59 throughput improvement compared with\nstate-of-the-art baseline. The presented architecture in this work provides\nactionable insights for next-generation RL training system designs.", "AI": {"tldr": "AsyncFlow\u662f\u4e00\u4e2a\u5f02\u6b65\u6d41\u5f0f\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u7684\u540e\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfRL\u6846\u67b6\u7684\u53ef\u6269\u5c55\u6027\u548c\u8d44\u6e90\u5229\u7528\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfRL\u6846\u67b6\u5728\u6269\u5c55\u6027\u548c\u8d44\u6e90\u5229\u7528\u4e0a\u5b58\u5728\u74f6\u9888\uff0c\u4e14\u4e0eLLM\u8bad\u7ec3\u6216\u63a8\u7406\u5f15\u64ce\u7d27\u5bc6\u8026\u5408\uff0c\u96be\u4ee5\u652f\u6301\u81ea\u5b9a\u4e49\u5f15\u64ce\u3002", "method": "\u63d0\u51fa\u5206\u5e03\u5f0f\u6570\u636e\u5b58\u50a8\u548c\u4f20\u8f93\u6a21\u5757\uff0c\u5b9e\u73b0\u7edf\u4e00\u6570\u636e\u7ba1\u7406\u548c\u7ec6\u7c92\u5ea6\u8c03\u5ea6\uff1b\u91c7\u7528\u751f\u4ea7\u8005-\u6d88\u8d39\u8005\u5f02\u6b65\u5de5\u4f5c\u6d41\uff0c\u51cf\u5c11\u8ba1\u7b97\u95f2\u7f6e\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u541e\u5410\u91cf\u5e73\u5747\u63d0\u53471.59\u500d\u3002", "conclusion": "AsyncFlow\u4e3a\u4e0b\u4e00\u4ee3RL\u8bad\u7ec3\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6a21\u5757\u5316\u548c\u53ef\u5b9a\u5236\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01654", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01654", "abs": "https://arxiv.org/abs/2507.01654", "authors": ["Martine Hjelkrem-Tan", "Marius Aasan", "Gabriel Y. Arteaga", "Ad\u00edn Ram\u00edrez Rivera"], "title": "SPoT: Subpixel Placement of Tokens in Vision Transformers", "comment": "To appear in Workshop on Efficient Computing under Limited Resources:\n  Visual Computing (ICCV 2025). Code available at\n  https://github.com/dsb-ifi/SPoT", "summary": "Vision Transformers naturally accommodate sparsity, yet standard tokenization\nmethods confine features to discrete patch grids. This constraint prevents\nmodels from fully exploiting sparse regimes, forcing awkward compromises. We\npropose Subpixel Placement of Tokens (SPoT), a novel tokenization strategy that\npositions tokens continuously within images, effectively sidestepping\ngrid-based limitations. With our proposed oracle-guided search, we uncover\nsubstantial performance gains achievable with ideal subpixel token positioning,\ndrastically reducing the number of tokens necessary for accurate predictions\nduring inference. SPoT provides a new direction for flexible, efficient, and\ninterpretable ViT architectures, redefining sparsity as a strategic advantage\nrather than an imposed limitation.", "AI": {"tldr": "SPoT\u662f\u4e00\u79cd\u65b0\u7684\u89c6\u89c9Transformer\u6807\u8bb0\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u8fde\u7eed\u653e\u7f6e\u6807\u8bb0\u7ed5\u8fc7\u7f51\u683c\u9650\u5236\uff0c\u63d0\u5347\u6027\u80fd\u5e76\u51cf\u5c11\u63a8\u7406\u6240\u9700\u7684\u6807\u8bb0\u6570\u91cf\u3002", "motivation": "\u6807\u51c6\u6807\u8bb0\u5316\u65b9\u6cd5\u5c06\u7279\u5f81\u9650\u5236\u5728\u79bb\u6563\u7684\u7f51\u683c\u4e2d\uff0c\u963b\u788d\u4e86\u6a21\u578b\u5728\u7a00\u758f\u573a\u666f\u4e0b\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51faSubpixel Placement of Tokens (SPoT)\uff0c\u901a\u8fc7\u8fde\u7eed\u6807\u8bb0\u653e\u7f6e\u548cOracle\u5f15\u5bfc\u641c\u7d22\u4f18\u5316\u6807\u8bb0\u4f4d\u7f6e\u3002", "result": "SPoT\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u51cf\u5c11\u4e86\u63a8\u7406\u65f6\u6240\u9700\u7684\u6807\u8bb0\u6570\u91cf\u3002", "conclusion": "SPoT\u4e3a\u7075\u6d3b\u3001\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684ViT\u67b6\u6784\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u5c06\u7a00\u758f\u6027\u8f6c\u5316\u4e3a\u4f18\u52bf\u3002"}}
{"id": "2507.01544", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01544", "abs": "https://arxiv.org/abs/2507.01544", "authors": ["Benjamin Feuer", "Lennart Purucker", "Oussama Elachqar", "Chinmay Hegde"], "title": "MARVIS: Modality Adaptive Reasoning over VISualizations", "comment": null, "summary": "Scientific applications of machine learning often rely on small, specialized\nmodels tuned to particular domains. Such models often achieve excellent\nperformance, but lack flexibility. Foundation models offer versatility, but\ntypically underperform specialized approaches, especially on non-traditional\nmodalities and long-tail domains. We propose MARVIS (Modality Adaptive\nReasoning over VISualizations), a training-free method that enables even small\nvision-language models to predict any data modality with high accuracy. MARVIS\ntransforms latent embedding spaces into visual representations and then\nleverages the spatial and fine-grained reasoning skills of VLMs to successfully\ninterpret and utilize them. MARVIS achieves competitive performance on vision,\naudio, biological, and tabular domains using a single 3B parameter model,\nachieving results that beat Gemini by 16\\% on average and approach specialized\nmethods, without exposing personally identifiable information (P.I.I.) or\nrequiring any domain-specific training. We open source our code and datasets at\nhttps://github.com/penfever/marvis", "AI": {"tldr": "MARVIS\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6f5c\u5728\u5d4c\u5165\u7a7a\u95f4\u8f6c\u6362\u4e3a\u89c6\u89c9\u8868\u793a\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u548c\u7ec6\u7c92\u5ea6\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u6570\u636e\u7684\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u3002", "motivation": "\u89e3\u51b3\u4e13\u4e1a\u6a21\u578b\u7075\u6d3b\u6027\u4e0d\u8db3\u548c\u57fa\u7840\u6a21\u578b\u5728\u975e\u4f20\u7edf\u6a21\u6001\u53ca\u957f\u5c3e\u9886\u57df\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "method": "\u5c06\u6f5c\u5728\u5d4c\u5165\u7a7a\u95f4\u8f6c\u6362\u4e3a\u89c6\u89c9\u8868\u793a\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u63a8\u7406\u3002", "result": "\u5728\u89c6\u89c9\u3001\u97f3\u9891\u3001\u751f\u7269\u548c\u8868\u683c\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u5e73\u5747\u6027\u80fd\u8d85\u8d8aGemini 16%\uff0c\u63a5\u8fd1\u4e13\u4e1a\u65b9\u6cd5\u3002", "conclusion": "MARVIS\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u4fdd\u62a4\u9690\u79c1\u7684\u591a\u6a21\u6001\u9884\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.01667", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01667", "abs": "https://arxiv.org/abs/2507.01667", "authors": ["Gianluca Monaci", "Philippe Weinzaepfel", "Christian Wolf"], "title": "What does really matter in image goal navigation?", "comment": null, "summary": "Image goal navigation requires two different skills: firstly, core navigation\nskills, including the detection of free space and obstacles, and taking\ndecisions based on an internal representation; and secondly, computing\ndirectional information by comparing visual observations to the goal image.\nCurrent state-of-the-art methods either rely on dedicated image-matching, or\npre-training of computer vision modules on relative pose estimation. In this\npaper, we study whether this task can be efficiently solved with end-to-end\ntraining of full agents with RL, as has been claimed by recent work. A positive\nanswer would have impact beyond Embodied AI and allow training of relative pose\nestimation from reward for navigation alone. In a large study we investigate\nthe effect of architectural choices like late fusion, channel stacking,\nspace-to-depth projections and cross-attention, and their role in the emergence\nof relative pose estimators from navigation training. We show that the success\nof recent methods is influenced up to a certain extent by simulator settings,\nleading to shortcuts in simulation. However, we also show that these\ncapabilities can be transferred to more realistic setting, up to some extend.\nWe also find evidence for correlations between navigation performance and\nprobed (emerging) relative pose estimation performance, an important sub skill.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u56fe\u50cf\u76ee\u6807\u5bfc\u822a\u4efb\u52a1\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u89e3\u51b3\uff0c\u5e76\u5206\u6790\u4e86\u67b6\u6784\u9009\u62e9\u548c\u6a21\u62df\u5668\u8bbe\u7f6e\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u9a8c\u8bc1\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u662f\u5426\u80fd\u9ad8\u6548\u89e3\u51b3\u56fe\u50cf\u76ee\u6807\u5bfc\u822a\u4efb\u52a1\uff0c\u5e76\u63a2\u7d22\u5176\u6f5c\u5728\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5927\u89c4\u6a21\u7814\u7a76\u5206\u6790\u4e0d\u540c\u67b6\u6784\u9009\u62e9\uff08\u5982\u5ef6\u8fdf\u878d\u5408\u3001\u901a\u9053\u5806\u53e0\u7b49\uff09\u548c\u6a21\u62df\u5668\u8bbe\u7f6e\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u6a21\u62df\u5668\u8bbe\u7f6e\u4f1a\u5f71\u54cd\u6027\u80fd\uff0c\u4f46\u90e8\u5206\u80fd\u529b\u53ef\u8fc1\u79fb\u5230\u66f4\u73b0\u5b9e\u7684\u573a\u666f\uff1b\u5bfc\u822a\u6027\u80fd\u4e0e\u76f8\u5bf9\u59ff\u6001\u4f30\u8ba1\u6027\u80fd\u76f8\u5173\u3002", "conclusion": "\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u5728\u56fe\u50cf\u76ee\u6807\u5bfc\u822a\u4e2d\u6709\u6548\uff0c\u4f46\u9700\u6ce8\u610f\u6a21\u62df\u5668\u8bbe\u7f6e\u7684\u5f71\u54cd\u3002"}}
{"id": "2507.01693", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01693", "abs": "https://arxiv.org/abs/2507.01693", "authors": ["Adrians Skapars", "Edoardo Manino", "Youcheng Sun", "Lucas C. Cordeiro"], "title": "GPT, But Backwards: Exactly Inverting Language Model Outputs", "comment": "9 pages, ICML 2025 Workshop on Reliable and Responsible Foundation\n  Models", "summary": "While existing auditing techniques attempt to identify potential unwanted\nbehaviours in large language models (LLMs), we address the complementary\nforensic problem of reconstructing the exact input that led to an existing LLM\noutput - enabling post-incident analysis and potentially the detection of fake\noutput reports. We formalize exact input reconstruction as a discrete\noptimisation problem with a unique global minimum and introduce SODA, an\nefficient gradient-based algorithm that operates on a continuous relaxation of\nthe input search space with periodic restarts and parameter decay. Through\ncomprehensive experiments on LLMs ranging in size from 33M to 3B parameters, we\ndemonstrate that SODA significantly outperforms existing approaches. We succeed\nin fully recovering 79.5% of shorter out-of-distribution inputs from next-token\nlogits, without a single false positive, but struggle to extract private\ninformation from the outputs of longer (15+ token) input sequences. This\nsuggests that standard deployment practices may currently provide adequate\nprotection against malicious use of our method. Our code is available at\nhttps://doi.org/10.5281/zenodo.15539879.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSODA\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u4ece\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8f93\u51fa\u4e2d\u7cbe\u786e\u91cd\u6784\u8f93\u5165\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5ba1\u8ba1\u6280\u672f\u7684\u8865\u5145\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5ba1\u8ba1\u6280\u672f\u4e13\u6ce8\u4e8e\u8bc6\u522bLLM\u4e2d\u7684\u6f5c\u5728\u4e0d\u826f\u884c\u4e3a\uff0c\u800c\u672c\u6587\u5219\u81f4\u529b\u4e8e\u89e3\u51b3\u4e8b\u540e\u5206\u6790\u7684\u8f93\u5165\u91cd\u6784\u95ee\u9898\uff0c\u4ee5\u68c0\u6d4b\u865a\u5047\u8f93\u51fa\u62a5\u544a\u3002", "method": "\u5c06\u8f93\u5165\u91cd\u6784\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u79bb\u6563\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51faSODA\u7b97\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u641c\u7d22\u548c\u5468\u671f\u6027\u91cd\u542f\u7b49\u6280\u672f\u5728\u8fde\u7eed\u677e\u5f1b\u7a7a\u95f4\u4e2d\u9ad8\u6548\u6c42\u89e3\u3002", "result": "\u572833M\u81f33B\u53c2\u6570\u7684LLM\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cSODA\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6210\u529f\u91cd\u678479.5%\u7684\u77ed\u8f93\u5165\uff0c\u4f46\u5bf9\u957f\u8f93\u5165\uff0815+ token\uff09\u7684\u9690\u79c1\u4fe1\u606f\u63d0\u53d6\u6548\u679c\u4e0d\u4f73\u3002", "conclusion": "\u6807\u51c6\u90e8\u7f72\u5b9e\u8df5\u53ef\u80fd\u5df2\u8db3\u591f\u9632\u8303\u6076\u610f\u4f7f\u7528SODA\u7b97\u6cd5\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u957f\u8f93\u5165\u7684\u4fdd\u62a4\u63aa\u65bd\u3002"}}
{"id": "2507.01673", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01673", "abs": "https://arxiv.org/abs/2507.01673", "authors": ["Muzammil Behzad"], "title": "Facial Emotion Learning with Text-Guided Multiview Fusion via Vision-Language Model for 3D/4D Facial Expression Recognition", "comment": null, "summary": "Facial expression recognition (FER) in 3D and 4D domains presents a\nsignificant challenge in affective computing due to the complexity of spatial\nand temporal facial dynamics. Its success is crucial for advancing applications\nin human behavior understanding, healthcare monitoring, and human-computer\ninteraction. In this work, we propose FACET-VLM, a vision-language framework\nfor 3D/4D FER that integrates multiview facial representation learning with\nsemantic guidance from natural language prompts. FACET-VLM introduces three key\ncomponents: Cross-View Semantic Aggregation (CVSA) for view-consistent fusion,\nMultiview Text-Guided Fusion (MTGF) for semantically aligned facial emotions,\nand a multiview consistency loss to enforce structural coherence across views.\nOur model achieves state-of-the-art accuracy across multiple benchmarks,\nincluding BU-3DFE, Bosphorus, BU-4DFE, and BP4D-Spontaneous. We further extend\nFACET-VLM to 4D micro-expression recognition (MER) on the 4DME dataset,\ndemonstrating strong performance in capturing subtle, short-lived emotional\ncues. The extensive experimental results confirm the effectiveness and\nsubstantial contributions of each individual component within the framework.\nOverall, FACET-VLM offers a robust, extensible, and high-performing solution\nfor multimodal FER in both posed and spontaneous settings.", "AI": {"tldr": "FACET-VLM\u662f\u4e00\u79cd\u7528\u4e8e3D/4D\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u7684\u89c6\u89c9-\u8bed\u8a00\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u9762\u90e8\u8868\u793a\u5b66\u4e60\u548c\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u7684\u8bed\u4e49\u5f15\u5bfc\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u8bc6\u522b\u3002", "motivation": "3D/4D\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u5728\u60c5\u611f\u8ba1\u7b97\u4e2d\u5177\u6709\u6311\u6218\u6027\uff0c\u4f46\u5bf9\u4eba\u7c7b\u884c\u4e3a\u7406\u89e3\u3001\u533b\u7597\u76d1\u6d4b\u548c\u4eba\u673a\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faFACET-VLM\u6846\u67b6\uff0c\u5305\u542b\u8de8\u89c6\u89d2\u8bed\u4e49\u805a\u5408\uff08CVSA\uff09\u3001\u591a\u89c6\u89d2\u6587\u672c\u5f15\u5bfc\u878d\u5408\uff08MTGF\uff09\u548c\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u635f\u5931\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5e76\u6210\u529f\u6269\u5c55\u52304D\u5fae\u8868\u60c5\u8bc6\u522b\u3002", "conclusion": "FACET-VLM\u4e3a\u591a\u6a21\u6001\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u63d0\u4f9b\u4e86\u5f3a\u5927\u3001\u53ef\u6269\u5c55\u4e14\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01700", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01700", "abs": "https://arxiv.org/abs/2507.01700", "authors": ["Andrea Piras", "Matteo Negro", "Ragib Ahsan", "David Arbour", "Elena Zheleva"], "title": "Relational Causal Discovery with Latent Confounders", "comment": "30 pages, 19 figures. Accepted for publication at the 41st Conference\n  on Uncertainty in Artificial Intelligence (UAI 2025). Andrea Piras and Matteo\n  Negro contributed equally to this work", "summary": "Estimating causal effects from real-world relational data can be challenging\nwhen the underlying causal model and potential confounders are unknown. While\nseveral causal discovery algorithms exist for learning causal models with\nlatent confounders from data, they assume that the data is independent and\nidentically distributed (i.i.d.) and are not well-suited for learning from\nrelational data. Similarly, existing relational causal discovery algorithms\nassume causal sufficiency, which is unrealistic for many real-world datasets.\nTo address this gap, we propose RelFCI, a sound and complete causal discovery\nalgorithm for relational data with latent confounders. Our work builds upon the\nFast Causal Inference (FCI) and Relational Causal Discovery (RCD) algorithms\nand it defines new graphical models, necessary to support causal discovery in\nrelational domains. We also establish soundness and completeness guarantees for\nrelational d-separation with latent confounders. We present experimental\nresults demonstrating the effectiveness of RelFCI in identifying the correct\ncausal structure in relational causal models with latent confounders.", "AI": {"tldr": "\u63d0\u51faRelFCI\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5173\u7cfb\u6570\u636e\u4e2d\u6f5c\u5728\u6df7\u6742\u56e0\u7d20\u7684\u56e0\u679c\u53d1\u73b0\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u56e0\u679c\u53d1\u73b0\u7b97\u6cd5\u5728\u5904\u7406\u5173\u7cfb\u6570\u636e\u548c\u6f5c\u5728\u6df7\u6742\u56e0\u7d20\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u65b0\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8eFCI\u548cRCD\u7b97\u6cd5\uff0c\u5b9a\u4e49\u65b0\u56fe\u5f62\u6a21\u578b\uff0c\u652f\u6301\u5173\u7cfb\u6570\u636e\u4e2d\u7684\u56e0\u679c\u53d1\u73b0\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eRelFCI\u80fd\u6709\u6548\u8bc6\u522b\u542b\u6f5c\u5728\u6df7\u6742\u56e0\u7d20\u7684\u5173\u7cfb\u56e0\u679c\u6a21\u578b\u3002", "conclusion": "RelFCI\u586b\u8865\u4e86\u5173\u7cfb\u6570\u636e\u56e0\u679c\u53d1\u73b0\u7684\u7a7a\u767d\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u8df5\u610f\u4e49\u3002"}}
{"id": "2507.01559", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01559", "abs": "https://arxiv.org/abs/2507.01559", "authors": ["Lapo Frati", "Neil Traft", "Jeff Clune", "Nick Cheney"], "title": "How Weight Resampling and Optimizers Shape the Dynamics of Continual Learning and Forgetting in Neural Networks", "comment": null, "summary": "Recent work in continual learning has highlighted the beneficial effect of\nresampling weights in the last layer of a neural network (``zapping\"). Although\nempirical results demonstrate the effectiveness of this approach, the\nunderlying mechanisms that drive these improvements remain unclear. In this\nwork, we investigate in detail the pattern of learning and forgetting that take\nplace inside a convolutional neural network when trained in challenging\nsettings such as continual learning and few-shot transfer learning, with\nhandwritten characters and natural images. Our experiments show that models\nthat have undergone zapping during training more quickly recover from the shock\nof transferring to a new domain. Furthermore, to better observe the effect of\ncontinual learning in a multi-task setting we measure how each individual task\nis affected. This shows that, not only zapping, but the choice of optimizer can\nalso deeply affect the dynamics of learning and forgetting, causing complex\npatterns of synergy/interference between tasks to emerge when the model learns\nsequentially at transfer time.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u6301\u7eed\u5b66\u4e60\u548c\u5c11\u6837\u672c\u8fc1\u79fb\u5b66\u4e60\u4e2d\uff0c\u795e\u7ecf\u7f51\u7edc\u6700\u540e\u4e00\u5c42\u6743\u91cd\u91cd\u91c7\u6837\uff08\u201czapping\u201d\uff09\u7684\u6548\u679c\u53ca\u5176\u673a\u5236\uff0c\u53d1\u73b0zapping\u80fd\u5e2e\u52a9\u6a21\u578b\u66f4\u5feb\u9002\u5e94\u65b0\u9886\u57df\uff0c\u4e14\u4f18\u5316\u5668\u9009\u62e9\u5bf9\u5b66\u4e60\u52a8\u6001\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u63a2\u7d22zapping\u5728\u6301\u7eed\u5b66\u4e60\u548c\u8fc1\u79fb\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u6027\u53ca\u5176\u80cc\u540e\u7684\u673a\u5236\u3002", "method": "\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u6311\u6218\u6027\u573a\u666f\uff08\u5982\u6301\u7eed\u5b66\u4e60\u548c\u5c11\u6837\u672c\u8fc1\u79fb\u5b66\u4e60\uff09\u4e2d\u8bad\u7ec3\uff0c\u5206\u6790\u5b66\u4e60\u548c\u9057\u5fd8\u6a21\u5f0f\u3002", "result": "zapping\u80fd\u52a0\u901f\u6a21\u578b\u9002\u5e94\u65b0\u9886\u57df\uff1b\u4f18\u5316\u5668\u9009\u62e9\u663e\u8457\u5f71\u54cd\u4efb\u52a1\u95f4\u7684\u5b66\u4e60\u52a8\u6001\uff0c\u5bfc\u81f4\u590d\u6742\u7684\u534f\u540c/\u5e72\u6270\u6a21\u5f0f\u3002", "conclusion": "zapping\u548c\u4f18\u5316\u5668\u9009\u62e9\u5bf9\u6301\u7eed\u5b66\u4e60\u548c\u8fc1\u79fb\u5b66\u4e60\u7684\u52a8\u6001\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u673a\u5236\u3002"}}
{"id": "2507.01711", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01711", "abs": "https://arxiv.org/abs/2507.01711", "authors": ["Mingfu Yan", "Jiancheng Huang", "Yifan Liu", "Shifeng Chen"], "title": "Component Adaptive Clustering for Generalized Category Discovery", "comment": "Accepted by IEEE ICME 2025", "summary": "Generalized Category Discovery (GCD) tackles the challenging problem of\ncategorizing unlabeled images into both known and novel classes within a\npartially labeled dataset, without prior knowledge of the number of unknown\ncategories. Traditional methods often rely on rigid assumptions, such as\npredefining the number of classes, which limits their ability to handle the\ninherent variability and complexity of real-world data. To address these\nshortcomings, we propose AdaGCD, a cluster-centric contrastive learning\nframework that incorporates Adaptive Slot Attention (AdaSlot) into the GCD\nframework. AdaSlot dynamically determines the optimal number of slots based on\ndata complexity, removing the need for predefined slot counts. This adaptive\nmechanism facilitates the flexible clustering of unlabeled data into known and\nnovel categories by dynamically allocating representational capacity. By\nintegrating adaptive representation with dynamic slot allocation, our method\ncaptures both instance-specific and spatially clustered features, improving\nclass discovery in open-world scenarios. Extensive experiments on public and\nfine-grained datasets validate the effectiveness of our framework, emphasizing\nthe advantages of leveraging spatial local information for category discovery\nin unlabeled image datasets.", "AI": {"tldr": "AdaGCD\u662f\u4e00\u79cd\u57fa\u4e8e\u81ea\u9002\u5e94\u69fd\u6ce8\u610f\u529b\u7684\u805a\u7c7b\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u90e8\u5206\u6807\u8bb0\u6570\u636e\u96c6\u4e2d\u5df2\u77e5\u548c\u672a\u77e5\u7c7b\u522b\u7684\u56fe\u50cf\u5206\u7c7b\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u9884\u5b9a\u4e49\u7c7b\u522b\u6570\u91cf\uff0c\u9650\u5236\u4e86\u5904\u7406\u771f\u5b9e\u4e16\u754c\u6570\u636e\u7684\u80fd\u529b\u3002AdaGCD\u901a\u8fc7\u52a8\u6001\u786e\u5b9a\u69fd\u6570\u91cf\uff0c\u89e3\u51b3\u4e86\u8fd9\u4e00\u95ee\u9898\u3002", "method": "AdaGCD\u7ed3\u5408\u81ea\u9002\u5e94\u69fd\u6ce8\u610f\u529b\uff08AdaSlot\uff09\uff0c\u52a8\u6001\u5206\u914d\u8868\u793a\u5bb9\u91cf\uff0c\u7075\u6d3b\u805a\u7c7b\u672a\u6807\u8bb0\u6570\u636e\u3002", "result": "\u5728\u516c\u5f00\u548c\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86AdaGCD\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u5229\u7528\u7a7a\u95f4\u5c40\u90e8\u4fe1\u606f\u8fdb\u884c\u7c7b\u522b\u53d1\u73b0\u65b9\u9762\u3002", "conclusion": "AdaGCD\u901a\u8fc7\u81ea\u9002\u5e94\u673a\u5236\u548c\u52a8\u6001\u69fd\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e0b\u7684\u7c7b\u522b\u53d1\u73b0\u80fd\u529b\u3002"}}
{"id": "2507.01581", "categories": ["cs.LG", "cs.CR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.01581", "abs": "https://arxiv.org/abs/2507.01581", "authors": ["Masood Jan", "Wafa Njima", "Xun Zhang"], "title": "A Privacy-Preserving Indoor Localization System based on Hierarchical Federated Learning", "comment": null, "summary": "Location information serves as the fundamental element for numerous Internet\nof Things (IoT) applications. Traditional indoor localization techniques often\nproduce significant errors and raise privacy concerns due to centralized data\ncollection. In response, Machine Learning (ML) techniques offer promising\nsolutions by capturing indoor environment variations. However, they typically\nrequire central data aggregation, leading to privacy, bandwidth, and server\nreliability issues. To overcome these challenges, in this paper, we propose a\nFederated Learning (FL)-based approach for dynamic indoor localization using a\nDeep Neural Network (DNN) model. Experimental results show that FL has the\nnearby performance to Centralized Model (CL) while keeping the data privacy,\nbandwidth efficiency and server reliability. This research demonstrates that\nour proposed FL approach provides a viable solution for privacy-enhanced indoor\nlocalization, paving the way for advancements in secure and efficient indoor\nlocalization systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u7684\u52a8\u6001\u5ba4\u5185\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u96c6\u4e2d\u5f0f\u65b9\u6cd5\u7684\u9690\u79c1\u3001\u5e26\u5bbd\u548c\u670d\u52a1\u5668\u53ef\u9760\u6027\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5ba4\u5185\u5b9a\u4f4d\u6280\u672f\u5b58\u5728\u8bef\u5dee\u5927\u548c\u9690\u79c1\u95ee\u9898\uff0c\u800c\u96c6\u4e2d\u5f0f\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u53c8\u5e26\u6765\u9690\u79c1\u3001\u5e26\u5bbd\u548c\u670d\u52a1\u5668\u53ef\u9760\u6027\u6311\u6218\u3002", "method": "\u91c7\u7528\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u6a21\u578b\u8fdb\u884c\u52a8\u6001\u5ba4\u5185\u5b9a\u4f4d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFL\u5728\u6027\u80fd\u4e0a\u63a5\u8fd1\u96c6\u4e2d\u5f0f\u6a21\u578b\uff08CL\uff09\uff0c\u540c\u65f6\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u3001\u63d0\u9ad8\u5e26\u5bbd\u6548\u7387\u548c\u670d\u52a1\u5668\u53ef\u9760\u6027\u3002", "conclusion": "FL\u4e3a\u9690\u79c1\u589e\u5f3a\u7684\u5ba4\u5185\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u5b89\u5168\u9ad8\u6548\u5ba4\u5185\u5b9a\u4f4d\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.01712", "categories": ["cs.CV", "eess.IV", "stat.AP"], "pdf": "https://arxiv.org/pdf/2507.01712", "abs": "https://arxiv.org/abs/2507.01712", "authors": ["Xinle Tian", "Matthew Nunes", "Emiko Dupont", "Shaunagh Downing", "Freddie Lichtenstein", "Matt Burns"], "title": "Using Wavelet Domain Fingerprints to Improve Source Camera Identification", "comment": null, "summary": "Camera fingerprint detection plays a crucial role in source identification\nand image forensics, with wavelet denoising approaches proving to be\nparticularly effective in extracting sensor pattern noise (SPN). In this\narticle, we propose a modification to wavelet-based SPN extraction. Rather than\nconstructing the fingerprint as an image, we introduce the notion of a wavelet\ndomain fingerprint. This avoids the final inversion step of the denoising\nalgorithm and allows fingerprint comparisons to be made directly in the wavelet\ndomain. As such, our modification streamlines the extraction and comparison\nprocess. Experimental results on real-world datasets demonstrate that our\nmethod not only achieves higher detection accuracy but can also significantly\nimprove processing speed.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u5c0f\u6ce2\u57df\u6307\u7eb9\u63d0\u53d6\u65b9\u6cd5\uff0c\u76f4\u63a5\u5728\u9891\u57df\u8fdb\u884c\u6307\u7eb9\u6bd4\u8f83\uff0c\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u548c\u5904\u7406\u901f\u5ea6\u3002", "motivation": "\u4f20\u7edf\u5c0f\u6ce2\u53bb\u566a\u65b9\u6cd5\u9700\u8981\u56fe\u50cf\u91cd\u5efa\u6b65\u9aa4\uff0c\u6539\u8fdb\u65b9\u6cd5\u65e8\u5728\u7b80\u5316\u6d41\u7a0b\u5e76\u63d0\u5347\u6027\u80fd\u3002", "method": "\u5c06\u6307\u7eb9\u6784\u5efa\u4e3a\u5c0f\u6ce2\u57df\u6307\u7eb9\uff0c\u907f\u514d\u53bb\u566a\u7b97\u6cd5\u7684\u6700\u7ec8\u53cd\u8f6c\u6b65\u9aa4\uff0c\u76f4\u63a5\u5728\u9891\u57df\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6539\u8fdb\u65b9\u6cd5\u5728\u68c0\u6d4b\u7cbe\u5ea6\u548c\u5904\u7406\u901f\u5ea6\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u5c0f\u6ce2\u57df\u6307\u7eb9\u63d0\u53d6\u65b9\u6cd5\u66f4\u9ad8\u6548\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u56fe\u50cf\u53d6\u8bc1\u4efb\u52a1\u3002"}}
{"id": "2507.01598", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01598", "abs": "https://arxiv.org/abs/2507.01598", "authors": ["Naoki Sato", "Hiroki Naganuma", "Hideaki Iiduka"], "title": "Analysis of Muon's Convergence and Critical Batch Size", "comment": null, "summary": "This paper presents a theoretical analysis of Muon, a new optimizer that\nleverages the inherent matrix structure of neural network parameters. We\nprovide convergence proofs for four practical variants of Muon: with and\nwithout Nesterov momentum, and with and without weight decay. We then show that\nadding weight decay leads to strictly tighter bounds on both the parameter and\ngradient norms, and we clarify the relationship between the weight decay\ncoefficient and the learning rate. Finally, we derive Muon's critical batch\nsize minimizing the stochastic first-order oracle (SFO) complexity, which is\nthe stochastic computational cost, and validate our theoretical findings with\nexperiments.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86Muon\u4f18\u5316\u5668\u7684\u7406\u8bba\u7279\u6027\uff0c\u5305\u62ec\u6536\u655b\u6027\u8bc1\u660e\u3001\u6743\u91cd\u8870\u51cf\u7684\u5f71\u54cd\u4ee5\u53ca\u4e34\u754c\u6279\u91cf\u5927\u5c0f\u7684\u63a8\u5bfc\u3002", "motivation": "\u7814\u7a76Muon\u4f18\u5316\u5668\u7684\u7406\u8bba\u57fa\u7840\uff0c\u63a2\u7d22\u5176\u5728\u4e0d\u540c\u53d8\u4f53\u4e0b\u7684\u6536\u655b\u6027\uff0c\u4ee5\u53ca\u6743\u91cd\u8870\u51cf\u5bf9\u53c2\u6570\u548c\u68af\u5ea6\u8303\u6570\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u4f9b\u4e86\u56db\u79cdMuon\u53d8\u4f53\u7684\u6536\u655b\u6027\u8bc1\u660e\uff08\u5e26/\u4e0d\u5e26Nesterov\u52a8\u91cf\uff0c\u5e26/\u4e0d\u5e26\u6743\u91cd\u8870\u51cf\uff09\uff0c\u5e76\u5206\u6790\u4e86\u6743\u91cd\u8870\u51cf\u7cfb\u6570\u4e0e\u5b66\u4e60\u7387\u7684\u5173\u7cfb\u3002\u63a8\u5bfc\u4e86\u4e34\u754c\u6279\u91cf\u5927\u5c0f\u4ee5\u6700\u5c0f\u5316SFO\u590d\u6742\u5ea6\u3002", "result": "\u6743\u91cd\u8870\u51cf\u80fd\u4e25\u683c\u6536\u7d27\u53c2\u6570\u548c\u68af\u5ea6\u8303\u6570\u7684\u754c\u9650\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u3002", "conclusion": "Muon\u4f18\u5316\u5668\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u6743\u91cd\u8870\u51cf\u548c\u4e34\u754c\u6279\u91cf\u5927\u5c0f\u7684\u5206\u6790\u4e3a\u5176\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2507.01721", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01721", "abs": "https://arxiv.org/abs/2507.01721", "authors": ["Zhongwen Zhang", "Yuri Boykov"], "title": "Soft Self-labeling and Potts Relaxations for Weakly-Supervised Segmentation", "comment": "published at CVPR 2025", "summary": "We consider weakly supervised segmentation where only a fraction of pixels\nhave ground truth labels (scribbles) and focus on a self-labeling approach\noptimizing relaxations of the standard unsupervised CRF/Potts loss on unlabeled\npixels. While WSSS methods can directly optimize such losses via gradient\ndescent, prior work suggests that higher-order optimization can improve network\ntraining by introducing hidden pseudo-labels and powerful CRF sub-problem\nsolvers, e.g. graph cut. However, previously used hard pseudo-labels can not\nrepresent class uncertainty or errors, which motivates soft self-labeling. We\nderive a principled auxiliary loss and systematically evaluate standard and new\nCRF relaxations (convex and non-convex), neighborhood systems, and terms\nconnecting network predictions with soft pseudo-labels. We also propose a\ngeneral continuous sub-problem solver. Using only standard architectures, soft\nself-labeling consistently improves scribble-based training and outperforms\nsignificantly more complex specialized WSSS systems. It can outperform full\npixel-precise supervision. Our general ideas apply to other weakly-supervised\nproblems/systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f6f\u81ea\u6807\u8bb0\u7684\u5f31\u76d1\u7763\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316CRF/Potts\u635f\u5931\u7684\u677e\u5f1b\u5f62\u5f0f\uff0c\u6539\u8fdb\u4e86\u7f51\u7edc\u8bad\u7ec3\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u786c\u4f2a\u6807\u7b7e\u65e0\u6cd5\u8868\u793a\u7c7b\u522b\u4e0d\u786e\u5b9a\u6027\u6216\u9519\u8bef\uff0c\u56e0\u6b64\u63d0\u51fa\u8f6f\u81ea\u6807\u8bb0\u4ee5\u6539\u8fdb\u5f31\u76d1\u7763\u5206\u5272\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f85\u52a9\u635f\u5931\u51fd\u6570\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u6807\u51c6\u548c\u65b0CRF\u677e\u5f1b\u3001\u90bb\u57df\u7cfb\u7edf\u53ca\u7f51\u7edc\u9884\u6d4b\u4e0e\u8f6f\u4f2a\u6807\u7b7e\u7684\u8fde\u63a5\u65b9\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u8fde\u7eed\u5b50\u95ee\u9898\u6c42\u89e3\u5668\u3002", "result": "\u8f6f\u81ea\u6807\u8bb0\u663e\u8457\u6539\u8fdb\u4e86\u57fa\u4e8e\u6d82\u9e26\u7684\u8bad\u7ec3\u6548\u679c\uff0c\u6027\u80fd\u4f18\u4e8e\u66f4\u590d\u6742\u7684\u4e13\u7528\u5f31\u76d1\u7763\u5206\u5272\u7cfb\u7edf\uff0c\u751a\u81f3\u4f18\u4e8e\u5168\u50cf\u7d20\u7ea7\u76d1\u7763\u3002", "conclusion": "\u8f6f\u81ea\u6807\u8bb0\u662f\u4e00\u79cd\u6709\u6548\u7684\u5f31\u76d1\u7763\u5206\u5272\u65b9\u6cd5\uff0c\u5176\u601d\u60f3\u53ef\u63a8\u5e7f\u5230\u5176\u4ed6\u5f31\u76d1\u7763\u95ee\u9898\u6216\u7cfb\u7edf\u3002"}}
{"id": "2507.01636", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.01636", "abs": "https://arxiv.org/abs/2507.01636", "authors": ["Ghasem Alipoor", "Karl Skretting"], "title": "Kernel Recursive Least Squares Dictionary Learning Algorithm", "comment": "Published in Digital Signal Processing, Volume 141, 2023. DOI:\n  https://doi.org/10.1016/j.dsp.2023.104159 12 pages, 8 figures. Code and data\n  available at: https://github.com/G-Alipoor/kernel-rls-dictionary-learning", "summary": "We propose an efficient online dictionary learning algorithm for kernel-based\nsparse representations. In this framework, input signals are nonlinearly mapped\nto a high-dimensional feature space and represented sparsely using a virtual\ndictionary. At each step, the dictionary is updated recursively using a novel\nalgorithm based on the recursive least squares (RLS) method. This update\nmechanism works with single samples or mini-batches and maintains low\ncomputational complexity. Experiments on four datasets across different domains\nshow that our method not only outperforms existing online kernel dictionary\nlearning approaches but also achieves classification accuracy close to that of\nbatch-trained models, while remaining significantly more efficient.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5728\u7ebf\u6838\u7a00\u758f\u8868\u793a\u5b57\u5178\u5b66\u4e60\u7b97\u6cd5\uff0c\u57fa\u4e8eRLS\u65b9\u6cd5\u9012\u5f52\u66f4\u65b0\u5b57\u5178\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5728\u7ebf\u6838\u7a00\u758f\u8868\u793a\u5b57\u5178\u5b66\u4e60\u7684\u9ad8\u6548\u6027\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u5206\u7c7b\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u9012\u5f52\u6700\u5c0f\u4e8c\u4e58\u6cd5\uff08RLS\uff09\u9012\u5f52\u66f4\u65b0\u5b57\u5178\uff0c\u652f\u6301\u5355\u6837\u672c\u6216\u5c0f\u6279\u91cf\u5904\u7406\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u4f4e\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u5728\u7ebf\u65b9\u6cd5\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u63a5\u8fd1\u6279\u91cf\u8bad\u7ec3\u6a21\u578b\uff0c\u4e14\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u5728\u7ebf\u5b66\u4e60\u573a\u666f\u3002"}}
{"id": "2507.01722", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01722", "abs": "https://arxiv.org/abs/2507.01722", "authors": ["Enrico Cassano", "Riccardo Renzulli", "Andrea Bragagnolo", "Marco Grangetto"], "title": "When Does Pruning Benefit Vision Representations?", "comment": null, "summary": "Pruning is widely used to reduce the complexity of deep learning models, but\nits effects on interpretability and representation learning remain poorly\nunderstood. This paper investigates how pruning influences vision models across\nthree key dimensions: (i) interpretability, (ii) unsupervised object discovery,\nand (iii) alignment with human perception. We first analyze different vision\nnetwork architectures to examine how varying sparsity levels affect feature\nattribution interpretability methods. Additionally, we explore whether pruning\npromotes more succinct and structured representations, potentially improving\nunsupervised object discovery by discarding redundant information while\npreserving essential features. Finally, we assess whether pruning enhances the\nalignment between model representations and human perception, investigating\nwhether sparser models focus on more discriminative features similarly to\nhumans. Our findings also reveal the presence of sweet spots, where sparse\nmodels exhibit higher interpretability, downstream generalization and human\nalignment. However, these spots highly depend on the network architectures and\ntheir size in terms of trainable parameters. Our results suggest a complex\ninterplay between these three dimensions, highlighting the importance of\ninvestigating when and how pruning benefits vision representations.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u526a\u679d\u5bf9\u89c6\u89c9\u6a21\u578b\u5728\u53ef\u89e3\u91ca\u6027\u3001\u65e0\u76d1\u7763\u76ee\u6807\u53d1\u73b0\u548c\u4eba\u7c7b\u611f\u77e5\u5bf9\u9f50\u4e09\u4e2a\u7ef4\u5ea6\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u7a00\u758f\u6a21\u578b\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u6548\u679c\u9ad8\u5ea6\u4f9d\u8d56\u7f51\u7edc\u67b6\u6784\u548c\u53c2\u6570\u89c4\u6a21\u3002", "motivation": "\u526a\u679d\u867d\u80fd\u964d\u4f4e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u590d\u6742\u5ea6\uff0c\u4f46\u5176\u5bf9\u53ef\u89e3\u91ca\u6027\u548c\u8868\u793a\u5b66\u4e60\u7684\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\uff0c\u56e0\u6b64\u9700\u8981\u6df1\u5165\u7814\u7a76\u3002", "method": "\u5206\u6790\u4e0d\u540c\u89c6\u89c9\u7f51\u7edc\u67b6\u6784\u5728\u4e0d\u540c\u7a00\u758f\u5ea6\u4e0b\u5bf9\u7279\u5f81\u5f52\u56e0\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u7684\u5f71\u54cd\uff0c\u5e76\u63a2\u7d22\u526a\u679d\u662f\u5426\u4fc3\u8fdb\u66f4\u7b80\u6d01\u7684\u7ed3\u6784\u5316\u8868\u793a\uff0c\u4ee5\u53ca\u662f\u5426\u589e\u5f3a\u6a21\u578b\u4e0e\u4eba\u7c7b\u611f\u77e5\u7684\u5bf9\u9f50\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u7a00\u758f\u6a21\u578b\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u53ef\u89e3\u91ca\u6027\u3001\u4e0b\u6e38\u6cdb\u5316\u80fd\u529b\u548c\u4eba\u7c7b\u5bf9\u9f50\u6027\uff0c\u4f46\u8fd9\u4e9b\u6548\u679c\u9ad8\u5ea6\u4f9d\u8d56\u7f51\u7edc\u67b6\u6784\u548c\u53c2\u6570\u89c4\u6a21\u3002", "conclusion": "\u526a\u679d\u5bf9\u89c6\u89c9\u8868\u793a\u7684\u5f71\u54cd\u590d\u6742\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u9002\u7528\u6761\u4ef6\u548c\u65b9\u5f0f\u3002"}}
{"id": "2507.01761", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.01761", "abs": "https://arxiv.org/abs/2507.01761", "authors": ["Nicolas Salvy", "Hugues Talbot", "Bertrand Thirion"], "title": "Enhanced Generative Model Evaluation with Clipped Density and Coverage", "comment": null, "summary": "Although generative models have made remarkable progress in recent years,\ntheir use in critical applications has been hindered by their incapacity to\nreliably evaluate sample quality. Quality refers to at least two complementary\nconcepts: fidelity and coverage. Current quality metrics often lack reliable,\ninterpretable values due to an absence of calibration or insufficient\nrobustness to outliers. To address these shortcomings, we introduce two novel\nmetrics, Clipped Density and Clipped Coverage. By clipping individual sample\ncontributions and, for fidelity, the radii of nearest neighbor balls, our\nmetrics prevent out-of-distribution samples from biasing the aggregated values.\nThrough analytical and empirical calibration, these metrics exhibit linear\nscore degradation as the proportion of poor samples increases. Thus, they can\nbe straightforwardly interpreted as equivalent proportions of good samples.\nExtensive experiments on synthetic and real-world datasets demonstrate that\nClipped Density and Clipped Coverage outperform existing methods in terms of\nrobustness, sensitivity, and interpretability for evaluating generative models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u6307\u6807\uff08Clipped Density\u548cClipped Coverage\uff09\uff0c\u7528\u4e8e\u66f4\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u5730\u8bc4\u4f30\u751f\u6210\u6a21\u578b\u6837\u672c\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6307\u6807\u5728\u9c81\u68d2\u6027\u548c\u6821\u51c6\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u5728\u5173\u952e\u5e94\u7528\u4e2d\u56e0\u6837\u672c\u8d28\u91cf\u8bc4\u4f30\u4e0d\u53ef\u9760\u800c\u53d7\u9650\uff0c\u73b0\u6709\u6307\u6807\u7f3a\u4e4f\u6821\u51c6\u548c\u5bf9\u5f02\u5e38\u503c\u7684\u9c81\u68d2\u6027\u3002", "method": "\u901a\u8fc7\u526a\u88c1\u5355\u4e2a\u6837\u672c\u8d21\u732e\u548c\u6700\u8fd1\u90bb\u7403\u534a\u5f84\uff0c\u9632\u6b62\u5f02\u5e38\u6837\u672c\u5f71\u54cd\u6574\u4f53\u8bc4\u4f30\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u548c\u7ecf\u9a8c\u6821\u51c6\u4f7f\u6307\u6807\u7ebf\u6027\u9000\u5316\u3002", "result": "\u65b0\u6307\u6807\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3001\u654f\u611f\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Clipped Density\u548cClipped Coverage\u4e3a\u751f\u6210\u6a21\u578b\u6837\u672c\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u548c\u76f4\u89c2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01644", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01644", "abs": "https://arxiv.org/abs/2507.01644", "authors": ["Miguel O'Malley"], "title": "Dance Dance ConvLSTM", "comment": "15 pages, 9 figures, 4 tables", "summary": "\\textit{Dance Dance Revolution} is a rhythm game consisting of songs and\naccompanying choreography, referred to as charts. Players press arrows on a\ndevice referred to as a dance pad in time with steps determined by the song's\nchart. In 2017, the authors of Dance Dance Convolution (DDC) developed an\nalgorithm for the automatic generation of \\textit{Dance Dance Revolution}\ncharts, utilizing a CNN-LSTM architecture. We introduce Dance Dance ConvLSTM\n(DDCL), a new method for the automatic generation of DDR charts using a\nConvLSTM based model, which improves upon the DDC methodology and substantially\nincreases the accuracy of chart generation.", "AI": {"tldr": "DDCL\u662f\u4e00\u79cd\u57fa\u4e8eConvLSTM\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210DDR\u6e38\u620f\u56fe\u8868\uff0c\u76f8\u6bd4\u4e4b\u524d\u7684DDC\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u51c6\u786e\u6027\u3002", "motivation": "\u6539\u8fdbDDR\u6e38\u620f\u56fe\u8868\u7684\u81ea\u52a8\u751f\u6210\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u751f\u6210\u51c6\u786e\u6027\u548c\u7528\u6237\u4f53\u9a8c\u3002", "method": "\u91c7\u7528ConvLSTM\u67b6\u6784\uff0c\u7ed3\u5408CNN\u548cLSTM\u7684\u4f18\u52bf\uff0c\u4f18\u5316\u56fe\u8868\u751f\u6210\u7b97\u6cd5\u3002", "result": "DDCL\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u56fe\u8868\u751f\u6210\u7684\u51c6\u786e\u6027\u3002", "conclusion": "DDCL\u662f\u4e00\u79cd\u6709\u6548\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u9002\u7528\u4e8eDDR\u6e38\u620f\u56fe\u8868\u7684\u81ea\u52a8\u751f\u6210\u3002"}}
{"id": "2507.01781", "categories": ["cs.LG", "cs.AI", "68T07 (Primary) 62H30, 68T05 (Secondary)"], "pdf": "https://arxiv.org/pdf/2507.01781", "abs": "https://arxiv.org/abs/2507.01781", "authors": ["Dalia Rodr\u00edguez-Salas", "Christian Riess"], "title": "BranchNet: A Neuro-Symbolic Learning Framework for Structured Multi-Class Classification", "comment": "18 pages, 3 figures (with two images each)", "summary": "We introduce BranchNet, a neuro-symbolic learning framework that transforms\ndecision tree ensembles into sparse, partially connected neural networks. Each\nbranch, defined as a decision path from root to a parent of leaves, is mapped\nto a hidden neuron, preserving symbolic structure while enabling gradient-based\noptimization. The resulting models are compact, interpretable, and require no\nmanual architecture tuning. Evaluated on a suite of structured multi-class\nclassification benchmarks, BranchNet consistently outperforms XGBoost in\naccuracy, with statistically significant gains. We detail the architecture,\ntraining procedure, and sparsity dynamics, and discuss the model's strengths in\nsymbolic interpretability as well as its current limitations, particularly on\nbinary tasks where further adaptive calibration may be beneficial.", "AI": {"tldr": "BranchNet\u5c06\u51b3\u7b56\u6811\u96c6\u6210\u8f6c\u5316\u4e3a\u7a00\u758f\u3001\u90e8\u5206\u8fde\u63a5\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u4fdd\u7559\u7b26\u53f7\u7ed3\u6784\u5e76\u652f\u6301\u68af\u5ea6\u4f18\u5316\uff0c\u6027\u80fd\u4f18\u4e8eXGBoost\u3002", "motivation": "\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u7684\u68af\u5ea6\u4f18\u5316\u80fd\u529b\u4e0e\u51b3\u7b56\u6811\u7684\u7b26\u53f7\u53ef\u89e3\u91ca\u6027\uff0c\u5f00\u53d1\u7d27\u51d1\u4e14\u65e0\u9700\u624b\u52a8\u8c03\u4f18\u7684\u6a21\u578b\u3002", "method": "\u5c06\u51b3\u7b56\u6811\u7684\u6bcf\u4e2a\u5206\u652f\u6620\u5c04\u4e3a\u9690\u85cf\u795e\u7ecf\u5143\uff0c\u6784\u5efa\u7a00\u758f\u795e\u7ecf\u7f51\u7edc\uff0c\u4fdd\u7559\u7b26\u53f7\u7ed3\u6784\u3002", "result": "\u5728\u591a\u7c7b\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBranchNet\u51c6\u786e\u7387\u663e\u8457\u4f18\u4e8eXGBoost\u3002", "conclusion": "BranchNet\u5728\u7b26\u53f7\u53ef\u89e3\u91ca\u6027\u548c\u6027\u80fd\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u4e8c\u5143\u4efb\u52a1\u4e2d\u53ef\u80fd\u9700\u8981\u8fdb\u4e00\u6b65\u6821\u51c6\u3002"}}
{"id": "2507.01737", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01737", "abs": "https://arxiv.org/abs/2507.01737", "authors": ["Lin Wu", "Zhixiang Chen", "Jianglin Lan"], "title": "HOI-Dyn: Learning Interaction Dynamics for Human-Object Motion Diffusion", "comment": null, "summary": "Generating realistic 3D human-object interactions (HOIs) remains a\nchallenging task due to the difficulty of modeling detailed interaction\ndynamics. Existing methods treat human and object motions independently,\nresulting in physically implausible and causally inconsistent behaviors. In\nthis work, we present HOI-Dyn, a novel framework that formulates HOI generation\nas a driver-responder system, where human actions drive object responses. At\nthe core of our method is a lightweight transformer-based interaction dynamics\nmodel that explicitly predicts how objects should react to human motion. To\nfurther enforce consistency, we introduce a residual-based dynamics loss that\nmitigates the impact of dynamics prediction errors and prevents misleading\noptimization signals. The dynamics model is used only during training,\npreserving inference efficiency. Through extensive qualitative and quantitative\nexperiments, we demonstrate that our approach not only enhances the quality of\nHOI generation but also establishes a feasible metric for evaluating the\nquality of generated interactions.", "AI": {"tldr": "HOI-Dyn\u6846\u67b6\u901a\u8fc7\u9a71\u52a8-\u54cd\u5e94\u7cfb\u7edf\u751f\u6210\u903c\u771f\u76843D\u4eba-\u7269\u4ea4\u4e92\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7Transformer\u6a21\u578b\u9884\u6d4b\u7269\u4f53\u5bf9\u4eba\u7269\u52a8\u4f5c\u7684\u53cd\u5e94\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u635f\u5931\u4f18\u5316\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u72ec\u7acb\u5904\u7406\u4eba\u7269\u548c\u7269\u4f53\u8fd0\u52a8\uff0c\u5bfc\u81f4\u4ea4\u4e92\u884c\u4e3a\u7269\u7406\u4e0a\u4e0d\u5408\u7406\u4e14\u56e0\u679c\u4e0d\u4e00\u81f4\u3002", "method": "\u63d0\u51faHOI-Dyn\u6846\u67b6\uff0c\u5c06\u4ea4\u4e92\u751f\u6210\u5efa\u6a21\u4e3a\u9a71\u52a8-\u54cd\u5e94\u7cfb\u7edf\uff0c\u4f7f\u7528Transformer\u6a21\u578b\u9884\u6d4b\u7269\u4f53\u54cd\u5e94\uff0c\u5e76\u5f15\u5165\u52a8\u6001\u635f\u5931\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u4ea4\u4e92\u751f\u6210\u8d28\u91cf\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u8bc4\u4f30\u6307\u6807\u3002", "conclusion": "HOI-Dyn\u6846\u67b6\u5728\u751f\u6210\u903c\u771f3D\u4eba-\u7269\u4ea4\u4e92\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2507.01738", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01738", "abs": "https://arxiv.org/abs/2507.01738", "authors": ["Ming Dai", "Wenxuan Cheng", "Jiang-jiang Liu", "Sen Yang", "Wenxiao Cai", "Yanpeng Sun", "Wankou Yang"], "title": "DeRIS: Decoupling Perception and Cognition for Enhanced Referring Image Segmentation through Loopback Synergy", "comment": "ICCV 2025", "summary": "Referring Image Segmentation (RIS) is a challenging task that aims to segment\nobjects in an image based on natural language expressions. While prior studies\nhave predominantly concentrated on improving vision-language interactions and\nachieving fine-grained localization, a systematic analysis of the fundamental\nbottlenecks in existing RIS frameworks remains underexplored. To bridge this\ngap, we propose DeRIS, a novel framework that decomposes RIS into two key\ncomponents: perception and cognition. This modular decomposition facilitates a\nsystematic analysis of the primary bottlenecks impeding RIS performance. Our\nfindings reveal that the predominant limitation lies not in perceptual\ndeficiencies, but in the insufficient multi-modal cognitive capacity of current\nmodels. To mitigate this, we propose a Loopback Synergy mechanism, which\nenhances the synergy between the perception and cognition modules, thereby\nenabling precise segmentation while simultaneously improving robust image-text\ncomprehension. Additionally, we analyze and introduce a simple non-referent\nsample conversion data augmentation to address the long-tail distribution issue\nrelated to target existence judgement in general scenarios. Notably, DeRIS\ndemonstrates inherent adaptability to both non- and multi-referents scenarios\nwithout requiring specialized architectural modifications, enhancing its\ngeneral applicability. The codes and models are available at\nhttps://github.com/Dmmm1997/DeRIS.", "AI": {"tldr": "DeRIS\u6846\u67b6\u5c06Referring Image Segmentation\uff08RIS\uff09\u5206\u89e3\u4e3a\u611f\u77e5\u548c\u8ba4\u77e5\u4e24\u4e2a\u6a21\u5757\uff0c\u901a\u8fc7Loopback Synergy\u673a\u5236\u63d0\u5347\u591a\u6a21\u6001\u8ba4\u77e5\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u6570\u636e\u589e\u5f3a\u89e3\u51b3\u957f\u5c3e\u5206\u5e03\u95ee\u9898\u3002", "motivation": "\u73b0\u6709RIS\u6846\u67b6\u7f3a\u4e4f\u5bf9\u6027\u80fd\u74f6\u9888\u7684\u7cfb\u7edf\u5206\u6790\uff0c\u5c24\u5176\u662f\u591a\u6a21\u6001\u8ba4\u77e5\u80fd\u529b\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51faDeRIS\u6846\u67b6\uff0c\u5206\u89e3RIS\u4e3a\u611f\u77e5\u548c\u8ba4\u77e5\u6a21\u5757\uff0c\u5f15\u5165Loopback Synergy\u673a\u5236\u548c\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002", "result": "DeRIS\u5728\u975e\u548c\u591a\u76ee\u6807\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u65e0\u9700\u4e13\u95e8\u67b6\u6784\u8c03\u6574\u3002", "conclusion": "DeRIS\u901a\u8fc7\u6a21\u5757\u5316\u5206\u6790\u548c\u673a\u5236\u6539\u8fdb\uff0c\u663e\u8457\u63d0\u5347\u4e86RIS\u7684\u6027\u80fd\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2507.01744", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01744", "abs": "https://arxiv.org/abs/2507.01744", "authors": ["Benjamin Jin", "Grant Mair", "Joanna M. Wardlaw", "Maria del C. Vald\u00e9s Hern\u00e1ndez"], "title": "Calibrated Self-supervised Vision Transformers Improve Intracranial Arterial Calcification Segmentation from Clinical CT Head Scans", "comment": null, "summary": "Vision Transformers (ViTs) have gained significant popularity in the natural\nimage domain but have been less successful in 3D medical image segmentation.\nNevertheless, 3D ViTs are particularly interesting for large medical imaging\nvolumes due to their efficient self-supervised training within the masked\nautoencoder (MAE) framework, which enables the use of imaging data without the\nneed for expensive manual annotations. intracranial arterial calcification\n(IAC) is an imaging biomarker visible on routinely acquired CT scans linked to\nneurovascular diseases such as stroke and dementia, and automated IAC\nquantification could enable their large-scale risk assessment. We pre-train\nViTs with MAE and fine-tune them for IAC segmentation for the first time. To\ndevelop our models, we use highly heterogeneous data from a large clinical\ntrial, the third International Stroke Trial (IST-3). We evaluate key aspects of\nMAE pre-trained ViTs in IAC segmentation, and analyse the clinical\nimplications. We show: 1) our calibrated self-supervised ViT beats a strong\nsupervised nnU-Net baseline by 3.2 Dice points, 2) low patch sizes are crucial\nfor ViTs for IAC segmentation and interpolation upsampling with regular\nconvolutions is preferable to transposed convolutions for ViT-based models, and\n3) our ViTs increase robustness to higher slice thicknesses and improve risk\ngroup classification in a clinical scenario by 46%. Our code is available\nonline.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8eMAE\u9884\u8bad\u7ec3\u7684ViT\u57283D\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u9885\u5185\u52a8\u8109\u9499\u5316\uff08IAC\uff09\u5206\u5272\u4efb\u52a1\u4e2d\uff0c\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "3D ViTs\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5e94\u7528\u8f83\u5c11\u3002IAC\u4f5c\u4e3a\u795e\u7ecf\u8840\u7ba1\u75be\u75c5\u7684\u751f\u7269\u6807\u5fd7\u7269\uff0c\u5176\u81ea\u52a8\u5316\u91cf\u5316\u6709\u52a9\u4e8e\u5927\u89c4\u6a21\u98ce\u9669\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528MAE\u6846\u67b6\u9884\u8bad\u7ec3ViT\uff0c\u5e76\u5728IST-3\u4e34\u5e8a\u8bd5\u9a8c\u6570\u636e\u4e0a\u5fae\u8c03\u7528\u4e8eIAC\u5206\u5272\u3002\u63a2\u8ba8\u4e86\u4f4epatch\u5927\u5c0f\u548c\u63d2\u503c\u4e0a\u91c7\u6837\u7684\u91cd\u8981\u6027\u3002", "result": "\u81ea\u76d1\u7763ViT\u5728Dice\u5206\u6570\u4e0a\u6bd4\u76d1\u7763nnU-Net\u57fa\u7ebf\u9ad83.2\u5206\uff0c\u5bf9\u9ad8\u5207\u7247\u539a\u5ea6\u66f4\u9c81\u68d2\uff0c\u4e34\u5e8a\u98ce\u9669\u5206\u7c7b\u63d0\u534746%\u3002", "conclusion": "MAE\u9884\u8bad\u7ec3\u7684ViT\u5728IAC\u5206\u5272\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u4e34\u5e8a\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.01788", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01788", "abs": "https://arxiv.org/abs/2507.01788", "authors": ["Montasir Shams", "Chashi Mahiul Islam", "Shaeke Salman", "Phat Tran", "Xiuwen Liu"], "title": "Are Vision Transformer Representations Semantically Meaningful? A Case Study in Medical Imaging", "comment": "9 pages", "summary": "Vision transformers (ViTs) have rapidly gained prominence in medical imaging\ntasks such as disease classification, segmentation, and detection due to their\nsuperior accuracy compared to conventional deep learning models. However, due\nto their size and complex interactions via the self-attention mechanism, they\nare not well understood. In particular, it is unclear whether the\nrepresentations produced by such models are semantically meaningful. In this\npaper, using a projected gradient-based algorithm, we show that their\nrepresentations are not semantically meaningful and they are inherently\nvulnerable to small changes. Images with imperceptible differences can have\nvery different representations; on the other hand, images that should belong to\ndifferent semantic classes can have nearly identical representations. Such\nvulnerability can lead to unreliable classification results; for example,\nunnoticeable changes cause the classification accuracy to be reduced by over\n60\\%. %. To the best of our knowledge, this is the first work to systematically\ndemonstrate this fundamental lack of semantic meaningfulness in ViT\nrepresentations for medical image classification, revealing a critical\nchallenge for their deployment in safety-critical systems.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u89c6\u89c9\u53d8\u6362\u5668\uff08ViT\uff09\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u8868\u5f81\u7f3a\u4e4f\u8bed\u4e49\u610f\u4e49\uff0c\u4e14\u5bf9\u5fae\u5c0f\u53d8\u5316\u654f\u611f\uff0c\u5bfc\u81f4\u5206\u7c7b\u7ed3\u679c\u4e0d\u53ef\u9760\u3002", "motivation": "\u63a2\u7d22ViT\u5728\u533b\u5b66\u56fe\u50cf\u4efb\u52a1\u4e2d\u7684\u8868\u5f81\u662f\u5426\u5177\u6709\u8bed\u4e49\u610f\u4e49\u53ca\u5176\u7a33\u5b9a\u6027\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u6295\u5f71\u68af\u5ea6\u7684\u7b97\u6cd5\u5206\u6790ViT\u7684\u8868\u5f81\u3002", "result": "ViT\u7684\u8868\u5f81\u5bf9\u5fae\u5c0f\u53d8\u5316\u654f\u611f\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u53ef\u964d\u4f4e60%\u4ee5\u4e0a\u3002", "conclusion": "ViT\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u8868\u5f81\u5b58\u5728\u6839\u672c\u6027\u7f3a\u9677\uff0c\u5f71\u54cd\u5176\u5728\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u4e2d\u7684\u90e8\u7f72\u3002"}}
{"id": "2507.01747", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01747", "abs": "https://arxiv.org/abs/2507.01747", "authors": ["Nora Gourmelon", "Marcel Dreier", "Martin Mayr", "Thorsten Seehaus", "Dakota Pyles", "Matthias Braun", "Andreas Maier", "Vincent Christlein"], "title": "SSL4SAR: Self-Supervised Learning for Glacier Calving Front Extraction from SAR Imagery", "comment": "in IEEE Transactions on Geoscience and Remote Sensing. arXiv admin\n  note: text overlap with arXiv:2501.05281", "summary": "Glaciers are losing ice mass at unprecedented rates, increasing the need for\naccurate, year-round monitoring to understand frontal ablation, particularly\nthe factors driving the calving process. Deep learning models can extract\ncalving front positions from Synthetic Aperture Radar imagery to track seasonal\nice losses at the calving fronts of marine- and lake-terminating glaciers. The\ncurrent state-of-the-art model relies on ImageNet-pretrained weights. However,\nthey are suboptimal due to the domain shift between the natural images in\nImageNet and the specialized characteristics of remote sensing imagery, in\nparticular for Synthetic Aperture Radar imagery. To address this challenge, we\npropose two novel self-supervised multimodal pretraining techniques that\nleverage SSL4SAR, a new unlabeled dataset comprising 9,563 Sentinel-1 and 14\nSentinel-2 images of Arctic glaciers, with one optical image per glacier in the\ndataset. Additionally, we introduce a novel hybrid model architecture that\ncombines a Swin Transformer encoder with a residual Convolutional Neural\nNetwork (CNN) decoder. When pretrained on SSL4SAR, this model achieves a mean\ndistance error of 293 m on the \"CAlving Fronts and where to Find thEm\" (CaFFe)\nbenchmark dataset, outperforming the prior best model by 67 m. Evaluating an\nensemble of the proposed model on a multi-annotator study of the benchmark\ndataset reveals a mean distance error of 75 m, approaching the human\nperformance of 38 m. This advancement enables precise monitoring of seasonal\nchanges in glacier calving fronts.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u81ea\u76d1\u7763\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u6280\u672f\u548c\u4e00\u79cd\u6df7\u5408\u6a21\u578b\u67b6\u6784\uff0c\u7528\u4e8e\u4ece\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\u56fe\u50cf\u4e2d\u63d0\u53d6\u51b0\u5ddd\u5d29\u89e3\u524d\u6cbf\u4f4d\u7f6e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76d1\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u51b0\u5ddd\u51b0\u91cf\u635f\u5931\u52a0\u5267\uff0c\u9700\u8981\u66f4\u51c6\u786e\u7684\u5168\u5e74\u76d1\u6d4b\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5d29\u89e3\u8fc7\u7a0b\u7684\u9a71\u52a8\u56e0\u7d20\u3002\u73b0\u6709\u57fa\u4e8eImageNet\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u56e0\u9886\u57df\u5dee\u5f02\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u81ea\u76d1\u7763\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u6280\u672f\uff0c\u4f7f\u7528\u65b0\u6570\u636e\u96c6SSL4SAR\uff0c\u5e76\u8bbe\u8ba1\u6df7\u5408\u6a21\u578b\u67b6\u6784\uff08Swin Transformer\u7f16\u7801\u5668+\u6b8b\u5deeCNN\u89e3\u7801\u5668\uff09\u3002", "result": "\u5728CaFFe\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u5e73\u5747\u8ddd\u79bb\u8bef\u5dee\u4e3a293\u7c73\uff0c\u4f18\u4e8e\u4e4b\u524d\u6700\u4f73\u6a21\u578b67\u7c73\uff1b\u96c6\u6210\u6a21\u578b\u8bef\u5dee\u964d\u81f375\u7c73\uff0c\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\u768438\u7c73\u3002", "conclusion": "\u65b0\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u51b0\u5ddd\u5d29\u89e3\u524d\u6cbf\u7684\u5b63\u8282\u6027\u53d8\u5316\u76d1\u6d4b\u7cbe\u5ea6\u3002"}}
{"id": "2507.01695", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01695", "abs": "https://arxiv.org/abs/2507.01695", "authors": ["Omkar Shende", "Gayathri Ananthanarayanan", "Marcello Traiola"], "title": "PERTINENCE: Input-based Opportunistic Neural Network Dynamic Execution", "comment": null, "summary": "Deep neural networks (DNNs) have become ubiquitous thanks to their remarkable\nability to model complex patterns across various domains such as computer\nvision, speech recognition, robotics, etc. While large DNN models are often\nmore accurate than simpler, lightweight models, they are also resource- and\nenergy-hungry. Hence, it is imperative to design methods to reduce reliance on\nsuch large models without significant degradation in output accuracy. The high\ncomputational cost of these models is often necessary only for a reduced set of\nchallenging inputs, while lighter models can handle most simple ones. Thus,\ncarefully combining properties of existing DNN models in a dynamic, input-based\nway opens opportunities to improve efficiency without impacting accuracy.\n  In this work, we introduce PERTINENCE, a novel online method designed to\nanalyze the complexity of input features and dynamically select the most\nsuitable model from a pre-trained set to process a given input effectively. To\nachieve this, we employ a genetic algorithm to explore the training space of an\nML-based input dispatcher, enabling convergence towards the Pareto front in the\nsolution space that balances overall accuracy and computational efficiency.\n  We showcase our approach on state-of-the-art Convolutional Neural Networks\n(CNNs) trained on the CIFAR-10 and CIFAR-100, as well as Vision Transformers\n(ViTs) trained on TinyImageNet dataset. We report results showing PERTINENCE's\nability to provide alternative solutions to existing state-of-the-art models in\nterms of trade-offs between accuracy and number of operations. By\nopportunistically selecting among models trained for the same task, PERTINENCE\nachieves better or comparable accuracy with up to 36% fewer operations.", "AI": {"tldr": "PERTINENCE\u662f\u4e00\u79cd\u52a8\u6001\u9009\u62e9\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f93\u5165\u7279\u5f81\u590d\u6742\u5ea6\u5206\u6790\uff0c\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u64cd\u4f5c\u6b21\u6570\u8fbe36%\u3002", "motivation": "\u5927\u578bDNN\u6a21\u578b\u867d\u7136\u51c6\u786e\u4f46\u8d44\u6e90\u6d88\u8017\u9ad8\uff0c\u9700\u8bbe\u8ba1\u65b9\u6cd5\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u51cf\u5c11\u5bf9\u5927\u578b\u6a21\u578b\u7684\u4f9d\u8d56\u3002", "method": "\u4f7f\u7528\u9057\u4f20\u7b97\u6cd5\u8bad\u7ec3\u8f93\u5165\u5206\u914d\u5668\uff0c\u52a8\u6001\u9009\u62e9\u6700\u9002\u5408\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u5904\u7406\u8f93\u5165\u3002", "result": "\u5728CIFAR-10\u3001CIFAR-100\u548cTinyImageNet\u6570\u636e\u96c6\u4e0a\uff0cPERTINENCE\u5728\u4fdd\u6301\u6216\u63d0\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u64cd\u4f5c\u6b21\u6570\u51cf\u5c11\u8fbe36%\u3002", "conclusion": "PERTINENCE\u901a\u8fc7\u52a8\u6001\u6a21\u578b\u9009\u62e9\uff0c\u6709\u6548\u5e73\u8861\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u8d44\u6e90\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.01756", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01756", "abs": "https://arxiv.org/abs/2507.01756", "authors": ["Peng Zheng", "Junke Wang", "Yi Chang", "Yizhou Yu", "Rui Ma", "Zuxuan Wu"], "title": "Rethinking Discrete Tokens: Treating Them as Conditions for Continuous Autoregressive Image Synthesis", "comment": "accepted by iccv 2025", "summary": "Recent advances in large language models (LLMs) have spurred interests in\nencoding images as discrete tokens and leveraging autoregressive (AR)\nframeworks for visual generation. However, the quantization process in AR-based\nvisual generation models inherently introduces information loss that degrades\nimage fidelity. To mitigate this limitation, recent studies have explored to\nautoregressively predict continuous tokens. Unlike discrete tokens that reside\nin a structured and bounded space, continuous representations exist in an\nunbounded, high-dimensional space, making density estimation more challenging\nand increasing the risk of generating out-of-distribution artifacts. Based on\nthe above findings, this work introduces DisCon (Discrete-Conditioned\nContinuous Autoregressive Model), a novel framework that reinterprets discrete\ntokens as conditional signals rather than generation targets. By modeling the\nconditional probability of continuous representations conditioned on discrete\ntokens, DisCon circumvents the optimization challenges of continuous token\nmodeling while avoiding the information loss caused by quantization. DisCon\nachieves a gFID score of 1.38 on ImageNet 256$\\times$256 generation,\noutperforming state-of-the-art autoregressive approaches by a clear margin.", "AI": {"tldr": "DisCon\u6846\u67b6\u901a\u8fc7\u5c06\u79bb\u6563\u6807\u8bb0\u4f5c\u4e3a\u6761\u4ef6\u4fe1\u53f7\u800c\u975e\u751f\u6210\u76ee\u6807\uff0c\u89e3\u51b3\u4e86\u8fde\u7eed\u6807\u8bb0\u5efa\u6a21\u7684\u4f18\u5316\u6311\u6218\uff0c\u540c\u65f6\u907f\u514d\u4e86\u91cf\u5316\u5e26\u6765\u7684\u4fe1\u606f\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u81ea\u56de\u5f52\u7684\u89c6\u89c9\u751f\u6210\u6a21\u578b\u4e2d\u91cf\u5316\u8fc7\u7a0b\u5bfc\u81f4\u7684\u4fe1\u606f\u635f\u5931\u95ee\u9898\uff0c\u4ee5\u53ca\u8fde\u7eed\u6807\u8bb0\u5efa\u6a21\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faDisCon\u6846\u67b6\uff0c\u5c06\u79bb\u6563\u6807\u8bb0\u4f5c\u4e3a\u6761\u4ef6\u4fe1\u53f7\uff0c\u5efa\u6a21\u8fde\u7eed\u8868\u793a\u7684\u6761\u4ef6\u6982\u7387\u3002", "result": "\u5728ImageNet 256\u00d7256\u751f\u6210\u4efb\u52a1\u4e0a\uff0cDisCon\u7684gFID\u5f97\u5206\u4e3a1.38\uff0c\u4f18\u4e8e\u73b0\u6709\u81ea\u56de\u5f52\u65b9\u6cd5\u3002", "conclusion": "DisCon\u901a\u8fc7\u7ed3\u5408\u79bb\u6563\u548c\u8fde\u7eed\u8868\u793a\u7684\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u751f\u6210\u7684\u4fdd\u771f\u5ea6\u548c\u6027\u80fd\u3002"}}
{"id": "2507.01699", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01699", "abs": "https://arxiv.org/abs/2507.01699", "authors": ["Illia Oleksiienko", "Juho Kanniainen", "Alexandros Iosifidis"], "title": "Variational Graph Convolutional Neural Networks", "comment": "This work has been submitted to the IEEE for possible publication. 9\n  pages, 6 figures", "summary": "Estimation of model uncertainty can help improve the explainability of Graph\nConvolutional Networks and the accuracy of the models at the same time.\nUncertainty can also be used in critical applications to verify the results of\nthe model by an expert or additional models. In this paper, we propose\nVariational Neural Network versions of spatial and spatio-temporal Graph\nConvolutional Networks. We estimate uncertainty in both outputs and layer-wise\nattentions of the models, which has the potential for improving model\nexplainability. We showcase the benefits of these models in the social trading\nanalysis and the skeleton-based human action recognition tasks on the Finnish\nboard membership, NTU-60, NTU-120 and Kinetics datasets, where we show\nimprovement in model accuracy in addition to estimated model uncertainties.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53d8\u5206\u795e\u7ecf\u7f51\u7edc\u7684\u56fe\u5377\u79ef\u7f51\u7edc\uff0c\u7528\u4e8e\u4f30\u8ba1\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u901a\u8fc7\u4f30\u8ba1\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\uff0c\u589e\u5f3a\u56fe\u5377\u79ef\u7f51\u7edc\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5728\u5173\u952e\u5e94\u7528\u4e2d\u9a8c\u8bc1\u6a21\u578b\u7ed3\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u7a7a\u95f4\u548c\u65f6\u7a7a\u56fe\u5377\u79ef\u7f51\u7edc\u7684\u53d8\u5206\u795e\u7ecf\u7f51\u7edc\u7248\u672c\uff0c\u4f30\u8ba1\u6a21\u578b\u8f93\u51fa\u548c\u5c42\u6ce8\u610f\u529b\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728\u793e\u4ea4\u4ea4\u6613\u5206\u6790\u548c\u57fa\u4e8e\u9aa8\u67b6\u7684\u4eba\u4f53\u52a8\u4f5c\u8bc6\u522b\u4efb\u52a1\u4e2d\uff0c\u6a21\u578b\u51c6\u786e\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5747\u6709\u6240\u63d0\u5347\u3002", "conclusion": "\u53d8\u5206\u56fe\u5377\u79ef\u7f51\u7edc\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u3002"}}
{"id": "2507.01825", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01825", "abs": "https://arxiv.org/abs/2507.01825", "authors": ["Franco Alberto Cardillo", "Hamza Khyari", "Umberto Straccia"], "title": "MILP-SAT-GNN: Yet Another Neural SAT Solver", "comment": null, "summary": "We proposes a novel method that enables Graph Neural Networks (GNNs) to solve\nSAT problems by leveraging a technique developed for applying GNNs to Mixed\nInteger Linear Programming (MILP). Specifically, k-CNF formulae are mapped into\nMILP problems, which are then encoded as weighted bipartite graphs and\nsubsequently fed into a GNN for training and testing. From a theoretical\nperspective: (i) we establish permutation and equivalence invariance results,\ndemonstrating that the method produces outputs that are stable under reordering\nof clauses and variables; (ii) we identify a theoretical limitation, showing\nthat for a class of formulae called foldable formulae, standard GNNs cannot\nalways distinguish satisfiable from unsatisfiable instances; (iii) we prove a\nuniversal approximation theorem, establishing that with Random Node\nInitialization (RNI), the method can approximate SAT solving to arbitrary\nprecision on finite datasets, that is, the GNN becomes approximately sound and\ncomplete on such datasets. Furthermore, we show that for unfoldable formulae,\nthe same approximation guarantee can be achieved without the need for RNI.\nFinally, we conduct an experimental evaluation of our approach, which show\nthat, despite the simplicity of the neural architecture, the method achieves\npromising results.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5229\u7528GNN\u89e3\u51b3SAT\u95ee\u9898\uff0c\u901a\u8fc7\u5c06k-CNF\u516c\u5f0f\u6620\u5c04\u4e3aMILP\u95ee\u9898\u5e76\u7f16\u7801\u4e3a\u52a0\u6743\u4e8c\u5206\u56fe\uff0c\u518d\u8f93\u5165GNN\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002\u7406\u8bba\u8bc1\u660e\u5176\u7a33\u5b9a\u6027\u548c\u5c40\u9650\u6027\uff0c\u5b9e\u9a8c\u663e\u793a\u6548\u679c\u826f\u597d\u3002", "motivation": "\u63a2\u7d22GNN\u5728\u89e3\u51b3SAT\u95ee\u9898\u4e2d\u7684\u5e94\u7528\uff0c\u7ed3\u5408MILP\u6280\u672f\uff0c\u63d0\u5347GNN\u7684\u901a\u7528\u6027\u548c\u8868\u73b0\u3002", "method": "\u5c06k-CNF\u516c\u5f0f\u8f6c\u4e3aMILP\u95ee\u9898\uff0c\u7f16\u7801\u4e3a\u52a0\u6743\u4e8c\u5206\u56fe\uff0c\u8f93\u5165GNN\u8bad\u7ec3\u548c\u6d4b\u8bd5\uff0c\u5e76\u5f15\u5165\u968f\u673a\u8282\u70b9\u521d\u59cb\u5316\uff08RNI\uff09\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u65b9\u6cd5\u5bf9\u91cd\u6392\u5e8f\u548c\u7b49\u4ef7\u6027\u7a33\u5b9a\uff0c\u4f46\u5bf9\u53ef\u6298\u53e0\u516c\u5f0f\u6709\u9650\u5236\uff1b\u5b9e\u9a8c\u663e\u793a\u65b9\u6cd5\u7b80\u5355\u4f46\u6548\u679c\u826f\u597d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aGNN\u89e3\u51b3SAT\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u7406\u8bba\u8bc1\u660e\u5176\u6f5c\u529b\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u53ef\u884c\u6027\u3002"}}
{"id": "2507.01791", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01791", "abs": "https://arxiv.org/abs/2507.01791", "authors": ["Zihong Guo", "Chen Wan", "Yayin Zheng", "Hailing Kuang", "Xiaohai Lu"], "title": "Boosting Adversarial Transferability Against Defenses via Multi-Scale Transformation", "comment": null, "summary": "The transferability of adversarial examples poses a significant security\nchallenge for deep neural networks, which can be attacked without knowing\nanything about them. In this paper, we propose a new Segmented Gaussian Pyramid\n(SGP) attack method to enhance the transferability, particularly against\ndefense models. Unlike existing methods that generally focus on single-scale\nimages, our approach employs Gaussian filtering and three types of downsampling\nto construct a series of multi-scale examples. Then, the gradients of the loss\nfunction with respect to each scale are computed, and their average is used to\ndetermine the adversarial perturbations. The proposed SGP can be considered an\ninput transformation with high extensibility that is easily integrated into\nmost existing adversarial attacks. Extensive experiments demonstrate that in\ncontrast to the state-of-the-art methods, SGP significantly enhances attack\nsuccess rates against black-box defense models, with average attack success\nrates increasing by 2.3% to 32.6%, based only on transferability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u6bb5\u9ad8\u65af\u91d1\u5b57\u5854\uff08SGP\uff09\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u56fe\u50cf\u5904\u7406\u589e\u5f3a\u5bf9\u6297\u6837\u672c\u7684\u8fc1\u79fb\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u9ed1\u76d2\u9632\u5fa1\u6a21\u578b\u7684\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u5bf9\u6297\u6837\u672c\u7684\u8fc1\u79fb\u6027\u5bf9\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6784\u6210\u5b89\u5168\u5a01\u80c1\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4ec5\u5173\u6ce8\u5355\u5c3a\u5ea6\u56fe\u50cf\uff0c\u9650\u5236\u4e86\u653b\u51fb\u6548\u679c\u3002", "method": "\u91c7\u7528\u9ad8\u65af\u6ee4\u6ce2\u548c\u4e09\u79cd\u4e0b\u91c7\u6837\u65b9\u6cd5\u6784\u5efa\u591a\u5c3a\u5ea6\u6837\u672c\uff0c\u8ba1\u7b97\u5404\u5c3a\u5ea6\u635f\u5931\u51fd\u6570\u7684\u68af\u5ea6\u5e76\u53d6\u5e73\u5747\u503c\u4ee5\u786e\u5b9a\u5bf9\u6297\u6270\u52a8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSGP\u663e\u8457\u63d0\u9ad8\u4e86\u653b\u51fb\u6210\u529f\u7387\uff0c\u5e73\u5747\u63d0\u53472.3%\u81f332.6%\u3002", "conclusion": "SGP\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u8f93\u5165\u53d8\u6362\u65b9\u6cd5\uff0c\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u653b\u51fb\u4e2d\uff0c\u6709\u6548\u589e\u5f3a\u5bf9\u6297\u6837\u672c\u7684\u8fc1\u79fb\u6027\u3002"}}
{"id": "2507.01829", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01829", "abs": "https://arxiv.org/abs/2507.01829", "authors": ["Tristan Torchet", "Christian Metzner", "Laura Kriener", "Melika Payvand"], "title": "mGRADE: Minimal Recurrent Gating Meets Delay Convolutions for Lightweight Sequence Modeling", "comment": null, "summary": "Edge devices for temporal processing demand models that capture both short-\nand long- range dynamics under tight memory constraints. While Transformers\nexcel at sequence modeling, their quadratic memory scaling with sequence length\nmakes them impractical for such settings. Recurrent Neural Networks (RNNs)\noffer constant memory but train sequentially, and Temporal Convolutional\nNetworks (TCNs), though efficient, scale memory with kernel size. To address\nthis, we propose mGRADE (mininally Gated Recurrent Architecture with Delay\nEmbedding), a hybrid-memory system that integrates a temporal 1D-convolution\nwith learnable spacings followed by a minimal gated recurrent unit (minGRU).\nThis design allows the convolutional layer to realize a flexible delay\nembedding that captures rapid temporal variations, while the recurrent module\nefficiently maintains global context with minimal memory overhead. We validate\nour approach on two synthetic tasks, demonstrating that mGRADE effectively\nseparates and preserves multi-scale temporal features. Furthermore, on\nchallenging pixel-by-pixel image classification benchmarks, mGRADE consistently\noutperforms both pure convolutional and pure recurrent counterparts using\napproximately 20% less memory footprint, highlighting its suitability for\nmemory-constrained temporal processing at the edge. This highlights mGRADE's\npromise as an efficient solution for memory-constrained multi-scale temporal\nprocessing at the edge.", "AI": {"tldr": "mGRADE\u662f\u4e00\u79cd\u6df7\u5408\u5185\u5b58\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e861D\u5377\u79ef\u548c\u6700\u5c0f\u95e8\u63a7\u5faa\u73af\u5355\u5143\uff0c\u9002\u7528\u4e8e\u5185\u5b58\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\uff0c\u80fd\u6709\u6548\u5904\u7406\u591a\u5c3a\u5ea6\u65f6\u95f4\u7279\u5f81\u3002", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u9700\u8981\u540c\u65f6\u6355\u6349\u77ed\u65f6\u548c\u957f\u65f6\u52a8\u6001\u7684\u6a21\u578b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\uff08\u5982Transformer\u3001RNN\u3001TCN\uff09\u5728\u5185\u5b58\u6216\u6548\u7387\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51famGRADE\uff0c\u7ed3\u54081D\u5377\u79ef\u548cminGRU\uff0c\u5377\u79ef\u5c42\u5b9e\u73b0\u7075\u6d3b\u5ef6\u8fdf\u5d4c\u5165\uff0c\u5faa\u73af\u6a21\u5757\u9ad8\u6548\u7ef4\u62a4\u5168\u5c40\u4e0a\u4e0b\u6587\u3002", "result": "\u5728\u5408\u6210\u4efb\u52a1\u548c\u50cf\u7d20\u7ea7\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cmGRADE\u8868\u73b0\u4f18\u4e8e\u7eaf\u5377\u79ef\u548c\u7eaf\u5faa\u73af\u6a21\u578b\uff0c\u5185\u5b58\u5360\u7528\u51cf\u5c1120%\u3002", "conclusion": "mGRADE\u662f\u5185\u5b58\u53d7\u9650\u8fb9\u7f18\u8bbe\u5907\u4e2d\u591a\u5c3a\u5ea6\u65f6\u95f4\u5904\u7406\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01714", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01714", "abs": "https://arxiv.org/abs/2507.01714", "authors": ["Kevin Innerebner", "Franz M. Rohrhofer", "Bernhard C. Geiger"], "title": "B-PL-PINN: Stabilizing PINN Training with Bayesian Pseudo Labeling", "comment": null, "summary": "Training physics-informed neural networks (PINNs) for forward problems often\nsuffers from severe convergence issues, hindering the propagation of\ninformation from regions where the desired solution is well-defined.\nHaitsiukevich and Ilin (2023) proposed an ensemble approach that extends the\nactive training domain of each PINN based on i) ensemble consensus and ii)\nvicinity to (pseudo-)labeled points, thus ensuring that the information from\nthe initial condition successfully propagates to the interior of the\ncomputational domain.\n  In this work, we suggest replacing the ensemble by a Bayesian PINN, and\nconsensus by an evaluation of the PINN's posterior variance. Our experiments\nshow that this mathematically principled approach outperforms the ensemble on a\nset of benchmark problems and is competitive with PINN ensembles trained with\ncombinations of Adam and LBFGS.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u7528\u8d1d\u53f6\u65af\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u66ff\u4ee3\u96c6\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc4\u4f30\u540e\u9a8c\u65b9\u5dee\u6539\u8fdb\u4fe1\u606f\u4f20\u64ad\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u96c6\u6210\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfPINN\u5728\u524d\u5411\u95ee\u9898\u4e2d\u4fe1\u606f\u4f20\u64ad\u4e0d\u8db3\u7684\u6536\u655b\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u8d1d\u53f6\u65afPINN\u66ff\u4ee3\u96c6\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u540e\u9a8c\u65b9\u5dee\u8bc4\u4f30\u6539\u8fdb\u4fe1\u606f\u4f20\u64ad\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u57fa\u51c6\u95ee\u9898\u4e0a\u4f18\u4e8e\u96c6\u6210\u65b9\u6cd5\uff0c\u5e76\u4e0eAdam\u548cLBFGS\u7ec4\u5408\u8bad\u7ec3\u7684PINN\u96c6\u6210\u7ade\u4e89\u3002", "conclusion": "\u8d1d\u53f6\u65afPINN\u662f\u4e00\u79cd\u6570\u5b66\u4e0a\u66f4\u4e25\u8c28\u7684\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u6539\u8fdbPINN\u7684\u4fe1\u606f\u4f20\u64ad\u6027\u80fd\u3002"}}
{"id": "2507.01792", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01792", "abs": "https://arxiv.org/abs/2507.01792", "authors": ["Peng Zheng", "Ye Wang", "Rui Ma", "Zuxuan Wu"], "title": "FreeLoRA: Enabling Training-Free LoRA Fusion for Autoregressive Multi-Subject Personalization", "comment": null, "summary": "Subject-driven image generation plays a crucial role in applications such as\nvirtual try-on and poster design. Existing approaches typically fine-tune\npretrained generative models or apply LoRA-based adaptations for individual\nsubjects. However, these methods struggle with multi-subject personalization,\nas combining independently adapted modules often requires complex re-tuning or\njoint optimization. We present FreeLoRA, a simple and generalizable framework\nthat enables training-free fusion of subject-specific LoRA modules for\nmulti-subject personalization. Each LoRA module is adapted on a few images of a\nspecific subject using a Full Token Tuning strategy, where it is applied across\nall tokens in the prompt to encourage weakly supervised token-content\nalignment. At inference, we adopt Subject-Aware Inference, activating each\nmodule only on its corresponding subject tokens. This enables training-free\nfusion of multiple personalized subjects within a single image, while\nmitigating overfitting and mutual interference between subjects. Extensive\nexperiments show that FreeLoRA achieves strong performance in both subject\nfidelity and prompt consistency.", "AI": {"tldr": "FreeLoRA\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u591a\u4e2a\u7279\u5b9a\u4e3b\u9898\u7684LoRA\u6a21\u5757\u5b9e\u73b0\u591a\u4e3b\u9898\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u4e3b\u9898\u4e2a\u6027\u5316\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u590d\u6742\u7684\u8c03\u6574\u6216\u8054\u5408\u4f18\u5316\u3002", "method": "\u91c7\u7528Full Token Tuning\u7b56\u7565\u8bad\u7ec3\u7279\u5b9a\u4e3b\u9898\u7684LoRA\u6a21\u5757\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u901a\u8fc7Subject-Aware Inference\u6fc0\u6d3b\u5bf9\u5e94\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u8868\u660eFreeLoRA\u5728\u4e3b\u9898\u4fdd\u771f\u5ea6\u548c\u63d0\u793a\u4e00\u81f4\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "FreeLoRA\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u4e14\u901a\u7528\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u591a\u4e3b\u9898\u4e2a\u6027\u5316\u4e2d\u7684\u6311\u6218\u3002"}}
{"id": "2507.01875", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01875", "abs": "https://arxiv.org/abs/2507.01875", "authors": ["Gast\u00f3n Garc\u00eda Gonz\u00e1lez", "Pedro Casas", "Emilio Mart\u00ednez", "Alicia Fern\u00e1ndez"], "title": "Towards Foundation Auto-Encoders for Time-Series Anomaly Detection", "comment": "Presented at ACM KDD 2024, MiLeTS 2024 Workshop, August 25, 2024,\n  Barcelona, Spain", "summary": "We investigate a novel approach to time-series modeling, inspired by the\nsuccesses of large pretrained foundation models. We introduce FAE (Foundation\nAuto-Encoders), a foundation generative-AI model for anomaly detection in\ntime-series data, based on Variational Auto-Encoders (VAEs). By foundation, we\nmean a model pretrained on massive amounts of time-series data which can learn\ncomplex temporal patterns useful for accurate modeling, forecasting, and\ndetection of anomalies on previously unseen datasets. FAE leverages VAEs and\nDilated Convolutional Neural Networks (DCNNs) to build a generic model for\nunivariate time-series modeling, which could eventually perform properly in\nout-of-the-box, zero-shot anomaly detection applications. We introduce the main\nconcepts of FAE, and present preliminary results in different multi-dimensional\ntime-series datasets from various domains, including a real dataset from an\noperational mobile ISP, and the well known KDD 2021 Anomaly Detection dataset.", "AI": {"tldr": "FAE\u662f\u4e00\u79cd\u57fa\u4e8e\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548c\u81a8\u80c0\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u5b66\u4e60\u590d\u6742\u65f6\u5e8f\u6a21\u5f0f\uff0c\u652f\u6301\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u3002", "motivation": "\u53d7\u5927\u578b\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u6210\u529f\u7684\u542f\u53d1\uff0c\u7814\u7a76\u5982\u4f55\u5229\u7528VAEs\u548cDCNNs\u6784\u5efa\u901a\u7528\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u5f02\u5e38\u68c0\u6d4b\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAEs\uff09\u548c\u81a8\u80c0\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08DCNNs\uff09\uff0c\u6784\u5efaFAE\u6a21\u578b\uff0c\u8fdb\u884c\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u7ef4\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\uff08\u5305\u62ec\u79fb\u52a8ISP\u6570\u636e\u548cKDD 2021\u6570\u636e\u96c6\uff09\u4e0a\u5c55\u793a\u4e86\u521d\u6b65\u7ed3\u679c\u3002", "conclusion": "FAE\u4e3a\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u3001\u96f6\u6837\u672c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u6f5c\u5728\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.01724", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01724", "abs": "https://arxiv.org/abs/2507.01724", "authors": ["Micha Henheik", "Theresa Eimer", "Marius Lindauer"], "title": "Revisiting Learning Rate Control", "comment": null, "summary": "The learning rate is one of the most important hyperparameters in deep\nlearning, and how to control it is an active area within both AutoML and deep\nlearning research. Approaches for learning rate control span from classic\noptimization to online scheduling based on gradient statistics. This paper\ncompares paradigms to assess the current state of learning rate control. We\nfind that methods from multi-fidelity hyperparameter optimization,\nfixed-hyperparameter schedules, and hyperparameter-free learning often perform\nvery well on selected deep learning tasks but are not reliable across settings.\nThis highlights the need for algorithm selection methods in learning rate\ncontrol, which have been neglected so far by both the AutoML and deep learning\ncommunities. We also observe a trend of hyperparameter optimization approaches\nbecoming less effective as models and tasks grow in complexity, even when\ncombined with multi-fidelity approaches for more expensive model trainings. A\nfocus on more relevant test tasks and new promising directions like finetunable\nmethods and meta-learning will enable the AutoML community to significantly\nstrengthen its impact on this crucial factor in deep learning.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u5b66\u4e60\u7387\u63a7\u5236\u65b9\u6cd5\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\u4f46\u7f3a\u4e4f\u666e\u9002\u6027\uff0c\u9700\u5173\u6ce8\u7b97\u6cd5\u9009\u62e9\u548c\u65b0\u5174\u65b9\u5411\u5982\u5143\u5b66\u4e60\u3002", "motivation": "\u5b66\u4e60\u7387\u662f\u6df1\u5ea6\u5b66\u4e60\u7684\u5173\u952e\u8d85\u53c2\u6570\uff0c\u4f46\u5176\u63a7\u5236\u65b9\u6cd5\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u6a21\u578b\u4e2d\u7684\u53ef\u9760\u6027\u4e0d\u8db3\uff0c\u9700\u63a2\u7d22\u66f4\u666e\u9002\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6bd4\u8f83\u4e86\u591a\u4fdd\u771f\u8d85\u53c2\u6570\u4f18\u5316\u3001\u56fa\u5b9a\u5b66\u4e60\u7387\u8c03\u5ea6\u548c\u65e0\u8d85\u53c2\u6570\u5b66\u4e60\u7b49\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u5728\u6df1\u5ea6\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u73b0\u6709\u65b9\u6cd5\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u590d\u6742\u6a21\u578b\u548c\u4efb\u52a1\u4e2d\u6548\u679c\u4e0b\u964d\uff0c\u9700\u7b97\u6cd5\u9009\u62e9\u548c\u65b0\u5174\u65b9\u5411\u5982\u5143\u5b66\u4e60\u3002", "conclusion": "AutoML\u793e\u533a\u9700\u5173\u6ce8\u66f4\u76f8\u5173\u7684\u6d4b\u8bd5\u4efb\u52a1\u548c\u65b0\u5174\u65b9\u5411\uff08\u5982\u53ef\u5fae\u8c03\u65b9\u6cd5\u548c\u5143\u5b66\u4e60\uff09\uff0c\u4ee5\u63d0\u5347\u5b66\u4e60\u7387\u63a7\u5236\u7684\u6548\u679c\u3002"}}
{"id": "2507.01800", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.01800", "abs": "https://arxiv.org/abs/2507.01800", "authors": ["Shengli Zhou", "Jianuo Zhu", "Qilin Huang", "Fangjing Wang", "Yanfu Zhang", "Feng Zheng"], "title": "HCNQA: Enhancing 3D VQA with Hierarchical Concentration Narrowing Supervision", "comment": "ICANN 2025", "summary": "3D Visual Question-Answering (3D VQA) is pivotal for models to perceive the\nphysical world and perform spatial reasoning. Answer-centric supervision is a\ncommonly used training method for 3D VQA models. Many models that utilize this\nstrategy have achieved promising results in 3D VQA tasks. However, the\nanswer-centric approach only supervises the final output of models and allows\nmodels to develop reasoning pathways freely. The absence of supervision on the\nreasoning pathway enables the potential for developing superficial shortcuts\nthrough common patterns in question-answer pairs. Moreover, although\nslow-thinking methods advance large language models, they suffer from\nunderthinking. To address these issues, we propose \\textbf{HCNQA}, a 3D VQA\nmodel leveraging a hierarchical concentration narrowing supervision method. By\nmimicking the human process of gradually focusing from a broad area to specific\nobjects while searching for answers, our method guides the model to perform\nthree phases of concentration narrowing through hierarchical supervision. By\nsupervising key checkpoints on a general reasoning pathway, our method can\nensure the development of a rational and effective reasoning pathway. Extensive\nexperimental results demonstrate that our method can effectively ensure that\nthe model develops a rational reasoning pathway and performs better. The code\nis available at https://github.com/JianuoZhu/HCNQA.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHCNQA\u76843D VQA\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u5c42\u76d1\u7763\u65b9\u6cd5\u89e3\u51b3\u73b0\u6709\u7b54\u6848\u4e2d\u5fc3\u76d1\u7763\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u786e\u4fdd\u6a21\u578b\u53d1\u5c55\u51fa\u5408\u7406\u7684\u63a8\u7406\u8def\u5f84\u3002", "motivation": "\u73b0\u67093D VQA\u6a21\u578b\u7684\u7b54\u6848\u4e2d\u5fc3\u76d1\u7763\u65b9\u6cd5\u4ec5\u76d1\u7763\u6700\u7ec8\u8f93\u51fa\uff0c\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u53d1\u5c55\u51fa\u6d45\u5c42\u7684\u63a8\u7406\u6377\u5f84\uff0c\u7f3a\u4e4f\u5bf9\u63a8\u7406\u8def\u5f84\u7684\u76d1\u7763\u3002", "method": "\u63d0\u51faHCNQA\u6a21\u578b\uff0c\u91c7\u7528\u5206\u5c42\u6d53\u5ea6\u7f29\u5c0f\u76d1\u7763\u65b9\u6cd5\uff0c\u6a21\u4eff\u4eba\u7c7b\u4ece\u5e7f\u6cdb\u533a\u57df\u9010\u6b65\u805a\u7126\u5230\u7279\u5b9a\u5bf9\u8c61\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u76d1\u7763\u786e\u4fdd\u5408\u7406\u7684\u63a8\u7406\u8def\u5f84\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHCNQA\u80fd\u6709\u6548\u786e\u4fdd\u6a21\u578b\u53d1\u5c55\u51fa\u5408\u7406\u7684\u63a8\u7406\u8def\u5f84\uff0c\u5e76\u53d6\u5f97\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "HCNQA\u901a\u8fc7\u5206\u5c42\u76d1\u7763\u65b9\u6cd5\u89e3\u51b3\u4e86\u73b0\u67093D VQA\u6a21\u578b\u7684\u4e0d\u8db3\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u6027\u80fd\u3002"}}
{"id": "2507.01740", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.01740", "abs": "https://arxiv.org/abs/2507.01740", "authors": ["Trung-Dung Hoang", "Alceu Bissoto", "Vihangkumar V. Naik", "Tim Fl\u00fchmann", "Artemii Shlychkov", "Jos\u00e9 Garcia-Tirado", "Lisa M. Koch"], "title": "A Real-Time Digital Twin for Type 1 Diabetes using Simulation-Based Inference", "comment": null, "summary": "Accurately estimating parameters of physiological models is essential to\nachieving reliable digital twins. For Type 1 Diabetes, this is particularly\nchallenging due to the complexity of glucose-insulin interactions. Traditional\nmethods based on Markov Chain Monte Carlo struggle with high-dimensional\nparameter spaces and fit parameters from scratch at inference time, making them\nslow and computationally expensive. In this study, we propose a\nSimulation-Based Inference approach based on Neural Posterior Estimation to\nefficiently capture the complex relationships between meal intake, insulin, and\nglucose level, providing faster, amortized inference. Our experiments\ndemonstrate that SBI not only outperforms traditional methods in parameter\nestimation but also generalizes better to unseen conditions, offering real-time\nposterior inference with reliable uncertainty quantification.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u540e\u9a8c\u4f30\u8ba1\u7684\u6a21\u62df\u63a8\u7406\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u4f30\u8ba11\u578b\u7cd6\u5c3f\u75c5\u751f\u7406\u6a21\u578b\u53c2\u6570\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u9ad8\u7ef4\u53c2\u6570\u7a7a\u95f4\u4e2d\u6548\u7387\u4f4e\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u5904\u7406\u8461\u8404\u7cd6-\u80f0\u5c9b\u7d20\u76f8\u4e92\u4f5c\u7528\u7684\u590d\u6742\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u795e\u7ecf\u540e\u9a8c\u4f30\u8ba1\u7684\u6a21\u62df\u63a8\u7406\u65b9\u6cd5\uff0c\u6355\u6349\u996e\u98df\u3001\u80f0\u5c9b\u7d20\u548c\u8840\u7cd6\u6c34\u5e73\u7684\u590d\u6742\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u53c2\u6570\u4f30\u8ba1\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e14\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u6761\u4ef6\uff0c\u63d0\u4f9b\u5b9e\u65f6\u540e\u9a8c\u63a8\u65ad\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a1\u578b\u7cd6\u5c3f\u75c5\u6570\u5b57\u5b6a\u751f\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u9760\u7684\u53c2\u6570\u4f30\u8ba1\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002"}}
{"id": "2507.01801", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01801", "abs": "https://arxiv.org/abs/2507.01801", "authors": ["Bin Rao", "Haicheng Liao", "Yanchen Guan", "Chengyue Wang", "Bonan Wang", "Jiaxun Zhang", "Zhenning Li"], "title": "AMD: Adaptive Momentum and Decoupled Contrastive Learning Framework for Robust Long-Tail Trajectory Prediction", "comment": null, "summary": "Accurately predicting the future trajectories of traffic agents is essential\nin autonomous driving. However, due to the inherent imbalance in trajectory\ndistributions, tail data in natural datasets often represents more complex and\nhazardous scenarios. Existing studies typically rely solely on a base model's\nprediction error, without considering the diversity and uncertainty of\nlong-tail trajectory patterns. We propose an adaptive momentum and decoupled\ncontrastive learning framework (AMD), which integrates unsupervised and\nsupervised contrastive learning strategies. By leveraging an improved momentum\ncontrast learning (MoCo-DT) and decoupled contrastive learning (DCL) module,\nour framework enhances the model's ability to recognize rare and complex\ntrajectories. Additionally, we design four types of trajectory random\naugmentation methods and introduce an online iterative clustering strategy,\nallowing the model to dynamically update pseudo-labels and better adapt to the\ndistributional shifts in long-tail data. We propose three different criteria to\ndefine long-tail trajectories and conduct extensive comparative experiments on\nthe nuScenes and ETH$/$UCY datasets. The results show that AMD not only\nachieves optimal performance in long-tail trajectory prediction but also\ndemonstrates outstanding overall prediction accuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u52a8\u91cf\u548c\u89e3\u8026\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff08AMD\uff09\uff0c\u7528\u4e8e\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u4e2d\u957f\u5c3e\u8f68\u8ff9\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u957f\u5c3e\u6570\u636e\u4ee3\u8868\u590d\u6742\u4e14\u5371\u9669\u7684\u573a\u666f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u57fa\u7840\u6a21\u578b\u7684\u9884\u6d4b\u8bef\u5dee\uff0c\u5ffd\u89c6\u4e86\u957f\u5c3e\u8f68\u8ff9\u6a21\u5f0f\u7684\u591a\u6837\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u7ed3\u5408\u6539\u8fdb\u7684\u52a8\u91cf\u5bf9\u6bd4\u5b66\u4e60\uff08MoCo-DT\uff09\u548c\u89e3\u8026\u5bf9\u6bd4\u5b66\u4e60\uff08DCL\uff09\u6a21\u5757\uff0c\u8bbe\u8ba1\u4e86\u56db\u79cd\u8f68\u8ff9\u968f\u673a\u589e\u5f3a\u65b9\u6cd5\u548c\u5728\u7ebf\u8fed\u4ee3\u805a\u7c7b\u7b56\u7565\u3002", "result": "\u5728nuScenes\u548cETH/UCY\u6570\u636e\u96c6\u4e0a\uff0cAMD\u5728\u957f\u5c3e\u8f68\u8ff9\u9884\u6d4b\u548c\u6574\u4f53\u9884\u6d4b\u51c6\u786e\u6027\u4e0a\u5747\u8868\u73b0\u6700\u4f18\u3002", "conclusion": "AMD\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u957f\u5c3e\u8f68\u8ff9\u9884\u6d4b\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6574\u4f53\u9884\u6d4b\u7684\u9ad8\u51c6\u786e\u6027\u3002"}}
{"id": "2507.01835", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01835", "abs": "https://arxiv.org/abs/2507.01835", "authors": ["Daniil Reutsky", "Daniil Vladimirov", "Yasin Mamedov", "Georgy Perevozchikov", "Nancy Mehta", "Egor Ershov", "Radu Timofte"], "title": "Modulate and Reconstruct: Learning Hyperspectral Imaging from Misaligned Smartphone Views", "comment": null, "summary": "Hyperspectral reconstruction (HSR) from RGB images is a fundamentally\nill-posed problem due to severe spectral information loss. Existing approaches\ntypically rely on a single RGB image, limiting reconstruction accuracy. In this\nwork, we propose a novel multi-image-to-hyperspectral reconstruction (MI-HSR)\nframework that leverages a triple-camera smartphone system, where two lenses\nare equipped with carefully selected spectral filters. Our configuration,\ngrounded in theoretical and empirical analysis, enables richer and more diverse\nspectral observations than conventional single-camera setups. To support this\nnew paradigm, we introduce Doomer, the first dataset for MI-HSR, comprising\naligned images from three smartphone cameras and a hyperspectral reference\ncamera across diverse scenes. We show that the proposed HSR model achieves\nconsistent improvements over existing methods on the newly proposed benchmark.\nIn a nutshell, our setup allows 30% towards more accurately estimated spectra\ncompared to an ordinary RGB camera. Our findings suggest that multi-view\nspectral filtering with commodity hardware can unlock more accurate and\npractical hyperspectral imaging solutions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u56fe\u50cf\u7684\u8d85\u5149\u8c31\u91cd\u5efa\uff08MI-HSR\uff09\u6846\u67b6\uff0c\u5229\u7528\u4e09\u6444\u50cf\u5934\u667a\u80fd\u624b\u673a\u7cfb\u7edf\u63d0\u5347\u91cd\u5efa\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u5355RGB\u56fe\u50cf\u91cd\u5efa\u65b9\u6cd5\u56e0\u5149\u8c31\u4fe1\u606f\u4e22\u5931\u4e25\u91cd\u800c\u7cbe\u5ea6\u53d7\u9650\uff0c\u9700\u66f4\u4e30\u5bcc\u7684\u89c2\u6d4b\u6570\u636e\u3002", "method": "\u91c7\u7528\u914d\u5907\u7279\u5b9a\u5149\u8c31\u6ee4\u955c\u7684\u53cc\u955c\u5934\u4e09\u6444\u50cf\u5934\u7cfb\u7edf\uff0c\u7ed3\u5408\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\uff0c\u6784\u5efa\u9996\u4e2aMI-HSR\u6570\u636e\u96c6Doomer\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5149\u8c31\u4f30\u8ba1\u7cbe\u5ea6\u63d0\u534730%\u3002", "conclusion": "\u591a\u89c6\u89d2\u5149\u8c31\u6ee4\u6ce2\u7ed3\u5408\u6d88\u8d39\u7ea7\u786c\u4ef6\u53ef\u5b9e\u73b0\u66f4\u51c6\u786e\u3001\u5b9e\u7528\u7684\u8d85\u5149\u8c31\u6210\u50cf\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01924", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01924", "abs": "https://arxiv.org/abs/2507.01924", "authors": ["Samirah Bakker", "Yao Ma", "Seyed Sahand Mohammadi Ziabari"], "title": "Exploring a Hybrid Deep Learning Approach for Anomaly Detection in Mental Healthcare Provider Billing: Addressing Label Scarcity through Semi-Supervised Anomaly Detection", "comment": null, "summary": "The complexity of mental healthcare billing enables anomalies, including\nfraud. While machine learning methods have been applied to anomaly detection,\nthey often struggle with class imbalance, label scarcity, and complex\nsequential patterns. This study explores a hybrid deep learning approach\ncombining Long Short-Term Memory (LSTM) networks and Transformers, with\npseudo-labeling via Isolation Forests (iForest) and Autoencoders (AE). Prior\nwork has not evaluated such hybrid models trained on pseudo-labeled data in the\ncontext of healthcare billing. The approach is evaluated on two real-world\nbilling datasets related to mental healthcare. The iForest LSTM baseline\nachieves the highest recall (0.963) on declaration-level data. On the\noperation-level data, the hybrid iForest-based model achieves the highest\nrecall (0.744), though at the cost of lower precision. These findings highlight\nthe potential of combining pseudo-labeling with hybrid deep learning in\ncomplex, imbalanced anomaly detection settings.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LSTM\u548cTransformer\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5229\u7528iForest\u548cAE\u8fdb\u884c\u4f2a\u6807\u8bb0\uff0c\u7528\u4e8e\u5fc3\u7406\u5065\u5eb7\u8d26\u5355\u5f02\u5e38\u68c0\u6d4b\uff0c\u89e3\u51b3\u4e86\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u6807\u7b7e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u5fc3\u7406\u5065\u5eb7\u8d26\u5355\u7684\u590d\u6742\u6027\u5bfc\u81f4\u5f02\u5e38\uff08\u5982\u6b3a\u8bc8\uff09\u9891\u53d1\uff0c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u56e0\u7c7b\u522b\u4e0d\u5e73\u8861\u3001\u6807\u7b7e\u7a00\u7f3a\u548c\u590d\u6742\u5e8f\u5217\u6a21\u5f0f\u800c\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08LSTM+Transformer\uff09\uff0c\u7ed3\u5408iForest\u548cAE\u8fdb\u884c\u4f2a\u6807\u8bb0\uff0c\u5e76\u5728\u4e24\u4e2a\u771f\u5b9e\u5fc3\u7406\u5065\u5eb7\u8d26\u5355\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u3002", "result": "iForest LSTM\u57fa\u7ebf\u5728\u58f0\u660e\u7ea7\u6570\u636e\u4e0a\u53ec\u56de\u7387\u6700\u9ad8\uff080.963\uff09\uff1b\u6df7\u5408iForest\u6a21\u578b\u5728\u64cd\u4f5c\u7ea7\u6570\u636e\u4e0a\u53ec\u56de\u7387\u6700\u9ad8\uff080.744\uff09\uff0c\u4f46\u7cbe\u5ea6\u8f83\u4f4e\u3002", "conclusion": "\u4f2a\u6807\u8bb0\u4e0e\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u7ed3\u5408\u5728\u590d\u6742\u3001\u4e0d\u5e73\u8861\u7684\u5f02\u5e38\u68c0\u6d4b\u573a\u666f\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2507.01838", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01838", "abs": "https://arxiv.org/abs/2507.01838", "authors": ["Hailong Yan", "Ao Li", "Xiangtao Zhang", "Zhe Liu", "Zenglin Shi", "Ce Zhu", "Le Zhang"], "title": "MobileIE: An Extremely Lightweight and Effective ConvNet for Real-Time Image Enhancement on Mobile Devices", "comment": "Accepted by ICCV 2025", "summary": "Recent advancements in deep neural networks have driven significant progress\nin image enhancement (IE). However, deploying deep learning models on\nresource-constrained platforms, such as mobile devices, remains challenging due\nto high computation and memory demands. To address these challenges and\nfacilitate real-time IE on mobile, we introduce an extremely lightweight\nConvolutional Neural Network (CNN) framework with around 4K parameters. Our\napproach integrates reparameterization with an Incremental Weight Optimization\nstrategy to ensure efficiency. Additionally, we enhance performance with a\nFeature Self-Transform module and a Hierarchical Dual-Path Attention mechanism,\noptimized with a Local Variance-Weighted loss. With this efficient framework,\nwe are the first to achieve real-time IE inference at up to 1,100 frames per\nsecond (FPS) while delivering competitive image quality, achieving the best\ntrade-off between speed and performance across multiple IE tasks. The code will\nbe available at https://github.com/AVC2-UESTC/MobileIE.git.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6781\u8f7b\u91cf\u7ea7\u7684CNN\u6846\u67b6\uff0c\u7528\u4e8e\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u56fe\u50cf\u589e\u5f3a\uff0c\u53c2\u6570\u4ec5\u7ea64K\uff0c\u5b9e\u73b0\u4e861,100 FPS\u7684\u9ad8\u6548\u63a8\u7406\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\uff08\u5982\u79fb\u52a8\u8bbe\u5907\uff09\u4e0a\u90e8\u7f72\u65f6\u7684\u9ad8\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u91cd\u53c2\u6570\u5316\u548c\u589e\u91cf\u6743\u91cd\u4f18\u5316\u7b56\u7565\uff0c\u5f15\u5165\u7279\u5f81\u81ea\u53d8\u6362\u6a21\u5757\u548c\u5206\u5c42\u53cc\u8def\u5f84\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u4f7f\u7528\u5c40\u90e8\u65b9\u5dee\u52a0\u6743\u635f\u5931\u4f18\u5316\u3002", "result": "\u9996\u6b21\u5b9e\u73b0\u9ad8\u8fbe1,100 FPS\u7684\u5b9e\u65f6\u56fe\u50cf\u589e\u5f3a\u63a8\u7406\uff0c\u540c\u65f6\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u56fe\u50cf\u8d28\u91cf\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u901f\u5ea6\u548c\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u4e86\u6700\u4f73\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u56fe\u50cf\u589e\u5f3a\u3002"}}
{"id": "2507.01882", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01882", "abs": "https://arxiv.org/abs/2507.01882", "authors": ["Guiqiu Liao", "Matjaz Jogan", "Marcel Hussing", "Edward Zhang", "Eric Eaton", "Daniel A. Hashimoto"], "title": "Future Slot Prediction for Unsupervised Object Discovery in Surgical Video", "comment": "Accepted by MICCAI2025", "summary": "Object-centric slot attention is an emerging paradigm for unsupervised\nlearning of structured, interpretable object-centric representations (slots).\nThis enables effective reasoning about objects and events at a low\ncomputational cost and is thus applicable to critical healthcare applications,\nsuch as real-time interpretation of surgical video. The heterogeneous scenes in\nreal-world applications like surgery are, however, difficult to parse into a\nmeaningful set of slots. Current approaches with an adaptive slot count perform\nwell on images, but their performance on surgical videos is low. To address\nthis challenge, we propose a dynamic temporal slot transformer (DTST) module\nthat is trained both for temporal reasoning and for predicting the optimal\nfuture slot initialization. The model achieves state-of-the-art performance on\nmultiple surgical databases, demonstrating that unsupervised object-centric\nmethods can be applied to real-world data and become part of the common arsenal\nin healthcare applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u65f6\u5e8f\u69fd\u53d8\u6362\u5668\uff08DTST\uff09\u6a21\u5757\uff0c\u7528\u4e8e\u89e3\u51b3\u624b\u672f\u89c6\u9891\u4e2d\u5bf9\u8c61\u4e2d\u5fc3\u8868\u793a\u5b66\u4e60\u7684\u6311\u6218\uff0c\u5e76\u5728\u591a\u4e2a\u624b\u672f\u6570\u636e\u5e93\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\uff08\u5982\u624b\u672f\uff09\u4e2d\u7684\u5f02\u6784\u573a\u666f\u96be\u4ee5\u89e3\u6790\u4e3a\u6709\u610f\u4e49\u7684\u4e00\u7ec4\u69fd\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u624b\u672f\u89c6\u9891\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u65f6\u5e8f\u69fd\u53d8\u6362\u5668\uff08DTST\uff09\u6a21\u5757\uff0c\u7ed3\u5408\u65f6\u5e8f\u63a8\u7406\u548c\u9884\u6d4b\u672a\u6765\u6700\u4f18\u69fd\u521d\u59cb\u5316\u3002", "result": "\u5728\u591a\u4e2a\u624b\u672f\u6570\u636e\u5e93\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u65e0\u76d1\u7763\u5bf9\u8c61\u4e2d\u5fc3\u65b9\u6cd5\u53ef\u5e94\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u6570\u636e\uff0c\u5e76\u6210\u4e3a\u533b\u7597\u5e94\u7528\u4e2d\u7684\u5e38\u89c1\u5de5\u5177\u3002"}}
{"id": "2507.01955", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01955", "abs": "https://arxiv.org/abs/2507.01955", "authors": ["Rahul Ramachandran", "Ali Garjani", "Roman Bachmann", "Andrei Atanov", "O\u011fuzhan Fatih Kar", "Amir Zamir"], "title": "How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation Models on Standard Computer Vision Tasks", "comment": "Project page at https://fm-vision-evals.epfl.ch/", "summary": "Multimodal foundation models, such as GPT-4o, have recently made remarkable\nprogress, but it is not clear where exactly these models stand in terms of\nunderstanding vision. In this paper, we benchmark the performance of popular\nmultimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0\nFlash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision\ntasks (semantic segmentation, object detection, image classification, depth and\nsurface normal prediction) using established datasets (e.g., COCO, ImageNet and\nits variants, etc).\n  The main challenges to performing this are: 1) most models are trained to\noutput text and cannot natively express versatile domains, such as segments or\n3D geometry, and 2) many leading models are proprietary and accessible only at\nan API level, i.e., there is no weight access to adapt them. We address these\nchallenges by translating standard vision tasks into equivalent text-promptable\nand API-compatible tasks via prompt chaining to create a standardized\nbenchmarking framework.\n  We observe that 1) the models are not close to the state-of-the-art\nspecialist models at any task. However, 2) they are respectable generalists;\nthis is remarkable as they are presumably trained on primarily image-text-based\ntasks. 3) They perform semantic tasks notably better than geometric ones. 4)\nWhile the prompt-chaining techniques affect performance, better models exhibit\nless sensitivity to prompt variations. 5) GPT-4o performs the best among\nnon-reasoning models, securing the top position in 4 out of 6 tasks, 6)\nreasoning models, e.g. o3, show improvements in geometric tasks, and 7) a\npreliminary analysis of models with native image generation, like the latest\nGPT-4o, shows they exhibit quirks like hallucinations and spatial\nmisalignments.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff08\u5982GPT-4o\u7b49\uff09\u5728\u6807\u51c6\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5b83\u4eec\u867d\u4e0d\u53ca\u4e13\u4e1a\u6a21\u578b\uff0c\u4f46\u4f5c\u4e3a\u901a\u7528\u6a21\u578b\u8868\u73b0\u5c1a\u53ef\uff0c\u4e14\u5728\u8bed\u4e49\u4efb\u52a1\u4e0a\u4f18\u4e8e\u51e0\u4f55\u4efb\u52a1\u3002", "motivation": "\u7814\u7a76\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5728\u89c6\u89c9\u7406\u89e3\u65b9\u9762\u7684\u5b9e\u9645\u80fd\u529b\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u63d0\u793a\u94fe\u5c06\u6807\u51c6\u89c6\u89c9\u4efb\u52a1\u8f6c\u5316\u4e3a\u6587\u672c\u53ef\u63d0\u793a\u548cAPI\u517c\u5bb9\u7684\u4efb\u52a1\uff0c\u5efa\u7acb\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u6a21\u578b\u5728\u8bed\u4e49\u4efb\u52a1\u4e0a\u8868\u73b0\u8f83\u597d\uff0c\u4f46\u4e0d\u53ca\u4e13\u4e1a\u6a21\u578b\uff1bGPT-4o\u5728\u975e\u63a8\u7406\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f73\uff1b\u63a8\u7406\u6a21\u578b\u5728\u51e0\u4f55\u4efb\u52a1\u4e0a\u6709\u6539\u8fdb\u3002", "conclusion": "\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u5c24\u5176\u662f\u5728\u51e0\u4f55\u4efb\u52a1\u548c\u63d0\u793a\u654f\u611f\u6027\u65b9\u9762\u3002"}}
{"id": "2507.01803", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01803", "abs": "https://arxiv.org/abs/2507.01803", "authors": ["Leyang Xue", "Meghana Madhyastha", "Randal Burns", "Myungjin Lee", "Mahesh K. Marina"], "title": "Towards Decentralized and Sustainable Foundation Model Training with the Edge", "comment": null, "summary": "Foundation models are at the forefront of AI research, appealing for their\nability to learn from vast datasets and cater to diverse tasks. Yet, their\nsignificant computational demands raise issues of environmental impact and the\nrisk of centralized control in their development. We put forward a vision\ntowards decentralized and sustainable foundation model training that leverages\nthe collective compute of sparingly used connected edge AI devices. We present\nthe rationale behind our vision, particularly in support of its sustainability\nbenefit. We further outline a set of challenges that need to be addressed to\nturn this vision into reality.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u3001\u53ef\u6301\u7eed\u7684\u57fa\u7840\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5229\u7528\u8fb9\u7f18AI\u8bbe\u5907\u7684\u95f2\u7f6e\u8ba1\u7b97\u8d44\u6e90\uff0c\u4ee5\u51cf\u5c11\u73af\u5883\u5f71\u54cd\u548c\u96c6\u4e2d\u63a7\u5236\u98ce\u9669\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u7684\u8ba1\u7b97\u9700\u6c42\u9ad8\uff0c\u5e26\u6765\u73af\u5883\u95ee\u9898\u548c\u96c6\u4e2d\u63a7\u5236\u98ce\u9669\uff0c\u9700\u8981\u66f4\u53ef\u6301\u7eed\u548c\u53bb\u4e2d\u5fc3\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528\u8fb9\u7f18AI\u8bbe\u5907\u7684\u95f2\u7f6e\u8ba1\u7b97\u8d44\u6e90\uff0c\u5b9e\u73b0\u53bb\u4e2d\u5fc3\u5316\u7684\u57fa\u7840\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u63d0\u51fa\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u6301\u7eed\u6027\u4f18\u52bf\uff0c\u5e76\u5217\u4e3e\u4e86\u5b9e\u73b0\u8fc7\u7a0b\u4e2d\u9700\u8981\u89e3\u51b3\u7684\u6311\u6218\u3002", "conclusion": "\u53bb\u4e2d\u5fc3\u5316\u548c\u53ef\u6301\u7eed\u7684\u57fa\u7840\u6a21\u578b\u8bad\u7ec3\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u4f46\u9700\u514b\u670d\u6280\u672f\u6311\u6218\u3002"}}
{"id": "2507.01884", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01884", "abs": "https://arxiv.org/abs/2507.01884", "authors": ["Kunlun Xu", "Fan Zhuo", "Jiangmeng Li", "Xu Zou", "Jiahuan Zhou"], "title": "Self-Reinforcing Prototype Evolution with Dual-Knowledge Cooperation for Semi-Supervised Lifelong Person Re-Identification", "comment": "Accepted by ICCV 2025", "summary": "Current lifelong person re-identification (LReID) methods predominantly rely\non fully labeled data streams. However, in real-world scenarios where\nannotation resources are limited, a vast amount of unlabeled data coexists with\nscarce labeled samples, leading to the Semi-Supervised LReID (Semi-LReID)\nproblem where LReID methods suffer severe performance degradation. Existing\nLReID methods, even when combined with semi-supervised strategies, suffer from\nlimited long-term adaptation performance due to struggling with the noisy\nknowledge occurring during unlabeled data utilization. In this paper, we\npioneer the investigation of Semi-LReID, introducing a novel Self-Reinforcing\nPrototype Evolution with Dual-Knowledge Cooperation framework (SPRED). Our key\ninnovation lies in establishing a self-reinforcing cycle between dynamic\nprototype-guided pseudo-label generation and new-old knowledge collaborative\npurification to enhance the utilization of unlabeled data. Specifically,\nlearnable identity prototypes are introduced to dynamically capture the\nidentity distributions and generate high-quality pseudo-labels. Then, the\ndual-knowledge cooperation scheme integrates current model specialization and\nhistorical model generalization, refining noisy pseudo-labels. Through this\ncyclic design, reliable pseudo-labels are progressively mined to improve\ncurrent-stage learning and ensure positive knowledge propagation over long-term\nlearning. Experiments on the established Semi-LReID benchmarks show that our\nSPRED achieves state-of-the-art performance. Our source code is available at\nhttps://github.com/zhoujiahuan1991/ICCV2025-SPRED", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SPRED\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u539f\u578b\u5f15\u5bfc\u7684\u4f2a\u6807\u7b7e\u751f\u6210\u548c\u65b0\u65e7\u77e5\u8bc6\u534f\u4f5c\u51c0\u5316\uff0c\u89e3\u51b3\u4e86\u534a\u76d1\u7763\u7ec8\u8eab\u884c\u4eba\u91cd\u8bc6\u522b\uff08Semi-LReID\uff09\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u6807\u6ce8\u8d44\u6e90\u6709\u9650\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\u65f6\u6027\u80fd\u4e0b\u964d\u4e25\u91cd\uff0c\u9700\u8981\u89e3\u51b3\u566a\u58f0\u77e5\u8bc6\u95ee\u9898\u3002", "method": "\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u8eab\u4efd\u539f\u578b\u52a8\u6001\u6355\u83b7\u8eab\u4efd\u5206\u5e03\u5e76\u751f\u6210\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\uff0c\u7ed3\u5408\u65b0\u65e7\u77e5\u8bc6\u534f\u4f5c\u51c0\u5316\u566a\u58f0\u4f2a\u6807\u7b7e\u3002", "result": "\u5728Semi-LReID\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSPRED\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "SPRED\u901a\u8fc7\u81ea\u589e\u5f3a\u5faa\u73af\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u672a\u6807\u6ce8\u6570\u636e\u7684\u5229\u7528\u6548\u7387\u548c\u957f\u671f\u5b66\u4e60\u6027\u80fd\u3002"}}
{"id": "2507.01957", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01957", "abs": "https://arxiv.org/abs/2507.01957", "authors": ["Zhuoyang Zhang", "Luke J. Huang", "Chengyue Wu", "Shang Yang", "Kelly Peng", "Yao Lu", "Song Han"], "title": "Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation", "comment": "The first two authors contributed equally to this work", "summary": "We present Locality-aware Parallel Decoding (LPD) to accelerate\nautoregressive image generation. Traditional autoregressive image generation\nrelies on next-patch prediction, a memory-bound process that leads to high\nlatency. Existing works have tried to parallelize next-patch prediction by\nshifting to multi-patch prediction to accelerate the process, but only achieved\nlimited parallelization. To achieve high parallelization while maintaining\ngeneration quality, we introduce two key techniques: (1) Flexible Parallelized\nAutoregressive Modeling, a novel architecture that enables arbitrary generation\nordering and degrees of parallelization. It uses learnable position query\ntokens to guide generation at target positions while ensuring mutual visibility\namong concurrently generated tokens for consistent parallel decoding. (2)\nLocality-aware Generation Ordering, a novel schedule that forms groups to\nminimize intra-group dependencies and maximize contextual support, enhancing\ngeneration quality. With these designs, we reduce the generation steps from 256\nto 20 (256$\\times$256 res.) and 1024 to 48 (512$\\times$512 res.) without\ncompromising quality on the ImageNet class-conditional generation, and\nachieving at least 3.4$\\times$ lower latency than previous parallelized\nautoregressive models.", "AI": {"tldr": "Locality-aware Parallel Decoding (LPD) accelerates autoregressive image generation by introducing flexible parallelization and locality-aware ordering, reducing steps significantly without quality loss.", "motivation": "Traditional autoregressive image generation is memory-bound and slow; existing parallelization methods achieve limited success.", "method": "LPD uses Flexible Parallelized Autoregressive Modeling for arbitrary generation ordering and Locality-aware Generation Ordering to minimize dependencies.", "result": "Reduces generation steps from 256 to 20 (256x256) and 1024 to 48 (512x512) with at least 3.4x lower latency.", "conclusion": "LPD achieves high parallelization and maintains generation quality, significantly improving efficiency."}}
{"id": "2507.01908", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01908", "abs": "https://arxiv.org/abs/2507.01908", "authors": ["Qingdong He", "Xueqin Chen", "Chaoyi Wang", "Yanjie Pan", "Xiaobin Hu", "Zhenye Gan", "Yabiao Wang", "Chengjie Wang", "Xiangtai Li", "Jiangning Zhang"], "title": "Reasoning to Edit: Hypothetical Instruction-Based Image Editing with Visual Reasoning", "comment": null, "summary": "Instruction-based image editing (IIE) has advanced rapidly with the success\nof diffusion models. However, existing efforts primarily focus on simple and\nexplicit instructions to execute editing operations such as adding, deleting,\nmoving, or swapping objects. They struggle to handle more complex implicit\nhypothetical instructions that require deeper reasoning to infer plausible\nvisual changes and user intent. Additionally, current datasets provide limited\nsupport for training and evaluating reasoning-aware editing capabilities.\nArchitecturally, these methods also lack mechanisms for fine-grained detail\nextraction that support such reasoning. To address these limitations, we\npropose Reason50K, a large-scale dataset specifically curated for training and\nevaluating hypothetical instruction reasoning image editing, along with\nReasonBrain, a novel framework designed to reason over and execute implicit\nhypothetical instructions across diverse scenarios. Reason50K includes over 50K\nsamples spanning four key reasoning scenarios: Physical, Temporal, Causal, and\nStory reasoning. ReasonBrain leverages Multimodal Large Language Models (MLLMs)\nfor editing guidance generation and a diffusion model for image synthesis,\nincorporating a Fine-grained Reasoning Cue Extraction (FRCE) module to capture\ndetailed visual and textual semantics essential for supporting instruction\nreasoning. To mitigate the semantic loss, we further introduce a Cross-Modal\nEnhancer (CME) that enables rich interactions between the fine-grained cues and\nMLLM-derived features. Extensive experiments demonstrate that ReasonBrain\nconsistently outperforms state-of-the-art baselines on reasoning scenarios\nwhile exhibiting strong zero-shot generalization to conventional IIE tasks. Our\ndataset and code will be released publicly.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Reason50K\u6570\u636e\u96c6\u548cReasonBrain\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u590d\u6742\u9690\u5f0f\u5047\u8bbe\u6307\u4ee4\u7684\u56fe\u50cf\u7f16\u8f91\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u7ec6\u7c92\u5ea6\u63a8\u7406\u7ebf\u7d22\u63d0\u53d6\u6a21\u5757\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u9700\u8981\u6df1\u5c42\u63a8\u7406\u7684\u590d\u6742\u9690\u5f0f\u5047\u8bbe\u6307\u4ee4\uff0c\u4e14\u6570\u636e\u96c6\u548c\u67b6\u6784\u7f3a\u4e4f\u652f\u6301\u63a8\u7406\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faReason50K\u6570\u636e\u96c6\u548cReasonBrain\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u7ec6\u7c92\u5ea6\u63a8\u7406\u7ebf\u7d22\u63d0\u53d6\u6a21\u5757\uff08FRCE\uff09\uff0c\u5e76\u5f15\u5165\u8de8\u6a21\u6001\u589e\u5f3a\u5668\uff08CME\uff09\u51cf\u5c11\u8bed\u4e49\u635f\u5931\u3002", "result": "ReasonBrain\u5728\u63a8\u7406\u573a\u666f\u4e2d\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5728\u4f20\u7edf\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Reason50K\u548cReasonBrain\u4e3a\u590d\u6742\u6307\u4ee4\u7684\u56fe\u50cf\u7f16\u8f91\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5c06\u516c\u5f00\u3002"}}
{"id": "2507.01823", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01823", "abs": "https://arxiv.org/abs/2507.01823", "authors": ["Dmytro Kuzmenko", "Nadiya Shvai"], "title": "TD-MPC-Opt: Distilling Model-Based Multi-Task Reinforcement Learning Agents", "comment": "Preprint of a manuscript submitted for peer review", "summary": "We present a novel approach to knowledge transfer in model-based\nreinforcement learning, addressing the critical challenge of deploying large\nworld models in resource-constrained environments. Our method efficiently\ndistills a high-capacity multi-task agent (317M parameters) into a compact\nmodel (1M parameters) on the MT30 benchmark, significantly improving\nperformance across diverse tasks. Our distilled model achieves a\nstate-of-the-art normalized score of 28.45, surpassing the original 1M\nparameter model score of 18.93. This improvement demonstrates the ability of\nour distillation technique to capture and consolidate complex multi-task\nknowledge. We further optimize the distilled model through FP16 post-training\nquantization, reducing its size by $\\sim$50\\%. Our approach addresses practical\ndeployment limitations and offers insights into knowledge representation in\nlarge world models, paving the way for more efficient and accessible multi-task\nreinforcement learning systems in robotics and other resource-constrained\napplications. Code available at https://github.com/dmytro-kuzmenko/td-mpc-opt.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u77e5\u8bc6\u8f6c\u79fb\u65b0\u65b9\u6cd5\uff0c\u5c06\u5927\u6a21\u578b\u9ad8\u6548\u538b\u7f29\u4e3a\u5c0f\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u5927\u578b\u4e16\u754c\u6a21\u578b\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u901a\u8fc7\u84b8\u998f\u6280\u672f\u5c06\u9ad8\u5bb9\u91cf\u591a\u4efb\u52a1\u4ee3\u7406\uff08317M\u53c2\u6570\uff09\u538b\u7f29\u4e3a\u7d27\u51d1\u6a21\u578b\uff081M\u53c2\u6570\uff09\uff0c\u5e76\u8fdb\u4e00\u6b65\u4f18\u5316\u91cf\u5316\u3002", "result": "\u84b8\u998f\u6a21\u578b\u5728MT30\u57fa\u51c6\u4e0a\u8fbe\u523028.45\u7684\u5f52\u4e00\u5316\u5206\u6570\uff0c\u4f18\u4e8e\u539f\u59cb1M\u6a21\u578b\u768418.93\uff0c\u4e14\u91cf\u5316\u540e\u6a21\u578b\u5927\u5c0f\u51cf\u5c11\u7ea650%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8d44\u6e90\u53d7\u9650\u5e94\u7528\u4e2d\u7684\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u63ed\u793a\u4e86\u5927\u578b\u4e16\u754c\u6a21\u578b\u7684\u77e5\u8bc6\u8868\u793a\u65b9\u5f0f\u3002"}}
{"id": "2507.01909", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01909", "abs": "https://arxiv.org/abs/2507.01909", "authors": ["Jorge Tapias Gomez", "Nishant Nadkarni", "Lando S. Bosma", "Jue Jiang", "Ergys D. Subashi", "William P. Segars", "James M. Balter", "Mert R Sabuncu", "Neelam Tyagi", "Harini Veeraraghavan"], "title": "Modality Agnostic, patient-specific digital twins modeling temporally varying digestive motion", "comment": "7 Pages, 6 figures, 4 tables", "summary": "Objective: Clinical implementation of deformable image registration (DIR)\nrequires voxel-based spatial accuracy metrics such as manually identified\nlandmarks, which are challenging to implement for highly mobile\ngastrointestinal (GI) organs. To address this, patient-specific digital twins\n(DT) modeling temporally varying motion were created to assess the accuracy of\nDIR methods. Approach: 21 motion phases simulating digestive GI motion as 4D\nsequences were generated from static 3D patient scans using published\nanalytical GI motion models through a semi-automated pipeline. Eleven datasets,\nincluding six T2w FSE MRI (T2w MRI), two T1w 4D golden-angle stack-of-stars,\nand three contrast-enhanced CT scans. The motion amplitudes of the DTs were\nassessed against real patient stomach motion amplitudes extracted from\nindependent 4D MRI datasets. The generated DTs were then used to assess six\ndifferent DIR methods using target registration error, Dice similarity\ncoefficient, and the 95th percentile Hausdorff distance using summary metrics\nand voxel-level granular visualizations. Finally, for a subset of T2w MRI scans\nfrom patients treated with MR-guided radiation therapy, dose distributions were\nwarped and accumulated to assess dose warping errors, including evaluations of\nDIR performance in both low- and high-dose regions for patient-specific error\nestimation. Main results: Our proposed pipeline synthesized DTs modeling\nrealistic GI motion, achieving mean and maximum motion amplitudes and a mean\nlog Jacobian determinant within 0.8 mm and 0.01, respectively, similar to\npublished real-patient gastric motion data. It also enables the extraction of\ndetailed quantitative DIR performance metrics and rigorous validation of dose\nmapping accuracy. Significance: The pipeline enables rigorously testing DIR\ntools for dynamic, anatomically complex regions enabling granular spatial and\ndosimetric accuracies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u5b57\u5b6a\u751f\uff08DT\uff09\u7684\u7ba1\u9053\uff0c\u7528\u4e8e\u8bc4\u4f30\u53d8\u5f62\u56fe\u50cf\u914d\u51c6\uff08DIR\uff09\u65b9\u6cd5\u5728\u80c3\u80a0\u9053\uff08GI\uff09\u5668\u5b98\u8fd0\u52a8\u4e2d\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4e34\u5e8a\u5b9e\u65bdDIR\u9700\u8981\u57fa\u4e8e\u4f53\u7d20\u7684\u7a7a\u95f4\u51c6\u786e\u6027\u6307\u6807\uff0c\u4f46\u624b\u52a8\u6807\u8bb0\u5730\u6807\u5bf9\u4e8e\u9ad8\u5ea6\u79fb\u52a8\u7684GI\u5668\u5b98\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u901a\u8fc7\u534a\u81ea\u52a8\u5316\u7ba1\u9053\uff0c\u4ece\u9759\u60013D\u60a3\u8005\u626b\u63cf\u751f\u621021\u4e2a\u8fd0\u52a8\u9636\u6bb5\u76844D\u5e8f\u5217\uff0c\u6a21\u62dfGI\u8fd0\u52a8\uff0c\u5e76\u8bc4\u4f30\u516d\u79cdDIR\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "result": "\u751f\u6210\u7684DT\u6a21\u62df\u4e86\u771f\u5b9e\u7684GI\u8fd0\u52a8\uff0c\u8fd0\u52a8\u5e45\u5ea6\u4e0e\u771f\u5b9e\u60a3\u8005\u6570\u636e\u76f8\u4f3c\uff0c\u5e76\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684DIR\u6027\u80fd\u6307\u6807\u548c\u5242\u91cf\u6620\u5c04\u51c6\u786e\u6027\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u7ba1\u9053\u80fd\u591f\u4e25\u683c\u6d4b\u8bd5DIR\u5de5\u5177\u5728\u52a8\u6001\u3001\u89e3\u5256\u590d\u6742\u533a\u57df\u7684\u6027\u80fd\uff0c\u652f\u6301\u7a7a\u95f4\u548c\u5242\u91cf\u7cbe\u5ea6\u7684\u8be6\u7ec6\u8bc4\u4f30\u3002"}}
{"id": "2507.01912", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01912", "abs": "https://arxiv.org/abs/2507.01912", "authors": ["Ranjan Sapkota", "Zhichao Meng", "Martin Churuvija", "Xiaoqiang Du", "Zenghong Ma", "Manoj Karkee"], "title": "3D Reconstruction and Information Fusion between Dormant and Canopy Seasons in Commercial Orchards Using Deep Learning and Fast GICP", "comment": "17 pages, 4 tables, 11 figures", "summary": "In orchard automation, dense foliage during the canopy season severely\noccludes tree structures, minimizing visibility to various canopy parts such as\ntrunks and branches, which limits the ability of a machine vision system.\nHowever, canopy structure is more open and visible during the dormant season\nwhen trees are defoliated. In this work, we present an information fusion\nframework that integrates multi-seasonal structural data to support robotic and\nautomated crop load management during the entire growing season. The framework\ncombines high-resolution RGB-D imagery from both dormant and canopy periods\nusing YOLOv9-Seg for instance segmentation, Kinect Fusion for 3D\nreconstruction, and Fast Generalized Iterative Closest Point (Fast GICP) for\nmodel alignment. Segmentation outputs from YOLOv9-Seg were used to extract\ndepth-informed masks, which enabled accurate 3D point cloud reconstruction via\nKinect Fusion; these reconstructed models from each season were subsequently\naligned using Fast GICP to achieve spatially coherent multi-season fusion. The\nYOLOv9-Seg model, trained on manually annotated images, achieved a mean squared\nerror (MSE) of 0.0047 and segmentation mAP@50 scores up to 0.78 for trunks in\ndormant season dataset. Kinect Fusion enabled accurate reconstruction of tree\ngeometry, validated with field measurements resulting in root mean square\nerrors (RMSE) of 5.23 mm for trunk diameter, 4.50 mm for branch diameter, and\n13.72 mm for branch spacing. Fast GICP achieved precise cross-seasonal\nregistration with a minimum fitness score of 0.00197, allowing integrated,\ncomprehensive tree structure modeling despite heavy occlusions during the\ngrowing season. This fused structural representation enables robotic systems to\naccess otherwise obscured architectural information, improving the precision of\npruning, thinning, and other automated orchard operations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5b63\u8282\u4fe1\u606f\u878d\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u4f11\u7720\u671f\u548c\u751f\u957f\u671f\u7684RGB-D\u56fe\u50cf\uff0c\u7528\u4e8e\u679c\u56ed\u81ea\u52a8\u5316\u7ba1\u7406\uff0c\u63d0\u5347\u673a\u5668\u89c6\u89c9\u7cfb\u7edf\u5728\u5bc6\u96c6\u6811\u51a0\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u679c\u56ed\u81ea\u52a8\u5316\u4e2d\uff0c\u751f\u957f\u671f\u5bc6\u96c6\u7684\u6811\u53f6\u906e\u6321\u4e86\u6811\u5e72\u548c\u6811\u679d\u7ed3\u6784\uff0c\u9650\u5236\u4e86\u673a\u5668\u89c6\u89c9\u7cfb\u7edf\u7684\u80fd\u529b\uff0c\u800c\u4f11\u7720\u671f\u6811\u51a0\u7ed3\u6784\u66f4\u6e05\u6670\u53ef\u89c1\u3002", "method": "\u4f7f\u7528YOLOv9-Seg\u8fdb\u884c\u5b9e\u4f8b\u5206\u5272\uff0cKinect Fusion\u8fdb\u884c3D\u91cd\u5efa\uff0cFast GICP\u8fdb\u884c\u6a21\u578b\u5bf9\u9f50\uff0c\u878d\u5408\u591a\u5b63\u8282\u6570\u636e\u3002", "result": "YOLOv9-Seg\u5728\u4f11\u7720\u671f\u6570\u636e\u96c6\u4e0aMSE\u4e3a0.0047\uff0cmAP@50\u4e3a0.78\uff1bKinect Fusion\u91cd\u5efa\u8bef\u5dee\u5c0f\uff08\u6811\u5e72\u76f4\u5f84RMSE\u4e3a5.23 mm\uff09\uff1bFast GICP\u6ce8\u518c\u7cbe\u5ea6\u9ad8\uff08\u6700\u5c0f\u9002\u5e94\u5206\u65700.00197\uff09\u3002", "conclusion": "\u591a\u5b63\u8282\u878d\u5408\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u679c\u56ed\u64cd\u4f5c\u4e2d\u7684\u7cbe\u5ea6\uff0c\u514b\u670d\u4e86\u751f\u957f\u671f\u906e\u6321\u95ee\u9898\u3002"}}
{"id": "2507.01926", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01926", "abs": "https://arxiv.org/abs/2507.01926", "authors": ["Yaowei Li", "Xiaoyu Li", "Zhaoyang Zhang", "Yuxuan Bian", "Gan Liu", "Xinyuan Li", "Jiale Xu", "Wenbo Hu", "Yating Liu", "Lingen Li", "Jing Cai", "Yuexian Zou", "Yancheng He", "Ying Shan"], "title": "IC-Custom: Diverse Image Customization via In-Context Learning", "comment": "Project page: https://liyaowei-stu.github.io/project/IC_Custom", "summary": "Image customization, a crucial technique for industrial media production,\naims to generate content that is consistent with reference images. However,\ncurrent approaches conventionally separate image customization into\nposition-aware and position-free customization paradigms and lack a universal\nframework for diverse customization, limiting their applications across various\nscenarios. To overcome these limitations, we propose IC-Custom, a unified\nframework that seamlessly integrates position-aware and position-free image\ncustomization through in-context learning. IC-Custom concatenates reference\nimages with target images to a polyptych, leveraging DiT's multi-modal\nattention mechanism for fine-grained token-level interactions. We introduce the\nIn-context Multi-Modal Attention (ICMA) mechanism with learnable task-oriented\nregister tokens and boundary-aware positional embeddings to enable the model to\ncorrectly handle different task types and distinguish various inputs in\npolyptych configurations. To bridge the data gap, we carefully curated a\nhigh-quality dataset of 12k identity-consistent samples with 8k from real-world\nsources and 4k from high-quality synthetic data, avoiding the overly glossy and\nover-saturated synthetic appearance. IC-Custom supports various industrial\napplications, including try-on, accessory placement, furniture arrangement, and\ncreative IP customization. Extensive evaluations on our proposed ProductBench\nand the publicly available DreamBench demonstrate that IC-Custom significantly\noutperforms community workflows, closed-source models, and state-of-the-art\nopen-source approaches. IC-Custom achieves approximately 73% higher human\npreference across identity consistency, harmonicity, and text alignment\nmetrics, while training only 0.4% of the original model parameters. Project\npage: https://liyaowei-stu.github.io/project/IC_Custom", "AI": {"tldr": "IC-Custom\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u56fe\u50cf\u5b9a\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u6574\u5408\u4f4d\u7f6e\u611f\u77e5\u548c\u65e0\u4f4d\u7f6e\u5b9a\u5236\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u56fe\u50cf\u5b9a\u5236\u65b9\u6cd5\u7f3a\u4e4f\u901a\u7528\u6846\u67b6\uff0c\u9650\u5236\u4e86\u591a\u6837\u5316\u573a\u666f\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faIC-Custom\u6846\u67b6\uff0c\u5229\u7528\u591a\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u548c\u4efb\u52a1\u5bfc\u5411\u7684\u6ce8\u518c\u6807\u8bb0\uff0c\u7ed3\u5408\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4eba\u7c7b\u504f\u597d\u63d0\u534773%\uff0c\u4ec5\u8bad\u7ec30.4%\u7684\u53c2\u6570\u91cf\u3002", "conclusion": "IC-Custom\u4e3a\u5de5\u4e1a\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u901a\u7528\u7684\u56fe\u50cf\u5b9a\u5236\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01831", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.01831", "abs": "https://arxiv.org/abs/2507.01831", "authors": ["Yucen Lily Li", "Daohan Lu", "Polina Kirichenko", "Shikai Qiu", "Tim G. J. Rudner", "C. Bayan Bruss", "Andrew Gordon Wilson"], "title": "Out-of-Distribution Detection Methods Answer the Wrong Questions", "comment": "Extended version of ICML 2025 paper", "summary": "To detect distribution shifts and improve model safety, many\nout-of-distribution (OOD) detection methods rely on the predictive uncertainty\nor features of supervised models trained on in-distribution data. In this\npaper, we critically re-examine this popular family of OOD detection\nprocedures, and we argue that these methods are fundamentally answering the\nwrong questions for OOD detection. There is no simple fix to this misalignment,\nsince a classifier trained only on in-distribution classes cannot be expected\nto identify OOD points; for instance, a cat-dog classifier may confidently\nmisclassify an airplane if it contains features that distinguish cats from\ndogs, despite generally appearing nothing alike. We find that uncertainty-based\nmethods incorrectly conflate high uncertainty with being OOD, while\nfeature-based methods incorrectly conflate far feature-space distance with\nbeing OOD. We show how these pathologies manifest as irreducible errors in OOD\ndetection and identify common settings where these methods are ineffective.\nAdditionally, interventions to improve OOD detection such as feature-logit\nhybrid methods, scaling of model and data size, epistemic uncertainty\nrepresentation, and outlier exposure also fail to address this fundamental\nmisalignment in objectives. We additionally consider unsupervised density\nestimation and generative models for OOD detection, which we show have their\nown fundamental limitations.", "AI": {"tldr": "\u8bba\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u57fa\u4e8e\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u548c\u7279\u5f81\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u6307\u51fa\u8fd9\u4e9b\u65b9\u6cd5\u5b58\u5728\u6839\u672c\u6027\u9519\u8bef\uff0c\u65e0\u6cd5\u6709\u6548\u8bc6\u522bOOD\u6570\u636e\u3002", "motivation": "\u73b0\u6709OOD\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u76d1\u7763\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u6216\u7279\u5f81\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u65e0\u6cd5\u6b63\u786e\u8bc6\u522bOOD\u6570\u636e\uff0c\u751a\u81f3\u53ef\u80fd\u4ea7\u751f\u8bef\u5bfc\u6027\u7ed3\u679c\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e0d\u786e\u5b9a\u6027\u65b9\u6cd5\u548c\u7279\u5f81\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63a2\u8ba8\u4e86\u5176\u9519\u8bef\u6839\u6e90\uff0c\u5e76\u8bc4\u4f30\u4e86\u6539\u8fdb\u65b9\u6cd5\u7684\u65e0\u6548\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5728OOD\u68c0\u6d4b\u4e2d\u5b58\u5728\u4e0d\u53ef\u7ea6\u8bef\u5dee\uff0c\u4e14\u5e38\u89c1\u6539\u8fdb\u63aa\u65bd\u65e0\u6cd5\u89e3\u51b3\u6839\u672c\u95ee\u9898\u3002", "conclusion": "\u8bba\u6587\u6307\u51faOOD\u68c0\u6d4b\u9700\u8981\u91cd\u65b0\u601d\u8003\u65b9\u6cd5\u8bbe\u8ba1\uff0c\u73b0\u6709\u6280\u672f\u5b58\u5728\u672c\u8d28\u7f3a\u9677\u3002"}}
{"id": "2507.01927", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01927", "abs": "https://arxiv.org/abs/2507.01927", "authors": ["Zhentan Zheng"], "title": "evMLP: An Efficient Event-Driven MLP Architecture for Vision", "comment": null, "summary": "Deep neural networks have achieved remarkable results in computer vision\ntasks. In the early days, Convolutional Neural Networks (CNNs) were the\nmainstream architecture. In recent years, Vision Transformers (ViTs) have\nbecome increasingly popular. In addition, exploring applications of multi-layer\nperceptrons (MLPs) has provided new perspectives for research into vision model\narchitectures. In this paper, we present evMLP accompanied by a simple\nevent-driven local update mechanism. The proposed evMLP can independently\nprocess patches on images or feature maps via MLPs. We define changes between\nconsecutive frames as \"events\". Under the event-driven local update mechanism,\nevMLP selectively processes patches where events occur. For sequential image\ndata (e.g., video processing), this approach improves computational performance\nby avoiding redundant computations. Through ImageNet image classification\nexperiments, evMLP attains accuracy competitive with state-of-the-art models.\nMore significantly, experimental results on multiple video datasets demonstrate\nthat evMLP reduces computational cost via its event-driven local update\nmechanism while maintaining output consistency with its non-event-driven\nbaseline. The code and trained models are available at\nhttps://github.com/i-evi/evMLP.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aevMLP\u7684\u65b0\u578b\u89c6\u89c9\u6a21\u578b\uff0c\u901a\u8fc7\u4e8b\u4ef6\u9a71\u52a8\u7684\u5c40\u90e8\u66f4\u65b0\u673a\u5236\uff0c\u9009\u62e9\u6027\u5730\u5904\u7406\u56fe\u50cf\u6216\u7279\u5f81\u56fe\u4e2d\u7684\u53d8\u5316\u533a\u57df\uff0c\u4ece\u800c\u5728\u89c6\u9891\u5904\u7406\u7b49\u4efb\u52a1\u4e2d\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u63a2\u7d22\u591a\u5c42\u611f\u77e5\u673a\uff08MLPs\uff09\u5728\u89c6\u89c9\u6a21\u578b\u67b6\u6784\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u5e8f\u5217\u56fe\u50cf\u6570\u636e\u65f6\u7684\u5197\u4f59\u8ba1\u7b97\u95ee\u9898\u3002", "method": "\u63d0\u51faevMLP\u6a21\u578b\uff0c\u7ed3\u5408\u4e8b\u4ef6\u9a71\u52a8\u7684\u5c40\u90e8\u66f4\u65b0\u673a\u5236\uff0c\u4ec5\u5904\u7406\u8fde\u7eed\u5e27\u95f4\u53d1\u751f\u53d8\u5316\u7684\u56fe\u50cf\u5757\uff08\u4e8b\u4ef6\uff09\u3002", "result": "\u5728ImageNet\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u5728\u591a\u4e2a\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "evMLP\u901a\u8fc7\u4e8b\u4ef6\u9a71\u52a8\u673a\u5236\u6709\u6548\u51cf\u5c11\u4e86\u8ba1\u7b97\u5197\u4f59\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\u7684\u8f93\u51fa\u4e00\u81f4\u6027\u3002"}}
{"id": "2507.01841", "categories": ["cs.LG", "cs.IT", "eess.SP", "math.IT", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.01841", "abs": "https://arxiv.org/abs/2507.01841", "authors": ["Yihang Gao", "Vincent Y. F. Tan"], "title": "Automatic Rank Determination for Low-Rank Adaptation via Submodular Function Maximization", "comment": null, "summary": "In this paper, we propose SubLoRA, a rank determination method for Low-Rank\nAdaptation (LoRA) based on submodular function maximization. In contrast to\nprior approaches, such as AdaLoRA, that rely on first-order (linearized)\napproximations of the loss function, SubLoRA utilizes second-order information\nto capture the potentially complex loss landscape by incorporating the Hessian\nmatrix. We show that the linearization becomes inaccurate and ill-conditioned\nwhen the LoRA parameters have been well optimized, motivating the need for a\nmore reliable and nuanced second-order formulation. To this end, we reformulate\nthe rank determination problem as a combinatorial optimization problem with a\nquadratic objective. However, solving this problem exactly is NP-hard in\ngeneral. To overcome the computational challenge, we introduce a submodular\nfunction maximization framework and devise a greedy algorithm with\napproximation guarantees. We derive a sufficient and necessary condition under\nwhich the rank-determination objective becomes submodular, and construct a\nclosed-form projection of the Hessian matrix that satisfies this condition\nwhile maintaining computational efficiency. Our method combines solid\ntheoretical foundations, second-order accuracy, and practical computational\nefficiency. We further extend SubLoRA to a joint optimization setting,\nalternating between LoRA parameter updates and rank determination under a rank\nbudget constraint. Extensive experiments on fine-tuning physics-informed neural\nnetworks (PINNs) for solving partial differential equations (PDEs) demonstrate\nthe effectiveness of our approach. Results show that SubLoRA outperforms\nexisting methods in both rank determination and joint training performance.", "AI": {"tldr": "SubLoRA\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b50\u6a21\u51fd\u6570\u6700\u5927\u5316\u7684LoRA\u79e9\u786e\u5b9a\u65b9\u6cd5\uff0c\u5229\u7528\u4e8c\u9636\u4fe1\u606f\uff08Hessian\u77e9\u9635\uff09\u6539\u8fdb\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5e76\u901a\u8fc7\u8d2a\u5fc3\u7b97\u6cd5\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u3002\u5b9e\u9a8c\u8868\u660e\u5176\u5728PINN\u5fae\u8c03\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982AdaLoRA\uff09\u4f9d\u8d56\u4e00\u9636\u8fd1\u4f3c\uff0c\u5728LoRA\u53c2\u6570\u4f18\u5316\u826f\u597d\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u53ef\u9760\u4e14\u7cbe\u786e\u7684\u4e8c\u9636\u65b9\u6cd5\u3002", "method": "\u5c06\u79e9\u786e\u5b9a\u95ee\u9898\u8f6c\u5316\u4e3a\u7ec4\u5408\u4f18\u5316\u95ee\u9898\uff0c\u5f15\u5165\u5b50\u6a21\u51fd\u6570\u6700\u5927\u5316\u6846\u67b6\uff0c\u8bbe\u8ba1\u8d2a\u5fc3\u7b97\u6cd5\uff0c\u5e76\u6269\u5c55\u81f3\u8054\u5408\u4f18\u5316\u8bbe\u7f6e\u3002", "result": "SubLoRA\u5728\u79e9\u786e\u5b9a\u548c\u8054\u5408\u8bad\u7ec3\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SubLoRA\u7ed3\u5408\u4e86\u7406\u8bba\u4e25\u8c28\u6027\u3001\u4e8c\u9636\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u4e3aLoRA\u79e9\u786e\u5b9a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01938", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01938", "abs": "https://arxiv.org/abs/2507.01938", "authors": ["Yiming Ju", "Jijin Hu", "Zhengxiong Luo", "Haoge Deng", "hanyu Zhao", "Li Du", "Chengwei Wu", "Donglin Hao", "Xinlong Wang", "Tengfei Pan"], "title": "CI-VID: A Coherent Interleaved Text-Video Dataset", "comment": null, "summary": "Text-to-video (T2V) generation has recently attracted considerable attention,\nresulting in the development of numerous high-quality datasets that have\npropelled progress in this area. However, existing public datasets are\nprimarily composed of isolated text-video (T-V) pairs and thus fail to support\nthe modeling of coherent multi-clip video sequences. To address this\nlimitation, we introduce CI-VID, a dataset that moves beyond isolated\ntext-to-video (T2V) generation toward text-and-video-to-video (TV2V)\ngeneration, enabling models to produce coherent, multi-scene video sequences.\nCI-VID contains over 340,000 samples, each featuring a coherent sequence of\nvideo clips with text captions that capture both the individual content of each\nclip and the transitions between them, enabling visually and textually grounded\ngeneration. To further validate the effectiveness of CI-VID, we design a\ncomprehensive, multi-dimensional benchmark incorporating human evaluation,\nVLM-based assessment, and similarity-based metrics. Experimental results\ndemonstrate that models trained on CI-VID exhibit significant improvements in\nboth accuracy and content consistency when generating video sequences. This\nfacilitates the creation of story-driven content with smooth visual transitions\nand strong temporal coherence, underscoring the quality and practical utility\nof the CI-VID dataset We release the CI-VID dataset and the accompanying code\nfor data construction and evaluation at: https://github.com/ymju-BAAI/CI-VID", "AI": {"tldr": "CI-VID\u662f\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6\uff0c\u652f\u6301\u4ece\u6587\u672c\u548c\u89c6\u9891\u751f\u6210\u8fde\u8d2f\u7684\u591a\u573a\u666f\u89c6\u9891\u5e8f\u5217\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u4ec5\u652f\u6301\u5b64\u7acb\u6587\u672c-\u89c6\u9891\u5bf9\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u516c\u5171\u6570\u636e\u96c6\u4e3b\u8981\u5305\u542b\u5b64\u7acb\u7684\u6587\u672c-\u89c6\u9891\u5bf9\uff0c\u65e0\u6cd5\u652f\u6301\u8fde\u8d2f\u591a\u573a\u666f\u89c6\u9891\u5e8f\u5217\u7684\u5efa\u6a21\u3002", "method": "\u5f15\u5165CI-VID\u6570\u636e\u96c6\uff0c\u5305\u542b34\u4e07\u6837\u672c\uff0c\u6bcf\u4e2a\u6837\u672c\u5305\u542b\u8fde\u8d2f\u7684\u89c6\u9891\u7247\u6bb5\u5e8f\u5217\u548c\u6587\u672c\u63cf\u8ff0\uff0c\u652f\u6301\u6587\u672c\u548c\u89c6\u9891\u5230\u89c6\u9891\u7684\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528CI-VID\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u751f\u6210\u89c6\u9891\u5e8f\u5217\u65f6\uff0c\u51c6\u786e\u6027\u548c\u5185\u5bb9\u4e00\u81f4\u6027\u663e\u8457\u63d0\u5347\u3002", "conclusion": "CI-VID\u6570\u636e\u96c6\u652f\u6301\u6545\u4e8b\u9a71\u52a8\u5185\u5bb9\u7684\u751f\u6210\uff0c\u5177\u6709\u9ad8\u8d28\u91cf\u7684\u89c6\u89c9\u8fc7\u6e21\u548c\u65f6\u95f4\u8fde\u8d2f\u6027\uff0c\u5b9e\u7528\u6027\u5f3a\u3002"}}
{"id": "2507.01945", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01945", "abs": "https://arxiv.org/abs/2507.01945", "authors": ["Nan Chen", "Mengqi Huang", "Yihao Meng", "Zhendong Mao"], "title": "LongAnimation: Long Animation Generation with Dynamic Global-Local Memory", "comment": null, "summary": "Animation colorization is a crucial part of real animation industry\nproduction. Long animation colorization has high labor costs. Therefore,\nautomated long animation colorization based on the video generation model has\nsignificant research value. Existing studies are limited to short-term\ncolorization. These studies adopt a local paradigm, fusing overlapping features\nto achieve smooth transitions between local segments. However, the local\nparadigm neglects global information, failing to maintain long-term color\nconsistency. In this study, we argue that ideal long-term color consistency can\nbe achieved through a dynamic global-local paradigm, i.e., dynamically\nextracting global color-consistent features relevant to the current generation.\nSpecifically, we propose LongAnimation, a novel framework, which mainly\nincludes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color\nConsistency Reward. The SketchDiT captures hybrid reference features to support\nthe DGLM module. The DGLM module employs a long video understanding model to\ndynamically compress global historical features and adaptively fuse them with\nthe current generation features. To refine the color consistency, we introduce\na Color Consistency Reward. During inference, we propose a color consistency\nfusion to smooth the video segment transition. Extensive experiments on both\nshort-term (14 frames) and long-term (average 500 frames) animations show the\neffectiveness of LongAnimation in maintaining short-term and long-term color\nconsistency for open-domain animation colorization task. The code can be found\nat https://cn-makers.github.io/long_animation_web/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u5168\u5c40-\u5c40\u90e8\u8303\u5f0f\uff08LongAnimation\u6846\u67b6\uff09\uff0c\u7528\u4e8e\u5b9e\u73b0\u957f\u52a8\u753b\u8272\u5f69\u4e00\u81f4\u6027\uff0c\u5305\u62ecSketchDiT\u3001DGLM\u6a21\u5757\u548c\u8272\u5f69\u4e00\u81f4\u6027\u5956\u52b1\u3002", "motivation": "\u957f\u52a8\u753b\u8272\u5f69\u5316\u5728\u52a8\u753b\u4ea7\u4e1a\u4e2d\u6210\u672c\u9ad8\uff0c\u73b0\u6709\u7814\u7a76\u4ec5\u5173\u6ce8\u77ed\u671f\u8272\u5f69\u5316\uff0c\u7f3a\u4e4f\u5168\u5c40\u4fe1\u606f\u5bfc\u81f4\u957f\u671f\u8272\u5f69\u4e00\u81f4\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51faLongAnimation\u6846\u67b6\uff0c\u7ed3\u5408SketchDiT\u63d0\u53d6\u53c2\u8003\u7279\u5f81\uff0cDGLM\u6a21\u5757\u52a8\u6001\u878d\u5408\u5168\u5c40\u5386\u53f2\u7279\u5f81\u4e0e\u5f53\u524d\u751f\u6210\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u8272\u5f69\u4e00\u81f4\u6027\u5956\u52b1\u4f18\u5316\u7ed3\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLongAnimation\u5728\u77ed\u671f\uff0814\u5e27\uff09\u548c\u957f\u671f\uff08\u5e73\u5747500\u5e27\uff09\u52a8\u753b\u4e2d\u5747\u80fd\u6709\u6548\u4fdd\u6301\u8272\u5f69\u4e00\u81f4\u6027\u3002", "conclusion": "\u52a8\u6001\u5168\u5c40-\u5c40\u90e8\u8303\u5f0f\u663e\u8457\u63d0\u5347\u4e86\u957f\u52a8\u753b\u8272\u5f69\u5316\u7684\u8272\u5f69\u4e00\u81f4\u6027\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.01949", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01949", "abs": "https://arxiv.org/abs/2507.01949", "authors": ["Kwai Keye Team", "Biao Yang", "Bin Wen", "Changyi Liu", "Chenglong Chu", "Chengru Song", "Chongling Rao", "Chuan Yi", "Da Li", "Dunju Zang", "Fan Yang", "Guorui Zhou", "Hao Peng", "Haojie Ding", "Jiaming Huang", "Jiangxia Cao", "Jiankang Chen", "Jingyun Hua", "Jin Ouyang", "Kaibing Chen", "Kaiyu Jiang", "Kaiyu Tang", "Kun Gai", "Shengnan Zhang", "Siyang Mao", "Sui Huang", "Tianke Zhang", "Tingting Gao", "Wei Chen", "Wei Yuan", "Xiangyu Wu", "Xiao Hu", "Xingyu Lu", "Yang Zhou", "Yi-Fan Zhang", "Yiping Yang", "Yulong Chen", "Zhenhua Wu", "Zhenyu Li", "Zhixin Ling", "Ziming Li", "Dehua Ma", "Di Xu", "Haixuan Gao", "Hang Li", "Jiawei Guo", "Jing Wang", "Lejian Ren", "Muhao Wei", "Qianqian Wang", "Qigen Hu", "Shiyao Wang", "Tao Yu", "Xinchen Luo", "Yan Li", "Yiming Liang", "Yuhang Hu", "Zeyi Lu", "Zhuoran Yang", "Zixing Zhang"], "title": "Kwai Keye-VL Technical Report", "comment": "Technical Report: https://github.com/Kwai-Keye/Keye", "summary": "While Multimodal Large Language Models (MLLMs) demonstrate remarkable\ncapabilities on static images, they often fall short in comprehending dynamic,\ninformation-dense short-form videos, a dominant medium in today's digital\nlandscape. To bridge this gap, we introduce \\textbf{Kwai Keye-VL}, an\n8-billion-parameter multimodal foundation model engineered for leading-edge\nperformance in short-video understanding while maintaining robust\ngeneral-purpose vision-language abilities. The development of Keye-VL rests on\ntwo core pillars: a massive, high-quality dataset exceeding 600 billion tokens\nwith a strong emphasis on video, and an innovative training recipe. This recipe\nfeatures a four-stage pre-training process for solid vision-language alignment,\nfollowed by a meticulous two-phase post-training process. The first\npost-training stage enhances foundational capabilities like instruction\nfollowing, while the second phase focuses on stimulating advanced reasoning. In\nthis second phase, a key innovation is our five-mode ``cold-start'' data\nmixture, which includes ``thinking'', ``non-thinking'', ``auto-think'', ``think\nwith image'', and high-quality video data. This mixture teaches the model to\ndecide when and how to reason. Subsequent reinforcement learning (RL) and\nalignment steps further enhance these reasoning capabilities and correct\nabnormal model behaviors, such as repetitive outputs. To validate our approach,\nwe conduct extensive evaluations, showing that Keye-VL achieves\nstate-of-the-art results on public video benchmarks and remains highly\ncompetitive on general image-based tasks (Figure 1). Furthermore, we develop\nand release the \\textbf{KC-MMBench}, a new benchmark tailored for real-world\nshort-video scenarios, where Keye-VL shows a significant advantage.", "AI": {"tldr": "Kwai Keye-VL\u662f\u4e00\u4e2a8B\u53c2\u6570\u7684\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u4e13\u6ce8\u4e8e\u77ed\u89c6\u9891\u7406\u89e3\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u65b9\u6cd5\u548c\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u5b9e\u73b0\u9886\u5148\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709MLLMs\u5728\u52a8\u6001\u3001\u4fe1\u606f\u5bc6\u96c6\u7684\u77ed\u89c6\u9891\u7406\u89e3\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u56db\u9636\u6bb5\u9884\u8bad\u7ec3\u548c\u4e24\u9636\u6bb5\u540e\u8bad\u7ec3\uff0c\u7ed3\u5408\u4e94\u79cd\u6a21\u5f0f\u7684\u6570\u636e\u6df7\u5408\u548c\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5728\u516c\u5171\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5e76\u5728\u901a\u7528\u56fe\u50cf\u4efb\u52a1\u4e2d\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "Kwai Keye-VL\u5728\u77ed\u89c6\u9891\u7406\u89e3\u65b9\u9762\u8868\u73b0\u5353\u8d8a\uff0c\u540c\u65f6\u5177\u5907\u5f3a\u5927\u7684\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u80fd\u529b\u3002"}}
{"id": "2507.01953", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01953", "abs": "https://arxiv.org/abs/2507.01953", "authors": ["Yukang Cao", "Chenyang Si", "Jinghao Wang", "Ziwei Liu"], "title": "FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model", "comment": "ICCV 2025. Project page: https://yukangcao.github.io/FreeMorph/", "summary": "We present FreeMorph, the first tuning-free method for image morphing that\naccommodates inputs with different semantics or layouts. Unlike existing\nmethods that rely on finetuning pre-trained diffusion models and are limited by\ntime constraints and semantic/layout discrepancies, FreeMorph delivers\nhigh-fidelity image morphing without requiring per-instance training. Despite\ntheir efficiency and potential, tuning-free methods face challenges in\nmaintaining high-quality results due to the non-linear nature of the multi-step\ndenoising process and biases inherited from the pre-trained diffusion model. In\nthis paper, we introduce FreeMorph to address these challenges by integrating\ntwo key innovations. 1) We first propose a guidance-aware spherical\ninterpolation design that incorporates explicit guidance from the input images\nby modifying the self-attention modules, thereby addressing identity loss and\nensuring directional transitions throughout the generated sequence. 2) We\nfurther introduce a step-oriented variation trend that blends self-attention\nmodules derived from each input image to achieve controlled and consistent\ntransitions that respect both inputs. Our extensive evaluations demonstrate\nthat FreeMorph outperforms existing methods, being 10x ~ 50x faster and\nestablishing a new state-of-the-art for image morphing.", "AI": {"tldr": "FreeMorph\u662f\u4e00\u79cd\u65e0\u9700\u8c03\u4f18\u7684\u56fe\u50cf\u53d8\u5f62\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u8bed\u4e49\u6216\u5e03\u5c40\u7684\u8f93\u5165\uff0c\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u5b9e\u73b0\u9ad8\u8d28\u91cf\u53d8\u5f62\uff0c\u901f\u5ea6\u63d0\u534710~50\u500d\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u5fae\u8c03\uff0c\u53d7\u9650\u4e8e\u65f6\u95f4\u548c\u8bed\u4e49/\u5e03\u5c40\u5dee\u5f02\uff0cFreeMorph\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "1\uff09\u63d0\u51fa\u6307\u5bfc\u611f\u77e5\u7684\u7403\u5f62\u63d2\u503c\u8bbe\u8ba1\uff0c\u4fee\u6539\u81ea\u6ce8\u610f\u529b\u6a21\u5757\uff1b2\uff09\u5f15\u5165\u6b65\u9aa4\u5bfc\u5411\u7684\u53d8\u5316\u8d8b\u52bf\uff0c\u6df7\u5408\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u3002", "result": "FreeMorph\u5728\u56fe\u50cf\u53d8\u5f62\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u901f\u5ea6\u63d0\u534710~50\u500d\uff0c\u8fbe\u5230\u65b0SOTA\u3002", "conclusion": "FreeMorph\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u53d8\u5f62\u3002"}}
