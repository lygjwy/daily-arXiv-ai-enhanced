<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 20]
- [cs.LG](#cs.LG) [Total: 20]
- [cs.CL](#cs.CL) [Total: 20]
- [cs.CV](#cs.CV) [Total: 23]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [MAIA: A Collaborative Medical AI Platform for Integrated Healthcare Innovation](https://arxiv.org/abs/2507.19489)
*Simone Bendazzoli,Sanna Persson,Mehdi Astaraki,Sebastian Pettersson,Vitali Grozman,Rodrigo Moreno*

Main category: cs.AI

TL;DR: MAIA是一个开源平台，旨在促进临床医生、研究人员和AI开发者之间的跨学科合作，加速AI研究向临床应用的转化。


<details>
  <summary>Details</summary>
Motivation: 解决AI技术在实际医疗应用中的落地问题，促进协作和互操作性。

Method: 基于Kubernetes构建，提供模块化、可扩展的环境，集成数据管理、模型开发、标注、部署和临床反馈工具。

Result: 在学术和临床环境中成功部署，支持医学影像AI的实际用例。

Conclusion: MAIA通过促进协作和互操作性，加速了AI研究向临床解决方案的转化，同时提升了可重复性、透明度和以用户为中心的设计。

Abstract: The integration of Artificial Intelligence (AI) into clinical workflows
requires robust collaborative platforms that are able to bridge the gap between
technical innovation and practical healthcare applications. This paper
introduces MAIA (Medical Artificial Intelligence Assistant), an open-source
platform designed to facilitate interdisciplinary collaboration among
clinicians, researchers, and AI developers. Built on Kubernetes, MAIA offers a
modular, scalable environment with integrated tools for data management, model
development, annotation, deployment, and clinical feedback. Key features
include project isolation, CI/CD automation, integration with high-computing
infrastructures and in clinical workflows. MAIA supports real-world use cases
in medical imaging AI, with deployments in both academic and clinical
environments. By promoting collaborations and interoperability, MAIA aims to
accelerate the translation of AI research into impactful clinical solutions
while promoting reproducibility, transparency, and user-centered design. We
showcase the use of MAIA with different projects, both at KTH Royal Institute
of Technology and Karolinska University Hospital.

</details>


### [2] [Agent WARPP: Workflow Adherence via Runtime Parallel Personalization](https://arxiv.org/abs/2507.19543)
*Maria Emilia Mazzolenis,Ruirui Zhang*

Main category: cs.AI

TL;DR: WARPP是一个无需训练、模块化的框架，通过多智能体编排和运行时个性化提升LLM在任务导向对话中的工作流依从性。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在长条件工作流中因外部工具调用和用户特定信息依赖而表现不佳的问题。

Method: 结合多智能体编排和运行时个性化，动态修剪条件分支，并行化架构中部署个性化代理和领域特定代理。

Result: 在五个复杂用户意图的测试中，WARPP优于非个性化方法和ReAct基线，提升参数保真度和工具准确性，同时减少令牌使用。

Conclusion: WARPP有效提升LLM在复杂工作流中的表现，无需额外训练。

Abstract: Large language models (LLMs) are increasingly applied in task-oriented
dialogue (TOD) systems but often struggle with long, conditional workflows that
involve external tool calls and depend on user-specific information. We present
Workflow Adherence via Runtime Parallel Personalization, or WARPP, a
training-free, modular framework that combines multi-agent orchestration with
runtime personalization to improve workflow adherence in LLM-based systems. By
dynamically pruning conditional branches based on user attributes, the
framework reduces reasoning overhead and narrows tool selection at runtime.
WARPP deploys a parallelized architecture where a dedicated Personalizer agent
operates alongside modular, domain-specific agents to dynamically tailor
execution paths in real time. The framework is evaluated across five
representative user intents of varying complexity within three domains:
banking, flights, and healthcare. Our evaluation leverages synthetic datasets
and LLM-powered simulated users to test scenarios with conditional
dependencies. Our results demonstrate that WARPP outperforms both the
non-personalized method and the ReAct baseline, achieving increasingly larger
gains in parameter fidelity and tool accuracy as intent complexity grows, while
also reducing average token usage, without any additional training.

</details>


### [3] [Hypergames: Modeling Misaligned Perceptions and Nested Beliefs for Multi-agent Systems](https://arxiv.org/abs/2507.19593)
*Vince Trencsenyi,Agnieszka Mensfelt,Kostas Stathis*

Main category: cs.AI

TL;DR: 本文综述了超博弈理论在动态多智能体系统中的应用，分析了44项研究，提出了智能体兼容性标准和分类框架，并指出了研究空白和未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统博弈理论假设理性、完全信息和共同知识，但现实多智能体系统常存在不确定性和认知差异。超博弈理论通过建模主观感知来弥补这些不足。

Method: 系统回顾了超博弈理论的应用，提出智能体兼容性标准和分类框架，分析研究趋势和挑战。

Result: 研究发现分层和图模型在欺骗推理中占主导，但HNF模型应用较少，且缺乏形式化语言。

Conclusion: 本文为超博弈理论在动态多智能体环境中的应用提供了新路线图，强调了未来研究方向。

Abstract: Classical game-theoretic models typically assume rational agents, complete
information, and common knowledge of payoffs - assumptions that are often
violated in real-world MAS characterized by uncertainty, misaligned
perceptions, and nested beliefs. To overcome these limitations, researchers
have proposed extensions that incorporate models of cognitive constraints,
subjective beliefs, and heterogeneous reasoning. Among these, hypergame theory
extends the classical paradigm by explicitly modeling agents' subjective
perceptions of the strategic scenario, known as perceptual games, in which
agents may hold divergent beliefs about the structure, payoffs, or available
actions. We present a systematic review of agent-compatible applications of
hypergame theory, examining how its descriptive capabilities have been adapted
to dynamic and interactive MAS contexts. We analyze 44 selected studies from
cybersecurity, robotics, social simulation, communications, and general
game-theoretic modeling. Building on a formal introduction to hypergame theory
and its two major extensions - hierarchical hypergames and HNF - we develop
agent-compatibility criteria and an agent-based classification framework to
assess integration patterns and practical applicability. Our analysis reveals
prevailing tendencies, including the prevalence of hierarchical and graph-based
models in deceptive reasoning and the simplification of extensive theoretical
frameworks in practical applications. We identify structural gaps, including
the limited adoption of HNF-based models, the lack of formal hypergame
languages, and unexplored opportunities for modeling human-agent and
agent-agent misalignment. By synthesizing trends, challenges, and open research
directions, this review provides a new roadmap for applying hypergame theory to
enhance the realism and effectiveness of strategic modeling in dynamic
multi-agent environments.

</details>


### [4] [DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference](https://arxiv.org/abs/2507.19608)
*Jiawen Qi,Chang Gao,Zhaochun Ren,Qinyu Chen*

Main category: cs.AI

TL;DR: DeltaLLM是一个无需训练的动态注意力剪枝框架，适用于资源受限的边缘设备，显著提高注意力稀疏性且保持准确性。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在边缘设备上部署时因序列长度增加而计算量激增的问题。

Method: 利用注意力模式的时间稀疏性，提出基于delta矩阵的构造策略和上下文感知的混合注意力机制。

Result: 在BitNet和Llama模型上，注意力稀疏性提升至60%，部分任务准确性略有提高。

Conclusion: DeltaLLM为边缘设备上的高效LLM推理提供了无需微调的解决方案。

Abstract: Deploying Large Language Models (LLMs) on edge devices remains challenging
due to their quadratically increasing computations with the sequence length.
Existing studies for dynamic attention pruning are designed for hardware with
massively parallel computation capabilities, such as GPUs or TPUs, and aim at
long context lengths (e.g., 64K), making them unsuitable for edge scenarios. We
present DeltaLLM, a training-free framework that exploits temporal sparsity in
attention patterns to enable efficient LLM inference across both the prefilling
and decoding stages, on resource-constrained edge devices. DeltaLLM introduces
an accuracy- and memory-aware delta matrix construction strategy that
introduces temporal sparsity, and a context-aware hybrid attention mechanism
that combines full attention in a local context window with delta approximation
outside it to increase accuracy. We evaluate our framework on the
edge-device-friendly BitNet-b1.58-2B-4T model and Llama3.2-1B-Instruct model
across diverse language tasks. The results show that on BitNet, our framework
increases the attention sparsity from 0% to 60% during the prefilling stage
with slight accuracy improvement on the WG task, and 0% to 57% across both the
prefilling and decoding stages, with even higher F1 score from 29.63 to 30.97
on SQuAD-v2 task. On the Llama model, it can also achieve up to 60% sparsity
during the prefilling stage and around 57% across both stages with negligible
accuracy drop. These results demonstrate that DeltaLLM offers a promising
solution for efficient edge deployment, requiring no fine-tuning and seamlessly
integrating with existing inference pipelines.

</details>


### [5] [Alignment and Safety in Large Language Models: Safety Mechanisms, Training Paradigms, and Emerging Challenges](https://arxiv.org/abs/2507.19672)
*Haoran Lu,Luyang Fang,Ruidong Zhang,Xinliang Li,Jiazhang Cai,Huimin Cheng,Lin Tang,Ziyu Liu,Zeliang Sun,Tao Wang,Yingchuan Zhang,Arif Hassan Zidan,Jinwen Xu,Jincheng Yu,Meizhi Yu,Hanqi Jiang,Xilin Gong,Weidi Luo,Bolun Sun,Yongkai Chen,Terry Ma,Shushan Wu,Yifan Zhou,Junhao Chen,Haotian Xiang,Jing Zhang,Afrar Jahin,Wei Ruan,Ke Deng,Yi Pan,Peilong Wang,Jiahui Li,Zhengliang Liu,Lu Zhang,Lin Zhao,Wei Liu,Dajiang Zhu,Xin Xing,Fei Dou,Wei Zhang,Chao Huang,Rongjie Liu,Mengrui Zhang,Yiwen Liu,Xiaoxiao Sun,Qin Lu,Zhen Xiang,Wenxuan Zhong,Tianming Liu,Ping Ma*

Main category: cs.AI

TL;DR: 这篇综述全面探讨了大语言模型（LLM）对齐人类价值观和意图的实践技术、训练协议和实证发现，分析了不同范式下的对齐方法，并总结了当前的最新技术和挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力的显著提升和社会影响的扩大，确保其与人类价值观和意图的对齐成为关键挑战。

Method: 综述分析了监督微调、基于偏好的方法等多种对齐范式，并讨论了包括DPO、Constitutional AI等在内的前沿技术。

Result: 研究发现，监督微调能实现基本指令跟随，而基于偏好的方法能更灵活地对齐复杂的人类意图。

Conclusion: 文章总结了当前实践中的策略，并提出了在监督、价值多元性、鲁棒性和持续对齐等方面的开放性问题。

Abstract: Due to the remarkable capabilities and growing impact of large language
models (LLMs), they have been deeply integrated into many aspects of society.
Thus, ensuring their alignment with human values and intentions has emerged as
a critical challenge. This survey provides a comprehensive overview of
practical alignment techniques, training protocols, and empirical findings in
LLM alignment. We analyze the development of alignment methods across diverse
paradigms, characterizing the fundamental trade-offs between core alignment
objectives. Our analysis shows that while supervised fine-tuning enables basic
instruction-following, preference-based methods offer more flexibility for
aligning with nuanced human intent. We discuss state-of-the-art techniques,
including Direct Preference Optimization (DPO), Constitutional AI,
brain-inspired methods, and alignment uncertainty quantification (AUQ),
highlighting their approaches to balancing quality and efficiency. We review
existing evaluation frameworks and benchmarking datasets, emphasizing
limitations such as reward misspecification, distributional robustness, and
scalable oversight. We summarize strategies adopted by leading AI labs to
illustrate the current state of practice. We conclude by outlining open
problems in oversight, value pluralism, robustness, and continuous alignment.
This survey aims to inform both researchers and practitioners navigating the
evolving landscape of LLM alignment.

</details>


### [6] [The wall confronting large language models](https://arxiv.org/abs/2507.19703)
*Peter V. Coveney,Sauro Succi*

Main category: cs.AI

TL;DR: 大型语言模型（LLM）的性能受限于其预测不确定性的提升能力，难以满足科学研究的可靠性标准。其学习机制可能导致错误累积和信息灾难，而数据规模的扩大加剧了虚假相关性。避免退化AI路径需更重视问题结构特性的理解。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM在提升预测可靠性方面的局限性，揭示其学习机制与准确性之间的冲突，并提出避免AI退化的可能途径。

Method: 通过分析LLM的缩放定律和学习机制，结合数据规模对虚假相关性的影响，论证其性能限制。

Result: LLM的预测不确定性难以改善，学习机制可能导致错误累积和信息灾难，数据规模扩大加剧问题。

Conclusion: 为避免AI退化，需更重视对问题结构特性的深入理解，而非单纯依赖数据规模和模型扩展。

Abstract: We show that the scaling laws which determine the performance of large
language models (LLMs) severely limit their ability to improve the uncertainty
of their predictions. As a result, raising their reliability to meet the
standards of scientific inquiry is intractable by any reasonable measure. We
argue that the very mechanism which fuels much of the learning power of LLMs,
namely the ability to generate non-Gaussian output distributions from Gaussian
input ones, might well be at the roots of their propensity to produce error
pileup, ensuing information catastrophes and degenerative AI behaviour. This
tension between learning and accuracy is a likely candidate mechanism
underlying the observed low values of the scaling components. It is
substantially compounded by the deluge of spurious correlations pointed out by
Calude and Longo which rapidly increase in any data set merely as a function of
its size, regardless of its nature. The fact that a degenerative AI pathway is
a very probable feature of the LLM landscape does not mean that it must
inevitably arise in all future AI research. Its avoidance, which we also
discuss in this paper, necessitates putting a much higher premium on insight
and understanding of the structural characteristics of the problems being
investigated.

</details>


### [7] [Minding Motivation: The Effect of Intrinsic Motivation on Agent Behaviors](https://arxiv.org/abs/2507.19725)
*Leonardo Villalobos-Arias,Grant Forbes,Jianxun Wang,David L Roberts,Arnav Jhala*

Main category: cs.AI

TL;DR: 论文研究了内在动机（IM）方法在强化学习（RL）中对游戏行为的影响，发现IM会导致奖励黑客行为，并提出广义奖励匹配（GRM）方法缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 游戏中的奖励稀疏性使得RL代理难以学习，IM方法虽能缓解这一问题，但可能导致奖励黑客行为，目前其影响尚不明确。

Method: 在MiniGrid环境中评估了三种IM技术的行为影响，并与GRM方法进行比较。

Result: IM显著改变了代理的行为，增加了初始奖励但也导致奖励黑客行为，GRM在部分场景中缓解了这一问题。

Conclusion: IM确实会改变RL代理的行为，GRM是一种潜在解决方案，但需进一步研究。

Abstract: Games are challenging for Reinforcement Learning~(RL) agents due to their
reward-sparsity, as rewards are only obtainable after long sequences of
deliberate actions. Intrinsic Motivation~(IM) methods -- which introduce
exploration rewards -- are an effective solution to reward-sparsity. However,
IM also causes an issue known as `reward hacking' where the agent optimizes for
the new reward at the expense of properly playing the game. The larger problem
is that reward hacking itself is largely unknown; there is no answer to
whether, and to what extent, IM rewards change the behavior of RL agents. This
study takes a first step by empirically evaluating the impact on behavior of
three IM techniques on the MiniGrid game-like environment. We compare these IM
models with Generalized Reward Matching~(GRM), a method that can be used with
any intrinsic reward function to guarantee optimality. Our results suggest that
IM causes noticeable change by increasing the initial rewards, but also
altering the way the agent plays; and that GRM mitigated reward hacking in some
scenarios.

</details>


### [8] [HypKG: Hypergraph-based Knowledge Graph Contextualization for Precision Healthcare](https://arxiv.org/abs/2507.19726)
*Yuzhang Xie,Xu Han,Ran Xu,Xiao Hu,Jiaying Lu,Carl Yang*

Main category: cs.AI

TL;DR: HypKG框架通过整合电子健康记录（EHRs）到知识图谱（KGs）中，生成上下文知识表示，提升医疗预测准确性。


<details>
  <summary>Details</summary>
Motivation: 通用知识图谱缺乏患者特定上下文信息，而电子健康记录提供了丰富的个人数据，两者结合可提升精准医疗的准确性。

Method: 使用实体链接技术连接通用KGs和EHRs，通过超图模型将知识“上下文化”，并利用超图变换器学习上下文化表示。

Result: 实验表明，HypKG在多个评估指标上显著提升了医疗预测任务的性能，并优化了知识图谱的实体和关系表示。

Conclusion: HypKG通过整合外部上下文，提升了知识图谱的实用性和质量，为精准医疗提供了有效支持。

Abstract: Knowledge graphs (KGs) are important products of the semantic web, which are
widely used in various application domains. Healthcare is one of such domains
where KGs are intensively used, due to the high requirement for knowledge
accuracy and interconnected nature of healthcare data. However, KGs storing
general factual information often lack the ability to account for important
contexts of the knowledge such as the status of specific patients, which are
crucial in precision healthcare. Meanwhile, electronic health records (EHRs)
provide rich personal data, including various diagnoses and medications, which
provide natural contexts for general KGs. In this paper, we propose HypKG, a
framework that integrates patient information from EHRs into KGs to generate
contextualized knowledge representations for accurate healthcare predictions.
Using advanced entity-linking techniques, we connect relevant knowledge from
general KGs with patient information from EHRs, and then utilize a hypergraph
model to "contextualize" the knowledge with the patient information. Finally,
we employ hypergraph transformers guided by downstream prediction tasks to
jointly learn proper contextualized representations for both KGs and patients,
fully leveraging existing knowledge in KGs and patient contexts in EHRs. In
experiments using a large biomedical KG and two real-world EHR datasets, HypKG
demonstrates significant improvements in healthcare prediction tasks across
multiple evaluation metrics. Additionally, by integrating external contexts,
HypKG can learn to adjust the representations of entities and relations in KG,
potentially improving the quality and real-world utility of knowledge.

</details>


### [9] [Integrating Activity Predictions in Knowledge Graphs](https://arxiv.org/abs/2507.19733)
*Alec Scully,Cameron Stockton,Forrest Hare*

Main category: cs.AI

TL;DR: 论文提出利用本体结构知识图谱预测未来事件，通过BFO和CCO组织数据并生成马尔可夫链模型，同时批判现有概率模型并提出替代方案。


<details>
  <summary>Details</summary>
Motivation: 探讨本体结构知识图谱在预测未来事件中的关键作用，并改进现有概率模型。

Method: 利用BFO和CCO构建知识图谱，组织数据并生成马尔可夫链模型，引入“时空实例”概念。

Result: 成功预测未来状态，并将概率计算无缝集成回知识图谱。

Conclusion: 本体结构知识图谱结合马尔可夫链模型能有效支持预测分析和决策。

Abstract: We argue that ontology-structured knowledge graphs can play a crucial role in
generating predictions about future events. By leveraging the semantic
framework provided by Basic Formal Ontology (BFO) and Common Core Ontologies
(CCO), we demonstrate how data such as the movements of a fishing vessel can be
organized in and retrieved from a knowledge graph. These query results are then
used to create Markov chain models, allowing us to predict future states based
on the vessel's history. To fully support this process, we introduce the term
`spatiotemporal instant' to complete the necessary structural semantics.
Additionally, we critique the prevailing ontological model of probability,
which conflates probability with likelihood and relies on the problematic
concept of modal measurements: measurements of future entities. We propose an
alternative view, where probabilities are treated as being about process
profiles, which better captures the dynamics of real world phenomena. Finally,
we demonstrate how our Markov chain based probability calculations can be
seamlessly integrated back into the knowledge graph, enabling further analysis
and decision-making. Keywords: predictive analytics, ontology, Markov chains,
probability, Basic Formal Ontology (BFO), knowledge graphs, SPARQL.

</details>


### [10] [Can LLMs Solve ASP Problems? Insights from a Benchmarking Study (Extended Version)](https://arxiv.org/abs/2507.19749)
*Lin Ren,Guohui Xiao,Guilin Qi,Yishuai Geng,Haohan Xue*

Main category: cs.AI

TL;DR: ASPBench是一个全面的ASP基准测试，揭示了当前大语言模型在ASP求解中的局限性，尤其是在核心任务上的表现不足。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型在ASP（答案集编程）中的评估过于简化，缺乏支持复杂逻辑结构和多答案集的基准测试。

Method: 引入ASPBench，包含三个ASP特定任务：ASP蕴含、答案集验证和答案集计算，并对14种先进大语言模型进行评估。

Result: 大语言模型在前两个简单任务上表现较好，但在核心的答案集计算任务上表现不佳。

Conclusion: 需要更有效整合符号推理能力的新方法，以提升大语言模型在ASP求解中的表现。

Abstract: Answer Set Programming (ASP) is a powerful paradigm for non-monotonic
reasoning. Recently, large language models (LLMs) have demonstrated promising
capabilities in logical reasoning. Despite this potential, current evaluations
of LLM capabilities in ASP are often limited. Existing works normally employ
overly simplified ASP programs, do not support negation, disjunction, or
multiple answer sets. Furthermore, there is a lack of benchmarks that introduce
tasks specifically designed for ASP solving. To bridge this gap, we introduce
ASPBench, a comprehensive ASP benchmark, including three ASP specific tasks:
ASP entailment, answer set verification, and answer set computation. Our
extensive evaluations on ASPBench reveal that while 14 state-of-the-art LLMs,
including \emph{deepseek-r1}, \emph{o4-mini}, and
\emph{gemini-2.5-flash-thinking}, perform relatively well on the first two
simpler tasks, they struggle with answer set computation, which is the core of
ASP solving. These findings offer insights into the current limitations of LLMs
in ASP solving. This highlights the need for new approaches that integrate
symbolic reasoning capabilities more effectively. The code and dataset are
available at https://github.com/HomuraT/ASPBench.

</details>


### [11] [Reinforcement Learning for Multi-Objective Multi-Echelon Supply Chain Optimisation](https://arxiv.org/abs/2507.19788)
*Rifny Rachman,Josh Tingey,Richard Allmendinger,Pradyumn Shukla,Wei Pan*

Main category: cs.AI

TL;DR: 本文提出了一种基于马尔可夫决策过程的多目标、多层次供应链优化模型，结合经济、环境和社会因素，并通过多目标强化学习方法进行评估。


<details>
  <summary>Details</summary>
Motivation: 解决非稳态市场中供应链的多目标优化问题，平衡经济、环境和社会效益。

Method: 采用多目标强化学习（RL）方法，与改进的单目标RL算法和多目标进化算法（MOEA）进行对比，通过可定制模拟器进行实验。

Result: 主方法在最优性、多样性和密度上表现最佳，复杂场景下超体积比MOEA高75%，解密度是改进单目标RL的11倍。

Conclusion: 该方法在复杂供应链中实现了稳定的生产和库存水平，同时最小化需求损失，提供了更优的多目标权衡。

Abstract: This study develops a generalised multi-objective, multi-echelon supply chain
optimisation model with non-stationary markets based on a Markov decision
process, incorporating economic, environmental, and social considerations. The
model is evaluated using a multi-objective reinforcement learning (RL) method,
benchmarked against an originally single-objective RL algorithm modified with
weighted sum using predefined weights, and a multi-objective evolutionary
algorithm (MOEA)-based approach. We conduct experiments on varying network
complexities, mimicking typical real-world challenges using a customisable
simulator. The model determines production and delivery quantities across
supply chain routes to achieve near-optimal trade-offs between competing
objectives, approximating Pareto front sets. The results demonstrate that the
primary approach provides the most balanced trade-off between optimality,
diversity, and density, further enhanced with a shared experience buffer that
allows knowledge transfer among policies. In complex settings, it achieves up
to 75\% higher hypervolume than the MOEA-based method and generates solutions
that are approximately eleven times denser, signifying better robustness, than
those produced by the modified single-objective RL method. Moreover, it ensures
stable production and inventory levels while minimising demand loss.

</details>


### [12] [Causality-aligned Prompt Learning via Diffusion-based Counterfactual Generation](https://arxiv.org/abs/2507.19882)
*Xinshu Li,Ruoyu Wang,Erdun Gao,Mingming Gong,Lina Yao*

Main category: cs.AI

TL;DR: DiCap模型通过扩散过程和反事实生成，解决了现有提示学习方法在因果不变性和泛化能力上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有提示学习方法因缺乏理论支持，难以生成因果不变的提示，导致泛化能力不足。

Method: DiCap利用扩散过程从因果模型的边际和条件分布中迭代采样梯度，生成满足最小充分性准则的反事实提示，并结合对比学习框架优化提示提取。

Result: 实验表明，DiCap在图像分类、图文检索和视觉问答等任务中表现优异，尤其在未见类别上优势明显。

Conclusion: DiCap通过理论驱动的反事实提示生成，显著提升了提示学习的因果不变性和泛化能力。

Abstract: Prompt learning has garnered attention for its efficiency over traditional
model training and fine-tuning. However, existing methods, constrained by
inadequate theoretical foundations, encounter difficulties in achieving
causally invariant prompts, ultimately falling short of capturing robust
features that generalize effectively across categories. To address these
challenges, we introduce the $\textit{\textbf{DiCap}}$ model, a theoretically
grounded $\textbf{Di}$ffusion-based $\textbf{C}$ounterf$\textbf{a}$ctual
$\textbf{p}$rompt learning framework, which leverages a diffusion process to
iteratively sample gradients from the marginal and conditional distributions of
the causal model, guiding the generation of counterfactuals that satisfy the
minimal sufficiency criterion. Grounded in rigorous theoretical derivations,
this approach guarantees the identifiability of counterfactual outcomes while
imposing strict bounds on estimation errors. We further employ a contrastive
learning framework that leverages the generated counterfactuals, thereby
enabling the refined extraction of prompts that are precisely aligned with the
causal features of the data. Extensive experimental results demonstrate that
our method performs excellently across tasks such as image classification,
image-text retrieval, and visual question answering, with particularly strong
advantages in unseen categories.

</details>


### [13] [What Does 'Human-Centred AI' Mean?](https://arxiv.org/abs/2507.19960)
*Olivia Guest*

Main category: cs.AI

TL;DR: 论文探讨了以人为中心的人工智能（AI）本质上是技术与人类认知的关系，分析了AI对人类认知劳动的替代、增强或取代，并强调忽视认知会导致AI设计的扭曲。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于澄清AI与人类认知的关系，避免因忽视认知而导致的AI设计问题，从而真正实现以人为中心的AI。

Method: 通过对比技术（如算盘、闹钟）与人类认知劳动（如心算、人工叫醒），提出新的定义和分析框架，将社会技术关系分为替代（有害）、增强（有益）和取代（中性）。

Result: 分析表明，所有AI都涉及人类认知，忽视这一点会扭曲AI设计，阻碍批判性思考，并限制真正以人为中心的AI工程。

Conclusion: 结论强调必须正视AI中的人类认知因素，才能真正去神秘化AI，实现以人为中心的设计。

Abstract: While it seems sensible that human-centred artificial intelligence (AI) means
centring "human behaviour and experience," it cannot be any other way. AI, I
argue, is usefully seen as a relationship between technology and humans where
it appears that artifacts can perform, to a greater or lesser extent, human
cognitive labour. This is evinced using examples that juxtapose technology with
cognition, inter alia: abacus versus mental arithmetic; alarm clock versus
knocker-upper; camera versus vision; and sweatshop versus tailor. Using novel
definitions and analyses, sociotechnical relationships can be analysed into
varying types of: displacement (harmful), enhancement (beneficial), and/or
replacement (neutral) of human cognitive labour. Ultimately, all AI implicates
human cognition; no matter what. Obfuscation of cognition in the AI context --
from clocks to artificial neural networks -- results in distortion, in slowing
critical engagement, perverting cognitive science, and indeed in limiting our
ability to truly centre humans and humanity in the engineering of AI systems.
To even begin to de-fetishise AI, we must look the human-in-the-loop in the
eyes.

</details>


### [14] [Leveraging Fine-Tuned Large Language Models for Interpretable Pancreatic Cystic Lesion Feature Extraction and Risk Categorization](https://arxiv.org/abs/2507.19973)
*Ebrahim Rasromani,Stella K. Kang,Yanqi Xu,Beisong Liu,Garvit Luhadia,Wan Fung Chui,Felicia L. Pasadyn,Yu Chih Hung,Julie Y. An,Edwin Mathieu,Zehui Gu,Carlos Fernandez-Granda,Ammar A. Javed,Greg D. Sacks,Tamas Gonda,Chenchan Huang,Yiqiu Shen*

Main category: cs.AI

TL;DR: 该论文提出了一种基于大型语言模型（LLMs）的方法，用于自动从MRI/CT报告中提取胰腺囊性病变（PCL）特征并分类风险，性能接近GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 手动提取PCL特征耗时且难以大规模研究，因此需要自动化工具。

Method: 使用GPT-4o生成的链式思维（CoT）数据微调开源LLMs（LLaMA和DeepSeek），并基于指南映射风险类别。

Result: 微调后模型在特征提取和风险分类上表现优异（准确率97%-98%，F1分数0.94-0.97），与放射科医生一致性高。

Conclusion: 微调的开源LLMs结合CoT监督可实现高效、准确的PCL研究，性能媲美GPT-4o。

Abstract: Background: Manual extraction of pancreatic cystic lesion (PCL) features from
radiology reports is labor-intensive, limiting large-scale studies needed to
advance PCL research. Purpose: To develop and evaluate large language models
(LLMs) that automatically extract PCL features from MRI/CT reports and assign
risk categories based on guidelines. Materials and Methods: We curated a
training dataset of 6,000 abdominal MRI/CT reports (2005-2024) from 5,134
patients that described PCLs. Labels were generated by GPT-4o using
chain-of-thought (CoT) prompting to extract PCL and main pancreatic duct
features. Two open-source LLMs were fine-tuned using QLoRA on GPT-4o-generated
CoT data. Features were mapped to risk categories per institutional guideline
based on the 2017 ACR White Paper. Evaluation was performed on 285 held-out
human-annotated reports. Model outputs for 100 cases were independently
reviewed by three radiologists. Feature extraction was evaluated using exact
match accuracy, risk categorization with macro-averaged F1 score, and
radiologist-model agreement with Fleiss' Kappa. Results: CoT fine-tuning
improved feature extraction accuracy for LLaMA (80% to 97%) and DeepSeek (79%
to 98%), matching GPT-4o (97%). Risk categorization F1 scores also improved
(LLaMA: 0.95; DeepSeek: 0.94), closely matching GPT-4o (0.97), with no
statistically significant differences. Radiologist inter-reader agreement was
high (Fleiss' Kappa = 0.888) and showed no statistically significant difference
with the addition of DeepSeek-FT-CoT (Fleiss' Kappa = 0.893) or GPT-CoT
(Fleiss' Kappa = 0.897), indicating that both models achieved agreement levels
on par with radiologists. Conclusion: Fine-tuned open-source LLMs with CoT
supervision enable accurate, interpretable, and efficient phenotyping for
large-scale PCL research, achieving performance comparable to GPT-4o.

</details>


### [15] [Digital Twin Channel-Enabled Online Resource Allocation for 6G: Principle, Architecture and Application](https://arxiv.org/abs/2507.19974)
*Tongjie Li,Jianhua Zhang,Li Yu,Yuxiang Zhang,Yunlong Cai,Fan Xu,Guangyi Liu*

Main category: cs.AI

TL;DR: 论文提出了一种基于数字孪生信道（DTC）的在线优化框架，用于6G网络中灵活、低延迟和可靠的资源分配。


<details>
  <summary>Details</summary>
Motivation: 6G网络中新兴应用（如全息通信、自动驾驶和工业物联网）对资源分配提出了严格要求，传统统计建模方法在动态环境中表现不佳，且实时信道状态信息（CSI）获取成本高。

Method: 利用DTC预测CSI，结合轻量级博弈论算法进行在线资源分配。

Result: 仿真结果显示，该方法比基于导频的理想CSI方案吞吐量提升高达11.5%。

Conclusion: 该方法为未来6G网络提供了可扩展、低开销和环境感知的通信解决方案。

Abstract: Emerging applications such as holographic communication, autonomous driving,
and the industrial Internet of Things impose stringent requirements on
flexible, low-latency, and reliable resource allocation in 6G networks.
Conventional methods, which rely on statistical modeling, have proven effective
in general contexts but may fail to achieve optimal performance in specific and
dynamic environments. Furthermore, acquiring real-time channel state
information (CSI) typically requires excessive pilot overhead. To address these
challenges, a digital twin channel (DTC)-enabled online optimization framework
is proposed, in which DTC is employed to predict CSI based on environmental
sensing. The predicted CSI is then utilized by lightweight game-theoretic
algorithms to perform online resource allocation in a timely and efficient
manner. Simulation results based on a digital replica of a realistic industrial
workshop demonstrate that the proposed method achieves throughput improvements
of up to 11.5\% compared with pilot-based ideal CSI schemes, validating its
effectiveness for scalable, low-overhead, and environment-aware communication
in future 6G networks.

</details>


### [16] [Matching Game Preferences Through Dialogical Large Language Models: A Perspective](https://arxiv.org/abs/2507.20000)
*Renaud Fabre,Daniel Egret,Patrice Bellot*

Main category: cs.AI

TL;DR: 论文探讨了如何结合大型语言模型（LLMs）和GRAPHYP网络系统，通过对话式智能提升对人类对话和偏好的理解，提出透明可追溯的AI框架D-LLM。


<details>
  <summary>Details</summary>
Motivation: 旨在通过透明化的AI推理过程，增强用户对AI决策的理解和信任，实现个性化偏好嵌入。

Method: 提出D-LLM框架，包含推理过程、偏好分类系统和对话方法，结合GRAPHYP网络分析用户偏好。

Result: 设想构建可解释的AI系统，用户可查看和理解AI决策过程，提升透明度和信任度。

Conclusion: 通过D-LLM框架，未来AI不仅能提供答案，还能展示推理过程，增强人机协作的透明性。

Abstract: This perspective paper explores the future potential of "conversational
intelligence" by examining how Large Language Models (LLMs) could be combined
with GRAPHYP's network system to better understand human conversations and
preferences. Using recent research and case studies, we propose a conceptual
framework that could make AI rea-soning transparent and traceable, allowing
humans to see and understand how AI reaches its conclusions. We present the
conceptual perspective of "Matching Game Preferences through Dialogical Large
Language Models (D-LLMs)," a proposed system that would allow multiple users to
share their different preferences through structured conversations. This
approach envisions personalizing LLMs by embedding individual user preferences
directly into how the model makes decisions. The proposed D-LLM framework would
require three main components: (1) reasoning processes that could analyze
different search experiences and guide performance, (2) classification systems
that would identify user preference patterns, and (3) dialogue approaches that
could help humans resolve conflicting information. This perspective framework
aims to create an interpretable AI system where users could examine,
understand, and combine the different human preferences that influence AI
responses, detected through GRAPHYP's search experience networks. The goal of
this perspective is to envision AI systems that would not only provide answers
but also show users how those answers were reached, making artificial
intelligence more transparent and trustworthy for human decision-making.

</details>


### [17] [Finding Personalized Good-Enough Solutions to Unsatisfiable Stable Roommates Problems](https://arxiv.org/abs/2507.20010)
*Müge Fidan,Esra Erdem*

Main category: cs.AI

TL;DR: 论文研究了稳定室友问题，提出了一种基于代理人偏好和朋友网络的个性化匹配方法，以解决无稳定解时的“足够好”匹配问题。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，稳定室友问题并不总有解，因此需要研究如何生成“足够好”的匹配。

Method: 结合代理人的习惯偏好和朋友网络，提出了一种生成个性化匹配的方法。

Result: 通过示例和实证评估验证了方法的有效性。

Conclusion: 该方法为解决无稳定解的室友匹配问题提供了实用方案。

Abstract: The Stable Roommates problems are characterized by the preferences of agents
over other agents as roommates. A solution is a partition of the agents into
pairs that are acceptable to each other (i.e., they are in the preference lists
of each other), and the matching is stable (i.e., there do not exist any two
agents who prefer each other to their roommates, and thus block the matching).
Motivated by real-world applications, and considering that stable roommates
problems do not always have solutions, we continue our studies to compute
"good-enough" matchings. In addition to the agents' habits and habitual
preferences, we consider their networks of preferred friends, and introduce a
method to generate personalized solutions to stable roommates problems. We
illustrate the usefulness of our method with examples and empirical
evaluations.

</details>


### [18] [PITA: Preference-Guided Inference-Time Alignment for LLM Post-Training](https://arxiv.org/abs/2507.20067)
*Sarat Chandra Bobbili,Ujwal Dinesha,Dheeraj Narasimha,Srinivas Shakkottai*

Main category: cs.AI

TL;DR: PITA框架通过直接整合偏好反馈到LLM的token生成中，无需预训练奖励模型，降低了计算成本并提高了对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预训练奖励模型，可能导致不稳定；PITA旨在消除这一依赖，直接利用偏好反馈。

Method: PITA通过学习小型偏好引导策略，在推理时修改token概率，无需微调LLM，采用随机搜索和迭代优化。

Result: 在数学推理和情感分类等任务中，PITA有效对齐LLM输出与用户偏好。

Conclusion: PITA提供了一种高效、低成本的方法，直接利用偏好反馈优化LLM输出。

Abstract: Inference-time alignment enables large language models (LLMs) to generate
outputs aligned with end-user preferences without further training. Recent
post-training methods achieve this by using small guidance models to modify
token generation during inference. These methods typically optimize a reward
function KL-regularized by the original LLM taken as the reference policy. A
critical limitation, however, is their dependence on a pre-trained reward
model, which requires fitting to human preference feedback--a potentially
unstable process. In contrast, we introduce PITA, a novel framework that
integrates preference feedback directly into the LLM's token generation,
eliminating the need for a reward model. PITA learns a small preference-based
guidance policy to modify token probabilities at inference time without LLM
fine-tuning, reducing computational cost and bypassing the pre-trained reward
model dependency. The problem is framed as identifying an underlying preference
distribution, solved through stochastic search and iterative refinement of the
preference-based guidance model. We evaluate PITA across diverse tasks,
including mathematical reasoning and sentiment classification, demonstrating
its effectiveness in aligning LLM outputs with user preferences.

</details>


### [19] [Concept Learning for Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2507.20143)
*Zhonghan Ge,Yuanyang Zhu,Chunlin Chen*

Main category: cs.AI

TL;DR: 论文提出了一种基于概念瓶颈模型的可解释价值分解框架CMQ，用于解决多智能体强化学习中的透明性和互操作性问题，并在性能和可解释性之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 尽管神经网络在多智能体强化学习中取得了进展，但其黑盒特性导致透明性和互操作性不足，隐含的合作机制尚未完全理解。

Method: 提出了CMQ方法，通过学习可解释的合作概念，将每个概念表示为监督向量，利用全局状态嵌入对个体动作价值进行条件化表示。

Result: 在StarCraft II和LBF任务中，CMQ优于现有方法，能够捕捉有意义的合作模式并支持概念干预。

Conclusion: CMQ在性能和可解释性方面取得了突破，为合作模式的检测和潜在偏差的识别提供了支持。

Abstract: Despite substantial progress in applying neural networks (NN) to multi-agent
reinforcement learning (MARL) areas, they still largely suffer from a lack of
transparency and interoperability. However, its implicit cooperative mechanism
is not yet fully understood due to black-box networks. In this work, we study
an interpretable value decomposition framework via concept bottleneck models,
which promote trustworthiness by conditioning credit assignment on an
intermediate level of human-like cooperation concepts. To address this problem,
we propose a novel value-based method, named Concepts learning for Multi-agent
Q-learning (CMQ), that goes beyond the current performance-vs-interpretability
trade-off by learning interpretable cooperation concepts. CMQ represents each
cooperation concept as a supervised vector, as opposed to existing models where
the information flowing through their end-to-end mechanism is concept-agnostic.
Intuitively, using individual action value conditioning on global state
embeddings to represent each concept allows for extra cooperation
representation capacity. Empirical evaluations on the StarCraft II
micromanagement challenge and level-based foraging (LBF) show that CMQ achieves
superior performance compared with the state-of-the-art counterparts. The
results also demonstrate that CMQ provides more cooperation concept
representation capturing meaningful cooperation modes, and supports test-time
concept interventions for detecting potential biases of cooperation mode and
identifying spurious artifacts that impact cooperation.

</details>


### [20] [The Policy Cliff: A Theoretical Analysis of Reward-Policy Maps in Large Language Models](https://arxiv.org/abs/2507.20150)
*Xingcheng Xu*

Main category: cs.AI

TL;DR: 本文提出了一个数学框架，用于分析强化学习（RL）中奖励函数到最优策略映射的稳定性，解释了策略脆弱性的根源，并探讨了熵正则化对稳定性的影响。


<details>
  <summary>Details</summary>
Motivation: 强化学习在大型语言和推理模型（LLMs/LRMs）中表现不稳定，导致虚假推理、欺骗性对齐和指令不服从等问题，缺乏统一的理论解释。

Method: 通过数学框架分析奖励函数到最优策略的映射稳定性，探讨非唯一最优动作的影响，并扩展到多奖励RL和熵正则化的作用。

Result: 理论分析揭示了策略脆弱性的根源，并通过实验验证了熵正则化能恢复稳定性但增加随机性。

Conclusion: 该框架为设计更安全、可信的AI系统提供了理论基础，将政策稳定性分析从经验启发提升到原则性理论。

Abstract: Reinforcement learning (RL) plays a crucial role in shaping the behavior of
large language and reasoning models (LLMs/LRMs). However, it often produces
brittle and unstable policies, leading to critical failures such as spurious
reasoning, deceptive alignment, and instruction disobedience that undermine the
trustworthiness and safety of LLMs/LRMs. Currently, these issues lack a unified
theoretical explanation and are typically addressed using ad-hoc heuristics.
This paper presents a rigorous mathematical framework for analyzing the
stability of the mapping from a reward function to the optimal policy. We show
that policy brittleness often stems from non-unique optimal actions, a common
occurrence when multiple valid traces exist in a reasoning task. This
theoretical lens provides a unified explanation for a range of seemingly
disparate failures, reframing them as rational outcomes of optimizing rewards
that may be incomplete or noisy, especially in the presence of action
degeneracy. We extend this analysis from the fundamental single-reward setting
to the more realistic multi-reward RL across diverse domains, showing how
stability is governed by an "effective reward" aggregation mechanism. We also
prove that entropy regularization restores policy stability at the cost of
increased stochasticity. Our framework provides a unified explanation for
recent empirical findings on deceptive reasoning, instruction-following
trade-offs, and RLHF-induced sophistry, and is further validated through
perturbation experiments in multi-reward RL. This work advances
policy-stability analysis from empirical heuristics towards a principled
theory, offering essential insights for designing safer and more trustworthy AI
systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [21] [Beyond 9-to-5: A Generative Model for Augmenting Mobility Data of Underrepresented Shift Workers](https://arxiv.org/abs/2507.19510)
*Haoxuan Ma,Xishun Liao,Yifan Liu,Chris Stanford,Jiaqi Ma*

Main category: cs.LG

TL;DR: 论文提出了一种基于Transformer的方法，利用GPS轨迹数据生成轮班工人的完整活动模式，填补了传统交通调查中对这一群体的忽视。


<details>
  <summary>Details</summary>
Motivation: 轮班工人占工业化社会劳动力的15-20%，但在传统交通调查和规划中代表性不足，导致对其出行需求的理解不充分。

Method: 采用基于Transformer的模型，结合周期性时间嵌入和过渡损失函数，从碎片化GPS数据生成完整的活动模式。

Result: 生成的模式与洛杉矶县的GPS数据高度一致（平均JSD < 0.02），验证了方法的有效性。

Conclusion: 该方法为交通规划者提供了强大的数据增强工具，有助于更全面地理解城市24/7的出行需求。

Abstract: This paper addresses a critical gap in urban mobility modeling by focusing on
shift workers, a population segment comprising 15-20% of the workforce in
industrialized societies yet systematically underrepresented in traditional
transportation surveys and planning. This underrepresentation is revealed in
this study by a comparative analysis of GPS and survey data, highlighting stark
differences between the bimodal temporal patterns of shift workers and the
conventional 9-to-5 schedules recorded in surveys. To address this bias, we
introduce a novel transformer-based approach that leverages fragmented GPS
trajectory data to generate complete, behaviorally valid activity patterns for
individuals working non-standard hours. Our method employs periodaware temporal
embeddings and a transition-focused loss function specifically designed to
capture the unique activity rhythms of shift workers and mitigate the inherent
biases in conventional transportation datasets. Evaluation shows that the
generated data achieves remarkable distributional alignment with GPS data from
Los Angeles County (Average JSD < 0.02 for all evaluation metrics). By
transforming incomplete GPS traces into complete, representative activity
patterns, our approach provides transportation planners with a powerful data
augmentation tool to fill critical gaps in understanding the 24/7 mobility
needs of urban populations, enabling precise and inclusive transportation
planning.

</details>


### [22] [Enhancing Spatiotemporal Networks with xLSTM: A Scalar LSTM Approach for Cellular Traffic Forecasting](https://arxiv.org/abs/2507.19513)
*Khalid Ali,Zineddine Bettouche,Andreas Kassler,Andreas Fischer*

Main category: cs.LG

TL;DR: 提出了一种轻量级的双路径时空网络，结合sLSTM和Conv3D模块，显著提升了交通预测的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 5G及未来网络中，精确的时空交通预测对资源管理至关重要，但传统AI方法难以捕捉复杂的时空模式。

Method: 采用双路径设计：sLSTM用于时间建模，三层Conv3D用于空间特征提取，通过融合层整合两者。

Result: 在真实数据集上表现优于ConvLSTM基线，MAE降低23%，泛化能力提升30%。

Conclusion: 该设计适合大规模下一代网络部署，具有梯度稳定性和快速收敛的优势。

Abstract: Accurate spatiotemporal traffic forecasting is vital for intelligent resource
management in 5G and beyond. However, conventional AI approaches often fail to
capture the intricate spatial and temporal patterns that exist, due to e.g.,
the mobility of users. We introduce a lightweight, dual-path Spatiotemporal
Network that leverages a Scalar LSTM (sLSTM) for efficient temporal modeling
and a three-layer Conv3D module for spatial feature extraction. A fusion layer
integrates both streams into a cohesive representation, enabling robust
forecasting. Our design improves gradient stability and convergence speed while
reducing prediction error. Evaluations on real-world datasets show superior
forecast performance over ConvLSTM baselines and strong generalization to
unseen regions, making it well-suited for large-scale, next-generation network
deployments. Experimental evaluation shows a 23% MAE reduction over ConvLSTM,
with a 30% improvement in model generalization.

</details>


### [23] [Wavelet Logic Machines: Learning and Reasoning in the Spectral Domain Without Neural Networks](https://arxiv.org/abs/2507.19514)
*Andrew Kiruluta*

Main category: cs.LG

TL;DR: 提出了一种完全基于小波域的学习框架，替代传统神经网络层，通过可学习的非线性变换和自适应小波基选择，实现了高效、紧凑的模型。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络（如Transformer）计算复杂度高且参数过多，希望设计一种更高效、可解释的替代方案。

Method: 在小波域中应用可学习的非线性变换（如软阈值和增益-相位调制），并支持自适应选择小波基（如Haar、Daubechies）。

Result: 在3D去噪和GLUE基准测试中，模型性能接近4层Transformer（89.3% vs 90.1%），但参数和内存分别减少72%和58%。

Conclusion: 光谱学习框架为视觉和语言任务提供了高效、可解释的替代方案，避免了过参数化架构。

Abstract: We introduce a fully spectral learning framework that eliminates traditional
neural layers by operating entirely in the wavelet domain. The model applies
learnable nonlinear transformations, including soft-thresholding and gain-phase
modulation, directly to wavelet coefficients. It also includes a differentiable
wavelet basis selection mechanism, enabling adaptive processing using families
such as Haar, Daubechies, and Biorthogonal wavelets.
  Implemented in PyTorch with full 3D support, the model maintains a spectral
pipeline without spatial convolutions or attention. On synthetic 3D denoising
and natural language tasks from the GLUE benchmark, including SST-2 sentiment
classification, the model achieves 89.3 percent accuracy, close to a 4-layer
Transformer baseline (90.1 percent), while using 72 percent fewer parameters
and 58 percent less peak memory. Faster early convergence is observed due to
spectral sparsity priors.
  In contrast to the quadratic complexity of self-attention and large matrix
multiplications in Transformers, our approach uses linear-time wavelet
transforms and pointwise nonlinearities, significantly reducing inference cost.
This yields a compact, interpretable, and efficient alternative to neural
models. Our results support the viability of principled spectral learning in
both vision and language tasks, offering new directions for model design
without overparameterized architectures.

</details>


### [24] [A Comparative Analysis of Traditional and Deep Learning Time Series Architectures for Influenza A Infectious Disease Forecasting](https://arxiv.org/abs/2507.19515)
*Edmund F. Agyemang,Hansapani Rodrigo,Vincent Agbenyeavu*

Main category: cs.LG

TL;DR: 该研究比较了传统模型（ARIMA和ETS）与六种深度学习模型（Simple RNN、LSTM、GRU、BiLSTM、BiGRU和Transformer）在预测甲型流感爆发中的表现，发现深度学习模型尤其是Transformer表现最优。


<details>
  <summary>Details</summary>
Motivation: 甲型流感每年导致大量死亡，研究旨在通过改进预测模型来提升公共卫生干预能力。

Method: 使用2009年至2023年的历史数据，对比传统模型和深度学习模型的性能。

Result: 所有深度学习模型均优于传统模型，其中Transformer表现最佳，测试MSE和MAE分别为0.0433和0.1126。

Conclusion: 深度学习模型能显著提升传染病预测能力，未来应探索如何将其整合到实时预测和监测系统中。

Abstract: Influenza A is responsible for 290,000 to 650,000 respiratory deaths a year,
though this estimate is an improvement from years past due to improvements in
sanitation, healthcare practices, and vaccination programs. In this study, we
perform a comparative analysis of traditional and deep learning models to
predict Influenza A outbreaks. Using historical data from January 2009 to
December 2023, we compared the performance of traditional ARIMA and Exponential
Smoothing(ETS) models with six distinct deep learning architectures: Simple
RNN, LSTM, GRU, BiLSTM, BiGRU, and Transformer. The results reveal a clear
superiority of all the deep learning models, especially the state-of-the-art
Transformer with respective average testing MSE and MAE of 0.0433 \pm 0.0020
and 0.1126 \pm 0.0016 for capturing the temporal complexities associated with
Influenza A data, outperforming well known traditional baseline ARIMA and ETS
models. These findings of this study provide evidence that state-of-the-art
deep learning architectures can enhance predictive modeling for infectious
diseases and indicate a more general trend toward using deep learning methods
to enhance public health forecasting and intervention planning strategies.
Future work should focus on how these models can be incorporated into real-time
forecasting and preparedness systems at an epidemic level, and integrated into
existing surveillance systems.

</details>


### [25] [BikeVAE-GNN: A Variational Autoencoder-Augmented Hybrid Graph Neural Network for Sparse Bicycle Volume Estimation](https://arxiv.org/abs/2507.19517)
*Mohit Gupta,Debjit Bhowmick,Ben Beck*

Main category: cs.LG

TL;DR: BikeVAE-GNN是一种结合变分自编码器（VAE）和图神经网络（GNN）的双任务框架，用于解决稀疏自行车网络的流量估计问题，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 城市自行车网络数据稀疏，传统方法难以准确估计自行车流量，需要一种新方法提升估计精度。

Method: 提出BikeVAE-GNN框架，结合Hybrid-GNN（GCN、GAT、GraphSAGE）和VAE，通过生成合成数据增强稀疏网络结构，同时进行回归和分类任务。

Result: 在墨尔本数据上，BikeVAE-GNN的MAE为30.82，准确率和F1-score均为0.99，优于基线模型。

Conclusion: BikeVAE-GNN为稀疏自行车网络流量估计提供了先进解决方案，对可持续交通规划有重要意义。

Abstract: Accurate link-level bicycle volume estimation is essential for informed urban
and transport planning but it is challenged by extremely sparse count data in
urban bicycling networks worldwide. We propose BikeVAE-GNN, a novel dual-task
framework augmenting a Hybrid Graph Neural Network (GNN) with Variational
Autoencoder (VAE) to estimate Average Daily Bicycle (ADB) counts, addressing
sparse bicycle networks. The Hybrid-GNN combines Graph Convolutional Networks
(GCN), Graph Attention Networks (GAT), and GraphSAGE to effectively model
intricate spatial relationships in sparse networks while VAE generates
synthetic nodes and edges to enrich the graph structure and enhance the
estimation performance. BikeVAE-GNN simultaneously performs - regression for
bicycling volume estimation and classification for bicycling traffic level
categorization. We demonstrate the effectiveness of BikeVAE-GNN using
OpenStreetMap data and publicly available bicycle count data within the City of
Melbourne - where only 141 of 15,933 road segments have labeled counts
(resulting in 99% count data sparsity). Our experiments show that BikeVAE-GNN
outperforms machine learning and baseline GNN models, achieving a mean absolute
error (MAE) of 30.82 bicycles per day, accuracy of 99% and F1-score of 0.99.
Ablation studies further validate the effective role of Hybrid-GNN and VAE
components. Our research advances bicycling volume estimation in sparse
networks using novel and state-of-the-art approaches, providing insights for
sustainable bicycling infrastructures.

</details>


### [26] [Target Circuit Matching in Large-Scale Netlists using GNN-Based Region Prediction](https://arxiv.org/abs/2507.19518)
*Sangwoo Seo,Jimin Seo,Yoonho Lee,Donghyeon Kim,Hyejin Shin,Banghyun Sung,Chanyoung Park*

Main category: cs.LG

TL;DR: 提出了一种基于图神经网络（GNN）的高效子图匹配方法，用于电子设计自动化（EDA）和电路验证。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的方法泛化能力有限，节点到节点匹配方法计算效率低，现有深度学习模型无法高效捕获全局子图嵌入或依赖低效匹配矩阵。

Method: 利用GNN预测目标电路的高概率区域，构建负样本以准确学习目标电路的存在，并直接从整个电路中提取子图嵌入以捕获全局信息。

Result: 实验表明，该方法在时间效率和目标区域预测上显著优于现有方法。

Conclusion: 该方法为大规模电路中的子图匹配提供了可扩展且高效的解决方案。

Abstract: Subgraph matching plays an important role in electronic design automation
(EDA) and circuit verification. Traditional rule-based methods have limitations
in generalizing to arbitrary target circuits. Furthermore, node-to-node
matching approaches tend to be computationally inefficient, particularly for
large-scale circuits. Deep learning methods have emerged as a potential
solution to address these challenges, but existing models fail to efficiently
capture global subgraph embeddings or rely on inefficient matching matrices,
which limits their effectiveness for large circuits. In this paper, we propose
an efficient graph matching approach that utilizes Graph Neural Networks (GNNs)
to predict regions of high probability for containing the target circuit.
Specifically, we construct various negative samples to enable GNNs to
accurately learn the presence of target circuits and develop an approach to
directly extracting subgraph embeddings from the entire circuit, which captures
global subgraph information and addresses the inefficiency of applying GNNs to
all candidate subgraphs. Extensive experiments demonstrate that our approach
significantly outperforms existing methods in terms of time efficiency and
target region prediction, offering a scalable and effective solution for
subgraph matching in large-scale circuits.

</details>


### [27] [Physics-informed transfer learning for SHM via feature selection](https://arxiv.org/abs/2507.19519)
*J. Poole,P. Gardner,A. J. Hughes,N. Dervilis,R. S. Mills,T. A. Dardeno,K. Worden*

Main category: cs.LG

TL;DR: 论文提出了一种基于物理知识的特征选择方法，利用模态保证准则（MAC）来量化健康结构模态之间的相似性，以解决结构健康监测（SHM）中数据稀缺和分布差异的问题。


<details>
  <summary>Details</summary>
Motivation: 结构健康监测（SHM）的训练数据昂贵且难以获取，尤其是标记数据。不同结构间的分布差异导致传统机器学习方法难以泛化。

Method: 利用转移学习（TL）跨域共享信息，提出基于物理知识的特征选择方法，使用MAC量化模态相似性，选择条件分布一致的跨域特征。

Result: MAC与监督度量高度一致，能有效选择条件分布不变的特征，验证了其在数值和实验案例中的有效性。

Conclusion: MAC可作为选择跨域一致性特征的度量，为SHM中的转移学习提供了一种有效方法。

Abstract: Data used for training structural health monitoring (SHM) systems are
expensive and often impractical to obtain, particularly labelled data.
Population-based SHM presents a potential solution to this issue by considering
the available data across a population of structures. However, differences
between structures will mean the training and testing distributions will
differ; thus, conventional machine learning methods cannot be expected to
generalise between structures. To address this issue, transfer learning (TL),
can be used to leverage information across related domains. An important
consideration is that the lack of labels in the target domain limits data-based
metrics to quantifying the discrepancy between the marginal distributions.
Thus, a prerequisite for the application of typical unsupervised TL methods is
to identify suitable source structures (domains), and a set of features, for
which the conditional distributions are related to the target structure.
Generally, the selection of domains and features is reliant on domain
expertise; however, for complex mechanisms, such as the influence of damage on
the dynamic response of a structure, this task is not trivial. In this paper,
knowledge of physics is leveraged to select more similar features, the modal
assurance criterion (MAC) is used to quantify the correspondence between the
modes of healthy structures. The MAC is shown to have high correspondence with
a supervised metric that measures joint-distribution similarity, which is the
primary indicator of whether a classifier will generalise between domains. The
MAC is proposed as a measure for selecting a set of features that behave
consistently across domains when subjected to damage, i.e. features with
invariance in the conditional distributions. This approach is demonstrated on
numerical and experimental case studies to verify its effectiveness in various
applications.

</details>


### [28] [Exoplanet Detection Using Machine Learning Models Trained on Synthetic Light Curves](https://arxiv.org/abs/2507.19520)
*Ethan Lo,Dan C. Lo*

Main category: cs.LG

TL;DR: 论文探讨了机器学习模型（如逻辑回归、K近邻和随机森林）在发现和验证系外行星中的应用，使用NASA开普勒太空望远镜的数据集，并通过数据增强技术提高了模型的召回率和精确度。


<details>
  <summary>Details</summary>
Motivation: 传统手动搜索系外行星效率低下，机器学习能高效处理大量数据，但现有模型依赖复杂算法和超级计算机。本研究旨在简化复杂性并验证多种ML模型的效果。

Method: 使用逻辑回归、K近邻和随机森林等ML模型，基于NASA开普勒太空望远镜的数据集进行训练和预测，并采用数据增强技术优化结果。

Result: 初步结果显示各模型表现良好，但存在潜在偏差和数据集不平衡问题。数据增强显著提高了召回率和精确度，但准确率因模型而异。

Conclusion: 在系外行星搜索中，数据增强技术能显著提升ML模型的召回率和精确度，但需进一步优化以解决准确率差异问题。

Abstract: With manual searching processes, the rate at which scientists and astronomers
discover exoplanets is slow because of inefficiencies that require an extensive
time of laborious inspections. In fact, as of now there have been about only
5,000 confirmed exoplanets since the late 1900s. Recently, machine learning
(ML) has proven to be extremely valuable and efficient in various fields,
capable of processing massive amounts of data in addition to increasing its
accuracy by learning. Though ML models for discovering exoplanets owned by
large corporations (e.g. NASA) exist already, they largely depend on complex
algorithms and supercomputers. In an effort to reduce such complexities, in
this paper, we report the results and potential benefits of various, well-known
ML models in the discovery and validation of extrasolar planets. The ML models
that are examined in this study include logistic regression, k-nearest
neighbors, and random forest. The dataset on which the models train and predict
is acquired from NASA's Kepler space telescope. The initial results show
promising scores for each model. However, potential biases and dataset
imbalances necessitate the use of data augmentation techniques to further
ensure fairer predictions and improved generalization. This study concludes
that, in the context of searching for exoplanets, data augmentation techniques
significantly improve the recall and precision, while the accuracy varies for
each model.

</details>


### [29] [Applications and Manipulations of Physics-Informed Neural Networks in Solving Differential Equations](https://arxiv.org/abs/2507.19522)
*Aarush Gupta,Kendric Hsu,Syna Mathod*

Main category: cs.LG

TL;DR: 论文介绍了物理信息神经网络（PINN）如何通过结合微分方程的先验知识，同时解决正向和逆向问题，并在稀疏数据下避免过拟合。


<details>
  <summary>Details</summary>
Motivation: 解决复杂微分方程的正向和逆向问题，同时利用先验知识提升模型在训练集外的性能。

Method: 通过将微分方程的残差嵌入损失函数，优化神经网络权重和模型参数，使用PyTorch实现。

Result: PINN能够有效解决线性、二次及热方程等复杂微分方程问题。

Conclusion: PINN是一种强大的工具，能够同时处理正向和逆向问题，并利用先验知识提升模型泛化能力。

Abstract: Mathematical models in neural networks are powerful tools for solving complex
differential equations and optimizing their parameters; that is, solving the
forward and inverse problems, respectively. A forward problem predicts the
output of a network for a given input by optimizing weights and biases. An
inverse problem finds equation parameters or coefficients that effectively
model the data. A Physics-Informed Neural Network (PINN) can solve both
problems. PINNs inject prior analytical information about the data into the
cost function to improve model performance outside the training set boundaries.
This also allows PINNs to efficiently solve problems with sparse data without
overfitting by extrapolating the model to fit larger trends in the data. The
prior information we implement is in the form of differential equations.
Residuals are the differences between the left-hand and right-hand sides of
corresponding differential equations; PINNs minimize these residuals to
effectively solve the differential equation and take advantage of prior
knowledge. In this way, the solution and parameters are embedded into the loss
function and optimized, allowing both the weights of the neural network and the
model parameters to be found simultaneously, solving both the forward and
inverse problems in the process. In this paper, we will create PINNs with
residuals of varying complexity, beginning with linear and quadratic models and
then expanding to fit models for the heat equation and other complex
differential equations. We will mainly use Python as the computing language,
using the PyTorch library to aid us in our research.

</details>


### [30] [Language Models for Controllable DNA Sequence Design](https://arxiv.org/abs/2507.19523)
*Xingyu Su,Xiner Li,Yuchao Lin,Ziqian Xie,Degui Zhi,Shuiwang Ji*

Main category: cs.LG

TL;DR: ATGC-Gen是一种基于Transformer的DNA序列生成模型，通过跨模态编码整合生物信号，支持自回归或掩码恢复目标，生成与特定生物属性对齐的序列。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型在DNA序列生成中的应用，解决现有方法在可控性和功能相关性上的不足。

Method: 采用解码器-编码器Transformer架构，结合跨模态编码，支持自回归或掩码恢复目标训练。

Result: 在启动子和增强子序列设计任务中表现优异，生成序列流畅、多样且生物相关。

Conclusion: ATGC-Gen展示了语言模型在可编程基因组设计中的潜力，代码已开源。

Abstract: We consider controllable DNA sequence design, where sequences are generated
by conditioning on specific biological properties. While language models (LMs)
such as GPT and BERT have achieved remarkable success in natural language
generation, their application to DNA sequence generation remains largely
underexplored. In this work, we introduce ATGC-Gen, an Automated Transformer
Generator for Controllable Generation, which leverages cross-modal encoding to
integrate diverse biological signals. ATGC-Gen is instantiated with both
decoder-only and encoder-only transformer architectures, allowing flexible
training and generation under either autoregressive or masked recovery
objectives. We evaluate ATGC-Gen on representative tasks including promoter and
enhancer sequence design, and further introduce a new dataset based on ChIP-Seq
experiments for modeling protein binding specificity. Our experiments
demonstrate that ATGC-Gen can generate fluent, diverse, and biologically
relevant sequences aligned with the desired properties. Compared to prior
methods, our model achieves notable improvements in controllability and
functional relevance, highlighting the potential of language models in
advancing programmable genomic design. The source code is released at
(https://github.com/divelab/AIRS/blob/main/OpenBio/ATGC_Gen).

</details>


### [31] [Kolmogorov Arnold Network Autoencoder in Medicine](https://arxiv.org/abs/2507.19524)
*Ugo Lomoio,Pierangelo Veltri,Pietro Hiram Guzzi*

Main category: cs.LG

TL;DR: 论文比较了传统自编码器（AE）与Kolmogorov-Arnold网络（KAN）变体在心脏信号处理任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索KAN架构在自编码任务中的潜力，尤其是在心脏信号处理领域。

Method: 方法包括对多种传统AE（线性、卷积、变分）与参数相当的KAN变体进行基准测试，任务包括重建、生成、去噪、修复和异常检测。

Result: 实验使用了心脏音频数据集，结果表明KAN变体在某些任务中表现更优。

Conclusion: 结论是KAN架构在心脏信号处理任务中具有潜力，尤其是在参数效率方面。

Abstract: Deep learning neural networks architectures such Multi Layer Perceptrons
(MLP) and Convolutional blocks still play a crucial role in nowadays research
advancements. From a topological point of view, these architecture may be
represented as graphs in which we learn the functions related to the nodes
while fixed edges convey the information from the input to the output. A recent
work introduced a new architecture called Kolmogorov Arnold Networks (KAN) that
reports how putting learnable activation functions on the edges of the neural
network leads to better performances in multiple scenarios. Multiple studies
are focusing on optimizing the KAN architecture by adding important features
such as dropout regularization, Autoencoders (AE), model benchmarking and last,
but not least, the KAN Convolutional Network (KCN) that introduced matrix
convolution with KANs learning. This study aims to benchmark multiple versions
of vanilla AEs (such as Linear, Convolutional and Variational) against their
Kolmogorov-Arnold counterparts that have same or less number of parameters.
Using cardiological signals as model input, a total of five different classic
AE tasks were studied: reconstruction, generation, denoising, inpainting and
anomaly detection. The proposed experiments uses a medical dataset
\textit{AbnormalHeartbeat} that contains audio signals obtained from the
stethoscope.

</details>


### [32] [MMCircuitEval: A Comprehensive Multimodal Circuit-Focused Benchmark for Evaluating LLMs](https://arxiv.org/abs/2507.19525)
*Chenchen Zhao,Zhengyuan Shi,Xiangyu Wen,Chengjie Liu,Yi Liu,Yunhao Zhou,Yuxiang Zhao,Hefei Feng,Yinan Zhu,Gwok-Waa Wan,Xin Cheng,Weiyu Chen,Yongqi Fu,Chujie Chen,Chenhao Xue,Guangyu Sun,Ying Wang,Yibo Lin,Jun Yang,Ning Xu,Xi Wang,Qiang Xu*

Main category: cs.LG

TL;DR: MMCircuitEval是首个针对EDA任务的多模态基准测试，旨在全面评估MLLM在电路设计中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试范围狭窄，难以全面评估MLLM在EDA任务中的表现，因此需要更全面的评估工具。

Method: 通过精心设计的3614个QA对，涵盖数字和模拟电路的关键EDA阶段，问题来源多样且经过专家审核。

Result: 评估显示现有LLM在后端设计和复杂计算方面表现不佳，需针对性改进。

Conclusion: MMCircuitEval为MLLM在EDA中的发展提供了基础资源，促进其在实际电路设计中的应用。

Abstract: The emergence of multimodal large language models (MLLMs) presents promising
opportunities for automation and enhancement in Electronic Design Automation
(EDA). However, comprehensively evaluating these models in circuit design
remains challenging due to the narrow scope of existing benchmarks. To bridge
this gap, we introduce MMCircuitEval, the first multimodal benchmark
specifically designed to assess MLLM performance comprehensively across diverse
EDA tasks. MMCircuitEval comprises 3614 meticulously curated question-answer
(QA) pairs spanning digital and analog circuits across critical EDA stages -
ranging from general knowledge and specifications to front-end and back-end
design. Derived from textbooks, technical question banks, datasheets, and
real-world documentation, each QA pair undergoes rigorous expert review for
accuracy and relevance. Our benchmark uniquely categorizes questions by design
stage, circuit type, tested abilities (knowledge, comprehension, reasoning,
computation), and difficulty level, enabling detailed analysis of model
capabilities and limitations. Extensive evaluations reveal significant
performance gaps among existing LLMs, particularly in back-end design and
complex computations, highlighting the critical need for targeted training
datasets and modeling approaches. MMCircuitEval provides a foundational
resource for advancing MLLMs in EDA, facilitating their integration into
real-world circuit design workflows. Our benchmark is available at
https://github.com/cure-lab/MMCircuitEval.

</details>


### [33] [Quantizing Text-attributed Graphs for Semantic-Structural Integration](https://arxiv.org/abs/2507.19526)
*Jianyuan Bo,Hao Wu,Yuan Fang*

Main category: cs.LG

TL;DR: STAG是一种自监督框架，通过量化图结构信息为离散令牌，解决了将结构信息嵌入LLM兼容格式的挑战，支持零样本迁移学习。


<details>
  <summary>Details</summary>
Motivation: 当前方法在将图结构信息嵌入LLM兼容格式时面临计算成本高或丢失关键细节的问题，且依赖源域标记数据，限制了适应性。

Method: STAG通过软分配和KL散度引导的量化，将图结构信息直接量化为离散令牌，无需自然标记化结构。

Result: 实验表明，STAG在多个节点分类基准上实现了最先进的性能，且兼容不同LLM架构。

Conclusion: STAG为图学习与LLM的结合提供了优雅解决方案，支持零样本迁移学习。

Abstract: Text-attributed graphs (TAGs) have emerged as a powerful representation for
modeling complex relationships across diverse domains. With the rise of large
language models (LLMs), there is growing interest in leveraging their
capabilities for graph learning. However, current approaches face significant
challenges in embedding structural information into LLM-compatible formats,
requiring either computationally expensive alignment mechanisms or manual graph
verbalization techniques that often lose critical structural details. Moreover,
these methods typically require labeled data from source domains for effective
transfer learning, significantly constraining their adaptability. We propose
STAG, a novel self-supervised framework that directly quantizes graph
structural information into discrete tokens using a frozen codebook. Unlike
traditional quantization approaches, our method employs soft assignment and KL
divergence guided quantization to address the unique challenges of graph data,
which lacks natural tokenization structures. Our framework enables both
LLM-based and traditional learning approaches, supporting true zero-shot
transfer learning without requiring labeled data even in the source domain.
Extensive experiments demonstrate state-of-the-art performance across multiple
node classification benchmarks while maintaining compatibility with different
LLM architectures, offering an elegant solution to bridging graph learning with
LLMs.

</details>


### [34] [Research on the application of graph data structure and graph neural network in node classification/clustering tasks](https://arxiv.org/abs/2507.19527)
*Yihan Wang,Jianing Zhao*

Main category: cs.LG

TL;DR: 论文研究了图数据结构、经典图算法和图神经网络（GNNs），并通过实验比较了传统算法与GNNs在节点分类和聚类任务中的性能差异。结果显示GNNs比传统方法准确率提升了43%至70%。


<details>
  <summary>Details</summary>
Motivation: 由于图数据的非欧几里得特性，传统机器学习方法难以处理，因此研究图数据结构和GNNs以提升性能。

Method: 通过理论分析和比较实验，评估传统算法与GNNs在节点分类和聚类任务中的表现。

Result: GNNs在准确率上比传统方法提升了43%至70%。

Conclusion: 研究为图表示学习提供了理论指导，并探索了经典算法与GNN架构的整合策略。

Abstract: Graph-structured data are pervasive across domains including social networks,
biological networks, and knowledge graphs. Due to their non-Euclidean nature,
such data pose significant challenges to conventional machine learning methods.
This study investigates graph data structures, classical graph algorithms, and
Graph Neural Networks (GNNs), providing comprehensive theoretical analysis and
comparative evaluation. Through comparative experiments, we quantitatively
assess performance differences between traditional algorithms and GNNs in node
classification and clustering tasks. Results show GNNs achieve substantial
accuracy improvements of 43% to 70% over traditional methods. We further
explore integration strategies between classical algorithms and GNN
architectures, providing theoretical guidance for advancing graph
representation learning research.

</details>


### [35] [Machine Learning Risk Intelligence for Green Hydrogen Investment: Insights for Duqm R3 Auction](https://arxiv.org/abs/2507.19529)
*Obumneme Nwafor,Mohammed Abdul Majeed Al Hooti*

Main category: cs.LG

TL;DR: 阿曼通过国家拍卖和国际合作推动绿氢发展，但缺乏大规模氢设施的历史数据。本文提出基于气象数据的AI决策支持系统，预测维护压力指数（MPI），以填补风险评估的知识空白。


<details>
  <summary>Details</summary>
Motivation: 全球绿氢投资增长，但沙漠环境下大规模氢设施的历史维护和性能数据稀缺，导致基础设施规划和拍卖决策的风险评估困难。

Method: 利用公开气象数据开发AI决策支持系统，生成维护压力指数（MPI），预测氢基础设施的维护需求和风险水平。

Result: 提出的MPI工具能够填补历史数据空白，为拍卖评估和运营决策提供时间基准和风险情报。

Conclusion: AI支持的MPI系统可增强监管预见性，优化绿氢基础设施的规划和运营决策。

Abstract: As green hydrogen emerges as a major component of global decarbonisation,
Oman has positioned itself strategically through national auctions and
international partnerships. Following two successful green hydrogen project
rounds, the country launched its third auction (R3) in the Duqm region. While
this area exhibits relative geospatial homogeneity, it is still vulnerable to
environmental fluctuations that pose inherent risks to productivity. Despite
growing global investment in green hydrogen, operational data remains scarce,
with major projects like Saudi Arabia's NEOM facility not expected to commence
production until 2026, and Oman's ACME Duqm project scheduled for 2028. This
absence of historical maintenance and performance data from large-scale
hydrogen facilities in desert environments creates a major knowledge gap for
accurate risk assessment for infrastructure planning and auction decisions.
Given this data void, environmental conditions emerge as accessible and
reliable proxy for predicting infrastructure maintenance pressures, because
harsh desert conditions such as dust storms, extreme temperatures, and humidity
fluctuations are well-documented drivers of equipment degradation in renewable
energy systems. To address this challenge, this paper proposes an Artificial
Intelligence decision support system that leverages publicly available
meteorological data to develop a predictive Maintenance Pressure Index (MPI),
which predicts risk levels and future maintenance demands on hydrogen
infrastructure. This tool strengthens regulatory foresight and operational
decision-making by enabling temporal benchmarking to assess and validate
performance claims over time. It can be used to incorporate temporal risk
intelligence into auction evaluation criteria despite the absence of historical
operational benchmarks.

</details>


### [36] [Clinical-Grade Blood Pressure Prediction in ICU Settings: An Ensemble Framework with Uncertainty Quantification and Cross-Institutional Validation](https://arxiv.org/abs/2507.19530)
*Md Basit Azam,Sarangthem Ibotombi Singh*

Main category: cs.LG

TL;DR: 该研究提出了一种基于电子健康记录（EHR）的血压预测框架，解决了现有机器学习方法在外部验证、不确定性量化和数据泄漏预防方面的不足。


<details>
  <summary>Details</summary>
Motivation: 重症监护病房（ICU）中血压监测至关重要，但现有方法存在外部验证不足、缺乏不确定性量化和数据泄漏问题。

Method: 采用集成框架（Gradient Boosting、Random Forest、XGBoost）结合74个生理特征，通过分位数回归量化不确定性，并防止数据泄漏。

Result: 内部验证表现良好（SBP: R²=0.86, RMSE=6.03 mmHg; DBP: R²=0.49, RMSE=7.13 mmHg），外部验证性能下降30%。不确定性量化生成有效预测区间。

Conclusion: 该框架为跨机构AI辅助血压监测提供了实用解决方案，代码已开源。

Abstract: Blood pressure (BP) monitoring is critical in in tensive care units (ICUs)
where hemodynamic instability can
  rapidly progress to cardiovascular collapse. Current machine
  learning (ML) approaches suffer from three limitations: lack of
  external validation, absence of uncertainty quantification, and
  inadequate data leakage prevention. This study presents the
  first comprehensive framework with novel algorithmic leakage
  prevention, uncertainty quantification, and cross-institutional
  validation for electronic health records (EHRs) based BP pre dictions. Our
methodology implemented systematic data leakage
  prevention, uncertainty quantification through quantile regres sion, and
external validation between the MIMIC-III and eICU
  databases. An ensemble framework combines Gradient Boosting,
  Random Forest, and XGBoost with 74 features across five
  physiological domains. Internal validation achieved a clinically
  acceptable performance (for SBP: R^2 = 0.86, RMSE = 6.03
  mmHg; DBP: R^2 = 0.49, RMSE = 7.13 mmHg), meeting AAMI
  standards. External validation showed 30% degradation with
  critical limitations in patients with hypotensive. Uncertainty
  quantification generated valid prediction intervals (80.3% SBP
  and 79.9% DBP coverage), enabling risk-stratified protocols
  with narrow intervals (< 15 mmHg) for standard monitoring
  and wide intervals (> 30 mmHg) for manual verification. This
  framework provides realistic deployment expectations for cross institutional
AI-assisted BP monitoring in critical care settings.
  The source code is publicly available at https://github.com/
  mdbasit897/clinical-bp-prediction-ehr.

</details>


### [37] [FedDPG: An Adaptive Yet Efficient Prompt-tuning Approach in Federated Learning Settings](https://arxiv.org/abs/2507.19534)
*Ali Shakeri,Wei Emma Zhang,Amin Beheshti,Weitong Chen,Jian Yang,Lishan Yang*

Main category: cs.LG

TL;DR: FedDPG是一种结合动态提示生成和联邦学习的方法，旨在提高模型灵活性和数据隐私保护，同时减少计算和通信开销。


<details>
  <summary>Details</summary>
Motivation: 传统提示调优方法缺乏灵活性，而联邦学习在数据隐私保护方面存在计算和通信限制。

Method: 提出FedDPG，通过动态提示生成网络生成上下文感知提示，结合联邦学习框架。

Result: 在三个NLP基准数据集上，FedDPG在全局模型性能上优于现有方法，并显著减少了计算时间和通信参数。

Conclusion: FedDPG在保持数据隐私的同时，提高了模型性能和效率，为联邦学习中的NLP任务提供了有效解决方案。

Abstract: Pre-trained Language Models (PLMs) have demonstrated impressive performance
in various NLP tasks. However, traditional fine-tuning methods for leveraging
PLMs for downstream tasks entail significant computational overhead.
Prompt-tuning has emerged as an efficient alternative that involves prepending
a limited number of parameters to the input sequence and only updating them
while the PLM's parameters are frozen. However, this technique's prompts remain
fixed for all inputs, reducing the model's flexibility. The Federated Learning
(FL) technique has gained attention in recent years to address the growing
concerns around data privacy. However, challenges such as communication and
computation limitations of clients still need to be addressed. To mitigate
these challenges, this paper introduces the Federated Dynamic Prompt Generator
(FedDPG), which incorporates a dynamic prompt generator network to generate
context-aware prompts based on the given input, ensuring flexibility and
adaptability while prioritising data privacy in federated learning settings.
Our experiments on three NLP benchmark datasets showcase that FedDPG
outperforms the state-of-the-art parameter-efficient fine-tuning methods in
terms of global model performance, and has significantly reduced the
calculation time and the number of parameters to be sent through the FL
network.

</details>


### [38] [Graph Learning Metallic Glass Discovery from Wikipedia](https://arxiv.org/abs/2507.19536)
*K. -C. Ouyang,S. -Y. Zhang,S. -L. Liu,J. Tian,Y. -H. Li,H. Tong,H. -Y. Bai,W. -H. Wang,Y. -C. Hu*

Main category: cs.LG

TL;DR: 提出了一种基于材料网络表示和语言模型的数据驱动方法，用于高效合成新材料，特别是金属玻璃。


<details>
  <summary>Details</summary>
Motivation: 传统材料合成方法效率低且成本高，数据稀缺和材料编码不成熟限制了机器学习在材料设计中的应用。

Method: 利用语言模型编码维基百科中的元素节点，设计图神经网络作为推荐系统，探索材料间的隐藏关系。

Result: 通过多语言维基百科嵌入评估自然语言在材料设计中的能力，为新材料发现提供了新范式。

Conclusion: 该方法为人工智能在非晶材料及其他领域的应用开辟了新途径。

Abstract: Synthesizing new materials efficiently is highly demanded in various research
fields. However, this process is usually slow and expensive, especially for
metallic glasses, whose formation strongly depends on the optimal combinations
of multiple elements to resist crystallization. This constraint renders only
several thousands of candidates explored in the vast material space since 1960.
Recently, data-driven approaches armed by advanced machine learning techniques
provided alternative routes for intelligent materials design. Due to data
scarcity and immature material encoding, the conventional tabular data is
usually mined by statistical learning algorithms, giving limited model
predictability and generalizability. Here, we propose sophisticated data
learning from material network representations. The node elements are encoded
from the Wikipedia by a language model. Graph neural networks with versatile
architectures are designed to serve as recommendation systems to explore hidden
relationships among materials. By employing Wikipedia embeddings from different
languages, we assess the capability of natural languages in materials design.
Our study proposes a new paradigm to harvesting new amorphous materials and
beyond with artificial intelligence.

</details>


### [39] [Swift-Sarsa: Fast and Robust Linear Control](https://arxiv.org/abs/2507.19539)
*Khurram Javed,Richard S. Sutton*

Main category: cs.LG

TL;DR: Javed等人（2024）提出了一种新的TD学习算法SwiftTD，并将其扩展到控制问题，开发了Swift-Sarsa算法。该算法在操作条件基准测试中表现出色，能够有效区分相关信号与噪声。


<details>
  <summary>Details</summary>
Motivation: 改进现有的TD学习算法，使其在控制问题中表现更优，并解决信号噪声问题。

Method: 结合SwiftTD和True Online Sarsa(λ)开发Swift-Sarsa算法，应用于操作条件基准测试。

Result: Swift-Sarsa能有效区分相关信号与噪声，无需先验知识。

Conclusion: Swift-Sarsa为大规模特征搜索提供了新思路，性能不受噪声特征影响。

Abstract: Javed, Sharifnassab, and Sutton (2024) introduced a new algorithm for TD
learning -- SwiftTD -- that augments True Online TD($\lambda$) with step-size
optimization, a bound on the effective learning rate, and step-size decay. In
their experiments SwiftTD outperformed True Online TD($\lambda$) and
TD($\lambda$) on a variety of prediction tasks derived from Atari games, and
its performance was robust to the choice of hyper-parameters. In this extended
abstract we extend SwiftTD to work for control problems. We combine the key
ideas behind SwiftTD with True Online Sarsa($\lambda$) to develop an on-policy
reinforcement learning algorithm called $\textit{Swift-Sarsa}$.
  We propose a simple benchmark for linear on-policy control called the
$\textit{operant conditioning benchmark}$. The key challenge in the operant
conditioning benchmark is that a very small subset of input signals are
relevant for decision making. The majority of the signals are noise sampled
from a non-stationary distribution. To learn effectively, the agent must learn
to differentiate between the relevant signals and the noisy signals, and
minimize prediction errors by assigning credit to the weight parameters
associated with the relevant signals.
  Swift-Sarsa, when applied to the operant conditioning benchmark, learned to
assign credit to the relevant signals without any prior knowledge of the
structure of the problem. It opens the door for solution methods that learn
representations by searching over hundreds of millions of features in parallel
without performance degradation due to noisy or bad features.

</details>


### [40] [Latent Representations of Intracardiac Electrograms for Atrial Fibrillation Driver Detection](https://arxiv.org/abs/2507.19547)
*Pablo Peiro-Corbacho,Long Lin,Pablo Ávila,Alejandro Carta-Bergaz,Ángel Arenal,Carlos Sevilla-Salcedo,Gonzalo R. Ríos-Muñoz*

Main category: cs.LG

TL;DR: 提出一种基于卷积自编码器的深度学习框架，用于从心房颤动（AF）的心内电图中无监督提取特征，以检测AF驱动源。


<details>
  <summary>Details</summary>
Motivation: 当前消融疗法对持续性AF效果不佳，因非肺静脉驱动源的存在。需自动化分析心内电图以识别AF驱动源。

Method: 使用卷积自编码器从单极和双极心内电图中无监督提取特征，并利用下游分类器检测旋转和局灶活动。

Result: 模型在检测旋转和局灶活动时表现中等（AUC 0.73-0.76），识别心房电图纠缠时表现优异（AUC 0.93）。

Conclusion: 该方法可实时运行并整合到临床系统中，展示了无监督学习在提取生理特征中的潜力。

Abstract: Atrial Fibrillation (AF) is the most prevalent sustained arrhythmia, yet
current ablation therapies, including pulmonary vein isolation, are frequently
ineffective in persistent AF due to the involvement of non-pulmonary vein
drivers. This study proposes a deep learning framework using convolutional
autoencoders for unsupervised feature extraction from unipolar and bipolar
intracavitary electrograms (EGMs) recorded during AF in ablation studies. These
latent representations of atrial electrical activity enable the
characterization and automation of EGM analysis, facilitating the detection of
AF drivers.
  The database consisted of 11,404 acquisitions recorded from 291 patients,
containing 228,080 unipolar EGMs and 171,060 bipolar EGMs. The autoencoders
successfully learned latent representations with low reconstruction loss,
preserving the morphological features. The extracted embeddings allowed
downstream classifiers to detect rotational and focal activity with moderate
performance (AUC 0.73-0.76) and achieved high discriminative performance in
identifying atrial EGM entanglement (AUC 0.93).
  The proposed method can operate in real-time and enables integration into
clinical electroanatomical mapping systems to assist in identifying
arrhythmogenic regions during ablation procedures. This work highlights the
potential of unsupervised learning to uncover physiologically meaningful
features from intracardiac signals.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [41] [Advancing Mental Disorder Detection: A Comparative Evaluation of Transformer and LSTM Architectures on Social Media](https://arxiv.org/abs/2507.19511)
*Khalid Hasan,Jamil Saquer,Mukulika Ghosh*

Main category: cs.CL

TL;DR: 论文评估了多种Transformer模型在心理健康障碍分类中的表现，发现RoBERTa性能最佳，同时LSTM结合BERT嵌入也表现优异。


<details>
  <summary>Details</summary>
Motivation: 心理健康障碍日益普遍，需要开发自动化工具进行早期检测和监测。

Method: 比较了BERT、RoBERTa等Transformer模型与LSTM方法，使用Reddit数据构建标注数据集。

Result: RoBERTa在测试集上F1分数达99.54%，LSTM结合BERT嵌入也表现优异。

Conclusion: Transformer模型在实时、可扩展的心理健康监测中效果显著，适合临床应用。

Abstract: The rising prevalence of mental health disorders necessitates the development
of robust, automated tools for early detection and monitoring. Recent advances
in Natural Language Processing (NLP), particularly transformer-based
architectures, have demonstrated significant potential in text analysis. This
study provides a comprehensive evaluation of state-of-the-art transformer
models (BERT, RoBERTa, DistilBERT, ALBERT, and ELECTRA) against Long Short-Term
Memory (LSTM) based approaches using different text embedding techniques for
mental health disorder classification on Reddit. We construct a large annotated
dataset, validating its reliability through statistical judgmental analysis and
topic modeling. Experimental results demonstrate the superior performance of
transformer models over traditional deep-learning approaches. RoBERTa achieved
the highest classification performance, with a 99.54% F1 score on the hold-out
test set and a 96.05% F1 score on the external test set. Notably, LSTM models
augmented with BERT embeddings proved highly competitive, achieving F1 scores
exceeding 94% on the external dataset while requiring significantly fewer
computational resources. These findings highlight the effectiveness of
transformer-based models for real-time, scalable mental health monitoring. We
discuss the implications for clinical applications and digital mental health
interventions, offering insights into the capabilities and limitations of
state-of-the-art NLP methodologies in mental disorder detection.

</details>


### [42] [Setting The Table with Intent: Intent-aware Schema Generation and Editing for Literature Review Tables](https://arxiv.org/abs/2507.19521)
*Vishakh Padmakumar,Joseph Chee Chang,Kyle Lo,Doug Downey,Aakanksha Naik*

Main category: cs.CL

TL;DR: 论文提出了一种利用大型语言模型（LLMs）生成文档比较框架的方法，解决了现有评估模糊和缺乏编辑方法的问题。通过合成意图增强数据集，并展示了编辑技术对生成框架的改进效果。


<details>
  <summary>Details</summary>
Motivation: 学术文献数量激增，需要有效组织、比较和对比文档。现有方法在生成比较框架时存在评估模糊和缺乏编辑方法的问题。

Method: 1. 合成意图增强未标注表格数据集，用于研究基于信息需求的框架生成；2. 提出多种基于LLM的框架编辑技术，并对比单次生成方法的性能。

Result: 合成意图显著提升了基准性能；小型开放权重模型通过微调可与先进提示LLMs竞争；编辑技术进一步改进了生成的框架。

Conclusion: 论文解决了框架生成中的关键问题，展示了合成意图和编辑技术的有效性，为文档比较提供了实用工具。

Abstract: The increasing volume of academic literature makes it essential for
researchers to organize, compare, and contrast collections of documents. Large
language models (LLMs) can support this process by generating schemas defining
shared aspects along which to compare papers. However, progress on schema
generation has been slow due to: (i) ambiguity in reference-based evaluations,
and (ii) lack of editing/refinement methods. Our work is the first to address
both issues. First, we present an approach for augmenting unannotated table
corpora with synthesized intents and apply it to create a dataset for studying
schema generation conditioned on a given information need, thus reducing
ambiguity. With this dataset, we show how incorporating table intents
significantly improves baseline performance in reconstructing reference
schemas. Next, we propose several LLM-based schema editing techniques. We start
by comprehensively benchmarking several single-shot schema generation methods,
including prompted LLM workflows and fine-tuned models, showing that smaller,
open-weight models can be fine-tuned to be competitive with state-of-the-art
prompted LLMs. Then we demonstrate that our editing techniques can further
improve schemas generated by these methods.

</details>


### [43] [Mind the Language Gap in Digital Humanities: LLM-Aided Translation of SKOS Thesauri](https://arxiv.org/abs/2507.19537)
*Felix Kraus,Nicolas Blumenröhr,Danah Tonne,Achim Streit*

Main category: cs.CL

TL;DR: WOKIE是一个开源、模块化且即用的SKOS词表自动翻译工具，旨在解决数字人文学科中语言多样性导致的资源访问和语义互操作性问题。


<details>
  <summary>Details</summary>
Motivation: 数字人文学科中语言多样性限制了知识资源的访问、重用和语义互操作性，WOKIE旨在解决这一问题。

Method: 结合外部翻译服务和大型语言模型（LLMs）进行针对性优化，平衡翻译质量、扩展性和成本。

Result: 在15种语言的多个词表上测试显示，WOKIE能提升词表的可访问性、重用性和跨语言互操作性。

Conclusion: WOKIE通过无障碍自动翻译和改进的ontology匹配性能，支持更包容和多语言的研究基础设施。

Abstract: We introduce WOKIE, an open-source, modular, and ready-to-use pipeline for
the automated translation of SKOS thesauri. This work addresses a critical need
in the Digital Humanities (DH), where language diversity can limit access,
reuse, and semantic interoperability of knowledge resources. WOKIE combines
external translation services with targeted refinement using Large Language
Models (LLMs), balancing translation quality, scalability, and cost. Designed
to run on everyday hardware and be easily extended, the application requires no
prior expertise in machine translation or LLMs. We evaluate WOKIE across
several DH thesauri in 15 languages with different parameters, translation
services and LLMs, systematically analysing translation quality, performance,
and ontology matching improvements. Our results show that WOKIE is suitable to
enhance the accessibility, reuse, and cross-lingual interoperability of
thesauri by hurdle-free automated translation and improved ontology matching
performance, supporting more inclusive and multilingual research
infrastructures.

</details>


### [44] [Mitigating Geospatial Knowledge Hallucination in Large Language Models: Benchmarking and Dynamic Factuality Aligning](https://arxiv.org/abs/2507.19586)
*Shengyuan Wang,Jie Feng,Tianhui Liu,Dan Pei,Yong Li*

Main category: cs.CL

TL;DR: 论文提出了一种评估和减少大语言模型（LLMs）地理空间幻觉的框架，通过知识图谱评估和动态事实对齐方法（KTO）提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: LLMs在地理空间任务中表现出知识不准确的问题（地理空间幻觉），但相关研究较少，亟需系统评估和解决方法。

Method: 利用结构化地理空间知识图谱进行评估，并提出基于Kahneman-Tversky优化（KTO）的动态事实对齐方法。

Result: 在20个先进LLMs上评估，发现地理空间幻觉问题，并通过KTO方法将性能提升29.6%。

Conclusion: 提出的框架和方法有效提升了LLMs在地理空间知识和推理任务中的可信度。

Abstract: Large language models (LLMs) possess extensive world knowledge, including
geospatial knowledge, which has been successfully applied to various geospatial
tasks such as mobility prediction and social indicator prediction. However,
LLMs often generate inaccurate geospatial knowledge, leading to geospatial
hallucinations (incorrect or inconsistent representations of geospatial
information) that compromise their reliability. While the phenomenon of general
knowledge hallucination in LLMs has been widely studied, the systematic
evaluation and mitigation of geospatial hallucinations remain largely
unexplored. To address this gap, we propose a comprehensive evaluation
framework for geospatial hallucinations, leveraging structured geospatial
knowledge graphs for controlled assessment. Through extensive evaluation across
20 advanced LLMs, we uncover the hallucinations in their geospatial knowledge.
Building on these insights, we introduce a dynamic factuality aligning method
based on Kahneman-Tversky Optimization (KTO) to mitigate geospatial
hallucinations in LLMs, leading to a performance improvement of over 29.6% on
the proposed benchmark. Extensive experimental results demonstrate the
effectiveness of our benchmark and learning algorithm in enhancing the
trustworthiness of LLMs in geospatial knowledge and reasoning tasks.

</details>


### [45] [Efficient Attention Mechanisms for Large Language Models: A Survey](https://arxiv.org/abs/2507.19595)
*Yutao Sun,Zhenyu Li,Yike Zhang,Tengyu Pan,Bowen Dong,Yuyi Guo,Jianyong Wang*

Main category: cs.CL

TL;DR: 本文综述了Transformer架构中高效注意力机制的两类方法：线性注意力和稀疏注意力，旨在解决自注意力机制的二次复杂度问题，推动高效长上下文建模的发展。


<details>
  <summary>Details</summary>
Motivation: 自注意力机制的二次时间和内存复杂度限制了长上下文建模的效率，因此需要研究高效的注意力机制以提升模型的可扩展性。

Method: 线性注意力通过核近似、循环公式或快速权重动态实现线性复杂度；稀疏注意力则通过固定模式、块路由或聚类策略选择部分令牌进行计算。

Result: 综述了高效注意力机制的算法创新和硬件级优化，并分析了其在预训练语言模型中的应用，包括纯高效注意力架构和混合设计。

Conclusion: 本文为设计和部署高效、可扩展的语言模型提供了理论基础和实践指导。

Abstract: Transformer-based architectures have become the prevailing backbone of large
language models. However, the quadratic time and memory complexity of
self-attention remains a fundamental obstacle to efficient long-context
modeling. To address this limitation, recent research has introduced two
principal categories of efficient attention mechanisms. Linear attention
methods achieve linear complexity through kernel approximations, recurrent
formulations, or fastweight dynamics, thereby enabling scalable inference with
reduced computational overhead. Sparse attention techniques, in contrast, limit
attention computation to selected subsets of tokens based on fixed patterns,
block-wise routing, or clustering strategies, enhancing efficiency while
preserving contextual coverage. This survey provides a systematic and
comprehensive overview of these developments, integrating both algorithmic
innovations and hardware-level considerations. In addition, we analyze the
incorporation of efficient attention into largescale pre-trained language
models, including both architectures built entirely on efficient attention and
hybrid designs that combine local and global components. By aligning
theoretical foundations with practical deployment strategies, this work aims to
serve as a foundational reference for advancing the design of scalable and
efficient language models.

</details>


### [46] [MOCHA: Are Code Language Models Robust Against Multi-Turn Malicious Coding Prompts?](https://arxiv.org/abs/2507.19598)
*Muntasir Wahed,Xiaona Zhou,Kiet A. Nguyen,Tianjiao Yu,Nirav Diwan,Gang Wang,Dilek Hakkani-Tür,Ismini Lourentzou*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLMs）在代码生成中的鲁棒性问题，提出了一种多轮恶意提示攻击方法，并引入了一个基准测试。通过微调模型，显著提高了对抗性攻击的防御能力。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在代码生成中对多轮恶意提示的鲁棒性，填补现有研究的空白。

Method: 提出代码分解攻击方法，并开发了大规模基准测试\benchmarkname{}，评估模型在单轮和多轮恶意提示下的表现。通过微调模型（MOCHA）提升防御能力。

Result: 实验显示模型在多轮攻击下存在漏洞，微调后拒绝率显著提升（最高32.4%），且不影响代码生成能力。

Conclusion: 多轮恶意提示对LLMs构成威胁，但通过微调可显著提高鲁棒性，为未来研究提供了基准和方法。

Abstract: Recent advancements in Large Language Models (LLMs) have significantly
enhanced their code generation capabilities. However, their robustness against
adversarial misuse, particularly through multi-turn malicious coding prompts,
remains underexplored. In this work, we introduce code decomposition attacks,
where a malicious coding task is broken down into a series of seemingly benign
subtasks across multiple conversational turns to evade safety filters. To
facilitate systematic evaluation, we introduce \benchmarkname{}, a large-scale
benchmark designed to evaluate the robustness of code LLMs against both
single-turn and multi-turn malicious prompts. Empirical results across open-
and closed-source models reveal persistent vulnerabilities, especially under
multi-turn scenarios. Fine-tuning on MOCHA improves rejection rates while
preserving coding ability, and importantly, enhances robustness on external
adversarial datasets with up to 32.4% increase in rejection rates without any
additional supervision.

</details>


### [47] [HITSZ's End-To-End Speech Translation Systems Combining Sequence-to-Sequence Auto Speech Recognition Model and Indic Large Language Model for IWSLT 2025 in Indic Track](https://arxiv.org/abs/2507.19616)
*Xuchen Wei,Yangxin Wu,Yaoyin Zhang,Henglyu Liu,Kehai Chen,Xuefeng Bai,Min Zhang*

Main category: cs.CL

TL;DR: HITSZ的IWSLT 2025 Indic赛道提交，提出了一种端到端系统，结合Whisper ASR和Krutrim LLM，提升低资源语言对的语音到文本翻译质量。


<details>
  <summary>Details</summary>
Motivation: 在低资源场景下提升英语与印度语言之间的语音到文本翻译质量。

Method: 集成预训练的Whisper ASR模型和印度语言专用LLM Krutrim，形成端到端系统，并探索Chain-of-Thought方法。

Result: 平均BLEU分数为英语到印度语言28.88，印度语言到英语27.86；CoT方法在某些情况下显著提升翻译质量（如泰米尔语到英语BLEU增加13.84）。

Conclusion: 端到端系统有效，但CoT方法的输出格式一致性仍需改进。

Abstract: This paper presents HITSZ's submission for the IWSLT 2025 Indic track,
focusing on speech-to-text translation (ST) for English-to-Indic and
Indic-to-English language pairs. To enhance translation quality in this
low-resource scenario, we propose an end-to-end system integrating the
pre-trained Whisper automated speech recognition (ASR) model with Krutrim, an
Indic-specialized large language model (LLM). Experimental results demonstrate
that our end-to-end system achieved average BLEU scores of $28.88$ for
English-to-Indic directions and $27.86$ for Indic-to-English directions.
Furthermore, we investigated the Chain-of-Thought (CoT) method. While this
method showed potential for significant translation quality improvements on
successfully parsed outputs (e.g. a $13.84$ BLEU increase for
Tamil-to-English), we observed challenges in ensuring the model consistently
adheres to the required CoT output format.

</details>


### [48] [MCIF: Multimodal Crosslingual Instruction-Following Benchmark from Scientific Talks](https://arxiv.org/abs/2507.19634)
*Sara Papi,Maike Züfle,Marco Gaido,Beatrice Savoldi,Danni Liu,Ioannis Douros,Luisa Bentivogli,Jan Niehues*

Main category: cs.CL

TL;DR: MCIF是一个多语言、多模态的基准测试，用于评估大型语言模型在多语言和多模态任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在评估多语言和多模态能力时存在不足，如局限于英语、单一模态或短文本，缺乏全面性。

Method: 引入MCIF基准测试，涵盖语音、视觉和文本三种模态及四种语言，基于科学讲座数据，支持长短期上下文评估。

Result: MCIF为多模态跨语言指令跟随提供了首个全面评估工具，支持开放研究。

Conclusion: MCIF填补了现有基准测试的空白，推动了多模态语言模型的发展。

Abstract: Recent advances in large language models have catalyzed the development of
multimodal LLMs (MLLMs) that integrate text, speech, and vision within unified
frameworks. As MLLMs evolve from narrow, monolingual, task-specific systems to
general-purpose instruction-following models, a key frontier lies in evaluating
their multilingual and multimodal capabilities over both long and short
contexts. However, existing benchmarks fall short in evaluating these
dimensions jointly: they are often limited to English, mostly focus on one
single modality at a time, rely on short-form contexts, or lack human
annotations -- hindering comprehensive assessment of model performance across
languages, modalities, and task complexity. To address these gaps, we introduce
MCIF (Multimodal Crosslingual Instruction Following), the first multilingual
human-annotated benchmark based on scientific talks that is designed to
evaluate instruction-following in crosslingual, multimodal settings over both
short- and long-form inputs. MCIF spans three core modalities -- speech,
vision, and text -- and four diverse languages (English, German, Italian, and
Chinese), enabling a comprehensive evaluation of MLLMs' abilities to interpret
instructions across languages and combine them with multimodal contextual
information. MCIF is released under a CC-BY 4.0 license to encourage open
research and progress in MLLMs development.

</details>


### [49] [RoD-TAL: A Benchmark for Answering Questions in Romanian Driving License Exams](https://arxiv.org/abs/2507.19666)
*Andrei Vlad Man,Răzvan-Alexandru Smădu,Cristian-George Craciun,Dumitru-Clementin Cercel,Florin Pop,Mihaela-Claudia Cercel*

Main category: cs.CL

TL;DR: 论文研究了AI在法律教育中的应用，特别是针对罗马尼亚语等资源不足的语言，通过多模态数据集RoD-TAL评估了LLMs和VLMs在罗马尼亚驾驶法律问答任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决资源不足语言（如罗马尼亚语）在法律教育中的AI支持需求，评估LLMs和VLMs在法律推理中的能力。

Method: 引入RoD-TAL多模态数据集，结合RAG管道、密集检索器和推理优化模型，评估文本和视觉问答任务。

Result: 领域特定微调显著提升检索性能，推理优化模型在问答任务中表现优异，但视觉推理仍有挑战。

Conclusion: LLMs和VLMs在法律教育中具有潜力，但视觉推理仍需改进。

Abstract: The intersection of AI and legal systems presents a growing need for tools
that support legal education, particularly in under-resourced languages such as
Romanian. In this work, we aim to evaluate the capabilities of Large Language
Models (LLMs) and Vision-Language Models (VLMs) in understanding and reasoning
about Romanian driving law through textual and visual question-answering tasks.
To facilitate this, we introduce RoD-TAL, a novel multimodal dataset comprising
Romanian driving test questions, text-based and image-based, alongside
annotated legal references and human explanations. We implement and assess
retrieval-augmented generation (RAG) pipelines, dense retrievers, and
reasoning-optimized models across tasks including Information Retrieval (IR),
Question Answering (QA), Visual IR, and Visual QA. Our experiments demonstrate
that domain-specific fine-tuning significantly enhances retrieval performance.
At the same time, chain-of-thought prompting and specialized reasoning models
improve QA accuracy, surpassing the minimum grades required to pass driving
exams. However, visual reasoning remains challenging, highlighting the
potential and the limitations of applying LLMs and VLMs to legal education.

</details>


### [50] [Towards Inclusive NLP: Assessing Compressed Multilingual Transformers across Diverse Language Benchmarks](https://arxiv.org/abs/2507.19699)
*Maitha Alshehhi,Ahmed Sharshar,Mohsen Guizani*

Main category: cs.CL

TL;DR: 论文研究了多语言和单语言大语言模型（LLMs）在阿拉伯语、英语和印度语中的表现，重点关注模型压缩策略（如剪枝和量化）的影响。发现多语言模型普遍优于单语言模型，量化能保持准确性，而剪枝会显著降低性能。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在低资源语言环境（如阿拉伯语和印度语）中的表现，以及模型压缩策略对性能的影响。

Method: 对多种LLMs（如BLOOMZ、AceGPT等）进行基准测试，分析量化（4位和8位）和剪枝策略的效果。

Result: 多语言模型表现更优，量化有效但剪枝损害性能，尤其是在大型模型中。

Conclusion: 研究为构建可扩展且公平的多语言NLP解决方案提供了关键策略，并指出需解决低资源环境中的幻觉和泛化错误。

Abstract: Although LLMs have attained significant success in high-resource languages,
their capacity in low-resource linguistic environments like Kannada and Arabic
is not yet fully understood. This work benchmarking the performance of
multilingual and monolingual Large Language Models (LLMs) across Arabic,
English, and Indic languages, with particular emphasis on the effects of model
compression strategies such as pruning and quantization. Findings shows
significant performance differences driven by linguistic diversity and resource
availability on SOTA LLMS as BLOOMZ, AceGPT, Jais, LLaMA-2, XGLM, and AraGPT2.
We find that multilingual versions of the model outperform their
language-specific counterparts across the board, indicating substantial
cross-lingual transfer benefits. Quantization (4-bit and 8-bit) is effective in
maintaining model accuracy while promoting efficiency, but aggressive pruning
significantly compromises performance, especially in bigger models. Our
findings pinpoint key strategies to construct scalable and fair multilingual
NLP solutions and underscore the need for interventions to address
hallucination and generalization errors in the low-resource setting.

</details>


### [51] [Ta-G-T: Subjectivity Capture in Table to Text Generation via RDF Graphs](https://arxiv.org/abs/2507.19710)
*Ronak Upasham,Tathagata Dey,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 提出了一种新颖的三阶段流水线方法，用于从表格数据生成包含主观性的文本，通过RDF三元组提取、文本聚合和主观性注入实现。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注表格数据的客观描述，而生成包含主观性（即超越原始数据的解释）的文本尚未充分探索。

Method: 1) 提取RDF三元组；2) 将文本聚合成连贯叙述；3) 注入主观性以丰富文本。使用小型T5模型而非大型语言模型。

Result: 在多项指标上表现与GPT-3.5相当，优于Mistral-7B和Llama-2，同时保持事实准确性和可解释性。

Conclusion: 这是首个通过中间表示增强事实正确性和主观性的结构化T2T生成方法。

Abstract: In Table-to-Text (T2T) generation, existing approaches predominantly focus on
providing objective descriptions of tabular data. However, generating text that
incorporates subjectivity, where subjectivity refers to interpretations beyond
raw numerical data, remains underexplored. To address this, we introduce a
novel pipeline that leverages intermediate representations to generate both
objective and subjective text from tables. Our three-stage pipeline consists
of: 1) extraction of Resource Description Framework (RDF) triples, 2)
aggregation of text into coherent narratives, and 3) infusion of subjectivity
to enrich the generated text. By incorporating RDFs, our approach enhances
factual accuracy while maintaining interpretability. Unlike large language
models (LLMs) such as GPT-3.5, Mistral-7B, and Llama-2, our pipeline employs
smaller, fine-tuned T5 models while achieving comparable performance to GPT-3.5
and outperforming Mistral-7B and Llama-2 in several metrics. We evaluate our
approach through quantitative and qualitative analyses, demonstrating its
effectiveness in balancing factual accuracy with subjective interpretation. To
the best of our knowledge, this is the first work to propose a structured
pipeline for T2T generation that integrates intermediate representations to
enhance both factual correctness and subjectivity.

</details>


### [52] [Basic Reading Distillation](https://arxiv.org/abs/2507.19741)
*Zhi Zhou,Sirui Miao,Xiangyu Duan,Hao Yang,Min Zhang*

Main category: cs.CL

TL;DR: 提出了一种名为基本阅读蒸馏（BRD）的方法，通过教育小型模型模仿大语言模型的基本阅读行为，使其在多种任务上表现优于或接近20倍大的模型。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型（LLMs）高计算资源需求的问题，同时避免现有蒸馏方法忽视对通用文本的基本阅读教育。

Method: 提出BRD方法，教育小型模型模仿LLMs的基本阅读行为（如命名实体识别、问答等），再应用于下游任务。

Result: 小型模型在语言推理和BIG-bench任务中表现优于或接近20倍大的LLMs。

Conclusion: BRD能有效影响小型模型的概率分布，并与知识蒸馏或任务蒸馏具有正交性。

Abstract: Large language models (LLMs) have demonstrated remarkable abilities in
various natural language processing areas, but they demand high computation
resources which limits their deployment in real-world. Distillation is one
technique to solve this problem through either knowledge distillation or task
distillation. Both distillation approaches train small models to imitate
specific features of LLMs, but they all neglect basic reading education for
small models on generic texts that are \emph{unrelated} to downstream tasks. In
this paper, we propose basic reading distillation (BRD) which educates a small
model to imitate LLMs basic reading behaviors, such as named entity
recognition, question raising and answering, on each sentence. After such basic
education, we apply the small model on various tasks including language
inference benchmarks and BIG-bench tasks. It shows that the small model can
outperform or perform comparable to over 20x bigger LLMs. Analysis reveals that
BRD effectively influences the probability distribution of the small model, and
has orthogonality to either knowledge distillation or task distillation.

</details>


### [53] [JT-Math: A Multi-Stage Framework for Advanced Mathematical Reasoning in Large Language Models](https://arxiv.org/abs/2507.19748)
*Yifan Hao,Fangning Chao,Yaqian Hao,Zhaojun Cui,Huan Bai,Haiyu Zhang,Yankai Liu,Chao Deng,Junlan Feng*

Main category: cs.CL

TL;DR: JT-Math-8B是一系列开源模型，通过多阶段优化框架提升数学推理能力，在类似规模的开源模型中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决现有大型语言模型在复杂数学问题中表现不足的问题。

Method: 采用多阶段优化框架，包括基础、指导和思考三个版本，结合高质量数据集和强化学习。

Result: 在开源模型中表现最优，超越OpenAI的O1-mini和GPT-4o。

Conclusion: JT-Math-8B展示了在数学推理任务中的强大能力，为开源模型树立了新标杆。

Abstract: Mathematical reasoning is a cornerstone of artificial general intelligence
and a primary benchmark for evaluating the capabilities of Large Language
Models (LLMs). While state-of-the-art models show promise, they often falter
when faced with complex problems that demand deep conceptual understanding and
intricate, multi-step deliberation. To address this challenge, we introduce
JT-Math-8B, a series of open-source models comprising base, instruct, and
thinking versions, built upon a systematic, multi-stage optimization framework.
Our pre-training corpus is a high-quality, 210B-token dataset curated through a
dedicated data pipeline that uses model-based validation to ensure quality and
diversity. The Instruct Model is optimized for direct, concise answers through
Supervised Fine-Tuning (SFT) and a GRPO-based reinforcement learning (RL)
method. The Thinking Model is trained for complex problem-solving using a Long
Chain-of-Thought (Long CoT) approach, combining SFT with a novel, multi-stage
RL curriculum that progressively increases task difficulty and context length
up to 32K tokens. JT-Math-8B achieves state-of-the-art results among
open-source models of similar size, surpassing prominent models like OpenAI's
O1-mini and GPT-4o , and demonstrating superior performance on
competition-level mathematics.

</details>


### [54] [Are You There God? Lightweight Narrative Annotation of Christian Fiction with LMs](https://arxiv.org/abs/2507.19756)
*Rebecca M. M. Hicke,Brian Haggard,Mia Ferrante,Rayhan Khanna,David Mimno*

Main category: cs.CL

TL;DR: 论文使用计算工具分析基督教小说中的‘神的行为’，发现《末日迷踪》系列与其他基督教小说及男女作者作品间存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 研究基督教小说的文化和文学层面，填补学术空白，尤其是对‘神的行为’的描绘。

Method: 结合人类注释和轻量级语言模型，定义并标注‘神的行为’，比较不同作品间的差异。

Result: 轻量级模型能匹配人类注释，发现《末日迷踪》与其他作品及男女作者间存在显著差异。

Conclusion: 计算工具能有效分析文学主题，揭示基督教小说中的多样性。

Abstract: In addition to its more widely studied political activities, the American
Evangelical movement has a well-developed but less externally visible cultural
and literary side. Christian Fiction, however, has been little studied, and
what scholarly attention there is has focused on the explosively popular Left
Behind series. In this work, we use computational tools to provide both a broad
topical overview of Christian Fiction as a genre and a more directed
exploration of how its authors depict divine acts. Working with human
annotators we first developed definitions and a codebook for "acts of God." We
then adapted those instructions designed for human annotators for use by a
recent, lightweight LM with the assistance of a much larger model. The
laptop-scale LM is capable of matching human annotations, even when the task is
subtle and challenging. Using these annotations, we show that significant and
meaningful differences exist between the Left Behind books and Christian
Fiction more broadly and between books by male and female authors.

</details>


### [55] [UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models' Reasoning Abilities](https://arxiv.org/abs/2507.19766)
*Dong Du,Shulin Liu,Tao Yang,Shaohua Chen,Yang Li*

Main category: cs.CL

TL;DR: 提出了一种针对超长输出序列的强化学习方法（UloRL），通过分段解码和动态掩码技术提升训练效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习框架在处理超长输出序列时效率低下，存在长尾分布和熵崩溃问题，需要改进。

Method: 将超长输出解码分为短段，引入动态掩码技术防止熵崩溃。

Result: 在Qwen3-30B-A3B模型上，训练速度提升2.06倍，性能在AIME2025和BeyondAIME任务上显著提升。

Conclusion: UloRL方法有效提升了LLMs在超长序列生成中的推理能力，具有广泛应用潜力。

Abstract: Recent advances in large language models (LLMs) have highlighted the
potential of reinforcement learning with verifiable rewards (RLVR) to enhance
reasoning capabilities through extended output sequences. However, traditional
RL frameworks face inefficiencies when handling ultra-long outputs due to
long-tail sequence distributions and entropy collapse during training. To
address these challenges, we propose an Ultra-Long Output Reinforcement
Learning (UloRL) approach for advancing large language models' reasoning
abilities. Specifically, we divide ultra long output decoding into short
segments, enabling efficient training by mitigating delays caused by long-tail
samples. Additionally, we introduce dynamic masking of well-Mastered Positive
Tokens (MPTs) to prevent entropy collapse. Experimental results demonstrate the
effectiveness of our approach. On the Qwen3-30B-A3B model, RL with segment
rollout achieved 2.06x increase in training speed, while RL training with
128k-token outputs improves the model's performance on AIME2025 from 70.9\% to
85.1\% and on BeyondAIME from 50.7\% to 61.9\%, even surpassing Qwen3-235B-A22B
with remarkable gains. These findings underscore the potential of our methods
to advance the reasoning capabilities of LLMs with ultra-long sequence
generation. We will release our code and model for further use by the
community.

</details>


### [56] [Flora: Effortless Context Construction to Arbitrary Length and Scale](https://arxiv.org/abs/2507.19786)
*Tianxiang Chen,Zhentao Tan,Xiaofan Bo,Yue Wu,Tao Gong,Qi Chu,Jieping Ye,Nenghai Yu*

Main category: cs.CL

TL;DR: Flora是一种无需人工或LLM干预的长上下文构建策略，通过组合短指令生成任意长度和多样性的长上下文，显著提升LLMs的长上下文性能，同时几乎不影响短上下文能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法构建长上下文依赖LLMs或人工干预，成本高且长度和多样性有限，且长上下文LLMs的短上下文性能下降明显。

Method: Flora通过基于类别的短指令组合和长上下文元指令生成响应，实现任意长度和多样性的上下文构建。

Result: 在Llama3-8B-Instruct和QwQ-32B上，Flora增强的LLMs在三个长上下文基准测试中表现优异，同时保持短上下文任务的强性能。

Conclusion: Flora提供了一种高效、低成本的长上下文构建方法，显著提升LLMs性能且几乎不影响短上下文能力。

Abstract: Effectively handling long contexts is challenging for Large Language Models
(LLMs) due to the rarity of long texts, high computational demands, and
substantial forgetting of short-context abilities. Recent approaches have
attempted to construct long contexts for instruction tuning, but these methods
often require LLMs or human interventions, which are both costly and limited in
length and diversity. Also, the drop in short-context performances of present
long-context LLMs remains significant. In this paper, we introduce Flora, an
effortless (human/LLM-free) long-context construction strategy. Flora can
markedly enhance the long-context performance of LLMs by arbitrarily assembling
short instructions based on categories and instructing LLMs to generate
responses based on long-context meta-instructions. This enables Flora to
produce contexts of arbitrary length and scale with rich diversity, while only
slightly compromising short-context performance. Experiments on
Llama3-8B-Instruct and QwQ-32B show that LLMs enhanced by Flora excel in three
long-context benchmarks while maintaining strong performances in short-context
tasks. Our data-construction code is available at
\href{https://github.com/txchen-USTC/Flora}{https://github.com/txchen-USTC/Flora}.

</details>


### [57] [HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs](https://arxiv.org/abs/2507.19823)
*Dongquan Yang,Yifan Yang,Xiaotian Yu,Xianbiao Qi,Rong Xiao*

Main category: cs.CL

TL;DR: HCAttention提出了一种异构注意力计算框架，通过关键量化、值卸载和动态KV淘汰，在极端内存限制下实现高效推理，将KV缓存内存占用缩小至25%，且无需微调模型。


<details>
  <summary>Details</summary>
Motivation: 处理长上下文输入时，KV缓存的巨大内存需求成为主要挑战，现有压缩方法在内存减少超过85%时性能显著下降。

Method: HCAttention结合关键量化、值卸载和动态KV淘汰，兼容现有Transformer架构，无需微调。

Result: 在LongBench基准测试中，HCAttention在KV缓存内存占用缩小至25%时保持全注意力模型的准确性，甚至在12.5%时仍具竞争力。

Conclusion: HCAttention在极端内存限制下实现了高效推理，扩展了Llama-3-8B模型处理4百万token的能力，成为KV缓存压缩的新标杆。

Abstract: Processing long-context inputs with large language models presents a
significant challenge due to the enormous memory requirements of the Key-Value
(KV) cache during inference. Existing KV cache compression methods exhibit
noticeable performance degradation when memory is reduced by more than 85%.
Additionally, strategies that leverage GPU-CPU collaboration for approximate
attention remain underexplored in this setting. We propose HCAttention, a
heterogeneous attention computation framework that integrates key quantization,
value offloading, and dynamic KV eviction to enable efficient inference under
extreme memory constraints. The method is compatible with existing transformer
architectures and does not require model fine-tuning. Experimental results on
the LongBench benchmark demonstrate that our approach preserves the accuracy of
full-attention model while shrinking the KV cache memory footprint to 25% of
its original size. Remarkably, it stays competitive with only 12.5% of the
cache, setting a new state-of-the-art in LLM KV cache compression. To the best
of our knowledge, HCAttention is the first to extend the Llama-3-8B model to
process 4 million tokens on a single A100 GPU with 80GB memory.

</details>


### [58] [DRIVE: Disfluency-Rich Synthetic Dialog Data Generation Framework for Intelligent Vehicle Environments](https://arxiv.org/abs/2507.19867)
*Anshul Chavda,M Jagadeesh,Chintalapalli Raja Kullayappa,B Jayaprakash,Medchalimi Sruthi,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: DiscoDrive是一个合成的车载对话数据集，包含3500个多轮对话，动态整合了真实对话中的不流畅现象，显著提升了对话AI的性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据集未能捕捉真实驾驶场景中的不流畅对话现象，如犹豫、重复和自我纠正，限制了对话AI的实际应用效果。

Method: 采用两阶段、提示驱动的合成方法，动态生成包含不流畅现象的多轮对话数据集。

Result: DiscoDrive在训练和低资源数据增强场景中均表现优异，显著提升了BLEU-4、METEOR等指标，并在人类评估中优于现有数据集。

Conclusion: DiscoDrive填补了现有资源的空白，为车载对话AI提供了更真实、鲁棒的训练和增强数据。

Abstract: In-car conversational AI is becoming increasingly critical as autonomous
vehicles and smart assistants gain widespread adoption. Yet, existing datasets
fail to capture the spontaneous disfluencies such as hesitations, false starts,
repetitions, and self-corrections that characterize real driver-AI dialogs. To
address this, we introduce DiscoDrive, a synthetic corpus of 3500 multi-turn
dialogs across seven automotive domains, generated using a two-stage,
prompt-driven pipeline that dynamically integrates disfluencies during
synthesis. We show that DiscoDrive is effective both as a training resource,
enabling DialoGPT-Medium and T5-Base to match or exceed KVRET-trained models on
the MultiWOZ 2.2 and Schema-Guided Dialogue (SGD) relevant test sets (BLEU-4
improvements of 0.26 to 0.61; METEOR +2.10; ROUGE-L +3.48; BERTScore F1
improvements of 1.35 to 3.48), and as a data augmentation resource in
low-resource scenarios, delivering additional gains of up to BLEU-4 +0.38,
METEOR +1.95, ROUGE-L +2.87, and BERTScore F1 +4.00 when combined with 10
percent of KVRET. Human evaluations further confirm that dialogs sampled from
DiscoDrive are rated higher than KVRET's human-collected dialogs in naturalness
(3.8 vs 3.6) and coherence (4.1 vs 4.0), and are perceived as more
context-appropriate than leading post-hoc methods (such as LARD), without
compromising clarity. DiscoDrive fills a critical gap in existing resources and
serves as a versatile corpus for both training and augmenting conversational
AI, enabling robust handling of real-world, disfluent in-car interactions.

</details>


### [59] [The Polish Vocabulary Size Test: A Novel Adaptive Test for Receptive Vocabulary Assessment](https://arxiv.org/abs/2507.19869)
*Danil Fokin,Monika Płużyczka,Grigory Golovin*

Main category: cs.CL

TL;DR: PVST是一种基于项目反应理论和计算机自适应测试的新工具，用于评估波兰语母语和非母语者的词汇量，测试时间短且准确度高。


<details>
  <summary>Details</summary>
Motivation: 开发一种高效、准确的工具来评估波兰语学习者和母语者的词汇量。

Method: 基于项目反应理论和计算机自适应测试，动态调整测试内容以适应受试者的水平。

Result: 验证研究表明，母语者的词汇量显著大于非母语者，且母语者的词汇量与年龄呈正相关。

Conclusion: PVST是一种有效的词汇量评估工具，已在myvocab.info/pl上线。

Abstract: We present the Polish Vocabulary Size Test (PVST), a novel tool for assessing
the receptive vocabulary size of both native and non-native Polish speakers.
Based on Item Response Theory and Computerized Adaptive Testing, PVST
dynamically adjusts to each test-taker's proficiency level, ensuring high
accuracy while keeping the test duration short. To validate the test, a pilot
study was conducted with 1.475 participants. Native Polish speakers
demonstrated significantly larger vocabularies compared to non-native speakers.
For native speakers, vocabulary size showed a strong positive correlation with
age. The PVST is available online at myvocab.info/pl.

</details>


### [60] [Zero-shot Performance of Generative AI in Brazilian Portuguese Medical Exam](https://arxiv.org/abs/2507.19885)
*Cesar Augusto Madid Truyts,Amanda Gomes Rabelo,Gabriel Mesquita de Souza,Daniel Scaldaferri Lages,Adriano Jose Pereira,Uri Adrian Prync Flato,Eduardo Pontes dos Reis,Joaquim Edson Vieira,Paulo Sergio Panse Silveira,Edson Amaro Junior*

Main category: cs.CL

TL;DR: 研究评估了六种LLMs和四种MLLMs在巴西葡萄牙语医学考试中的表现，发现部分模型表现接近人类，但多模态问题和语言差异仍需改进。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型评估多集中于英语，可能导致其他语言表现偏差，研究旨在填补这一空白。

Method: 使用巴西葡萄牙语医学考试题目，对比六种LLMs和四种MLLMs与人类考生的准确性、处理时间和解释连贯性。

Result: 部分模型（如Claude-3.5-Sonnet和Claude-3-Opus）准确性接近人类，但多模态问题和语言差异仍存在差距。

Conclusion: 需进一步优化非英语医学AI应用，强调多语言和临床环境评估的重要性。

Abstract: Artificial intelligence (AI) has shown the potential to revolutionize
healthcare by improving diagnostic accuracy, optimizing workflows, and
personalizing treatment plans. Large Language Models (LLMs) and Multimodal
Large Language Models (MLLMs) have achieved notable advancements in natural
language processing and medical applications. However, the evaluation of these
models has focused predominantly on the English language, leading to potential
biases in their performance across different languages.
  This study investigates the capability of six LLMs (GPT-4.0 Turbo,
LLaMA-3-8B, LLaMA-3-70B, Mixtral 8x7B Instruct, Titan Text G1-Express, and
Command R+) and four MLLMs (Claude-3.5-Sonnet, Claude-3-Opus, Claude-3-Sonnet,
and Claude-3-Haiku) to answer questions written in Brazilian spoken portuguese
from the medical residency entrance exam of the Hospital das Cl\'inicas da
Faculdade de Medicina da Universidade de S\~ao Paulo (HCFMUSP) - the largest
health complex in South America. The performance of the models was benchmarked
against human candidates, analyzing accuracy, processing time, and coherence of
the generated explanations.
  The results show that while some models, particularly Claude-3.5-Sonnet and
Claude-3-Opus, achieved accuracy levels comparable to human candidates,
performance gaps persist, particularly in multimodal questions requiring image
interpretation. Furthermore, the study highlights language disparities,
emphasizing the need for further fine-tuning and data set augmentation for
non-English medical AI applications.
  Our findings reinforce the importance of evaluating generative AI in various
linguistic and clinical settings to ensure a fair and reliable deployment in
healthcare. Future research should explore improved training methodologies,
improved multimodal reasoning, and real-world clinical integration of AI-driven
medical assistance.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [61] [Tuning adaptive gamma correction (TAGC) for enhancing images in low ligh](https://arxiv.org/abs/2507.19574)
*Ghufran Abualhail Alhamzawi,Ali Saeed Alfoudi,Ali Hakem Alsaeedi,Suha Mohammed Hadi,Amjed Abbas Ahmed,Md. Riad Hassan,Nurhizam Safie Mohd Satar,Waeel Yahya Yasseen*

Main category: cs.CV

TL;DR: 论文提出了一种名为TAGC的自适应伽马校正模型，用于增强低光照图像，通过分析颜色亮度和计算平均颜色来自动确定伽马值，有效提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 低光照条件下图像质量下降，表现为低对比度、噪声和细节模糊，需要一种无需人工干预的自动增强方法。

Method: 基于颜色亮度分析和平均颜色计算，自动确定自适应伽马系数，适用于不同光照水平的图像。

Result: TAGC模型在定性和定量评估中均有效提升低光照图像质量，保持细节、自然对比度和正确色彩分布。

Conclusion: TAGC是一种高效的低光照图像处理解决方案，适用于夜间监控、医学图像增强和低光照摄影等多种应用。

Abstract: Enhancing images in low-light conditions is an important challenge in
computer vision. Insufficient illumination negatively affects the quality of
images, resulting in low contrast, intensive noise, and blurred details. This
paper presents a model for enhancing low-light images called tuning adaptive
gamma correction (TAGC). The model is based on analyzing the color luminance of
the low-light image and calculating the average color to determine the adaptive
gamma coefficient. The gamma value is calculated automatically and adaptively
at different illumination levels suitable for the image without human
intervention or manual adjustment. Based on qualitative and quantitative
evaluation, tuning adaptive gamma correction model has effectively improved
low-light images while maintaining details, natural contrast, and correct color
distribution. It also provides natural visual quality. It can be considered a
more efficient solution for processing low-light images in multiple
applications such as night surveillance, improving the quality of medical
images, and photography in low-light environments.

</details>


### [62] [Is Exchangeability better than I.I.D to handle Data Distribution Shifts while Pooling Data for Data-scarce Medical image segmentation?](https://arxiv.org/abs/2507.19575)
*Ayush Roy,Samin Enam,Jun Xia,Vishnu Suresh Lokhande,Won Hwa Kim*

Main category: cs.CV

TL;DR: 论文探讨了医学影像中数据稀缺问题，提出了一种通过控制深度网络特征差异的方法来优化数据池化和数据添加的效果，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 医学影像数据稀缺，数据池化和数据添加虽能提升模型性能，但可能引发分布偏移问题（Data Addition Dilemma），影响下游任务表现。

Method: 基于因果框架，提出了一种控制深度网络中前景-背景特征差异的方法，优化特征表示。

Result: 在五个数据集（包括新整理的超声数据集）上实现了最先进的分割性能，定性结果显示分割图更精细准确。

Conclusion: 该方法有效解决了数据添加带来的分布偏移问题，提升了医学影像分割的精度和鲁棒性。

Abstract: Data scarcity is a major challenge in medical imaging, particularly for deep
learning models. While data pooling (combining datasets from multiple sources)
and data addition (adding more data from a new dataset) have been shown to
enhance model performance, they are not without complications. Specifically,
increasing the size of the training dataset through pooling or addition can
induce distributional shifts, negatively affecting downstream model
performance, a phenomenon known as the "Data Addition Dilemma". While the
traditional i.i.d. assumption may not hold in multi-source contexts, assuming
exchangeability across datasets provides a more practical framework for data
pooling. In this work, we investigate medical image segmentation under these
conditions, drawing insights from causal frameworks to propose a method for
controlling foreground-background feature discrepancies across all layers of
deep networks. This approach improves feature representations, which are
crucial in data-addition scenarios. Our method achieves state-of-the-art
segmentation performance on histopathology and ultrasound images across five
datasets, including a novel ultrasound dataset that we have curated and
contributed. Qualitative results demonstrate more refined and accurate
segmentation maps compared to prominent baselines across three model
architectures. The code will be available on Github.

</details>


### [63] [T-MPEDNet: Unveiling the Synergy of Transformer-aware Multiscale Progressive Encoder-Decoder Network with Feature Recalibration for Tumor and Liver Segmentation](https://arxiv.org/abs/2507.19590)
*Chandravardhan Singh Raghaw,Jasmer Singh Sanjotra,Mohammad Zia Ur Rehman,Shubhi Bansal,Shahid Shafi Dar,Nagendra Kumar*

Main category: cs.CV

TL;DR: 提出了一种新型Transformer感知的多尺度渐进编码器-解码器网络（T-MPEDNet），用于CT扫描中肝脏和肿瘤的自动分割，显著提高了分割精度。


<details>
  <summary>Details</summary>
Motivation: 肝脏和肿瘤的自动分割在快速诊断和治疗计划中至关重要，但由于肿瘤的异质性和肝脏视觉特征的多样性，现有方法面临挑战。

Method: T-MPEDNet采用渐进编码器-解码器结构，结合Transformer动态注意力机制和多尺度特征利用，并通过形态学边界细化优化分割结果。

Result: 在LiTS和3DIRCADb数据集上，T-MPEDNet的肝脏和肿瘤分割Dice系数分别达到97.6%/89.1%和98.3%/83.3%，优于12种先进方法。

Conclusion: T-MPEDNet是一种高效可靠的肝脏和肿瘤自动分割框架，具有显著的临床应用潜力。

Abstract: Precise and automated segmentation of the liver and its tumor within CT scans
plays a pivotal role in swift diagnosis and the development of optimal
treatment plans for individuals with liver diseases and malignancies. However,
automated liver and tumor segmentation faces significant hurdles arising from
the inherent heterogeneity of tumors and the diverse visual characteristics of
livers across a broad spectrum of patients. Aiming to address these challenges,
we present a novel Transformer-aware Multiscale Progressive Encoder-Decoder
Network (T-MPEDNet) for automated segmentation of tumor and liver. T-MPEDNet
leverages a deep adaptive features backbone through a progressive
encoder-decoder structure, enhanced by skip connections for recalibrating
channel-wise features while preserving spatial integrity. A
Transformer-inspired dynamic attention mechanism captures long-range contextual
relationships within the spatial domain, further enhanced by multi-scale
feature utilization for refined local details, leading to accurate prediction.
Morphological boundary refinement is then employed to address indistinct
boundaries with neighboring organs, capturing finer details and yielding
precise boundary labels. The efficacy of T-MPEDNet is comprehensively assessed
on two widely utilized public benchmark datasets, LiTS and 3DIRCADb. Extensive
quantitative and qualitative analyses demonstrate the superiority of T-MPEDNet
compared to twelve state-of-the-art methods. On LiTS, T-MPEDNet achieves
outstanding Dice Similarity Coefficients (DSC) of 97.6% and 89.1% for liver and
tumor segmentation, respectively. Similar performance is observed on 3DIRCADb,
with DSCs of 98.3% and 83.3% for liver and tumor segmentation, respectively.
Our findings prove that T-MPEDNet is an efficacious and reliable framework for
automated segmentation of the liver and its tumor in CT scans.

</details>


### [64] [SurgPIS: Surgical-instrument-level Instances and Part-level Semantics for Weakly-supervised Part-aware Instance Segmentation](https://arxiv.org/abs/2507.19592)
*Meng Wei,Charlie Budd,Oluwatosin Alabi,Miaojing Shi,Tom Vercauteren*

Main category: cs.CV

TL;DR: SurgPIS提出了一种统一的部分感知实例分割（PIS）方法，用于手术器械分割，结合了实例级和部分级分割任务，并通过弱监督学习和师生框架解决了数据标注不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅单独处理手术器械的实例级分割（IIS）或部分级语义分割（PSS），缺乏任务间的交互。SurgPIS旨在统一这两项任务，提升分割效果。

Method: 采用基于Transformer的掩码分类方法，引入部分特定查询，明确将部分与父器械实例关联。通过弱监督学习和师生框架处理部分标注数据。

Result: 在多个数据集上验证，SurgPIS在PIS、IIS、PSS和器械级语义分割任务中均达到最先进性能。

Conclusion: SurgPIS为手术器械分割提供了一种统一的解决方案，显著提升了分割性能，尤其在部分标注数据的情况下表现优异。

Abstract: Consistent surgical instrument segmentation is critical for automation in
robot-assisted surgery. Yet, existing methods only treat instrument-level
instance segmentation (IIS) or part-level semantic segmentation (PSS)
separately, without interaction between these tasks. In this work, we formulate
a surgical tool segmentation as a unified part-aware instance segmentation
(PIS) problem and introduce SurgPIS, the first PIS model for surgical
instruments. Our method adopts a transformer-based mask classification approach
and introduces part-specific queries derived from instrument-level object
queries, explicitly linking parts to their parent instrument instances. In
order to address the lack of large-scale datasets with both instance- and
part-level labels, we propose a weakly-supervised learning strategy for SurgPIS
to learn from disjoint datasets labelled for either IIS or PSS purposes. During
training, we aggregate our PIS predictions into IIS or PSS masks, thereby
allowing us to compute a loss against partially labelled datasets. A
student-teacher approach is developed to maintain prediction consistency for
missing PIS information in the partially labelled data, e.g., parts of the IIS
labelled data. Extensive experiments across multiple datasets validate the
effectiveness of SurgPIS, achieving state-of-the-art performance in PIS as well
as IIS, PSS, and instrument-level semantic segmentation.

</details>


### [65] [Object-centric Video Question Answering with Visual Grounding and Referring](https://arxiv.org/abs/2507.19599)
*Haochen Wang,Qirui Chen,Cilin Yan,Jiayin Cai,Xiaolong Jiang,Yao Hu,Weidi Xie,Stratis Gavves*

Main category: cs.CV

TL;DR: 论文提出了一种支持多模态交互的VideoLLM模型，解决了现有模型仅支持文本响应的限制，并引入了STOM模块和VideoInfer数据集，实验表明其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有VideoLLM模型仅支持文本响应，缺乏对象中心的多轮交互灵活性，限制了视频理解的深度。

Method: 提出支持文本和视觉提示的VideoLLM模型，引入STOM模块传播视觉提示，并构建VideoInfer数据集。

Result: 在12个基准测试的6项任务中，模型在视频问答和分割任务上均优于基线。

Conclusion: 该模型在多模态、对象中心的视频理解中表现出色，验证了其鲁棒性和灵活性。

Abstract: Video Large Language Models (VideoLLMs) have recently demonstrated remarkable
progress in general video understanding. However, existing models primarily
focus on high-level comprehension and are limited to text-only responses,
restricting the flexibility for object-centric, multiround interactions. In
this paper, we make three contributions: (i) we address these limitations by
introducing a VideoLLM model, capable of performing both object referring for
input and grounding for output in video reasoning tasks, i.e., allowing users
to interact with videos using both textual and visual prompts; (ii) we propose
STOM (Spatial-Temporal Overlay Module), a novel approach that propagates
arbitrary visual prompts input at any single timestamp to the remaining frames
within a video; (iii) we present VideoInfer, a manually curated object-centric
video instruction dataset featuring questionanswering pairs that require
reasoning. We conduct comprehensive experiments on VideoInfer and other
existing benchmarks across video question answering and referring object
segmentation. The results on 12 benchmarks of 6 tasks show that our proposed
model consistently outperforms baselines in both video question answering and
segmentation, underscoring its robustness in multimodal, object-centric video
and image understanding. Project page:
https://qirui-chen.github.io/RGA3-release/.

</details>


### [66] [Exemplar Med-DETR: Toward Generalized and Robust Lesion Detection in Mammogram Images and beyond](https://arxiv.org/abs/2507.19621)
*Sheethal Bhat,Bogdan Georgescu,Adarsh Bhandary Panambur,Mathias Zinnen,Tri-Thien Nguyen,Awais Mansoor,Karim Khalifa Elbarbary,Siming Bayer,Florin-Cristian Ghesu,Sasa Grbic,Andreas Maier*

Main category: cs.CV

TL;DR: 提出了一种名为Exemplar Med-DETR的多模态对比检测器，用于医学图像异常检测，显著提升了多种成像模态下的检测性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像异常检测面临特征表示差异和复杂解剖结构关系的挑战，现有方法难以学习有效的类别特定特征。

Method: 采用多模态对比检测器，结合交叉注意力和类别特定示例特征，通过迭代策略训练。

Result: 在四种公共数据集上取得最佳性能，如越南密集乳腺钼靶中肿块检测mAP达0.7，中国队列中病变检测性能提升两倍。

Conclusion: 该方法具有推动医学图像检测系统稳健性和通用性的潜力。

Abstract: Detecting abnormalities in medical images poses unique challenges due to
differences in feature representations and the intricate relationship between
anatomical structures and abnormalities. This is especially evident in
mammography, where dense breast tissue can obscure lesions, complicating
radiological interpretation. Despite leveraging anatomical and semantic
context, existing detection methods struggle to learn effective class-specific
features, limiting their applicability across different tasks and imaging
modalities. In this work, we introduce Exemplar Med-DETR, a novel multi-modal
contrastive detector that enables feature-based detection. It employs
cross-attention with inherently derived, intuitive class-specific exemplar
features and is trained with an iterative strategy. We achieve state-of-the-art
performance across three distinct imaging modalities from four public datasets.
On Vietnamese dense breast mammograms, we attain an mAP of 0.7 for mass
detection and 0.55 for calcifications, yielding an absolute improvement of 16
percentage points. Additionally, a radiologist-supported evaluation of 100
mammograms from an out-of-distribution Chinese cohort demonstrates a twofold
gain in lesion detection performance. For chest X-rays and angiography, we
achieve an mAP of 0.25 for mass and 0.37 for stenosis detection, improving
results by 4 and 7 percentage points, respectively. These results highlight the
potential of our approach to advance robust and generalizable detection systems
for medical imaging.

</details>


### [67] [Pre- and Post-Treatment Glioma Segmentation with the Medical Imaging Segmentation Toolkit](https://arxiv.org/abs/2507.19626)
*Adrian Celaya,Tucker Netherton,Dawid Schellingerhout,Caroline Chung,Beatrice Riviere,David Fuentes*

Main category: cs.CV

TL;DR: MIST是一个模块化的医学图像分割工具包，特别针对BraTS 2025挑战赛设计了灵活的后期处理框架，支持多种变换和用户自定义策略，提升了分割质量。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割方法缺乏标准化和可定制工具，难以进行严格比较。

Method: 扩展了MIST的后期处理模块，支持多种变换（如小对象移除、形态学操作等），并可组合成用户自定义策略。

Result: 评估了三种策略，结果显示MIST能快速实验并优化分割质量。

Conclusion: MIST开源且可扩展，支持医学图像分割的可重复和规模化研究。

Abstract: Medical image segmentation continues to advance rapidly, yet rigorous
comparison between methods remains challenging due to a lack of standardized
and customizable tooling. In this work, we present the current state of the
Medical Imaging Segmentation Toolkit (MIST), with a particular focus on its
flexible and modular postprocessing framework designed for the BraTS 2025 pre-
and post-treatment glioma segmentation challenge. Since its debut in the 2024
BraTS adult glioma post-treatment segmentation challenge, MIST's postprocessing
module has been significantly extended to support a wide range of transforms,
including removal or replacement of small objects, extraction of the largest
connected components, and morphological operations such as hole filling and
closing. These transforms can be composed into user-defined strategies,
enabling fine-grained control over the final segmentation output. We evaluate
three such strategies - ranging from simple small-object removal to more
complex, class-specific pipelines - and rank their performance using the BraTS
ranking protocol. Our results highlight how MIST facilitates rapid
experimentation and targeted refinement, ultimately producing high-quality
segmentations for the BraTS 2025 challenge. MIST remains open source and
extensible, supporting reproducible and scalable research in medical image
segmentation.

</details>


### [68] [SynPAIN: A Synthetic Dataset of Pain and Non-Pain Facial Expressions](https://arxiv.org/abs/2507.19673)
*Babak Taati,Muhammad Muzammil,Yasamin Zarghami,Abhishek Moturu,Airhossein Kazerouni,Hailey Reimer,Alex Mihailidis,Thomas Hadjistavropoulos*

Main category: cs.CV

TL;DR: SynPAIN是一个大规模合成数据集，用于解决疼痛评估研究中种族/年龄多样性不足的问题，并通过合成数据增强提高了疼痛检测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 解决疼痛评估中因数据集多样性不足导致的算法偏见问题，特别是针对老年痴呆症患者等沟通受限人群。

Method: 使用商业生成AI工具创建了包含不同种族、年龄和性别的合成疼痛表情数据集，并通过临床验证工具评估其有效性。

Result: 合成数据展示了预期的疼痛模式，显著提高了疼痛检测性能（平均精度提升7.0%），并揭示了现有模型的算法偏见。

Conclusion: SynPAIN填补了疼痛评估研究的空白，提供了首个公开的多样化合成数据集，并为测量和缓解算法偏见提供了框架。

Abstract: Accurate pain assessment in patients with limited ability to communicate,
such as older adults with dementia, represents a critical healthcare challenge.
Robust automated systems of pain detection may facilitate such assessments.
Existing pain detection datasets, however, suffer from limited ethnic/racial
diversity, privacy constraints, and underrepresentation of older adults who are
the primary target population for clinical deployment. We present SynPAIN, a
large-scale synthetic dataset containing 10,710 facial expression images (5,355
neutral/expressive pairs) across five ethnicities/races, two age groups (young:
20-35, old: 75+), and two genders. Using commercial generative AI tools, we
created demographically balanced synthetic identities with clinically
meaningful pain expressions. Our validation demonstrates that synthetic pain
expressions exhibit expected pain patterns, scoring significantly higher than
neutral and non-pain expressions using clinically validated pain assessment
tools based on facial action unit analysis. We experimentally demonstrate
SynPAIN's utility in identifying algorithmic bias in existing pain detection
models. Through comprehensive bias evaluation, we reveal substantial
performance disparities across demographic characteristics. These performance
disparities were previously undetectable with smaller, less diverse datasets.
Furthermore, we demonstrate that age-matched synthetic data augmentation
improves pain detection performance on real clinical data, achieving a 7.0%
improvement in average precision. SynPAIN addresses critical gaps in pain
assessment research by providing the first publicly available, demographically
diverse synthetic dataset specifically designed for older adult pain detection,
while establishing a framework for measuring and mitigating algorithmic bias.
The dataset is available at https://doi.org/10.5683/SP3/WCXMAP

</details>


### [69] [Efficient Learning for Product Attributes with Compact Multimodal Models](https://arxiv.org/abs/2507.19679)
*Mandar Kulkarni*

Main category: cs.CV

TL;DR: 本文研究了基于半监督学习的标签高效微调策略，利用未标注数据通过直接偏好优化（DPO）提升紧凑视觉语言模型（VLM）的性能。


<details>
  <summary>Details</summary>
Motivation: 由于手动或API标注成本高昂，监督微调视觉语言模型面临规模挑战，因此探索如何利用未标注数据提升模型性能。

Method: 采用PEFT训练低秩适配器模块，通过DPO利用未标注数据生成偏好链并更新模型，实现高效收敛。

Result: 在12个电商领域的数据集上，DPO微调显著优于监督模型，且未标注数据越多，性能提升越明显。

Conclusion: 通过DPO和未标注数据，可以在计算开销最小的情况下显著提升模型性能，证明了未标注数据的有效性。

Abstract: Image-based product attribute prediction in e-commerce is a crucial task with
numerous applications. The supervised fine-tuning of Vision Language Models
(VLMs) faces significant scale challenges due to the cost of manual or API
based annotation. In this paper, we investigate label-efficient semi-supervised
fine-tuning strategies for compact VLMs (2B-3B parameters) that leverage
unlabeled product listings through Direct Preference Optimization (DPO).
Beginning with a small, API-based, annotated, and labeled set, we first employ
PEFT to train low-rank adapter modules. To update the adapter weights with
unlabeled data, we generate multiple reasoning-and-answer chains per unlabeled
sample and segregate these chains into preferred and dispreferred based on
self-consistency. We then fine-tune the model with DPO loss and use the updated
model for the next iteration. By using PEFT fine-tuning with DPO, our method
achieves efficient convergence with minimal compute overhead. On a dataset
spanning twelve e-commerce verticals, DPO-based fine-tuning, which utilizes
only unlabeled data, demonstrates a significant improvement over the supervised
model. Moreover, experiments demonstrate that accuracy with DPO training
improves with more unlabeled data, indicating that a large pool of unlabeled
samples can be effectively leveraged to improve performance.

</details>


### [70] [DeepJIVE: Learning Joint and Individual Variation Explained from Multimodal Data Using Deep Learning](https://arxiv.org/abs/2507.19682)
*Matthew Drexler,Benjamin Risk,James J Lah,Suprateek Kundu,Deqiang Qiu*

Main category: cs.CV

TL;DR: DeepJIVE是一种基于深度学习的多模态数据集成方法，能够处理高维数据并识别非线性结构，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统多模态数据集成方法无法处理高维数据和非线性结构，因此需要一种更强大的方法。

Method: 提出DeepJIVE，通过深度学习实现联合和个体方差解释（JIVE），探索了三种可行的损失函数。

Result: DeepJIVE成功揭示了多模态数据的联合和个体变化，并在ADNI数据中发现了生物学上合理的协变模式。

Conclusion: DeepJIVE是多模态数据分析的有用工具。

Abstract: Conventional multimodal data integration methods provide a comprehensive
assessment of the shared or unique structure within each individual data type
but suffer from several limitations such as the inability to handle
high-dimensional data and identify nonlinear structures. In this paper, we
introduce DeepJIVE, a deep-learning approach to performing Joint and Individual
Variance Explained (JIVE). We perform mathematical derivation and experimental
validations using both synthetic and real-world 1D, 2D, and 3D datasets.
Different strategies of achieving the identity and orthogonality constraints
for DeepJIVE were explored, resulting in three viable loss functions. We found
that DeepJIVE can successfully uncover joint and individual variations of
multimodal datasets. Our application of DeepJIVE to the Alzheimer's Disease
Neuroimaging Initiative (ADNI) also identified biologically plausible
covariation patterns between the amyloid positron emission tomography (PET) and
magnetic resonance (MR) images. In conclusion, the proposed DeepJIVE can be a
useful tool for multimodal data analysis.

</details>


### [71] [Co-Win: Joint Object Detection and Instance Segmentation in LiDAR Point Clouds via Collaborative Window Processing](https://arxiv.org/abs/2507.19691)
*Haichuan Li,Tomi Westerlund*

Main category: cs.CV

TL;DR: Co-Win是一个新颖的鸟瞰图（BEV）感知框架，通过点云编码和并行窗口特征提取解决复杂环境的多模态问题。


<details>
  <summary>Details</summary>
Motivation: 在复杂城市环境中实现准确的感知和场景理解是确保自动驾驶安全高效的关键挑战。

Method: 采用分层架构，包括专用编码器、窗口式主干和查询式解码器，结合变分方法和掩码实例分割。

Result: 框架能够生成数据一致且上下文相关的预测掩码，并提供可解释的多样化实例预测。

Conclusion: Co-Win提升了自动驾驶系统的下游决策和规划能力。

Abstract: Accurate perception and scene understanding in complex urban environments is
a critical challenge for ensuring safe and efficient autonomous navigation. In
this paper, we present Co-Win, a novel bird's eye view (BEV) perception
framework that integrates point cloud encoding with efficient parallel
window-based feature extraction to address the multi-modality inherent in
environmental understanding. Our method employs a hierarchical architecture
comprising a specialized encoder, a window-based backbone, and a query-based
decoder head to effectively capture diverse spatial features and object
relationships. Unlike prior approaches that treat perception as a simple
regression task, our framework incorporates a variational approach with
mask-based instance segmentation, enabling fine-grained scene decomposition and
understanding. The Co-Win architecture processes point cloud data through
progressive feature extraction stages, ensuring that predicted masks are both
data-consistent and contextually relevant. Furthermore, our method produces
interpretable and diverse instance predictions, enabling enhanced downstream
decision-making and planning in autonomous driving systems.

</details>


### [72] [Bias Analysis for Synthetic Face Detection: A Case Study of the Impact of Facial Attribute](https://arxiv.org/abs/2507.19705)
*Asmae Lamsaf,Lucia Cascone,Hugo Proença,João Neves*

Main category: cs.CV

TL;DR: 本文提出了一种评估框架，用于分析合成人脸检测器的偏见，并通过合成数据生成和属性平衡来减少数据偏差。研究还通过案例研究揭示了五种先进检测器的偏见来源。


<details>
  <summary>Details</summary>
Motivation: 合成人脸检测器的偏见可能导致对某些人口群体的检测失败，引发社会、法律和伦理问题，因此需要系统分析。

Method: 利用合成数据生成和均匀分布的属性标签构建评估框架，并通过案例研究分析五种检测器的偏见。

Result: 研究发现合成人脸检测器普遍存在对特定面部属性的偏见，并揭示了偏见与训练数据属性平衡及检测器激活图的相关性。

Conclusion: 研究强调了偏见分析的重要性，并提供了减少偏见的框架和方法。

Abstract: Bias analysis for synthetic face detection is bound to become a critical
topic in the coming years. Although many detection models have been developed
and several datasets have been released to reliably identify synthetic content,
one crucial aspect has been largely overlooked: these models and training
datasets can be biased, leading to failures in detection for certain
demographic groups and raising significant social, legal, and ethical issues.
In this work, we introduce an evaluation framework to contribute to the
analysis of bias of synthetic face detectors with respect to several facial
attributes. This framework exploits synthetic data generation, with evenly
distributed attribute labels, for mitigating any skew in the data that could
otherwise influence the outcomes of bias analysis. We build on the proposed
framework to provide an extensive case study of the bias level of five
state-of-the-art detectors in synthetic datasets with 25 controlled facial
attributes. While the results confirm that, in general, synthetic face
detectors are biased towards the presence/absence of specific facial
attributes, our study also sheds light on the origins of the observed bias
through the analysis of the correlations with the balancing of facial
attributes in the training sets of the detectors, and the analysis of detectors
activation maps in image pairs with controlled attribute modifications.

</details>


### [73] [Quaternion-Based Robust PCA for Efficient Moving Target Detection and Background Recovery in Color Videos](https://arxiv.org/abs/2507.19730)
*Liyang Wang,Shiqian Wu,Shun Fang,Qile Zhu,Jiaxin Wu,Sos Again*

Main category: cs.CV

TL;DR: 论文提出了一种基于四元数黎曼流形的uQRPCA+框架，用于高效处理彩色视频中的移动目标检测和背景恢复任务，显著降低了计算复杂度并实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决彩色视频处理中QSVD的高计算成本和传统QRPCA在颜色通道中无法实现rank-1分解的问题。

Method: 利用四元数黎曼流形降低QSVD计算复杂度，提出uQRPCA框架平衡目标分割和背景恢复，并通过CR1B方法进一步优化背景恢复。

Result: uQRPCA+在移动目标检测和背景恢复任务上实现了SOTA性能。

Conclusion: uQRPCA+是一种高效且性能优越的彩色视频处理方法，其实现已开源。

Abstract: Moving target detection is a challenging computer vision task aimed at
generating accurate segmentation maps in diverse in-the-wild color videos
captured by static cameras. If backgrounds and targets can be simultaneously
extracted and recombined, such synthetic data can significantly enrich
annotated in-the-wild datasets and enhance the generalization ability of deep
models. Quaternion-based RPCA (QRPCA) is a promising unsupervised paradigm for
color image processing. However, in color video processing, Quaternion Singular
Value Decomposition (QSVD) incurs high computational costs, and rank-1
quaternion matrix fails to yield rank-1 color channels. In this paper, we
reduce the computational complexity of QSVD to o(1) by utilizing a quaternion
Riemannian manifold. Furthermor, we propose the universal QRPCA (uQRPCA)
framework, which achieves a balance in simultaneously segmenting targets and
recovering backgrounds from color videos. Moreover, we expand to uQRPCA+ by
introducing the Color Rank-1 Batch (CR1B) method to further process and obtain
the ideal low-rank background across color channels. Experiments demonstrate
our uQRPCA+ achieves State Of The Art (SOTA) performance on moving target
detection and background recovery tasks compared to existing open-source
methods. Our implementation is publicly available on GitHub at
https://github.com/Ruchtech/uQRPCA

</details>


### [74] [Leveraging Sparse LiDAR for RAFT-Stereo: A Depth Pre-Fill Perspective](https://arxiv.org/abs/2507.19738)
*Jinsu Yoo,Sooyoung Jeon,Zanming Huang,Tai-Yu Pan,Wei-Lun Chao*

Main category: cs.CV

TL;DR: 论文研究了在RAFT-Stereo框架中引入LiDAR引导以提高立体匹配精度，发现稀疏LiDAR点会导致性能下降，并提出了一种简单的插值预填充解决方案。结合两种预填充方法，GRAFT-Stereo在稀疏LiDAR条件下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 通过LiDAR深度信息提升立体匹配精度，尤其是在LiDAR点稀疏的情况下。

Method: 在初始视差图中通过插值预填充稀疏LiDAR点，并结合早期融合中的预填充方法。

Result: GRAFT-Stereo在稀疏LiDAR条件下显著优于现有方法。

Conclusion: 研究为LiDAR引导的立体匹配提供了新思路，并展示了简单插值预填充的有效性。

Abstract: We investigate LiDAR guidance within the RAFT-Stereo framework, aiming to
improve stereo matching accuracy by injecting precise LiDAR depth into the
initial disparity map. We find that the effectiveness of LiDAR guidance
drastically degrades when the LiDAR points become sparse (e.g., a few hundred
points per frame), and we offer a novel explanation from a signal processing
perspective. This insight leads to a surprisingly simple solution that enables
LiDAR-guided RAFT-Stereo to thrive: pre-filling the sparse initial disparity
map with interpolation. Interestingly, we find that pre-filling is also
effective when injecting LiDAR depth into image features via early fusion, but
for a fundamentally different reason, necessitating a distinct pre-filling
approach. By combining both solutions, the proposed Guided RAFT-Stereo
(GRAFT-Stereo) significantly outperforms existing LiDAR-guided methods under
sparse LiDAR conditions across various datasets. We hope this study inspires
more effective LiDAR-guided stereo methods.

</details>


### [75] [Latest Object Memory Management for Temporally Consistent Video Instance Segmentation](https://arxiv.org/abs/2507.19754)
*Seunghun Lee,Jiwan Seo,Minwoo Choi,Kiljoon Han,Jaehoon Jeong,Zane Durante,Ehsan Adeli,Sang Hyun Park,Sunghoon Im*

Main category: cs.CV

TL;DR: LOMM方法通过最新对象内存（LOM）和解耦对象关联（DOA）显著提升视频实例分割的长期跟踪能力，实现了54.0的AP高分。


<details>
  <summary>Details</summary>
Motivation: 解决视频实例分割中长期实例跟踪的挑战，提升跟踪的一致性和身份管理的准确性。

Method: 使用LOM跟踪和更新对象状态，DOA策略分别处理新出现和已存在的对象。

Result: 在YouTube-VIS 2022数据集上达到54.0的AP，优于传统方法。

Conclusion: LOMM在视频实例分割中实现了显著的性能提升，成为新的基准。

Abstract: In this paper, we present Latest Object Memory Management (LOMM) for
temporally consistent video instance segmentation that significantly improves
long-term instance tracking. At the core of our method is Latest Object Memory
(LOM), which robustly tracks and continuously updates the latest states of
objects by explicitly modeling their presence in each frame. This enables
consistent tracking and accurate identity management across frames, enhancing
both performance and reliability through the VIS process. Moreover, we
introduce Decoupled Object Association (DOA), a strategy that separately
handles newly appearing and already existing objects. By leveraging our memory
system, DOA accurately assigns object indices, improving matching accuracy and
ensuring stable identity consistency, even in dynamic scenes where objects
frequently appear and disappear. Extensive experiments and ablation studies
demonstrate the superiority of our method over traditional approaches, setting
a new benchmark in VIS. Notably, our LOMM achieves state-of-the-art AP score of
54.0 on YouTube-VIS 2022, a dataset known for its challenging long videos.
Project page: https://seung-hun-lee.github.io/projects/LOMM/

</details>


### [76] [MoFRR: Mixture of Diffusion Models for Face Retouching Restoration](https://arxiv.org/abs/2507.19770)
*Jiaxin Liu,Qichao Ying,Zhenxing Qian,Sheng Li,Runqi Zhang,Jian Liu,Xinpeng Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新任务——人脸修图恢复（FRR），旨在从修图后的人脸图像中恢复原始人脸。作者提出了一种混合扩散模型（MoFRR），通过专家隔离策略处理不同类型的修图操作，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上广泛使用的人脸修图技术引发了关于图像真实性的担忧，现有方法仅关注检测修图，而如何从修图后图像中恢复原始人脸尚未解决。

Method: 提出MoFRR模型，结合稀疏激活的专用专家处理特定修图类型，以及共享专家处理通用修图痕迹。专用专家采用双分支结构，包括基于DDIM的低频分支和基于交叉注意力的高频分支。

Result: 在新建的数据集RetouchingFFHQ++上进行了广泛实验，证明了MoFRR在FRR任务中的有效性。

Conclusion: MoFRR通过混合扩散模型和专家隔离策略，成功解决了人脸修图恢复的复杂问题，为恢复原始人脸提供了有效方法。

Abstract: The widespread use of face retouching on social media platforms raises
concerns about the authenticity of face images. While existing methods focus on
detecting face retouching, how to accurately recover the original faces from
the retouched ones has yet to be answered. This paper introduces Face
Retouching Restoration (FRR), a novel computer vision task aimed at restoring
original faces from their retouched counterparts. FRR differs from traditional
image restoration tasks by addressing the complex retouching operations with
various types and degrees, which focuses more on the restoration of the
low-frequency information of the faces. To tackle this challenge, we propose
MoFRR, Mixture of Diffusion Models for FRR. Inspired by DeepSeek's expert
isolation strategy, the MoFRR uses sparse activation of specialized experts
handling distinct retouching types and the engagement of a shared expert
dealing with universal retouching traces. Each specialized expert follows a
dual-branch structure with a DDIM-based low-frequency branch guided by an
Iterative Distortion Evaluation Module (IDEM) and a Cross-Attention-based
High-Frequency branch (HFCAM) for detail refinement. Extensive experiments on a
newly constructed face retouching dataset, RetouchingFFHQ++, demonstrate the
effectiveness of MoFRR for FRR.

</details>


### [77] [Self-Guided Masked Autoencoder](https://arxiv.org/abs/2507.19773)
*Jeongwoo Shin,Inseo Lee,Junho Lee,Joonseok Lee*

Main category: cs.CV

TL;DR: 论文提出了一种自引导的掩码自编码器（MAE），通过利用其内部学习的补丁聚类信息生成掩码，替代了原始MAE的随机掩码，显著提升了学习效果。


<details>
  <summary>Details</summary>
Motivation: 尽管MAE在自监督表示学习中取得了成功，但其具体学习机制尚未完全明确。本文旨在揭示MAE的学习机制，并基于此改进其性能。

Method: 通过深入分析发现MAE在预训练早期就学习到了基于模式的补丁级聚类。基于此，提出自引导MAE，利用内部聚类信息生成掩码，替代随机掩码。

Result: 在各种下游任务上的实验表明，该方法显著提升了学习效果，且无需依赖外部模型或额外信息。

Conclusion: 自引导MAE通过利用内部聚类信息改进掩码生成，有效提升了性能，同时保持了自监督学习的优势。

Abstract: Masked Autoencoder (MAE) is a self-supervised approach for representation
learning, widely applicable to a variety of downstream tasks in computer
vision. In spite of its success, it is still not fully uncovered what and how
MAE exactly learns. In this paper, with an in-depth analysis, we discover that
MAE intrinsically learns pattern-based patch-level clustering from surprisingly
early stages of pretraining. Upon this understanding, we propose self-guided
masked autoencoder, which internally generates informed mask by utilizing its
progress in patch clustering, substituting the naive random masking of the
vanilla MAE. Our approach significantly boosts its learning process without
relying on any external models or supplementary information, keeping the
benefit of self-supervised nature of MAE intact. Comprehensive experiments on
various downstream tasks verify the effectiveness of the proposed method.

</details>


### [78] [HydraMamba: Multi-Head State Space Model for Global Point Cloud Learning](https://arxiv.org/abs/2507.19778)
*Kanglin Qu,Pan Gao,Qun Dai,Yuanhao Sun*

Main category: cs.CV

TL;DR: HydraMamba提出了一种基于状态空间模型的点云网络，通过改进点云序列化和局部学习，解决了现有注意力机制在点云学习中的局限性。


<details>
  <summary>Details</summary>
Motivation: 注意力机制在点云学习中存在二次复杂度问题，限制了长距离依赖建模。状态空间模型（S6）因其线性复杂度和优秀的长距离建模能力被引入点云学习，但仍存在序列化和局部学习不足的问题。

Method: 设计了shuffle序列化策略以适应S6的因果性，并提出ConvBiS6层以协同捕获局部几何和全局上下文依赖。此外，通过多头设计扩展S6为MHS6，增强建模能力。

Result: HydraMamba在对象级和场景级任务中均取得了最先进的结果。

Conclusion: HydraMamba通过改进序列化和局部学习，显著提升了点云学习的长距离依赖建模能力。

Abstract: The attention mechanism has become a dominant operator in point cloud
learning, but its quadratic complexity leads to limited inter-point
interactions, hindering long-range dependency modeling between objects. Due to
excellent long-range modeling capability with linear complexity, the selective
state space model (S6), as the core of Mamba, has been exploited in point cloud
learning for long-range dependency interactions over the entire point cloud.
Despite some significant progress, related works still suffer from imperfect
point cloud serialization and lack of locality learning. To this end, we
explore a state space model-based point cloud network termed HydraMamba to
address the above challenges. Specifically, we design a shuffle serialization
strategy, making unordered point sets better adapted to the causal nature of
S6. Meanwhile, to overcome the deficiency of existing techniques in locality
learning, we propose a ConvBiS6 layer, which is capable of capturing local
geometries and global context dependencies synergistically. Besides, we propose
MHS6 by extending the multi-head design to S6, further enhancing its modeling
capability. HydraMamba achieves state-of-the-art results on various tasks at
both object-level and scene-level. The code is available at
https://github.com/Point-Cloud-Learning/HydraMamba.

</details>


### [79] [JDATT: A Joint Distillation Framework for Atmospheric Turbulence Mitigation and Target Detection](https://arxiv.org/abs/2507.19780)
*Zhiming Liu,Paul Hill,Nantheera Anantrasirichai*

Main category: cs.CV

TL;DR: JDATT是一个联合蒸馏框架，用于大气湍流抑制和目标检测，通过知识蒸馏压缩模型，同时保持性能，适用于实时应用。


<details>
  <summary>Details</summary>
Motivation: 大气湍流（AT）导致图像质量下降，影响下游视觉任务，现有深度学习方法计算复杂，不适合实时应用，且湍流抑制与目标检测分离导致效率低下。

Method: 提出JDATT框架，结合AT抑制和目标检测模块，采用特征级（CWD和MGD）和输出级（KL散度）知识蒸馏策略。

Result: 在合成和真实湍流数据集上，JDATT在视觉恢复和检测精度上表现优异，同时显著减少模型大小和推理时间。

Conclusion: JDATT通过联合蒸馏实现了高效的大气湍流抑制和目标检测，适合实时部署。

Abstract: Atmospheric turbulence (AT) introduces severe degradations, such as rippling,
blur, and intensity fluctuations, that hinder both image quality and downstream
vision tasks like target detection. While recent deep learning-based approaches
have advanced AT mitigation using transformer and Mamba architectures, their
high complexity and computational cost make them unsuitable for real-time
applications, especially in resource-constrained settings such as remote
surveillance. Moreover, the common practice of separating turbulence mitigation
and object detection leads to inefficiencies and suboptimal performance. To
address these challenges, we propose JDATT, a Joint Distillation framework for
Atmospheric Turbulence mitigation and Target detection. JDATT integrates
state-of-the-art AT mitigation and detection modules and introduces a unified
knowledge distillation strategy that compresses both components while
minimizing performance loss. We employ a hybrid distillation scheme:
feature-level distillation via Channel-Wise Distillation (CWD) and Masked
Generative Distillation (MGD), and output-level distillation via
Kullback-Leibler divergence. Experiments on synthetic and real-world turbulence
datasets demonstrate that JDATT achieves superior visual restoration and
detection accuracy while significantly reducing model size and inference time,
making it well-suited for real-time deployment.

</details>


### [80] [TransFlow: Motion Knowledge Transfer from Video Diffusion Models to Video Salient Object Detection](https://arxiv.org/abs/2507.19789)
*Suhwan Cho,Minhyeok Lee,Jungho Lee,Sunghun Yang,Sangyoun Lee*

Main category: cs.CV

TL;DR: TransFlow利用预训练的视频扩散模型生成逼真的视频数据，提升视频显著目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 视频显著目标检测依赖运动线索，但视频数据集稀缺，现有方法生成的视频序列缺乏真实的运动语义。

Method: 通过预训练的视频扩散模型迁移运动知识，从静态图像生成语义感知的光流。

Result: 在多个基准测试中性能提升，验证了运动知识迁移的有效性。

Conclusion: TransFlow通过生成逼真的训练数据，解决了视频显著目标检测中数据稀缺的问题。

Abstract: Video salient object detection (SOD) relies on motion cues to distinguish
salient objects from backgrounds, but training such models is limited by scarce
video datasets compared to abundant image datasets. Existing approaches that
use spatial transformations to create video sequences from static images fail
for motion-guided tasks, as these transformations produce unrealistic optical
flows that lack semantic understanding of motion. We present TransFlow, which
transfers motion knowledge from pre-trained video diffusion models to generate
realistic training data for video SOD. Video diffusion models have learned rich
semantic motion priors from large-scale video data, understanding how different
objects naturally move in real scenes. TransFlow leverages this knowledge to
generate semantically-aware optical flows from static images, where objects
exhibit natural motion patterns while preserving spatial boundaries and
temporal coherence. Our method achieves improved performance across multiple
benchmarks, demonstrating effective motion knowledge transfer.

</details>


### [81] [DepthFlow: Exploiting Depth-Flow Structural Correlations for Unsupervised Video Object Segmentation](https://arxiv.org/abs/2507.19790)
*Suhwan Cho,Minhyeok Lee,Jungho Lee,Donghyeong Kim,Sangyoun Lee*

Main category: cs.CV

TL;DR: DepthFlow通过从单张图像合成光流数据，解决了无监督视频对象分割中训练数据不足的问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决无监督视频对象分割中因训练数据稀缺导致的性能限制问题。

Method: 提出DepthFlow方法，通过从单张图像估计深度图并转换为合成光流，生成大规模训练数据。

Result: 在所有公开VOS基准测试中达到新的最先进性能。

Conclusion: DepthFlow是一种可扩展且有效的解决方案，显著缓解了数据稀缺问题。

Abstract: Unsupervised video object segmentation (VOS) aims to detect the most
prominent object in a video. Recently, two-stream approaches that leverage both
RGB images and optical flow have gained significant attention, but their
performance is fundamentally constrained by the scarcity of training data. To
address this, we propose DepthFlow, a novel data generation method that
synthesizes optical flow from single images. Our approach is driven by the key
insight that VOS models depend more on structural information embedded in flow
maps than on their geometric accuracy, and that this structure is highly
correlated with depth. We first estimate a depth map from a source image and
then convert it into a synthetic flow field that preserves essential structural
cues. This process enables the transformation of large-scale image-mask pairs
into image-flow-mask training pairs, dramatically expanding the data available
for network training. By training a simple encoder-decoder architecture with
our synthesized data, we achieve new state-of-the-art performance on all public
VOS benchmarks, demonstrating a scalable and effective solution to the data
scarcity problem.

</details>


### [82] [Smaller, Faster, Cheaper: Architectural Designs for Efficient Machine Learning](https://arxiv.org/abs/2507.19795)
*Steven Walton*

Main category: cs.CV

TL;DR: 论文探讨了如何通过改进神经网络架构设计，在减少计算资源需求的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着计算机视觉模型的广泛应用，对计算资源的需求不断增加，但在资源受限的环境中，需要更高效的架构。

Method: 通过三个方向实现目标：优化数据输入输出、改进核心神经网络架构（如受限注意力机制）、利用Normalizing Flows的特性。

Result: 研究表明，精心设计的架构可以提高算法效率，使模型更小、更快、更经济。

Conclusion: 通过架构优化，可以在减少计算资源的同时提升模型性能，适用于资源受限的环境。

Abstract: Major advancements in the capabilities of computer vision models have been
primarily fueled by rapid expansion of datasets, model parameters, and
computational budgets, leading to ever-increasing demands on computational
infrastructure. However, as these models are deployed in increasingly diverse
and resource-constrained environments, there is a pressing need for
architectures that can deliver high performance while requiring fewer
computational resources.
  This dissertation focuses on architectural principles through which models
can achieve increased performance while reducing their computational demands.
We discuss strides towards this goal through three directions. First, we focus
on data ingress and egress, investigating how information may be passed into
and retrieved from our core neural processing units. This ensures that our
models make the most of available data, allowing smaller architectures to
become more performant. Second, we investigate modifications to the core neural
architecture, applied to restricted attention in vision transformers. This
section explores how removing uniform context windows in restricted attention
increases the expressivity of the underlying neural architecture. Third, we
explore the natural structures of Normalizing Flows and how we can leverage
these properties to better distill model knowledge.
  These contributions demonstrate that careful design of neural architectures
can increase the efficiency of machine learning algorithms, allowing them to
become smaller, faster, and cheaper.

</details>


### [83] [ForCenNet: Foreground-Centric Network for Document Image Rectification](https://arxiv.org/abs/2507.19804)
*Peng Cai,Qiang Li,Kaicheng Yang,Dong Guo,Jia Li,Nan Zhou,Xiang An,Ninghua Yang,Jiankang Deng*

Main category: cs.CV

TL;DR: 论文提出Foreground-Centric Network (ForCenNet)，通过关注前景元素消除文档图像中的几何变形，显著提升文本识别效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视前景元素的重要性，而前景元素为文档图像校正提供关键几何参考和布局信息。

Method: 提出前景中心标签生成方法、前景中心掩码机制和曲率一致性损失，利用详细前景标签帮助模型理解变形几何分布。

Result: 在四个真实基准测试中达到最新最优性能，有效校正文本行和表格边框等布局元素。

Conclusion: ForCenNet通过关注前景元素，显著提升文档图像校正效果，为后续研究提供资源支持。

Abstract: Document image rectification aims to eliminate geometric deformation in
photographed documents to facilitate text recognition. However, existing
methods often neglect the significance of foreground elements, which provide
essential geometric references and layout information for document image
correction. In this paper, we introduce Foreground-Centric Network (ForCenNet)
to eliminate geometric distortions in document images. Specifically, we
initially propose a foreground-centric label generation method, which extracts
detailed foreground elements from an undistorted image. Then we introduce a
foreground-centric mask mechanism to enhance the distinction between readable
and background regions. Furthermore, we design a curvature consistency loss to
leverage the detailed foreground labels to help the model understand the
distorted geometric distribution. Extensive experiments demonstrate that
ForCenNet achieves new state-of-the-art on four real-world benchmarks, such as
DocUNet, DIR300, WarpDoc, and DocReal. Quantitative analysis shows that the
proposed method effectively undistorts layout elements, such as text lines and
table borders. The resources for further comparison are provided at
https://github.com/caipeng328/ForCenNet.

</details>
